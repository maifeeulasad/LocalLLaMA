import{j as e}from"./index-BOnf-UhU.js";import{R as t}from"./RedditPostRenderer-Ce39qICS.js";import"./index-CDase6VD.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hi, what do you all think for sort of a medium / smallest model to use as an orchestrator model that runs with whisper (speech in) and tts (speech out). I also want it to view my screen to get context to pass to other other models / mcp so it knows what is going on so it can respond etc, then route and call tools / MCP. I intend to do most heavy lifting and anything with real output using Claude code sdk since have unlimited max plan. \\n\\nI was am looking at using Grafiti for memory and building some consensus between models based on Zen mcp implementation:   \\n\\nI have a 64 gb macbook pro M1 and I’m looking at Qwen3-30B-A3B-MLX-4bit ([hugging face link](https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-MLX-4bit)),.\\n\\nI would welcome any advice! I've looked at Jan and related though seems too small. Is there anything that will run on my MBP that can serve as this brain (I looked at Gemma 3n, but its not fully mutli-modal out of the box as is). Would the be possible with this hardware?\\n\\nThis is the potential stack I came up with in chatting with Claude and o3:\\n\\n    User Input (speech/screen/events)\\n               ↓\\n        Local Processing\\n        ├── VAD → STT → Text\\n        ├── Screen → OCR → Context  \\n        └── Events → MCP → Actions\\n               ↓\\n         Qwen3-30B Router\\n        \\"Is this simple?\\"\\n          ↓         ↓\\n        Yes        No\\n         ↓          ↓\\n      Local     Claude API\\n      Response  + MCP tools\\n         ↓          ↓\\n         └────┬─────┘\\n              ↓\\n        Graphiti Memory\\n              ↓\\n        Response Stream\\n              ↓\\n        Kyutai TTS        \\n    \\n\\nThoughts?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Best small to medium size Local LLM Orchestrator for calling Tools, managing STT, TTS, screen OCR, and with passing heavy lift calls to Claude Code SDK, running on Macbook Pro.","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m7hq4w","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":1,"author_flair_background_color":null,"subreddit_type":"public","ups":5,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_846pg","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":5,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1753296737,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi, what do you all think for sort of a medium / smallest model to use as an orchestrator model that runs with whisper (speech in) and tts (speech out). I also want it to view my screen to get context to pass to other other models / mcp so it knows what is going on so it can respond etc, then route and call tools / MCP. I intend to do most heavy lifting and anything with real output using Claude code sdk since have unlimited max plan. &lt;/p&gt;\\n\\n&lt;p&gt;I was am looking at using Grafiti for memory and building some consensus between models based on Zen mcp implementation:   &lt;/p&gt;\\n\\n&lt;p&gt;I have a 64 gb macbook pro M1 and I’m looking at Qwen3-30B-A3B-MLX-4bit (&lt;a href=\\"https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-MLX-4bit\\"&gt;hugging face link&lt;/a&gt;),.&lt;/p&gt;\\n\\n&lt;p&gt;I would welcome any advice! I&amp;#39;ve looked at Jan and related though seems too small. Is there anything that will run on my MBP that can serve as this brain (I looked at Gemma 3n, but its not fully mutli-modal out of the box as is). Would the be possible with this hardware?&lt;/p&gt;\\n\\n&lt;p&gt;This is the potential stack I came up with in chatting with Claude and o3:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;User Input (speech/screen/events)\\n           ↓\\n    Local Processing\\n    ├── VAD → STT → Text\\n    ├── Screen → OCR → Context  \\n    └── Events → MCP → Actions\\n           ↓\\n     Qwen3-30B Router\\n    &amp;quot;Is this simple?&amp;quot;\\n      ↓         ↓\\n    Yes        No\\n     ↓          ↓\\n  Local     Claude API\\n  Response  + MCP tools\\n     ↓          ↓\\n     └────┬─────┘\\n          ↓\\n    Graphiti Memory\\n          ↓\\n    Response Stream\\n          ↓\\n    Kyutai TTS        \\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;Thoughts?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/EBLrlrQ_ze2lgA1gLs6eweAZ3a9siHivrp_8a72Wf0k.png?auto=webp&amp;s=6cf13531259fc6a43addb217a67da463735f0aaf","width":1200,"height":648},"resolutions":[{"url":"https://external-preview.redd.it/EBLrlrQ_ze2lgA1gLs6eweAZ3a9siHivrp_8a72Wf0k.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a97d235ba5ee0d377655e74657048733e66c0c80","width":108,"height":58},{"url":"https://external-preview.redd.it/EBLrlrQ_ze2lgA1gLs6eweAZ3a9siHivrp_8a72Wf0k.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f65ca22ff036e0e09c3072baa25e27599adcf38d","width":216,"height":116},{"url":"https://external-preview.redd.it/EBLrlrQ_ze2lgA1gLs6eweAZ3a9siHivrp_8a72Wf0k.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=48ce65ed5b5bd70fd126bbde8a6424436ca04f3a","width":320,"height":172},{"url":"https://external-preview.redd.it/EBLrlrQ_ze2lgA1gLs6eweAZ3a9siHivrp_8a72Wf0k.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2ffb0cf505dc6f2aa6cfcb7c7f84e77a38833283","width":640,"height":345},{"url":"https://external-preview.redd.it/EBLrlrQ_ze2lgA1gLs6eweAZ3a9siHivrp_8a72Wf0k.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8fb2173e54335250d945cddd3c72954d45d353c7","width":960,"height":518},{"url":"https://external-preview.redd.it/EBLrlrQ_ze2lgA1gLs6eweAZ3a9siHivrp_8a72Wf0k.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d321923d5c25125f2212fa01cabda39fcb114276","width":1080,"height":583}],"variants":{},"id":"EBLrlrQ_ze2lgA1gLs6eweAZ3a9siHivrp_8a72Wf0k"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m7hq4w","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"matznerd","discussion_type":null,"num_comments":8,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m7hq4w/best_small_to_medium_size_local_llm_orchestrator/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m7hq4w/best_small_to_medium_size_local_llm_orchestrator/","subreddit_subscribers":503518,"created_utc":1753296737,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4sgf68","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MKU64","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4ry3d2","score":1,"author_fullname":"t2_wn7it","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I tend to work with anything between 0.6B-4B for small tasks, they are surprisingly really good for normal tasks, regardless what I was mostly trying to help with was latency but if you are okay with it then you don’t lose anything with the 30B-A3B, insanely good model for absolutely anything tbh\\n\\nQuick Note: They are all absolutely awful at multilingual tasks, if you plan to speak to it in anything but English or Chinese then it wouldn’t be as good, the 30B-A3B is still your best bet if that’s the case (as size grows they get slightly better at other languages)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4sgf68","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I tend to work with anything between 0.6B-4B for small tasks, they are surprisingly really good for normal tasks, regardless what I was mostly trying to help with was latency but if you are okay with it then you don’t lose anything with the 30B-A3B, insanely good model for absolutely anything tbh&lt;/p&gt;\\n\\n&lt;p&gt;Quick Note: They are all absolutely awful at multilingual tasks, if you plan to speak to it in anything but English or Chinese then it wouldn’t be as good, the 30B-A3B is still your best bet if that’s the case (as size grows they get slightly better at other languages)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7hq4w","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7hq4w/best_small_to_medium_size_local_llm_orchestrator/n4sgf68/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753306662,"author_flair_text":null,"treatment_tags":[],"created_utc":1753306662,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4ry3d2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"matznerd","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4rlqip","score":3,"author_fullname":"t2_846pg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What smaller ones than 30B, these are what I see on HF is there somewhere else, and if I use a less popular model random model, do I risk not being able to update it etc?\\n\\nSee these direct on qwen site:\\n\\n Qwen3-32B\\n\\nQwen3-14B\\n\\nQwen3-8B, \\n\\nQwen3-4B,\\n\\n Qwen3-1.7B,\\n\\nQwen3-0.6B\\n\\nAnd on HF: \\nhttps://huggingface.co/kalomaze/Qwen3-16B-A3B\\nhttps://huggingface.co/mradermacher/Qwen3-18B-A3B-Stranger-Thoughts-IPONDER-Abliterated-Uncensored-i1-GGUF\\nhttps://huggingface.co/mlx-community/Qwen3-1.7B-dwq-3bit-gs128\\n\\nThanks!","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4ry3d2","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What smaller ones than 30B, these are what I see on HF is there somewhere else, and if I use a less popular model random model, do I risk not being able to update it etc?&lt;/p&gt;\\n\\n&lt;p&gt;See these direct on qwen site:&lt;/p&gt;\\n\\n&lt;p&gt;Qwen3-32B&lt;/p&gt;\\n\\n&lt;p&gt;Qwen3-14B&lt;/p&gt;\\n\\n&lt;p&gt;Qwen3-8B, &lt;/p&gt;\\n\\n&lt;p&gt;Qwen3-4B,&lt;/p&gt;\\n\\n&lt;p&gt;Qwen3-1.7B,&lt;/p&gt;\\n\\n&lt;p&gt;Qwen3-0.6B&lt;/p&gt;\\n\\n&lt;p&gt;And on HF: \\n&lt;a href=\\"https://huggingface.co/kalomaze/Qwen3-16B-A3B\\"&gt;https://huggingface.co/kalomaze/Qwen3-16B-A3B&lt;/a&gt;\\n&lt;a href=\\"https://huggingface.co/mradermacher/Qwen3-18B-A3B-Stranger-Thoughts-IPONDER-Abliterated-Uncensored-i1-GGUF\\"&gt;https://huggingface.co/mradermacher/Qwen3-18B-A3B-Stranger-Thoughts-IPONDER-Abliterated-Uncensored-i1-GGUF&lt;/a&gt;\\n&lt;a href=\\"https://huggingface.co/mlx-community/Qwen3-1.7B-dwq-3bit-gs128\\"&gt;https://huggingface.co/mlx-community/Qwen3-1.7B-dwq-3bit-gs128&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Thanks!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7hq4w","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7hq4w/best_small_to_medium_size_local_llm_orchestrator/n4ry3d2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753301445,"author_flair_text":null,"treatment_tags":[],"created_utc":1753301445,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n4rlqip","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MKU64","can_mod_post":false,"created_utc":1753297933,"send_replies":true,"parent_id":"t1_n4rl2mz","score":1,"author_fullname":"t2_wn7it","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I recommend thinking mode smaller Qwens btw","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4rlqip","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I recommend thinking mode smaller Qwens btw&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7hq4w","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7hq4w/best_small_to_medium_size_local_llm_orchestrator/n4rlqip/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753297933,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4rv82q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"matznerd","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4rml59","score":1,"author_fullname":"t2_846pg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I’m okay on the latency and am working on having the LLM itself being able to generate dashboards on the fly using DearPyGui. So it can just make an inbox of tasks etc whatever is needed and save them. So it has interfaces created based on specific scenarios, such as when in meeting. Or just for me asking (this local llm would call Claude code to build it under TDD, then it other models or cc subagents verify it works).\\n\\nMake sense why I need some local power? I think a 10-20b model would be perfect or can smaller work for this? Ones like Jan seem too small, though with MLX I could run multiple local models and keep the baby one just for TTS+SST, which steps up for the new models etc. I am okay with few seconds to reply etc as long as STT capture is instant like I’m using Wispr Flow now, want to change to local model powered by whisperX itself etc.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4rv82q","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I’m okay on the latency and am working on having the LLM itself being able to generate dashboards on the fly using DearPyGui. So it can just make an inbox of tasks etc whatever is needed and save them. So it has interfaces created based on specific scenarios, such as when in meeting. Or just for me asking (this local llm would call Claude code to build it under TDD, then it other models or cc subagents verify it works).&lt;/p&gt;\\n\\n&lt;p&gt;Make sense why I need some local power? I think a 10-20b model would be perfect or can smaller work for this? Ones like Jan seem too small, though with MLX I could run multiple local models and keep the baby one just for TTS+SST, which steps up for the new models etc. I am okay with few seconds to reply etc as long as STT capture is instant like I’m using Wispr Flow now, want to change to local model powered by whisperX itself etc.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7hq4w","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7hq4w/best_small_to_medium_size_local_llm_orchestrator/n4rv82q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753300642,"author_flair_text":null,"treatment_tags":[],"created_utc":1753300642,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4rml59","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MKU64","can_mod_post":false,"created_utc":1753298176,"send_replies":true,"parent_id":"t1_n4rl2mz","score":1,"author_fullname":"t2_wn7it","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"From someone who uses an M4 you are looking at 100-600ms of latency by having an orchestrator by the way. It depends on the size of your system prompt. If you are more interested in fast responses then be aware","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4rml59","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;From someone who uses an M4 you are looking at 100-600ms of latency by having an orchestrator by the way. It depends on the size of your system prompt. If you are more interested in fast responses then be aware&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7hq4w","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7hq4w/best_small_to_medium_size_local_llm_orchestrator/n4rml59/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753298176,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4rl2mz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MKU64","can_mod_post":false,"created_utc":1753297746,"send_replies":true,"parent_id":"t3_1m7hq4w","score":4,"author_fullname":"t2_wn7it","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Nice idea. I’m gonna be honest with you, if all you want is a Router model your best bet is to train a really small model or give it a nice system prompt. I would recommend a way smaller Qwen, then give it Few-Shot Prompting and it should be theoretically good, especially if you have a sort of grasp of what you mean by simple","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4rl2mz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nice idea. I’m gonna be honest with you, if all you want is a Router model your best bet is to train a really small model or give it a nice system prompt. I would recommend a way smaller Qwen, then give it Few-Shot Prompting and it should be theoretically good, especially if you have a sort of grasp of what you mean by simple&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7hq4w/best_small_to_medium_size_local_llm_orchestrator/n4rl2mz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753297746,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7hq4w","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4rsfkg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"matznerd","can_mod_post":false,"created_utc":1753299855,"send_replies":true,"parent_id":"t1_n4rqkt3","score":1,"author_fullname":"t2_846pg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Essentially an always on Jarvis style model that can see my screen, take voice and text in and route to the right model with the context of what is on screen etc. I want it to hand off to Claude code where I use opus 4 for most things, even writing etc. But I want the local model to be able to use Apple accessibility mode to manipulate computer directly and in future do things like eye tracking via the camera to know my intent even further. Immediate needs etc is something like meeting diarization e.g. detect who is speaking in meeting, analyzing what is on the screen share for the notes. Real time is ideal with transcript and constant updating summary etc. I want it to basically manage sending tasks and things to Claude code including the context and text to speech text etc (including commands). Helpful?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4rsfkg","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Essentially an always on Jarvis style model that can see my screen, take voice and text in and route to the right model with the context of what is on screen etc. I want it to hand off to Claude code where I use opus 4 for most things, even writing etc. But I want the local model to be able to use Apple accessibility mode to manipulate computer directly and in future do things like eye tracking via the camera to know my intent even further. Immediate needs etc is something like meeting diarization e.g. detect who is speaking in meeting, analyzing what is on the screen share for the notes. Real time is ideal with transcript and constant updating summary etc. I want it to basically manage sending tasks and things to Claude code including the context and text to speech text etc (including commands). Helpful?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7hq4w","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7hq4w/best_small_to_medium_size_local_llm_orchestrator/n4rsfkg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753299855,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4rqkt3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AIEchoesHumanity","can_mod_post":false,"created_utc":1753299315,"send_replies":true,"parent_id":"t3_1m7hq4w","score":1,"author_fullname":"t2_t4oqvl2rk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"why not just detect the input file format and route it to the appropriate models? Maybe im not understanding your system well.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4rqkt3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;why not just detect the input file format and route it to the appropriate models? Maybe im not understanding your system well.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7hq4w/best_small_to_medium_size_local_llm_orchestrator/n4rqkt3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753299315,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7hq4w","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),s=()=>e.jsx(t,{data:l});export{s as default};
