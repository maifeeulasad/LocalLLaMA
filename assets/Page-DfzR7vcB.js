import{j as e}from"./index-CWmJdUH_.js";import{R as l}from"./RedditPostRenderer-D2iunoQ9.js";import"./index-BCg9RP6g.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I’ve been testing Qwen3-Coder-480B (on Hyperbolics) and  Kimi K2 (on Groq) for Rust and Go projects. Neither model is built for deep problem-solving, but in real-world use, the differences are pretty clear.  \\n\\nQwen3-Coder often ignores system prompts, struggles with context, and its tool calls are rigid, like it’s just filling in templates rather than thinking through the task. It’s not just about raw capability; the responses are too formulaic, making it hard to use for actual coding tasks.  \\n\\nSome of this might be because Hyperbolics hasn’t fully optimized their setup for Qwen3 yet. But I suspect the bigger issue is the fine-tuning, it seems trained on overly structured responses, so it fails to adapt to natural prompts.\\n\\nKimi K2 works much better. Even though it’s not a reasoning-focused model, it stays on task, handles edits and helper functions smoothly, and just feels more responsive when working with multi-file projects. For Rust and Go, it’s consistently the better option.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Kimi K2 vs Qwen3 Coder 480B","link_flair_richtext":[{"e":"text","t":"New Model"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m6zz1v","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.89,"author_flair_background_color":null,"subreddit_type":"public","ups":60,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_a29pmyxj","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"New Model","can_mod_post":false,"score":60,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753245282,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I’ve been testing Qwen3-Coder-480B (on Hyperbolics) and  Kimi K2 (on Groq) for Rust and Go projects. Neither model is built for deep problem-solving, but in real-world use, the differences are pretty clear.  &lt;/p&gt;\\n\\n&lt;p&gt;Qwen3-Coder often ignores system prompts, struggles with context, and its tool calls are rigid, like it’s just filling in templates rather than thinking through the task. It’s not just about raw capability; the responses are too formulaic, making it hard to use for actual coding tasks.  &lt;/p&gt;\\n\\n&lt;p&gt;Some of this might be because Hyperbolics hasn’t fully optimized their setup for Qwen3 yet. But I suspect the bigger issue is the fine-tuning, it seems trained on overly structured responses, so it fails to adapt to natural prompts.&lt;/p&gt;\\n\\n&lt;p&gt;Kimi K2 works much better. Even though it’s not a reasoning-focused model, it stays on task, handles edits and helper functions smoothly, and just feels more responsive when working with multi-file projects. For Rust and Go, it’s consistently the better option.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ced98442-f5d3-11ed-b657-66d3b15490c6","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#ffb000","id":"1m6zz1v","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Ok-Pattern9779","discussion_type":null,"num_comments":10,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m6zz1v/kimi_k2_vs_qwen3_coder_480b/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m6zz1v/kimi_k2_vs_qwen3_coder_480b/","subreddit_subscribers":503256,"created_utc":1753245282,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ope78","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok-Pattern9779","can_mod_post":false,"created_utc":1753265047,"send_replies":true,"parent_id":"t1_n4o6l72","score":3,"author_fullname":"t2_a29pmyxj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah, good point — I’ve actually tested Qwen3-Coder using both the new Qwen Code CLI and my own custom coding agent.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ope78","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, good point — I’ve actually tested Qwen3-Coder using both the new Qwen Code CLI and my own custom coding agent.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6zz1v","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6zz1v/kimi_k2_vs_qwen3_coder_480b/n4ope78/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753265047,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n4o6l72","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ResearchCrafty1804","can_mod_post":false,"created_utc":1753254342,"send_replies":true,"parent_id":"t3_1m6zz1v","score":19,"author_fullname":"t2_c705ri9b","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You haven’t mentioned how you interact with the models. \\n\\nThrough chat or are you using any agentic tool e.g. cline? \\n\\nKeep in mind that some models are very sensitive to the system prompt and template which these agentic tools are using. Right now, the best agentic coding experience with Qwen3-coder is through the official Qwen Code CLI which was released with the model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4o6l72","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You haven’t mentioned how you interact with the models. &lt;/p&gt;\\n\\n&lt;p&gt;Through chat or are you using any agentic tool e.g. cline? &lt;/p&gt;\\n\\n&lt;p&gt;Keep in mind that some models are very sensitive to the system prompt and template which these agentic tools are using. Right now, the best agentic coding experience with Qwen3-coder is through the official Qwen Code CLI which was released with the model.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6zz1v/kimi_k2_vs_qwen3_coder_480b/n4o6l72/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753254342,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6zz1v","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":19}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4o6mq7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"kamikazechaser","can_mod_post":false,"created_utc":1753254365,"send_replies":true,"parent_id":"t3_1m6zz1v","score":10,"author_fullname":"t2_ycqip","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"On a Go codebase, Kimi K2 is the best I have used for Go. It is slightly better than Claude 4 Sonnet. Deepseek R1 is up there as well if one has patience. For a very complex problem, Deepseek is the only one that managed to come up with an elegant solution, even better than my own solution.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4o6mq7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;On a Go codebase, Kimi K2 is the best I have used for Go. It is slightly better than Claude 4 Sonnet. Deepseek R1 is up there as well if one has patience. For a very complex problem, Deepseek is the only one that managed to come up with an elegant solution, even better than my own solution.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6zz1v/kimi_k2_vs_qwen3_coder_480b/n4o6mq7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753254365,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6zz1v","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4nzjjw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SixZer0","can_mod_post":false,"created_utc":1753250549,"send_replies":true,"parent_id":"t3_1m6zz1v","score":3,"author_fullname":"t2_sczby","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"In my experience it is very knowledgable, actually one of the OS models which pass one of my test (although not perfect solution but it 1shots it), but yeah, when I ask it to optimalize the solution it just fails it, where Kimi could do it. \\nIt is not exactly following my requests, when I ask only optimalize X or Y function, it still rewrites all functions.\\n\\nIt might also has the tendency to say: \\"You're absolutely right...\\" :O","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4nzjjw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;In my experience it is very knowledgable, actually one of the OS models which pass one of my test (although not perfect solution but it 1shots it), but yeah, when I ask it to optimalize the solution it just fails it, where Kimi could do it. \\nIt is not exactly following my requests, when I ask only optimalize X or Y function, it still rewrites all functions.&lt;/p&gt;\\n\\n&lt;p&gt;It might also has the tendency to say: &amp;quot;You&amp;#39;re absolutely right...&amp;quot; :O&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6zz1v/kimi_k2_vs_qwen3_coder_480b/n4nzjjw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753250549,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6zz1v","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4p574t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Babouche_Le_Singe","can_mod_post":false,"created_utc":1753272218,"send_replies":true,"parent_id":"t3_1m6zz1v","score":1,"author_fullname":"t2_1mlog17z2z","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Keep in min that Hyperbolics is hosting an FP8 isntance rather than the full FP16. The difference is not usually noticeable in vibe checks but it's definitely there.  \\nI have not tried Qwen3-Coder-480B or Kimi K2 yet so I cannot say this it for sure, but I suggest you try the FP16 variant before you settle.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4p574t","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Keep in min that Hyperbolics is hosting an FP8 isntance rather than the full FP16. The difference is not usually noticeable in vibe checks but it&amp;#39;s definitely there.&lt;br/&gt;\\nI have not tried Qwen3-Coder-480B or Kimi K2 yet so I cannot say this it for sure, but I suggest you try the FP16 variant before you settle.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6zz1v/kimi_k2_vs_qwen3_coder_480b/n4p574t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753272218,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6zz1v","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4oy3lr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4oauf9","score":1,"author_fullname":"t2_h5utwre7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I dunno about *wrong* but definitely exaggerated. Qwen models are ok, but short real world data in favor of stem and benchmark related training.\\n\\nThey run around claiming 235b is equal (or better) to deepseek/kimi and they clearly aren't. I think this time it even trained for EQ bench and the maker noticed.\\n\\nContext is supposedly super high yet it just has YARN enabled and the actual model is ~40k. The newest release is *only* this way, sabotaging low ctx performance in favor of hype.\\n\\nQwen team releases a decent sedan but markets it as an F1 supercar. The 480b likely falls between 235b and deepseek so you end up with posts like op's because of the sales pitch and incorrect expectations.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4oy3lr","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I dunno about &lt;em&gt;wrong&lt;/em&gt; but definitely exaggerated. Qwen models are ok, but short real world data in favor of stem and benchmark related training.&lt;/p&gt;\\n\\n&lt;p&gt;They run around claiming 235b is equal (or better) to deepseek/kimi and they clearly aren&amp;#39;t. I think this time it even trained for EQ bench and the maker noticed.&lt;/p&gt;\\n\\n&lt;p&gt;Context is supposedly super high yet it just has YARN enabled and the actual model is ~40k. The newest release is &lt;em&gt;only&lt;/em&gt; this way, sabotaging low ctx performance in favor of hype.&lt;/p&gt;\\n\\n&lt;p&gt;Qwen team releases a decent sedan but markets it as an F1 supercar. The 480b likely falls between 235b and deepseek so you end up with posts like op&amp;#39;s because of the sales pitch and incorrect expectations.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6zz1v","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6zz1v/kimi_k2_vs_qwen3_coder_480b/n4oy3lr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753269310,"author_flair_text":null,"treatment_tags":[],"created_utc":1753269310,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4oauf9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"RuthlessCriticismAll","can_mod_post":false,"created_utc":1753256755,"send_replies":false,"parent_id":"t1_n4oa4s8","score":14,"author_fullname":"t2_7777b0y0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This is, of course, completely wrong.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4oauf9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is, of course, completely wrong.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6zz1v","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6zz1v/kimi_k2_vs_qwen3_coder_480b/n4oauf9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753256755,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":14}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4oaxhp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MelodicRecognition7","can_mod_post":false,"created_utc":1753256805,"send_replies":true,"parent_id":"t1_n4oa4s8","score":4,"author_fullname":"t2_1eex9ug5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"lol that's quite unpopular opinion but I've felt the same. Could you elaborate more please?\\nIn my experience Qwen MoE models were worse than Qwen dense models with comparable active-dense parameters amount, but I suspect that it is the same with all models not only Qwen because it is a limitation of MoE architecture.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4oaxhp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;lol that&amp;#39;s quite unpopular opinion but I&amp;#39;ve felt the same. Could you elaborate more please?\\nIn my experience Qwen MoE models were worse than Qwen dense models with comparable active-dense parameters amount, but I suspect that it is the same with all models not only Qwen because it is a limitation of MoE architecture.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6zz1v","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6zz1v/kimi_k2_vs_qwen3_coder_480b/n4oaxhp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753256805,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4oz3dz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok-Internal9317","can_mod_post":false,"created_utc":1753269743,"send_replies":true,"parent_id":"t1_n4oa4s8","score":1,"author_fullname":"t2_77yd9w74","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Why?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4oz3dz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6zz1v","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6zz1v/kimi_k2_vs_qwen3_coder_480b/n4oz3dz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753269743,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4oa4s8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"cantgetthistowork","can_mod_post":false,"created_utc":1753256345,"send_replies":true,"parent_id":"t3_1m6zz1v","score":-9,"author_fullname":"t2_j1i0o","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":true,"body":"Qwen has always been benchmaxed garbage unusable in the real world. Surprised they still had to cheat with such a large model","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4oa4s8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen has always been benchmaxed garbage unusable in the real world. Surprised they still had to cheat with such a large model&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6zz1v/kimi_k2_vs_qwen3_coder_480b/n4oa4s8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753256345,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6zz1v","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-9}}],"before":null}}]`),r=()=>e.jsx(l,{data:t});export{r as default};
