import{j as e}from"./index-CeRg6Q3f.js";import{R as l}from"./RedditPostRenderer-D7n1g-D8.js";import"./index-DPToWe3n.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I pay for Claude to assist with coding / tool calling which I use for my job all day. I feel a strong urge to waste tons of money on a nice GPU, but realistically the models aren't as strong or even as cheap as the cloud models.\\n\\nI'm trying to self-reflect hard and in this moment of clarity, I see this as a distract of an expensive new toy I won't use much.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Please convince me not to get a GPU I don't need. Can any local LLM compare with cloud models?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lnsax9","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.77,"author_flair_background_color":null,"subreddit_type":"public","ups":55,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1bl579qtd6","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":55,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751238331,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I pay for Claude to assist with coding / tool calling which I use for my job all day. I feel a strong urge to waste tons of money on a nice GPU, but realistically the models aren&amp;#39;t as strong or even as cheap as the cloud models.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m trying to self-reflect hard and in this moment of clarity, I see this as a distract of an expensive new toy I won&amp;#39;t use much.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lnsax9","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"TumbleweedDeep825","discussion_type":null,"num_comments":160,"send_replies":false,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/","subreddit_subscribers":493242,"created_utc":1751238331,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0pde64","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Effect3325","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0lc3a6","score":1,"author_fullname":"t2_kkv4u2t2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I topup $10 and probably I use it for testing around like $1 per day for an A4000 GPU. If you use flex worker, you pay per second of the GPU running ($0.00016/s). It means if you don't run the inference, you pay nothing.","edited":1751344064,"author_flair_css_class":null,"name":"t1_n0pde64","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I topup $10 and probably I use it for testing around like $1 per day for an A4000 GPU. If you use flex worker, you pay per second of the GPU running ($0.00016/s). It means if you don&amp;#39;t run the inference, you pay nothing.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0pde64/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751343771,"author_flair_text":null,"collapsed":false,"created_utc":1751343771,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0lc3a6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"my_name_isnt_clever","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0jayyn","score":6,"author_fullname":"t2_5o567","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What do the actual costs come out to? I want to try it but price per GPU hour is hard for me to grasp what I'd actually be paying.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0lc3a6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What do the actual costs come out to? I want to try it but price per GPU hour is hard for me to grasp what I&amp;#39;d actually be paying.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0lc3a6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751297379,"author_flair_text":null,"treatment_tags":[],"created_utc":1751297379,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n0jayyn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No_Effect3325","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0iai45","score":12,"author_fullname":"t2_kkv4u2t2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I use serverless Runpod to run my own model. Upload your model to HuggingFace and then you can run your model from Runpod serverless taking the model from the HuggingFace. It costs you per second of model inference (I use flex worker)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0jayyn","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I use serverless Runpod to run my own model. Upload your model to HuggingFace and then you can run your model from Runpod serverless taking the model from the HuggingFace. It costs you per second of model inference (I use flex worker)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jayyn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751265114,"author_flair_text":null,"treatment_tags":[],"created_utc":1751265114,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0iyasl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Karyo_Ten","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0iigj4","score":11,"author_fullname":"t2_tbdqg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt;\\"Provably secure\\" depends on your threat model but it's also a well solved problem.\\n\\nI would be out of a job if that was the case (cryptography).\\n\\nBut yeah, if you're asking, it's good enough for you.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0iyasl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;&amp;quot;Provably secure&amp;quot; depends on your threat model but it&amp;#39;s also a well solved problem.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I would be out of a job if that was the case (cryptography).&lt;/p&gt;\\n\\n&lt;p&gt;But yeah, if you&amp;#39;re asking, it&amp;#39;s good enough for you.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0iyasl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751258423,"author_flair_text":null,"treatment_tags":[],"created_utc":1751258423,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}}],"before":null}},"user_reports":[],"saved":false,"id":"n0iigj4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Steve_Streza","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0iai45","score":5,"author_fullname":"t2_35vw6osy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Probably not too long. A weekend if you know what you're doing, a few days or a week or so if you aren't familiar with the... quirks of AWS or GCM or Azure or whatever.\\n\\nOnce you get the tools set up, it's really not much different from setting up a virtual machine. You'd just SSH into it and set up your software and model. Shut it down when you're not using it (important! maybe set up a \\"shut down automatically after inactivity\\" script), and it'll probably only take a couple minutes (maybe a little longer for a model of that size) to boot up again.\\n\\n\\"Provably secure\\" depends on your threat model but it's also a well solved problem.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0iigj4","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Probably not too long. A weekend if you know what you&amp;#39;re doing, a few days or a week or so if you aren&amp;#39;t familiar with the... quirks of AWS or GCM or Azure or whatever.&lt;/p&gt;\\n\\n&lt;p&gt;Once you get the tools set up, it&amp;#39;s really not much different from setting up a virtual machine. You&amp;#39;d just SSH into it and set up your software and model. Shut it down when you&amp;#39;re not using it (important! maybe set up a &amp;quot;shut down automatically after inactivity&amp;quot; script), and it&amp;#39;ll probably only take a couple minutes (maybe a little longer for a model of that size) to boot up again.&lt;/p&gt;\\n\\n&lt;p&gt;&amp;quot;Provably secure&amp;quot; depends on your threat model but it&amp;#39;s also a well solved problem.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0iigj4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751251487,"author_flair_text":null,"treatment_tags":[],"created_utc":1751251487,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jouun","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Daemontatox","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0iai45","score":2,"author_fullname":"t2_1rm9syq1nb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Depends really on how complicated you want it to be , \\n\\nFor example , aws and azure ? Couple of minutes , the instances come preinstalled with torch , so its a matter of getting vllm /lightingai or torch serve.\\n\\nYou can get cheaper gpus on vastai or lambda but they might take abit longer to setup.\\n\\nAll and all its not too long of  a setup , so i will probably say couple of hours if its bare metal to a weekend depending on how experienced you are.\\n\\nNote: there are alot of guides that walk you through it , so dont worry about getting lost.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0jouun","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Depends really on how complicated you want it to be , &lt;/p&gt;\\n\\n&lt;p&gt;For example , aws and azure ? Couple of minutes , the instances come preinstalled with torch , so its a matter of getting vllm /lightingai or torch serve.&lt;/p&gt;\\n\\n&lt;p&gt;You can get cheaper gpus on vastai or lambda but they might take abit longer to setup.&lt;/p&gt;\\n\\n&lt;p&gt;All and all its not too long of  a setup , so i will probably say couple of hours if its bare metal to a weekend depending on how experienced you are.&lt;/p&gt;\\n\\n&lt;p&gt;Note: there are alot of guides that walk you through it , so dont worry about getting lost.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jouun/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751273503,"author_flair_text":null,"treatment_tags":[],"created_utc":1751273503,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0iai45","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ElectronSpiderwort","can_mod_post":false,"created_utc":1751248392,"send_replies":true,"parent_id":"t1_n0hocvs","score":19,"author_fullname":"t2_mxbu5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm curious what is the time investment for this; I know GPUs can be had for like $1.50/gpu/hour, but I don't know how long it takes to spin up a container or whatever that has the right software and is provably secure etc. etc..  How long does it take, starting from nothing, to get to hosting a quite large model (say Qwen 235b for instance) on a rented GPU cluster?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0iai45","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m curious what is the time investment for this; I know GPUs can be had for like $1.50/gpu/hour, but I don&amp;#39;t know how long it takes to spin up a container or whatever that has the right software and is provably secure etc. etc..  How long does it take, starting from nothing, to get to hosting a quite large model (say Qwen 235b for instance) on a rented GPU cluster?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0iai45/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751248392,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":19}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0lfg23","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"TheRealGentlefox","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0joeje","score":8,"author_fullname":"t2_f471r","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Their question is still whether any local LLM can compare with cloud models. The fastest and easiest way to do that is Openrouter. The logistics of running the model is a different story.\\n\\nThe calculations outside of that aren't that hard. Find the VRAM of the largest/most expensive GPU you're willing to get. Find out what parameter count that can fit at Q4/Q8. Try out models in that size category.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0lfg23","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Their question is still whether any local LLM can compare with cloud models. The fastest and easiest way to do that is Openrouter. The logistics of running the model is a different story.&lt;/p&gt;\\n\\n&lt;p&gt;The calculations outside of that aren&amp;#39;t that hard. Find the VRAM of the largest/most expensive GPU you&amp;#39;re willing to get. Find out what parameter count that can fit at Q4/Q8. Try out models in that size category.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0lfg23/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751298359,"author_flair_text":null,"treatment_tags":[],"created_utc":1751298359,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}}],"before":null}},"user_reports":[],"saved":false,"id":"n0joeje","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Daemontatox","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0imbmv","score":11,"author_fullname":"t2_1rm9syq1nb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Well openrouter isn't really an accurate representation to the gpu , codts and model setup , they wont know how much it costs , how hard it is , which models they will be able to realisticaly host locally .\\n\\nTheir goal is getting a gpu , not just making calls to free models.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0joeje","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Well openrouter isn&amp;#39;t really an accurate representation to the gpu , codts and model setup , they wont know how much it costs , how hard it is , which models they will be able to realisticaly host locally .&lt;/p&gt;\\n\\n&lt;p&gt;Their goal is getting a gpu , not just making calls to free models.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0joeje/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751273225,"author_flair_text":null,"treatment_tags":[],"created_utc":1751273225,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0lrcbq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"philosophical_lens","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0lfley","score":2,"author_fullname":"t2_1pc5yp6c","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"By using openrouter, OP won't understand the relationship between the relationship between GPU resources and model inference workloads. If you're planning to buy a GPU for model inference, it's essential to understand this. With this GPU, which models can I run locally, how fast will inference be, what will be the resource consumption, etc.","edited":false,"author_flair_css_class":null,"name":"t1_n0lrcbq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;By using openrouter, OP won&amp;#39;t understand the relationship between the relationship between GPU resources and model inference workloads. If you&amp;#39;re planning to buy a GPU for model inference, it&amp;#39;s essential to understand this. With this GPU, which models can I run locally, how fast will inference be, what will be the resource consumption, etc.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0lrcbq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751301782,"author_flair_text":null,"collapsed":false,"created_utc":1751301782,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0lfley","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"TheRealGentlefox","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0l6hrg","score":2,"author_fullname":"t2_f471r","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"How so? Aside from speed, what's the difference?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0lfley","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How so? Aside from speed, what&amp;#39;s the difference?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0lfley/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751298401,"author_flair_text":null,"treatment_tags":[],"created_utc":1751298401,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0lge16","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"InsideYork","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0l6hrg","score":1,"author_fullname":"t2_12s3hn4y0b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Bad analogy. You’re comparing renting a fleet car vs a test drive of a new car.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0lge16","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Bad analogy. You’re comparing renting a fleet car vs a test drive of a new car.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0lge16/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751298629,"author_flair_text":null,"treatment_tags":[],"created_utc":1751298629,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0l6hrg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"philosophical_lens","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0imbmv","score":3,"author_fullname":"t2_1pc5yp6c","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It's like advising someone to rent a car before they decide to buy it. Your response is like \\"why rent a car when you can just use Uber instead?\\". That doesn't help you understand the car you want to buy.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0l6hrg","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s like advising someone to rent a car before they decide to buy it. Your response is like &amp;quot;why rent a car when you can just use Uber instead?&amp;quot;. That doesn&amp;#39;t help you understand the car you want to buy.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0l6hrg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751295746,"author_flair_text":null,"treatment_tags":[],"created_utc":1751295746,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0imbmv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"TheRealGentlefox","can_mod_post":false,"created_utc":1751253061,"send_replies":true,"parent_id":"t1_n0hocvs","score":10,"author_fullname":"t2_f471r","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"No reason to rent a GPU, they can set up OpenRouter in literally 5 minutes to try out every model known to man. No configuring needed, just run them all in chat completion mode.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0imbmv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No reason to rent a GPU, they can set up OpenRouter in literally 5 minutes to try out every model known to man. No configuring needed, just run them all in chat completion mode.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0imbmv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751253061,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hocvs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Daemontatox","can_mod_post":false,"created_utc":1751240112,"send_replies":true,"parent_id":"t3_1lnsax9","score":83,"author_fullname":"t2_1rm9syq1nb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"How about renting a gpu online and trying out the models you think will do great in use case before committing to buying a gpu.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hocvs","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How about renting a gpu online and trying out the models you think will do great in use case before committing to buying a gpu.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hocvs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751240112,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":83}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0lp1f4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"-my_dude","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0i7iig","score":6,"author_fullname":"t2_6bzci4ja","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I would not make a major hardware purchase right now. 70b is nearly 7 months old and all recent models have been MoE. You're gonna need more than just a GPU to run those at a decent speed and quant.\\n\\nI'd take a look at some 24b and 36b models on OpenRouter since those still get dense releases, but the quants are probably gonna be better than what would run on just a 3090.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0lp1f4","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I would not make a major hardware purchase right now. 70b is nearly 7 months old and all recent models have been MoE. You&amp;#39;re gonna need more than just a GPU to run those at a decent speed and quant.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;d take a look at some 24b and 36b models on OpenRouter since those still get dense releases, but the quants are probably gonna be better than what would run on just a 3090.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0lp1f4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751301119,"author_flair_text":null,"treatment_tags":[],"created_utc":1751301119,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0j5d25","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No_Elderberry_9132","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0i7iig","score":8,"author_fullname":"t2_byh4ysuoh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I run it on 4 GPU strap, not a big problem, you can even run the full model on CPU with 2-3 tokens generation per second, but takes 1TB of ram :)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0j5d25","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I run it on 4 GPU strap, not a big problem, you can even run the full model on CPU with 2-3 tokens generation per second, but takes 1TB of ram :)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0j5d25/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751262016,"author_flair_text":null,"treatment_tags":[],"created_utc":1751262016,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0obv6b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"YouDontSeemRight","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0jygee","score":2,"author_fullname":"t2_1b7gjxtue9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Exactly, totally agree. 48GB is likely where a lot of local enthusiasts will end up given power and system constraints. With 48GB GPU you can fit the static layers in GPU and put the expert layers in CPU RAM. More CPU RAM the bigger the model you can run. When META gets their act together and make them good, Scout and Maverick can actually require run on identical GPU specs and only require more CPU RAM to fit the bigger model. It makes very little difference trying to upgrade the GPU RAM and your better off focusing on CPU processing power (first bottle neck) and lastly RAM bandwidth, but in most cases you likely don't hit the ram bandwidth. You may on the newest 395 max. Anyway, that means you can prioritize slower higher capacity ram. In the end I can run Maverick at a decent 20fps last I tried which is actually faster than my 70B numbers. Technically the 235B IS the new 70B. The geometric average thing brings it roughly to 70B equivalent. Qwen 235B's biggest issue is the parallel experts required. If they knock that in half to four it'll be my go to model.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n0obv6b","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Exactly, totally agree. 48GB is likely where a lot of local enthusiasts will end up given power and system constraints. With 48GB GPU you can fit the static layers in GPU and put the expert layers in CPU RAM. More CPU RAM the bigger the model you can run. When META gets their act together and make them good, Scout and Maverick can actually require run on identical GPU specs and only require more CPU RAM to fit the bigger model. It makes very little difference trying to upgrade the GPU RAM and your better off focusing on CPU processing power (first bottle neck) and lastly RAM bandwidth, but in most cases you likely don&amp;#39;t hit the ram bandwidth. You may on the newest 395 max. Anyway, that means you can prioritize slower higher capacity ram. In the end I can run Maverick at a decent 20fps last I tried which is actually faster than my 70B numbers. Technically the 235B IS the new 70B. The geometric average thing brings it roughly to 70B equivalent. Qwen 235B&amp;#39;s biggest issue is the parallel experts required. If they knock that in half to four it&amp;#39;ll be my go to model.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0obv6b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751329679,"author_flair_text":null,"treatment_tags":[],"created_utc":1751329679,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0k9a9c","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Karyo_Ten","can_mod_post":false,"created_utc":1751284408,"send_replies":true,"parent_id":"t1_n0k3nzb","score":1,"author_fullname":"t2_tbdqg","approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt;at acceptable* speeds (5t/s or so).\\n\\nI explicitly said that 10 tok/s is too slow.\\n\\n&gt;All the hostility and downvotes are completely uncalled for and a disgrace imo.\\n\\nMy first comment was:\\n\\n&gt; You mean upgrade from 2x 5090 to 8x5090 ?\\n\\nThen that guy went on a passive-aggressive crusade saying people here have no clue that when you solve 70b 235b is just around the corner.\\n\\nHe played the contempt and patronizing game, he gets called out on that.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n0k9a9c","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;at acceptable* speeds (5t/s or so).&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I explicitly said that 10 tok/s is too slow.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;All the hostility and downvotes are completely uncalled for and a disgrace imo.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;My first comment was:&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;You mean upgrade from 2x 5090 to 8x5090 ?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Then that guy went on a passive-aggressive crusade saying people here have no clue that when you solve 70b 235b is just around the corner.&lt;/p&gt;\\n\\n&lt;p&gt;He played the contempt and patronizing game, he gets called out on that.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0k9a9c/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751284408,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0kukt3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Threatening-Silence-","can_mod_post":false,"created_utc":1751292141,"send_replies":true,"parent_id":"t1_n0ksr41","score":1,"author_fullname":"t2_15wqsifdjf","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Something like this, a used Xeon server, 256gb for £695\\n\\nhttps://ebay.us/m/Dq5CU3","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0kukt3","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Something like this, a used Xeon server, 256gb for £695&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://ebay.us/m/Dq5CU3\\"&gt;https://ebay.us/m/Dq5CU3&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0kukt3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751292141,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":8,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ksr41","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Current-Ticket4214","can_mod_post":false,"created_utc":1751291572,"send_replies":true,"parent_id":"t1_n0k3nzb","score":0,"author_fullname":"t2_6yvd3nyy","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Show me the hardware list. I’m certainly interested.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n0ksr41","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Show me the hardware list. I’m certainly interested.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0ksr41/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751291572,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n0k3nzb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Threatening-Silence-","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0jygee","score":5,"author_fullname":"t2_15wqsifdjf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You can run Qwen3-235B-A22B on 256gb system ram and a single 3090 at acceptable* speeds (5t/s or so).\\n\\nYou can put that together for under a grand. \\n\\nAll the hostility and downvotes are completely uncalled for and a disgrace imo.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n0k3nzb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can run Qwen3-235B-A22B on 256gb system ram and a single 3090 at acceptable* speeds (5t/s or so).&lt;/p&gt;\\n\\n&lt;p&gt;You can put that together for under a grand. &lt;/p&gt;\\n\\n&lt;p&gt;All the hostility and downvotes are completely uncalled for and a disgrace imo.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0k3nzb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751281852,"author_flair_text":null,"treatment_tags":[],"created_utc":1751281852,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n0jygee","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Karyo_Ten","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0jxt70","score":-3,"author_fullname":"t2_tbdqg","approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt;No, apparently local llama has no clue about running models locally or what a dense vs MOE model is. If you have a 70B your likely running it partially on GPU.\\n\\nStats that you pulled out of your ass?\\n\\nIt's more likely that people here running 70b are actually running them on 2x3090, rather than deal with less than 10 tok/s due to CPU.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0jygee","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;No, apparently local llama has no clue about running models locally or what a dense vs MOE model is. If you have a 70B your likely running it partially on GPU.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Stats that you pulled out of your ass?&lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s more likely that people here running 70b are actually running them on 2x3090, rather than deal with less than 10 tok/s due to CPU.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jygee/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751279182,"author_flair_text":null,"treatment_tags":[],"created_utc":1751279182,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0jxt70","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"YouDontSeemRight","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ixyvg","score":3,"author_fullname":"t2_1b7gjxtue9","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"No, apparently local llama has no clue about running models locally or what a dense vs MOE model is. If you have a 70B your likely running it partially on GPU. With a MOE you dump the static weights on GPU and the experts in CPU RAM so in the end 235B just requires cheaper CPU RAM to run.","edited":false,"author_flair_css_class":null,"name":"t1_n0jxt70","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No, apparently local llama has no clue about running models locally or what a dense vs MOE model is. If you have a 70B your likely running it partially on GPU. With a MOE you dump the static weights on GPU and the experts in CPU RAM so in the end 235B just requires cheaper CPU RAM to run.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jxt70/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751278824,"author_flair_text":null,"collapsed":false,"created_utc":1751278824,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ixyvg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Karyo_Ten","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0imk68","score":24,"author_fullname":"t2_tbdqg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You mean upgrade from 2x 5090 to 8x5090 ?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ixyvg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You mean upgrade from 2x 5090 to 8x5090 ?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0ixyvg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751258262,"author_flair_text":null,"treatment_tags":[],"created_utc":1751258262,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":24}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jyj4e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"night0x63","can_mod_post":false,"created_utc":1751279224,"send_replies":true,"parent_id":"t1_n0jewgf","score":1,"author_fullname":"t2_3h2irqtz","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks for the information! Super super helpful. I greatly appreciate!\\n\\n\\nI haven't run any MOE models. And only started with llama3.1, 3.2, 3.3.\\n\\n\\nI haven't tried llama4 because everyone and their mother have negative reviews for llama4.\\n\\n\\nYour post gives me the encouragement to try: mixtral 8x22b, llama4.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n0jyj4e","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for the information! Super super helpful. I greatly appreciate!&lt;/p&gt;\\n\\n&lt;p&gt;I haven&amp;#39;t run any MOE models. And only started with llama3.1, 3.2, 3.3.&lt;/p&gt;\\n\\n&lt;p&gt;I haven&amp;#39;t tried llama4 because everyone and their mother have negative reviews for llama4.&lt;/p&gt;\\n\\n&lt;p&gt;Your post gives me the encouragement to try: mixtral 8x22b, llama4.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jyj4e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751279224,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0jewgf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Serprotease","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0jechj","score":6,"author_fullname":"t2_odh3w8c","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It’s a MoE.  \\nThe token output speed is about twice that of a 70b q8 in my setup.  \\nFrom ~7 tps to ~15 tps.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n0jewgf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It’s a MoE.&lt;br/&gt;\\nThe token output speed is about twice that of a 70b q8 in my setup.&lt;br/&gt;\\nFrom ~7 tps to ~15 tps.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jewgf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751267385,"author_flair_text":null,"treatment_tags":[],"created_utc":1751267385,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jufj7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Threatening-Silence-","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0jechj","score":3,"author_fullname":"t2_15wqsifdjf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The full name of it is Qwen3 235b-A22b.\\n\\n22b active parameters, versus all 70b for the dense model.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n0jufj7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The full name of it is Qwen3 235b-A22b.&lt;/p&gt;\\n\\n&lt;p&gt;22b active parameters, versus all 70b for the dense model.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jufj7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751276894,"author_flair_text":null,"treatment_tags":[],"created_utc":1751276894,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0jechj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"night0x63","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0jc5pa","score":1,"author_fullname":"t2_3h2irqtz","approved_by":null,"mod_note":null,"all_awardings":[],"body":"How is 235b faster than 70b?\\n\\n\\nI would expect about 4x slower.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0jechj","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How is 235b faster than 70b?&lt;/p&gt;\\n\\n&lt;p&gt;I would expect about 4x slower.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jechj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751267058,"author_flair_text":null,"treatment_tags":[],"created_utc":1751267058,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jeho3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"night0x63","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0jc5pa","score":1,"author_fullname":"t2_3h2irqtz","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Re similar to llama3.1 to llama3.3 gap: that sounds like a big jump!","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0jeho3","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Re similar to llama3.1 to llama3.3 gap: that sounds like a big jump!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jeho3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751267143,"author_flair_text":null,"treatment_tags":[],"created_utc":1751267143,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0jc5pa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Serprotease","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ipfbf","score":3,"author_fullname":"t2_odh3w8c","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It feels somewhat similar to the gap llama3.1 and llama3.3. \\nIt’s better, for sure. But maybe not as large as the gap between llama2 and llama3 for example.  \\n\\nThe big thing is that if you can run it locally, it’s most likely faster than any 70b on the same hardware.","edited":false,"author_flair_css_class":null,"name":"t1_n0jc5pa","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It feels somewhat similar to the gap llama3.1 and llama3.3. \\nIt’s better, for sure. But maybe not as large as the gap between llama2 and llama3 for example.  &lt;/p&gt;\\n\\n&lt;p&gt;The big thing is that if you can run it locally, it’s most likely faster than any 70b on the same hardware.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jc5pa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751265799,"author_flair_text":null,"collapsed":false,"created_utc":1751265799,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ipfbf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"night0x63","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0imk68","score":7,"author_fullname":"t2_3h2irqtz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"How does qwen 235b compare to llama3.3:70b?\\n\\n\\nAre there other 70b?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ipfbf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How does qwen 235b compare to llama3.3:70b?&lt;/p&gt;\\n\\n&lt;p&gt;Are there other 70b?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0ipfbf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751254374,"author_flair_text":null,"treatment_tags":[],"created_utc":1751254374,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0k36rm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Threatening-Silence-","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0imk68","score":5,"author_fullname":"t2_15wqsifdjf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Not sure why you're getting downvoted, I think from a lot of ignorant people who don't know Qwen3-235B-**A22B**  is a MoE model with only 22B active parameters, very amenable to partial offloading to RAM. You can run the whole thing with 256GB system ram and one 3090.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0k36rm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not sure why you&amp;#39;re getting downvoted, I think from a lot of ignorant people who don&amp;#39;t know Qwen3-235B-&lt;strong&gt;A22B&lt;/strong&gt;  is a MoE model with only 22B active parameters, very amenable to partial offloading to RAM. You can run the whole thing with 256GB system ram and one 3090.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0k36rm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751281617,"author_flair_text":null,"treatment_tags":[],"created_utc":1751281617,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0je9rb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mxforest","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0imk68","score":1,"author_fullname":"t2_kenmq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Wow!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0je9rb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Wow!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0je9rb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751267014,"author_flair_text":null,"treatment_tags":[],"created_utc":1751267014,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0imk68","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"YouDontSeemRight","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0i7iig","score":-12,"author_fullname":"t2_1b7gjxtue9","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"If you can run a 70B, Qwen 235B isn't far off.\\n\\nEdit: wow look at these downvotes. Guess not many on local llama have ran 70b or 235B locally. 235B just required more system RAM. Static weights go to GPU.","edited":1751278660,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0imk68","is_submitter":false,"collapsed":true,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you can run a 70B, Qwen 235B isn&amp;#39;t far off.&lt;/p&gt;\\n\\n&lt;p&gt;Edit: wow look at these downvotes. Guess not many on local llama have ran 70b or 235B locally. 235B just required more system RAM. Static weights go to GPU.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0imk68/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751253159,"author_flair_text":null,"treatment_tags":[],"created_utc":1751253159,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-12}}],"before":null}},"user_reports":[],"saved":false,"id":"n0i7iig","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Edzomatic","can_mod_post":false,"created_utc":1751247257,"send_replies":true,"parent_id":"t1_n0hk6bd","score":40,"author_fullname":"t2_9ist3ny0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Come close means running deepseek which is most likely not gonna happen. But 70b at a decent quant can serve you well depending on the use case","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0i7iig","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Come close means running deepseek which is most likely not gonna happen. But 70b at a decent quant can serve you well depending on the use case&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0i7iig/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751247257,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":40}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0kh7cr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Voxandr","can_mod_post":false,"created_utc":1751287579,"send_replies":true,"parent_id":"t1_n0hk6bd","score":1,"author_fullname":"t2_86dk0gye","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Qwen3 fair betters than cluad3.5 sonnet in several cases.\\n\\n\\\\- instruction following  \\n\\\\- tool calling","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0kh7cr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen3 fair betters than cluad3.5 sonnet in several cases.&lt;/p&gt;\\n\\n&lt;p&gt;- instruction following&lt;br/&gt;\\n- tool calling&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0kh7cr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751287579,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0j1tw3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"BlueSwordM","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0irxsl","score":11,"author_fullname":"t2_qhqon","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You use the 3090 to greatly boost prompt processing and context.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0j1tw3","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You use the 3090 to greatly boost prompt processing and context.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0j1tw3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751260164,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1751260164,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0izxk6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0irxsl","score":14,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Prompt processing.GPU massively speeds it up, even if inference us 100% on cpu.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0izxk6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Prompt processing.GPU massively speeds it up, even if inference us 100% on cpu.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0izxk6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751259212,"author_flair_text":null,"treatment_tags":[],"created_utc":1751259212,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":14}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0iztan","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ixip8","score":10,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"No gpu = non existent PP speed","edited":false,"author_flair_css_class":null,"name":"t1_n0iztan","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No gpu = non existent PP speed&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0iztan/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751259154,"author_flair_text":null,"collapsed":false,"created_utc":1751259154,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0juvkr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Threatening-Silence-","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ixip8","score":3,"author_fullname":"t2_15wqsifdjf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Prompt processing speed on CPU only is about the same as inference token speed on CPU only. A couple of tokens per second. \\n\\nRoo Code's initial prompt seems to be around 12,000 tokens. \\n\\nMy 3090 can do 77t/s processing that. So I get my first reply in about two minutes (Deepseek R1 0528 - IQ3_XXS, ubatch 1024).\\n\\nThe same prompt on CPU only would take about 40 minutes until you get your first reply 💀","edited":false,"author_flair_css_class":null,"name":"t1_n0juvkr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Prompt processing speed on CPU only is about the same as inference token speed on CPU only. A couple of tokens per second. &lt;/p&gt;\\n\\n&lt;p&gt;Roo Code&amp;#39;s initial prompt seems to be around 12,000 tokens. &lt;/p&gt;\\n\\n&lt;p&gt;My 3090 can do 77t/s processing that. So I get my first reply in about two minutes (Deepseek R1 0528 - IQ3_XXS, ubatch 1024).&lt;/p&gt;\\n\\n&lt;p&gt;The same prompt on CPU only would take about 40 minutes until you get your first reply 💀&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0juvkr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751277153,"author_flair_text":null,"collapsed":false,"created_utc":1751277153,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ixip8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ambitious_Subject108","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0irxsl","score":-2,"author_fullname":"t2_iaffzj2q","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It'll hurt more than it helps. CPU to memory latency is an order of magnitude faster than CPU to GPU.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ixip8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;ll hurt more than it helps. CPU to memory latency is an order of magnitude faster than CPU to GPU.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0ixip8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751258044,"author_flair_text":null,"treatment_tags":[],"created_utc":1751258044,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0irxsl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"gigaflops_","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0iivh7","score":9,"author_fullname":"t2_1t2n2s9f","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Question: what good would the 3090 do in this build? My understanding is that since the vast majority of the model (~85% of it) is stored on system RAM, it'll be computer almost entirely on the CPU. Unless of course you configure it to run entirely on GPU and transfer hundreds of gigabytes across the pcie lane every token, which in my experience is slower than CPU inference.\\n\\nI'm trying to understand this stuff better, so lmk where my thinking is wrong.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0irxsl","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Question: what good would the 3090 do in this build? My understanding is that since the vast majority of the model (~85% of it) is stored on system RAM, it&amp;#39;ll be computer almost entirely on the CPU. Unless of course you configure it to run entirely on GPU and transfer hundreds of gigabytes across the pcie lane every token, which in my experience is slower than CPU inference.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m trying to understand this stuff better, so lmk where my thinking is wrong.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0irxsl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751255472,"author_flair_text":null,"treatment_tags":[],"created_utc":1751255472,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0kz4sj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Steuern_Runter","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0kl4ow","score":2,"author_fullname":"t2_w8pggsa4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; You can buy a Lenovo P520 with quad channel memory\\n\\nIt has quad channel DDR4 which is as fast as dual channel DDR5.\\n\\nBut you are right that big MoE models are a huge improvement for CPU inference.","edited":false,"author_flair_css_class":null,"name":"t1_n0kz4sj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;You can buy a Lenovo P520 with quad channel memory&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;It has quad channel DDR4 which is as fast as dual channel DDR5.&lt;/p&gt;\\n\\n&lt;p&gt;But you are right that big MoE models are a huge improvement for CPU inference.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0kz4sj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751293559,"author_flair_text":null,"collapsed":false,"created_utc":1751293559,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":0,"name":"t1__","id":"_","parent_id":"t1_n0ohhat","depth":10,"children":[]}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ohhat","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"created_utc":1751331678,"send_replies":true,"parent_id":"t1_n0o2ivp","score":2,"author_fullname":"t2_t6glzswk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"No, you need nvlink support on the mobo as well. \\n\\nSource: I tried nvlinking 3090 FEs together before on an old i5 box, no dice. BIOS didn’t support it and you need pcie bifurcation support.\\n\\nFeel free to ask chatgpt “What does a motherboard need to support nvlink?”\\n\\nAnd Mi50s are a better idea overall, but you def can’t fit Deepseek on it unless you buy like 10 lol. I do wonder how fast a dozen MI50 box can run deepseek Q4 though, especially since Q4 seems the sweet spot for most model quants.","edited":1751331879,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ohhat","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No, you need nvlink support on the mobo as well. &lt;/p&gt;\\n\\n&lt;p&gt;Source: I tried nvlinking 3090 FEs together before on an old i5 box, no dice. BIOS didn’t support it and you need pcie bifurcation support.&lt;/p&gt;\\n\\n&lt;p&gt;Feel free to ask chatgpt “What does a motherboard need to support nvlink?”&lt;/p&gt;\\n\\n&lt;p&gt;And Mi50s are a better idea overall, but you def can’t fit Deepseek on it unless you buy like 10 lol. I do wonder how fast a dozen MI50 box can run deepseek Q4 though, especially since Q4 seems the sweet spot for most model quants.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0ohhat/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751331678,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":9,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0o2ivp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1751326489,"send_replies":true,"parent_id":"t1_n0nxx96","score":1,"author_fullname":"t2_h5utwre7","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Nvlink is based on cards having it themselves. Will it fit the 2nd 3090 without using a riser to relocate it?\\n\\nOne can cheap out completely and do 32gb Mi50s (I see them under $300). To me it seems like a terrible deal to buy expensive 64gb ddr4 chips vs the cheap 32s to run larger quants it's gonna struggle with anyway. \\n\\nA lot more attractive as a new mikubox with some mid-size text/image capabilities than a deepseek rig. Below the cost of a 4090 basically all in.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0o2ivp","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nvlink is based on cards having it themselves. Will it fit the 2nd 3090 without using a riser to relocate it?&lt;/p&gt;\\n\\n&lt;p&gt;One can cheap out completely and do 32gb Mi50s (I see them under $300). To me it seems like a terrible deal to buy expensive 64gb ddr4 chips vs the cheap 32s to run larger quants it&amp;#39;s gonna struggle with anyway. &lt;/p&gt;\\n\\n&lt;p&gt;A lot more attractive as a new mikubox with some mid-size text/image capabilities than a deepseek rig. Below the cost of a 4090 basically all in.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0o2ivp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751326489,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":8,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0nxx96","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"created_utc":1751324999,"send_replies":true,"parent_id":"t1_n0nwk6r","score":2,"author_fullname":"t2_t6glzswk","approved_by":null,"mod_note":null,"all_awardings":[],"body":"It does support nvlink though, unlike consumer machines, so it’s better for finetuning. That also makes PCIe a lot less of an issue. It’s the best setup to do ML work with 2x 3090s.\\n\\nFor old workstations, if you don’t get 512gb ram, and buy 2x 3090 and a nvlink bridge instead, it makes a lot more sense as a purchase. \\n\\nIn that case, if you’re grabbing an older workstation to run nvlink, dropping another $250ish for 256gb of ram to run Deepseek is a fun extra, if not a primary goal.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n0nxx96","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It does support nvlink though, unlike consumer machines, so it’s better for finetuning. That also makes PCIe a lot less of an issue. It’s the best setup to do ML work with 2x 3090s.&lt;/p&gt;\\n\\n&lt;p&gt;For old workstations, if you don’t get 512gb ram, and buy 2x 3090 and a nvlink bridge instead, it makes a lot more sense as a purchase. &lt;/p&gt;\\n\\n&lt;p&gt;In that case, if you’re grabbing an older workstation to run nvlink, dropping another $250ish for 256gb of ram to run Deepseek is a fun extra, if not a primary goal.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0nxx96/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751324999,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0nwk6r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0nczqt","score":1,"author_fullname":"t2_h5utwre7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Sure but I think you have a better time renting than using this rig beyond \\"i ran it\\". \\n\\nWhen you go to try other models which give reasonable speeds, it's gonna be disappointing. Not fast enough to CPUmaxx, not enough PCIE to stack GPUs over time.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n0nwk6r","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sure but I think you have a better time renting than using this rig beyond &amp;quot;i ran it&amp;quot;. &lt;/p&gt;\\n\\n&lt;p&gt;When you go to try other models which give reasonable speeds, it&amp;#39;s gonna be disappointing. Not fast enough to CPUmaxx, not enough PCIE to stack GPUs over time.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0nwk6r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751324549,"author_flair_text":null,"treatment_tags":[],"created_utc":1751324549,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0nczqt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ltqlt","score":2,"author_fullname":"t2_t6glzswk","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I mean, if the goal is running deepseek, this is the cheapest path by far. Especially since old server DDR4 is a lot cheaper than consumer DDR5, and you get ECC as well.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0nczqt","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I mean, if the goal is running deepseek, this is the cheapest path by far. Especially since old server DDR4 is a lot cheaper than consumer DDR5, and you get ECC as well.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0nczqt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751318299,"author_flair_text":null,"treatment_tags":[],"created_utc":1751318299,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ltqlt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0kl4ow","score":1,"author_fullname":"t2_h5utwre7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"max memory bandwidth: 93.85 GB/s\\n\\nJust don't end up with the skylake version without vnni and only slightly faster b/w vs ddr-5 consumer boards.\\n\\nI can see how this build works and *looks* like you save money, but then has enough lack where you wanna upgrade while having nowhere to go.","edited":false,"author_flair_css_class":null,"name":"t1_n0ltqlt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;max memory bandwidth: 93.85 GB/s&lt;/p&gt;\\n\\n&lt;p&gt;Just don&amp;#39;t end up with the skylake version without vnni and only slightly faster b/w vs ddr-5 consumer boards.&lt;/p&gt;\\n\\n&lt;p&gt;I can see how this build works and &lt;em&gt;looks&lt;/em&gt; like you save money, but then has enough lack where you wanna upgrade while having nowhere to go.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0ltqlt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751302470,"author_flair_text":null,"collapsed":false,"created_utc":1751302470,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0kl4ow","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ker9w","score":3,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Nope. You can buy a Lenovo P520 with **quad channel** memory for $300 easy. Then throw in 64gb*8 sticks of RAM for $500 ish. \\n\\nThen when you run inference for Deepseek you want to set all layers to GPU but then add \`   -ot \\".ffn_.*_exps.=CPU\\"\` which will leave the ffn (up/down/gate) routed experts in system RAM, which makes it run substantially faster without needing a few hundred GBs of vram.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0kl4ow","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nope. You can buy a Lenovo P520 with &lt;strong&gt;quad channel&lt;/strong&gt; memory for $300 easy. Then throw in 64gb*8 sticks of RAM for $500 ish. &lt;/p&gt;\\n\\n&lt;p&gt;Then when you run inference for Deepseek you want to set all layers to GPU but then add &lt;code&gt;-ot &amp;quot;.ffn_.*_exps.=CPU&amp;quot;&lt;/code&gt; which will leave the ffn (up/down/gate) routed experts in system RAM, which makes it run substantially faster without needing a few hundred GBs of vram.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0kl4ow/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751289003,"author_flair_text":null,"treatment_tags":[],"created_utc":1751289003,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ker9w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0iivh7","score":1,"author_fullname":"t2_h5utwre7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"To make it worthwhile, that workstation won't be $300. You need decent memory bandwidth and the processor supporting modern extensions. More like $3000 since you're only using a single GPU and can't offload but a drop.  \\n\\nDeepseek is more runable than people think, but nowhere near API speeds of major providers despite the financial outlay. For chat and with reasoning disabled, its neat. I wouldn't want to main it for code completion unless I absolutely had to.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0ker9w","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;To make it worthwhile, that workstation won&amp;#39;t be $300. You need decent memory bandwidth and the processor supporting modern extensions. More like $3000 since you&amp;#39;re only using a single GPU and can&amp;#39;t offload but a drop.  &lt;/p&gt;\\n\\n&lt;p&gt;Deepseek is more runable than people think, but nowhere near API speeds of major providers despite the financial outlay. For chat and with reasoning disabled, its neat. I wouldn&amp;#39;t want to main it for code completion unless I absolutely had to.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0ker9w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751286646,"author_flair_text":null,"treatment_tags":[],"created_utc":1751286646,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jft2p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0jfhea","score":2,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Spend ~$300 for 256gb\\n\\nOr spend 200 more ~$500 for 512gb","edited":1751289522,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jft2p","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Spend ~$300 for 256gb&lt;/p&gt;\\n\\n&lt;p&gt;Or spend 200 more ~$500 for 512gb&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jft2p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751267921,"author_flair_text":null,"treatment_tags":[],"created_utc":1751267921,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0kdhi5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RickyRickC137","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0jfhea","score":0,"author_fullname":"t2_mhdt7ir5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You'll get repeated customer discounts","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0kdhi5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You&amp;#39;ll get repeated customer discounts&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0kdhi5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751286146,"author_flair_text":null,"treatment_tags":[],"created_utc":1751286146,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n0jfhea","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"forhorglingrads","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0iivh7","score":1,"author_fullname":"t2_4pqxe","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"why is the 2nd half of the ram $100 cheaper than the first half","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0jfhea","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;why is the 2nd half of the ram $100 cheaper than the first half&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jfhea/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751267728,"author_flair_text":null,"treatment_tags":[],"created_utc":1751267728,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0iivh7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"created_utc":1751251652,"send_replies":true,"parent_id":"t1_n0hk6bd","score":-4,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"At full speed? Yeah, you can’t come close to Claude at home.\\n\\nSlowly? You can buy an old workstation on ebay for $300, spend $300 to drop in 256gb of RAM, put a 3090 in, and run Deepseek R1 0528 quantized to Q2 pretty easily. You’ll just get only 5 tokens/sec. \\n\\nIf Q2 is too small for you, spend $200 more and you’ll get 512gb RAM, and that lets you run Deepseek at Q5 which is pretty close to full quality.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0iivh7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;At full speed? Yeah, you can’t come close to Claude at home.&lt;/p&gt;\\n\\n&lt;p&gt;Slowly? You can buy an old workstation on ebay for $300, spend $300 to drop in 256gb of RAM, put a 3090 in, and run Deepseek R1 0528 quantized to Q2 pretty easily. You’ll just get only 5 tokens/sec. &lt;/p&gt;\\n\\n&lt;p&gt;If Q2 is too small for you, spend $200 more and you’ll get 512gb RAM, and that lets you run Deepseek at Q5 which is pretty close to full quality.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0iivh7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751251652,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-4}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hk6bd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Current-Ticket4214","can_mod_post":false,"created_utc":1751238618,"send_replies":true,"parent_id":"t3_1lnsax9","score":136,"author_fullname":"t2_6yvd3nyy","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You won’t match Claude with a local LLM running on high end consumer hardware. You need a commercial grade server with a ton of compute to even come close.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hk6bd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You won’t match Claude with a local LLM running on high end consumer hardware. You need a commercial grade server with a ton of compute to even come close.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hk6bd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751238618,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":136}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0i96k6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Secure_Reflection409","can_mod_post":false,"created_utc":1751247890,"send_replies":true,"parent_id":"t3_1lnsax9","score":45,"author_fullname":"t2_by77ogdhr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You won't match the cloud for speed, cost or competence.\\n\\n\\nThe cloud will never match local for privacy.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0i96k6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You won&amp;#39;t match the cloud for speed, cost or competence.&lt;/p&gt;\\n\\n&lt;p&gt;The cloud will never match local for privacy.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0i96k6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751247890,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":45}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hrkvg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Terminator857","can_mod_post":false,"created_utc":1751241278,"send_replies":true,"parent_id":"t3_1lnsax9","score":17,"author_fullname":"t2_m40tjcn","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Maybe next year it won't cost too much to run deepseek at home.  As soon as the hardware costs drop to under $5K for reasonable speed I'm buying.  \\n\\nMaybe software improvements like diffusion for LLMs will make this more probable.    [https://www.reddit.com/r/LocalLLM/comments/1ljbajp/diffusion\\\\_language\\\\_models\\\\_will\\\\_cut\\\\_the\\\\_cost\\\\_of/](https://www.reddit.com/r/LocalLLM/comments/1ljbajp/diffusion_language_models_will_cut_the_cost_of/)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hrkvg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Maybe next year it won&amp;#39;t cost too much to run deepseek at home.  As soon as the hardware costs drop to under $5K for reasonable speed I&amp;#39;m buying.  &lt;/p&gt;\\n\\n&lt;p&gt;Maybe software improvements like diffusion for LLMs will make this more probable.    &lt;a href=\\"https://www.reddit.com/r/LocalLLM/comments/1ljbajp/diffusion_language_models_will_cut_the_cost_of/\\"&gt;https://www.reddit.com/r/LocalLLM/comments/1ljbajp/diffusion_language_models_will_cut_the_cost_of/&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hrkvg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751241278,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":17}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0j0s90","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Karyo_Ten","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0j0d9v","score":3,"author_fullname":"t2_tbdqg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"https://www.youtube.com/watch?v=hSoCmAoIMOU","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0j0s90","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://www.youtube.com/watch?v=hSoCmAoIMOU\\"&gt;https://www.youtube.com/watch?v=hSoCmAoIMOU&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0j0s90/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751259635,"author_flair_text":null,"treatment_tags":[],"created_utc":1751259635,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0j0d9v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"moncallikta","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0iygcb","score":3,"author_fullname":"t2_15ju4l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Can it run Doom?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0j0d9v","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Can it run Doom?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0j0d9v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751259427,"author_flair_text":null,"treatment_tags":[],"created_utc":1751259427,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0iygcb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Karyo_Ten","can_mod_post":false,"created_utc":1751258496,"send_replies":true,"parent_id":"t1_n0htp47","score":6,"author_fullname":"t2_tbdqg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"If we follow your reasoning, OP should buy a dam instead.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0iygcb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If we follow your reasoning, OP should buy a dam instead.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0iygcb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751258496,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n0htp47","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FunnyAsparagus1253","can_mod_post":false,"created_utc":1751242066,"send_replies":true,"parent_id":"t3_1lnsax9","score":34,"author_fullname":"t2_i6c8tay3w","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Dude just buy a 3090 for fun. Then when the apocalypse hits and nobody has internet any more you’ll still have someone smart to chat to. Run quantized mistral small or whatever - sure it’s not as smart or good at coding as claude but so what? :)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0htp47","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Dude just buy a 3090 for fun. Then when the apocalypse hits and nobody has internet any more you’ll still have someone smart to chat to. Run quantized mistral small or whatever - sure it’s not as smart or good at coding as claude but so what? :)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0htp47/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751242066,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":34}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0iu05c","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"hapliniste","can_mod_post":false,"created_utc":1751256404,"send_replies":true,"parent_id":"t1_n0hk8tr","score":-7,"author_fullname":"t2_fc7rd","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Who upvote this? 🤨","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0iu05c","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Who upvote this? 🤨&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0iu05c/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751256404,"author_flair_text":null,"treatment_tags":[],"collapsed":true,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0kbjhn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"photodesignch","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0k7vke","score":2,"author_fullname":"t2_dnfi3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Cline is multi-agents.  Same with continue. But performance is pretty bad.  If I need code analysis I can probably run a deepseek and wait for a couple of hours for it to reply.  Otherwise, it’s impossible to get the Claude quality to be honest.  To code a “hello world” any SLM can do it. But to do real coding, I only found Claude and Gemini can do the task now.  Copilot is okay, but far from accurate.\\n\\nAs you code a lot, the answer is pretty obvious.  local LLMs can’t match remote.\\n\\nHowever! One I suggest to do is running SLM on autocomplete. That you can do easily and it’s fairly accurately.  Just not writing code for you and give you runnable completed answers.\\n\\nSLM can do code analysis decently though.\\n\\nWhat remote LLM can do is to go through the whole project and write up the design architecture, api swagger fairly easily.  Thats super helpful for documentation tasks and remote LLM do it nicely and quick.  Local LLM might take ages to do it and the result is somewhat 50/50.\\n\\nWhat local SLM does for me is to consolidate and give fast general informations like summarize what a repository of code does.  I often use fabric ai with SLM to summarize code for me into a few sentences or bullet points.  Great for presentation PPT.","edited":false,"author_flair_css_class":null,"name":"t1_n0kbjhn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Cline is multi-agents.  Same with continue. But performance is pretty bad.  If I need code analysis I can probably run a deepseek and wait for a couple of hours for it to reply.  Otherwise, it’s impossible to get the Claude quality to be honest.  To code a “hello world” any SLM can do it. But to do real coding, I only found Claude and Gemini can do the task now.  Copilot is okay, but far from accurate.&lt;/p&gt;\\n\\n&lt;p&gt;As you code a lot, the answer is pretty obvious.  local LLMs can’t match remote.&lt;/p&gt;\\n\\n&lt;p&gt;However! One I suggest to do is running SLM on autocomplete. That you can do easily and it’s fairly accurately.  Just not writing code for you and give you runnable completed answers.&lt;/p&gt;\\n\\n&lt;p&gt;SLM can do code analysis decently though.&lt;/p&gt;\\n\\n&lt;p&gt;What remote LLM can do is to go through the whole project and write up the design architecture, api swagger fairly easily.  Thats super helpful for documentation tasks and remote LLM do it nicely and quick.  Local LLM might take ages to do it and the result is somewhat 50/50.&lt;/p&gt;\\n\\n&lt;p&gt;What local SLM does for me is to consolidate and give fast general informations like summarize what a repository of code does.  I often use fabric ai with SLM to summarize code for me into a few sentences or bullet points.  Great for presentation PPT.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0kbjhn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751285358,"author_flair_text":null,"collapsed":false,"created_utc":1751285358,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0k7vke","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Subject_Ratio6842","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0jfcs8","score":4,"author_fullname":"t2_y8qz0s2mk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I use cline with a local llm , and they can modify and create documents using the mcp.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0k7vke","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I use cline with a local llm , and they can modify and create documents using the mcp.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0k7vke/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751283792,"author_flair_text":null,"treatment_tags":[],"created_utc":1751283792,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n0jfcs8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"photodesignch","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0i0yrd","score":4,"author_fullname":"t2_dnfi3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"LLM do not directly modify your file system.  Claude does.  When it presents a code, one agent goes into your file I/O to do file open and file save. When editing current file, there is one agent does file diff so it automatically marks diff code and you can apply over current file. Etc etc… it’s a multi-agent task from the cloud services under the hood.\\n\\nNormally LLM just takes input and report back output to you. It can’t interact with your system with separate agents with tools.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0jfcs8","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;LLM do not directly modify your file system.  Claude does.  When it presents a code, one agent goes into your file I/O to do file open and file save. When editing current file, there is one agent does file diff so it automatically marks diff code and you can apply over current file. Etc etc… it’s a multi-agent task from the cloud services under the hood.&lt;/p&gt;\\n\\n&lt;p&gt;Normally LLM just takes input and report back output to you. It can’t interact with your system with separate agents with tools.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jfcs8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751267652,"author_flair_text":null,"treatment_tags":[],"created_utc":1751267652,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n0i0yrd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"pineh2","can_mod_post":false,"created_utc":1751244798,"send_replies":true,"parent_id":"t1_n0hk8tr","score":-6,"author_fullname":"t2_fifter7i","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This is awesome. \\n\\nCan you tell us about Claude being  a a multi agents system behind it?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0i0yrd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is awesome. &lt;/p&gt;\\n\\n&lt;p&gt;Can you tell us about Claude being  a a multi agents system behind it?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0i0yrd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751244798,"author_flair_text":null,"treatment_tags":[],"collapsed":true,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-6}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hk8tr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"photodesignch","can_mod_post":false,"created_utc":1751238642,"send_replies":true,"parent_id":"t3_1lnsax9","score":36,"author_fullname":"t2_dnfi3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Don’t. You won’t get the same performance locally. Invest gpu only if you need to run normal LLM.  Claude is not just LLM itself. It’s multi agents system behind it which isn’t gonna to be replaced locally easily.  There is a point why cloud services are popular.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hk8tr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Don’t. You won’t get the same performance locally. Invest gpu only if you need to run normal LLM.  Claude is not just LLM itself. It’s multi agents system behind it which isn’t gonna to be replaced locally easily.  There is a point why cloud services are popular.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hk8tr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751238642,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":36}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0k9fyz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BumbleSlob","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0j2cbe","score":2,"author_fullname":"t2_1j7fhlcqkp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Qwen3 30B MoE: around 50tps\\n\\nQwen3 32B: probably closer to 17Tps\\n\\nFor me, M2 Max 64Gb I bought new like 2 years ago for ~$4k","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0k9fyz","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen3 30B MoE: around 50tps&lt;/p&gt;\\n\\n&lt;p&gt;Qwen3 32B: probably closer to 17Tps&lt;/p&gt;\\n\\n&lt;p&gt;For me, M2 Max 64Gb I bought new like 2 years ago for ~$4k&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0k9fyz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751284476,"author_flair_text":null,"treatment_tags":[],"created_utc":1751284476,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0j2cbe","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"edeltoaster","can_mod_post":false,"created_utc":1751260426,"send_replies":true,"parent_id":"t1_n0hl8fe","score":3,"author_fullname":"t2_c7rlb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Which M processor do you have and what is the generation speed?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0j2cbe","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Which M processor do you have and what is the generation speed?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0j2cbe/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751260426,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hl8fe","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"mantafloppy","can_mod_post":false,"created_utc":1751238998,"send_replies":true,"parent_id":"t3_1lnsax9","score":18,"author_fullname":"t2_co2hf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I bought a Mac with 64gb ram/VRAM to play with LLM.\\n\\n\\nI can run 70b q4k_m guff model.\\n\\n\\n\\nMy main use is coding.\\n\\n\\nI pay for Claude for when i need serious help coding.\\n\\n\\nThe only \\"local\\" model that comes close to closed source model are the 600b+ model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hl8fe","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I bought a Mac with 64gb ram/VRAM to play with LLM.&lt;/p&gt;\\n\\n&lt;p&gt;I can run 70b q4k_m guff model.&lt;/p&gt;\\n\\n&lt;p&gt;My main use is coding.&lt;/p&gt;\\n\\n&lt;p&gt;I pay for Claude for when i need serious help coding.&lt;/p&gt;\\n\\n&lt;p&gt;The only &amp;quot;local&amp;quot; model that comes close to closed source model are the 600b+ model.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hl8fe/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751238998,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":18}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jzeru","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"stoppableDissolution","can_mod_post":false,"created_utc":1751279703,"send_replies":true,"parent_id":"t1_n0hytpp","score":1,"author_fullname":"t2_1n0su21k4z","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Not quite that high yet, but #metoo, lol. Zero regret tho. My only issue is that I turned my workstation into AI rig, so its inconvenient to keep the model loaded 24/7, gotta move it into separate server.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jzeru","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not quite that high yet, but #metoo, lol. Zero regret tho. My only issue is that I turned my workstation into AI rig, so its inconvenient to keep the model loaded 24/7, gotta move it into separate server.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jzeru/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751279703,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hytpp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"grim-432","can_mod_post":false,"created_utc":1751243998,"send_replies":true,"parent_id":"t3_1lnsax9","score":18,"author_fullname":"t2_l3u9f39ym","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Spent $10,000 on my local llm rig.\\n\\nStill use cloud every day.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hytpp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Spent $10,000 on my local llm rig.&lt;/p&gt;\\n\\n&lt;p&gt;Still use cloud every day.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hytpp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751243998,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":18}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0j19l9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"entsnack","can_mod_post":false,"created_utc":1751259876,"send_replies":true,"parent_id":"t3_1lnsax9","score":5,"author_fullname":"t2_1a48h7vf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Don't buy one for LLMs. Buy one for ComfyUI. No good APIs for the image/video generation stuff yet.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0j19l9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Don&amp;#39;t buy one for LLMs. Buy one for ComfyUI. No good APIs for the image/video generation stuff yet.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0j19l9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751259876,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hzg2v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"CMDR-Bugsbunny","can_mod_post":false,"created_utc":1751244230,"send_replies":true,"parent_id":"t1_n0hu8ri","score":18,"author_fullname":"t2_lj6an","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah, it really depends on the use case. Anyone that says that model x is better than model y without context is clueless.\\n\\nQwen 32b (and others) with Context7 can be run locally and would have very solid coding capabilities. If I was using Claude for coding this would come close. \\n\\nIf you wanted writing capabilities, then there are many choices Mixtral, Gemma 3, Llama and combined with RAG can create a serious domain specific workspace. You can easily use OpenRouter suggested by No-Refrigerator-1672 and use a vector database and n8n for RAG.\\n\\nDon't want to figure out how to use n8n, vector database, etc. Then get AnythingLLM and drop a bunch of PDFs and viola - a smarter local LLM with specific domain knowledge.\\n\\nI dropped 50,000+ pages of PDF on a business topic with Gemma 3 27b QAT and it performs near ChatGPT/Claude/Gemini on this domain. I used AnythingLLM with LM Studio and used over 1GB of PDFs. It took about 10-15 minutes and I was prompting against a vast library.\\n\\nSo it really depends.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hzg2v","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, it really depends on the use case. Anyone that says that model x is better than model y without context is clueless.&lt;/p&gt;\\n\\n&lt;p&gt;Qwen 32b (and others) with Context7 can be run locally and would have very solid coding capabilities. If I was using Claude for coding this would come close. &lt;/p&gt;\\n\\n&lt;p&gt;If you wanted writing capabilities, then there are many choices Mixtral, Gemma 3, Llama and combined with RAG can create a serious domain specific workspace. You can easily use OpenRouter suggested by No-Refrigerator-1672 and use a vector database and n8n for RAG.&lt;/p&gt;\\n\\n&lt;p&gt;Don&amp;#39;t want to figure out how to use n8n, vector database, etc. Then get AnythingLLM and drop a bunch of PDFs and viola - a smarter local LLM with specific domain knowledge.&lt;/p&gt;\\n\\n&lt;p&gt;I dropped 50,000+ pages of PDF on a business topic with Gemma 3 27b QAT and it performs near ChatGPT/Claude/Gemini on this domain. I used AnythingLLM with LM Studio and used over 1GB of PDFs. It took about 10-15 minutes and I was prompting against a vast library.&lt;/p&gt;\\n\\n&lt;p&gt;So it really depends.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hzg2v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751244230,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":18}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0lsh7c","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"TheRealMasonMac","can_mod_post":false,"created_utc":1751302110,"send_replies":true,"parent_id":"t1_n0hu8ri","score":2,"author_fullname":"t2_101haj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"There is also [nano-gpt.com](http://nano-gpt.com) which has access to more models than on OpenRouter.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0lsh7c","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There is also &lt;a href=\\"http://nano-gpt.com\\"&gt;nano-gpt.com&lt;/a&gt; which has access to more models than on OpenRouter.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0lsh7c/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751302110,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hu8ri","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No-Refrigerator-1672","can_mod_post":false,"created_utc":1751242269,"send_replies":true,"parent_id":"t3_1lnsax9","score":21,"author_fullname":"t2_baavelp5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Whoah, so much comments and none gave you the correct answer. Most of the locally-runnable models are awailable on OpenRouter; go ahead, spend 10 bucks on credits, try them out and determine it for yourself. Other commenters keep telling you how non-local models are smarter; and while it is for the flagship ones, it's actually possible that your particular tasks don't require this flagship intelligence, so you really should to your own tests instead of relying on crowd instinct. And should you find out that locals actually can do your tasks, then you can just divide the price of the GPU by your monthly credit spending and use that number to get objective judgement what's actually better for you.","edited":1751242471,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hu8ri","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Whoah, so much comments and none gave you the correct answer. Most of the locally-runnable models are awailable on OpenRouter; go ahead, spend 10 bucks on credits, try them out and determine it for yourself. Other commenters keep telling you how non-local models are smarter; and while it is for the flagship ones, it&amp;#39;s actually possible that your particular tasks don&amp;#39;t require this flagship intelligence, so you really should to your own tests instead of relying on crowd instinct. And should you find out that locals actually can do your tasks, then you can just divide the price of the GPU by your monthly credit spending and use that number to get objective judgement what&amp;#39;s actually better for you.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hu8ri/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751242269,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":21}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hkd9i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No_Reveal_7826","can_mod_post":false,"created_utc":1751238685,"send_replies":true,"parent_id":"t3_1lnsax9","score":5,"author_fullname":"t2_1i5q2gxarw","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Some find local good enough, but nothing can compare to online models. At least when it comes to what can load in 24 GB GPU memory which is what I have.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hkd9i","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Some find local good enough, but nothing can compare to online models. At least when it comes to what can load in 24 GB GPU memory which is what I have.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hkd9i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751238685,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ixauk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Space__Whiskey","can_mod_post":false,"created_utc":1751257941,"send_replies":true,"parent_id":"t3_1lnsax9","score":3,"author_fullname":"t2_1cuwlonegr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"run comfyUI on it, and go on a comfyUI bender. That will pay it off.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ixauk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;run comfyUI on it, and go on a comfyUI bender. That will pay it off.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0ixauk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751257941,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hq1jx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Federal_Order4324","can_mod_post":false,"created_utc":1751240722,"send_replies":true,"parent_id":"t1_n0hkbc2","score":0,"author_fullname":"t2_rlhobztpn","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"No one realistically does.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hq1jx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No one realistically does.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hq1jx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751240722,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hvnpk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Koksny","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0htyqi","score":18,"author_fullname":"t2_olk3n","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"\\\\* For coding only  \\n\\\\*\\\\* Requires 400GB of DDR  \\n\\\\*\\\\*\\\\* At Q4  \\n\\\\*\\\\*\\\\*\\\\* OoM@&gt;8k context.","edited":1751242997,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0hvnpk","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;* For coding only&lt;br/&gt;\\n** Requires 400GB of DDR&lt;br/&gt;\\n*** At Q4&lt;br/&gt;\\n**** OoM@&amp;gt;8k context.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hvnpk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751242799,"author_flair_text":null,"treatment_tags":[],"created_utc":1751242799,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":18}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0i1ab3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullstackSensei","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0hxflk","score":3,"author_fullname":"t2_17n3nqtj56","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The CPUs are cheap. It's the DDR5 RAM and motherboard where you'll spend a lot.\\n\\nAnd if you need more than 8k context, you'll need a second 24GB GPU","edited":false,"author_flair_css_class":null,"name":"t1_n0i1ab3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The CPUs are cheap. It&amp;#39;s the DDR5 RAM and motherboard where you&amp;#39;ll spend a lot.&lt;/p&gt;\\n\\n&lt;p&gt;And if you need more than 8k context, you&amp;#39;ll need a second 24GB GPU&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0i1ab3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751244918,"author_flair_text":null,"collapsed":false,"created_utc":1751244918,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hxflk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Agreeable_Patience47","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0hw4vd","score":-2,"author_fullname":"t2_8levrzbi","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It’s pretty cheap in my country and it's not required, it's just faster","edited":1751246718,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hxflk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It’s pretty cheap in my country and it&amp;#39;s not required, it&amp;#39;s just faster&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hxflk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751243475,"author_flair_text":null,"treatment_tags":[],"created_utc":1751243475,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hw4vd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"netvyper","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0htyqi","score":2,"author_fullname":"t2_v6nk8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"But doesn't that require an AMX compatible CPU, which is more than the 24GB GPU at this point?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0hw4vd","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;But doesn&amp;#39;t that require an AMX compatible CPU, which is more than the 24GB GPU at this point?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hw4vd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751242982,"author_flair_text":null,"treatment_tags":[],"created_utc":1751242982,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0htyqi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"Agreeable_Patience47","can_mod_post":false,"created_utc":1751242166,"send_replies":true,"parent_id":"t1_n0hkbc2","score":-6,"author_fullname":"t2_8levrzbi","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Your information is outdated. R1 actually runs on a single 24G GPU. https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/DeepseekR1_V3_tutorial.md","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0htyqi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Your information is outdated. R1 actually runs on a single 24G GPU. &lt;a href=\\"https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/DeepseekR1_V3_tutorial.md\\"&gt;https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/DeepseekR1_V3_tutorial.md&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0htyqi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751242166,"author_flair_text":null,"treatment_tags":[],"collapsed":true,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-6}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hkbc2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"z_3454_pfk","can_mod_post":false,"created_utc":1751238667,"send_replies":true,"parent_id":"t3_1lnsax9","score":5,"author_fullname":"t2_askwa","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"no local model can really match claude for swe. r1 0528 is decent but requires too much VRAM (unless you have that kind of money)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hkbc2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;no local model can really match claude for swe. r1 0528 is decent but requires too much VRAM (unless you have that kind of money)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hkbc2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751238667,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hpq0w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"loyalekoinu88","can_mod_post":false,"created_utc":1751240605,"send_replies":true,"parent_id":"t3_1lnsax9","score":2,"author_fullname":"t2_1x5p0ubz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Tiny Qwen3 models can do tool calling. Coding doesn’t have the best options right now.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hpq0w","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Tiny Qwen3 models can do tool calling. Coding doesn’t have the best options right now.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hpq0w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751240605,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0p3016","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"nini2352","can_mod_post":false,"created_utc":1751339480,"send_replies":true,"parent_id":"t1_n0j4xvq","score":1,"author_fullname":"t2_7s203ubo","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Lol… this is so funny to imagine someone with 0 knowledge try and run Qwen-32B distilled fine-tuned coding-specific LLMs on the 5090 they purchase, overhead sounds insane for a noob 😂","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0p3016","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Lol… this is so funny to imagine someone with 0 knowledge try and run Qwen-32B distilled fine-tuned coding-specific LLMs on the 5090 they purchase, overhead sounds insane for a noob 😂&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0p3016/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751339480,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0j4xvq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"redditisunproductive","can_mod_post":false,"created_utc":1751261790,"send_replies":true,"parent_id":"t3_1lnsax9","score":2,"author_fullname":"t2_19353jsswd","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Finetuned local models can outperform SOTA models on very narrow tasks.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0j4xvq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Finetuned local models can outperform SOTA models on very narrow tasks.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0j4xvq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751261790,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0j6u3p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Noiselexer","can_mod_post":false,"created_utc":1751262816,"send_replies":true,"parent_id":"t3_1lnsax9","score":2,"author_fullname":"t2_n4xpz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I try models, never use them and use cloud for all my coding stuff.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0j6u3p","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I try models, never use them and use cloud for all my coding stuff.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0j6u3p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751262816,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jfi1n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"madaradess007","can_mod_post":false,"created_utc":1751267738,"send_replies":true,"parent_id":"t3_1lnsax9","score":2,"author_fullname":"t2_79slapln","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"no, local cant compare to cloud, but you can build workflows locally that will scale if you make a switch to external APIs","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jfi1n","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;no, local cant compare to cloud, but you can build workflows locally that will scale if you make a switch to external APIs&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jfi1n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751267738,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jqw9l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Fox-Lopsided","can_mod_post":false,"created_utc":1751274752,"send_replies":true,"parent_id":"t3_1lnsax9","score":2,"author_fullname":"t2_7ivwbs3t","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Nothing comes Close to Gemini and Claude especially because of the context window :(\\nBut who knows what the future will Bring","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jqw9l","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nothing comes Close to Gemini and Claude especially because of the context window :(\\nBut who knows what the future will Bring&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jqw9l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751274752,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0js5l8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"vinylger","can_mod_post":false,"created_utc":1751275529,"send_replies":true,"parent_id":"t3_1lnsax9","score":2,"author_fullname":"t2_f1nbx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Huh, is this me writing on another account w/o being aware of it?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0js5l8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Huh, is this me writing on another account w/o being aware of it?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0js5l8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751275529,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0k7vsc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"thallazar","can_mod_post":false,"created_utc":1751283794,"send_replies":true,"parent_id":"t3_1lnsax9","score":2,"author_fullname":"t2_4m3wz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Have never seen a local model that can perform as good as the remote, and generally at a fraction of the price. APIs are dirt cheap. Local has some utility, especially in privacy matters but for sheer performance, nothing compares.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0k7vsc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Have never seen a local model that can perform as good as the remote, and generally at a fraction of the price. APIs are dirt cheap. Local has some utility, especially in privacy matters but for sheer performance, nothing compares.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0k7vsc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751283794,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0k08x4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FunnyAsparagus1253","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0jqxqw","score":1,"author_fullname":"t2_i6c8tay3w","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Omg you’re straight back to ‘spending thousands to goon’ and ‘addiction’ 😅\\n\\nIt’s not a linear spectrum. It’s a big multidimensional reality not a two-ended stick. There are many different individuals who do different things for different reasons with different levels of spending and different levels of nfsw-ness, different levels of ‘taking it seriously’ different amounts of tech knowledge, different resources available, different life situations, different personal histories, different aims, etc etc etc etc etc.\\n\\nThis hasn’t been the worst conversation I’ve had on reddit, not even close. I don’t disagree with you that the cloud models are cheaper and better overall. Have a nice day, MengerianMango. I don’t want to argue about this anymore- I have tidying to do and bunnies to look after. Take care :) 🫡🖖","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0k08x4","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Omg you’re straight back to ‘spending thousands to goon’ and ‘addiction’ 😅&lt;/p&gt;\\n\\n&lt;p&gt;It’s not a linear spectrum. It’s a big multidimensional reality not a two-ended stick. There are many different individuals who do different things for different reasons with different levels of spending and different levels of nfsw-ness, different levels of ‘taking it seriously’ different amounts of tech knowledge, different resources available, different life situations, different personal histories, different aims, etc etc etc etc etc.&lt;/p&gt;\\n\\n&lt;p&gt;This hasn’t been the worst conversation I’ve had on reddit, not even close. I don’t disagree with you that the cloud models are cheaper and better overall. Have a nice day, MengerianMango. I don’t want to argue about this anymore- I have tidying to do and bunnies to look after. Take care :) 🫡🖖&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0k08x4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751280145,"author_flair_text":null,"treatment_tags":[],"created_utc":1751280145,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0mur1g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MengerianMango","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0mtxqh","score":1,"author_fullname":"t2_mcvyi","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Do you have trouble with reading comprehension? I answered OP's question. It's the peanut gallery that took issue with my antigooner commentary.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n0mur1g","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Do you have trouble with reading comprehension? I answered OP&amp;#39;s question. It&amp;#39;s the peanut gallery that took issue with my antigooner commentary.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0mur1g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751312977,"author_flair_text":null,"treatment_tags":[],"created_utc":1751312977,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0mtxqh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Thatisverytrue54321","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0jqxqw","score":1,"author_fullname":"t2_behl6etu","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Tl;dr - I don’t care about your actual question - I’m just here to say I’m better than you","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0mtxqh","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Tl;dr - I don’t care about your actual question - I’m just here to say I’m better than you&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0mtxqh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751312736,"author_flair_text":null,"treatment_tags":[],"created_utc":1751312736,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0jqxqw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MengerianMango","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0jjbch","score":-1,"author_fullname":"t2_mcvyi","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Using 3090 sized models for work feels like trying to prove the infinite monkey theorem empirically. It sounds hyperbolic but it's not. Even the best closed models are still pretty annoying rn. It comes down to success rate. Low success rates are just really annoying. You end up feeling you'd have been better off just writing the code yourself. I'm assuming OP already knows how to code.\\n\\nI'm perplexed how it's a point of debate that sexual obsessions are bad for you. There's a lot of room on the spectrum between techbro and gooner. Objectively, we are overdressed monkeys. Tech today is like cocaine in the 1920s. It's designed to hijack our instincts and create addiction. Spending thousands to goon on custom erotica is some late stage capitalism level addictive consumption","edited":false,"author_flair_css_class":null,"name":"t1_n0jqxqw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Using 3090 sized models for work feels like trying to prove the infinite monkey theorem empirically. It sounds hyperbolic but it&amp;#39;s not. Even the best closed models are still pretty annoying rn. It comes down to success rate. Low success rates are just really annoying. You end up feeling you&amp;#39;d have been better off just writing the code yourself. I&amp;#39;m assuming OP already knows how to code.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m perplexed how it&amp;#39;s a point of debate that sexual obsessions are bad for you. There&amp;#39;s a lot of room on the spectrum between techbro and gooner. Objectively, we are overdressed monkeys. Tech today is like cocaine in the 1920s. It&amp;#39;s designed to hijack our instincts and create addiction. Spending thousands to goon on custom erotica is some late stage capitalism level addictive consumption&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jqxqw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751274777,"author_flair_text":null,"collapsed":false,"created_utc":1751274777,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0jjbch","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FunnyAsparagus1253","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0jdfhv","score":4,"author_fullname":"t2_i6c8tay3w","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You’re using the word ‘objectively’ a lot while sharing a lot of your own opinions.  Really unpleasant judgemental opinions IMO. Calling it ‘illegal’, using phrases like ‘your mom’s basement’, ‘any sane person’ - and using ‘…..’ in place of the word ‘erotic’. A *lot* of people use LLMs for things other than ‘putting money in the bank’. It’s not *objectively wrong* no matter what you think.\\n\\nOP has said they want assistance with coding and tool-calling. It wouldn’t be the worst thing in the world to get a used 3090 and keep a coding model on hand for times when their internet is down, or for when Anthropic is experiencing an outage. Not everything has to be the ultra best 1 trillion parameters, 10xing your productivity and getting you ahead in the techbro capitalist rat race. This is r/LocalLLaMa, dude. We have all sorts here. Don’t be such a hater. Peace 🫶✌️","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jjbch","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You’re using the word ‘objectively’ a lot while sharing a lot of your own opinions.  Really unpleasant judgemental opinions IMO. Calling it ‘illegal’, using phrases like ‘your mom’s basement’, ‘any sane person’ - and using ‘…..’ in place of the word ‘erotic’. A &lt;em&gt;lot&lt;/em&gt; of people use LLMs for things other than ‘putting money in the bank’. It’s not &lt;em&gt;objectively wrong&lt;/em&gt; no matter what you think.&lt;/p&gt;\\n\\n&lt;p&gt;OP has said they want assistance with coding and tool-calling. It wouldn’t be the worst thing in the world to get a used 3090 and keep a coding model on hand for times when their internet is down, or for when Anthropic is experiencing an outage. Not everything has to be the ultra best 1 trillion parameters, 10xing your productivity and getting you ahead in the techbro capitalist rat race. This is &lt;a href=\\"/r/LocalLLaMa\\"&gt;r/LocalLLaMa&lt;/a&gt;, dude. We have all sorts here. Don’t be such a hater. Peace 🫶✌️&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jjbch/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751270069,"author_flair_text":null,"treatment_tags":[],"created_utc":1751270069,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n0jdfhv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"MengerianMango","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0jchum","score":-5,"author_fullname":"t2_mcvyi","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Bro i like fancy toys too but OP is asking for objectivity. Literal fact is that you're not getting your money's worth out of your hardware. Unless you live in your mom's basement, assuming you have a job and arent running the tits off your gpus 34/7, you could rent better for the same amount of actual usage/time for way less than you paid. I could too. I mean i do get it. Fancy toys are nice and nothing gets me harder than caressing my 6000, but yk, ion need it and it's not putting money in my bank account. It's not paying my rent. Except it kinda might. I got the thing bc pnl gets me a profit bonus. Excessive success in the project I'm working on could easily net me a 10x return on my money.\\n\\nObjectively, any sane person will tell you that you shouldn't spend 5k+ on an item that doesn't generate some kinda return for you. Unless you're making 400k, in which case why even ask?\\n\\nBut yeah the amount of people judging models by their...... fiction writing abilities. It might not be illegal but its certainly not a use of time or money that I respect very much","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0jdfhv","is_submitter":false,"collapsed":true,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Bro i like fancy toys too but OP is asking for objectivity. Literal fact is that you&amp;#39;re not getting your money&amp;#39;s worth out of your hardware. Unless you live in your mom&amp;#39;s basement, assuming you have a job and arent running the tits off your gpus 34/7, you could rent better for the same amount of actual usage/time for way less than you paid. I could too. I mean i do get it. Fancy toys are nice and nothing gets me harder than caressing my 6000, but yk, ion need it and it&amp;#39;s not putting money in my bank account. It&amp;#39;s not paying my rent. Except it kinda might. I got the thing bc pnl gets me a profit bonus. Excessive success in the project I&amp;#39;m working on could easily net me a 10x return on my money.&lt;/p&gt;\\n\\n&lt;p&gt;Objectively, any sane person will tell you that you shouldn&amp;#39;t spend 5k+ on an item that doesn&amp;#39;t generate some kinda return for you. Unless you&amp;#39;re making 400k, in which case why even ask?&lt;/p&gt;\\n\\n&lt;p&gt;But yeah the amount of people judging models by their...... fiction writing abilities. It might not be illegal but its certainly not a use of time or money that I respect very much&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jdfhv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751266524,"author_flair_text":null,"treatment_tags":[],"created_utc":1751266524,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-5}}],"before":null}},"user_reports":[],"saved":false,"id":"n0jchum","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FunnyAsparagus1253","can_mod_post":false,"created_utc":1751265992,"send_replies":true,"parent_id":"t1_n0hll17","score":5,"author_fullname":"t2_i6c8tay3w","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Saying that the only real use case for having a home setup is *illegal porn* is a really weird response given that we’re in r/LocalLLaMa . Is that *seriously* what you think? Maybe you’re in the wrong place 🤨","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jchum","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Saying that the only real use case for having a home setup is &lt;em&gt;illegal porn&lt;/em&gt; is a really weird response given that we’re in &lt;a href=\\"/r/LocalLLaMa\\"&gt;r/LocalLLaMa&lt;/a&gt; . Is that &lt;em&gt;seriously&lt;/em&gt; what you think? Maybe you’re in the wrong place 🤨&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jchum/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751265992,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0iey5z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MerlinTrashMan","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0i64gu","score":1,"author_fullname":"t2_7ghjkhuo","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It should be a lot of fun to play with the data.  I found the first step for me was to study the correlated items and build better signals from them and then use uncorrelated values to either scale the new signals or invalidate them.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0iey5z","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It should be a lot of fun to play with the data.  I found the first step for me was to study the correlated items and build better signals from them and then use uncorrelated values to either scale the new signals or invalidate them.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0iey5z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751250097,"author_flair_text":null,"treatment_tags":[],"created_utc":1751250097,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0i64gu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MengerianMango","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0i0r4o","score":0,"author_fullname":"t2_mcvyi","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Meta modeling. We already have awesome models. But they're generally simple and derived from first principles. To greatly oversimplify, we basically have built a pretty successful fund out of simply adding 500ish simple models. I want to use NNs to distill a better signal from the model soup than we're currently doing by just summing.\\n\\nIn theory, I should be using xgboost for this. But we'll see. Anything uncorrelated is valuable.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0i64gu","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Meta modeling. We already have awesome models. But they&amp;#39;re generally simple and derived from first principles. To greatly oversimplify, we basically have built a pretty successful fund out of simply adding 500ish simple models. I want to use NNs to distill a better signal from the model soup than we&amp;#39;re currently doing by just summing.&lt;/p&gt;\\n\\n&lt;p&gt;In theory, I should be using xgboost for this. But we&amp;#39;ll see. Anything uncorrelated is valuable.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0i64gu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751246728,"author_flair_text":null,"treatment_tags":[],"created_utc":1751246728,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n0i0r4o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MerlinTrashMan","can_mod_post":false,"created_utc":1751244721,"send_replies":true,"parent_id":"t1_n0hll17","score":1,"author_fullname":"t2_7ghjkhuo","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Id love to chat on your GPU use for quant finance.  I have been writing kernels for option portfolio optimization that run on a 3090 but I am running out of vram.  I am curious what you plan to do.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0i0r4o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Id love to chat on your GPU use for quant finance.  I have been writing kernels for option portfolio optimization that run on a 3090 but I am running out of vram.  I am curious what you plan to do.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0i0r4o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751244721,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0qogpu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0pg60f","score":1,"author_fullname":"t2_h5utwre7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"TabbyAPI has tool calling support, simply have to add the tools. Web search and image gen are part of sillytavern whether I use text or chat completion. Even for the latter, template can be edited before you run the model. Don't have to put up with secret prompt injections.\\n\\nOffload to dual xeons with close to 200gb/s combined using ddr4. While that may not work for you, the model fits in 96g using exllama. \\n\\nMy whole system, all the mis-steps and my desktop card combined, still clocks in under the price of that blackwell funny enough.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0qogpu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;TabbyAPI has tool calling support, simply have to add the tools. Web search and image gen are part of sillytavern whether I use text or chat completion. Even for the latter, template can be edited before you run the model. Don&amp;#39;t have to put up with secret prompt injections.&lt;/p&gt;\\n\\n&lt;p&gt;Offload to dual xeons with close to 200gb/s combined using ddr4. While that may not work for you, the model fits in 96g using exllama. &lt;/p&gt;\\n\\n&lt;p&gt;My whole system, all the mis-steps and my desktop card combined, still clocks in under the price of that blackwell funny enough.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0qogpu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751369399,"author_flair_text":null,"treatment_tags":[],"created_utc":1751369399,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0pg60f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MengerianMango","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ki8qc","score":2,"author_fullname":"t2_mcvyi","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That's gotta be painful for tool calling tho, no? What are you offloading to? My desktop is just a ryzen/dual channel so offloading even just a bit is about as fun as watching paint dry","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0pg60f","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s gotta be painful for tool calling tho, no? What are you offloading to? My desktop is just a ryzen/dual channel so offloading even just a bit is about as fun as watching paint dry&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0pg60f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751345045,"author_flair_text":null,"treatment_tags":[],"created_utc":1751345045,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ki8qc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1751287967,"send_replies":true,"parent_id":"t1_n0hll17","score":1,"author_fullname":"t2_h5utwre7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"EXL3 fits it at 96gb at about ~3bits. Quality is similar to the API and IQ4_XS which I partially offload through ik_llama.\\n\\nSetting expandable segments to true and Q6 cache fits a bit of context. Besides \\"illegal\\" porn, you do get much more control over the chat template and sampling by doing it locally. It does have worth and no rugpulls. \\n\\nNot spending money you can't miss goes without saying, same as taking a tropical vacation on credit in place of your rent/mortgage. Lots of people have hobbies that cost as much or more and don't generate revenue. No sense in harshing them for it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ki8qc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;EXL3 fits it at 96gb at about ~3bits. Quality is similar to the API and IQ4_XS which I partially offload through ik_llama.&lt;/p&gt;\\n\\n&lt;p&gt;Setting expandable segments to true and Q6 cache fits a bit of context. Besides &amp;quot;illegal&amp;quot; porn, you do get much more control over the chat template and sampling by doing it locally. It does have worth and no rugpulls. &lt;/p&gt;\\n\\n&lt;p&gt;Not spending money you can&amp;#39;t miss goes without saying, same as taking a tropical vacation on credit in place of your rent/mortgage. Lots of people have hobbies that cost as much or more and don&amp;#39;t generate revenue. No sense in harshing them for it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0ki8qc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751287967,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ibq0a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MengerianMango","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0i773r","score":2,"author_fullname":"t2_mcvyi","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"PP=prompt processing=prefill.\\n\\nThanks for all the info. I'm super jelly ngl. I'm a straight man but id give my booty for a dual epyc system. Unfortunately this is a side project so I dont have company funding for it. The 6000 could directly yield ROI for me in terms of performance bonus but I dont think id feel the same positive impact in my wallet from a dual epyc, as much as id love have one.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n0ibq0a","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;PP=prompt processing=prefill.&lt;/p&gt;\\n\\n&lt;p&gt;Thanks for all the info. I&amp;#39;m super jelly ngl. I&amp;#39;m a straight man but id give my booty for a dual epyc system. Unfortunately this is a side project so I dont have company funding for it. The 6000 could directly yield ROI for me in terms of performance bonus but I dont think id feel the same positive impact in my wallet from a dual epyc, as much as id love have one.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0ibq0a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751248852,"author_flair_text":null,"treatment_tags":[],"created_utc":1751248852,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0i773r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"createthiscom","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0i5ngq","score":1,"author_fullname":"t2_ozxxf","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I use Open Hands AI. I’m not sure what you mean by PP. I get 22 tok/s starting out. prefill is between 18 and 45 tok/s depending on the input. By 50k context I’m usually down to 18 tok/s. I’m quite happy with that performance for V3. It’s a little slow for R1 with the extra thinking, but I use R1 sometimes anyway for more complex tasks and sometimes it’s worthwhile. I used to prefer ktransformers because their ktransformer backend had zero delay for long context. The prefill cache was perfect because there was only one session. However, I get a lot of crashes with ktransformers and I have a lot of available VRAM with the 6000 pro, so I use llama.cpp now because it’s so reliable. Check the video I linked for examples.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0i773r","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I use Open Hands AI. I’m not sure what you mean by PP. I get 22 tok/s starting out. prefill is between 18 and 45 tok/s depending on the input. By 50k context I’m usually down to 18 tok/s. I’m quite happy with that performance for V3. It’s a little slow for R1 with the extra thinking, but I use R1 sometimes anyway for more complex tasks and sometimes it’s worthwhile. I used to prefer ktransformers because their ktransformer backend had zero delay for long context. The prefill cache was perfect because there was only one session. However, I get a lot of crashes with ktransformers and I have a lot of available VRAM with the 6000 pro, so I use llama.cpp now because it’s so reliable. Check the video I linked for examples.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0i773r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751247137,"author_flair_text":null,"treatment_tags":[],"created_utc":1751247137,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0i5ngq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MengerianMango","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0i06d4","score":1,"author_fullname":"t2_mcvyi","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What tps do you get tho, especially for PP? Personally, I'm pretty sensitive to delay for \\"vibe\\"/agentic coding. It would need to be something close to as fast as the APIs to feel useful. Do you use it for agentic coding (eg cline/aider) or just chat?","edited":false,"author_flair_css_class":null,"name":"t1_n0i5ngq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What tps do you get tho, especially for PP? Personally, I&amp;#39;m pretty sensitive to delay for &amp;quot;vibe&amp;quot;/agentic coding. It would need to be something close to as fast as the APIs to feel useful. Do you use it for agentic coding (eg cline/aider) or just chat?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0i5ngq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751246553,"author_flair_text":null,"collapsed":false,"created_utc":1751246553,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0iot40","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"createthiscom","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ih8sa","score":1,"author_fullname":"t2_ozxxf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yeah I haven’t noticed R1’s code to be better than V3’s. It tends to not follow instructions quite as faithfully too. I keep using it now and then trying to find a niche I like it in though.\\n\\nEDIT: I did find something R1 solved that V3 had no luck solving today: https://github.com/createthis/diffcalculia-ts/pull/4\\n\\nIt seems R1’s ability to “comprehend” a complex problem is slightly better than V3’s.","edited":1751303721,"gildings":{},"author_flair_css_class":null,"name":"t1_n0iot40","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah I haven’t noticed R1’s code to be better than V3’s. It tends to not follow instructions quite as faithfully too. I keep using it now and then trying to find a niche I like it in though.&lt;/p&gt;\\n\\n&lt;p&gt;EDIT: I did find something R1 solved that V3 had no luck solving today: &lt;a href=\\"https://github.com/createthis/diffcalculia-ts/pull/4\\"&gt;https://github.com/createthis/diffcalculia-ts/pull/4&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;It seems R1’s ability to “comprehend” a complex problem is slightly better than V3’s.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0iot40/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751254108,"author_flair_text":null,"treatment_tags":[],"created_utc":1751254108,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ih8sa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MengerianMango","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0igv0u","score":3,"author_fullname":"t2_mcvyi","approved_by":null,"mod_note":null,"all_awardings":[],"body":"r1 (and thinking models in general) tend to be worse at just doing stuff. Like v3 will outperform at editing code given the same quality instructions. People often use r1 as a master model to command v3 for ex. r1 will do the high level architectural design and hand off to v3 to do the grunt work.\\n\\nI think for ppl who already know how to code, v3 is more useful. I generally can do the architectural stuff myself as well or better than r1. I just want a \\"dumb\\" grunt to do the work 10x faster than I could","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0ih8sa","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;r1 (and thinking models in general) tend to be worse at just doing stuff. Like v3 will outperform at editing code given the same quality instructions. People often use r1 as a master model to command v3 for ex. r1 will do the high level architectural design and hand off to v3 to do the grunt work.&lt;/p&gt;\\n\\n&lt;p&gt;I think for ppl who already know how to code, v3 is more useful. I generally can do the architectural stuff myself as well or better than r1. I just want a &amp;quot;dumb&amp;quot; grunt to do the work 10x faster than I could&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0ih8sa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751251004,"author_flair_text":null,"treatment_tags":[],"created_utc":1751251004,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0igv0u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Few-Yam9901","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0i06d4","score":1,"author_fullname":"t2_1rhlf3bcfk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Isn’t r1 0528 better? I run it with mixed Nvidia gpus 2bit, 3-400 pp ts and 30-40 gen ts. Its Good","edited":false,"author_flair_css_class":null,"name":"t1_n0igv0u","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Isn’t r1 0528 better? I run it with mixed Nvidia gpus 2bit, 3-400 pp ts and 30-40 gen ts. Its Good&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0igv0u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751250850,"author_flair_text":null,"collapsed":false,"created_utc":1751250850,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0i06d4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"createthiscom","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0hzefi","score":4,"author_fullname":"t2_ozxxf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I don’t know if threadripper’s memory bandwidth is sufficient, but yes. I have a dual CPU EPYC system that was roughly 14k without the GPU. However, I can literally run V3 all day for only about $30/mo in electricity.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0i06d4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don’t know if threadripper’s memory bandwidth is sufficient, but yes. I have a dual CPU EPYC system that was roughly 14k without the GPU. However, I can literally run V3 all day for only about $30/mo in electricity.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0i06d4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751244505,"author_flair_text":null,"treatment_tags":[],"created_utc":1751244505,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hzefi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MengerianMango","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0hz6y4","score":4,"author_fullname":"t2_mcvyi","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"So roughly another 10k for a ddr5 threadripper or epyc build. You're still talking 9k gpu + ~10k host.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0hzefi","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So roughly another 10k for a ddr5 threadripper or epyc build. You&amp;#39;re still talking 9k gpu + ~10k host.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hzefi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751244213,"author_flair_text":null,"treatment_tags":[],"created_utc":1751244213,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0loyto","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"sc166","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0kjrne","score":1,"author_fullname":"t2_twa0i","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks, will watch it!","edited":false,"author_flair_css_class":null,"name":"t1_n0loyto","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks, will watch it!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0loyto/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751301097,"author_flair_text":null,"collapsed":false,"created_utc":1751301097,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0kjrne","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"createthiscom","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0jd4lw","score":1,"author_fullname":"t2_ozxxf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I show how to run it in detail in the video linked. The description links to two other videos. One has the full machine build.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0kjrne","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I show how to run it in detail in the video linked. The description links to two other videos. One has the full machine build.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0kjrne/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751288522,"author_flair_text":null,"treatment_tags":[],"created_utc":1751288522,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0jd4lw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"sc166","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0hz6y4","score":1,"author_fullname":"t2_twa0i","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Do you mind sharing instructions? I have 7985x with 768gb (8 channel 6000mt) and single 6000 pro (waiting for 2nd one). I don’t think my system has as high memory bandwidth but would like to test how close I can in terms of tok/s. Thanks","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0jd4lw","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Do you mind sharing instructions? I have 7985x with 768gb (8 channel 6000mt) and single 6000 pro (waiting for 2nd one). I don’t think my system has as high memory bandwidth but would like to test how close I can in terms of tok/s. Thanks&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jd4lw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751266352,"author_flair_text":null,"treatment_tags":[],"created_utc":1751266352,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hz6y4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"createthiscom","can_mod_post":false,"created_utc":1751244135,"send_replies":true,"parent_id":"t1_n0hll17","score":1,"author_fullname":"t2_ozxxf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You can run V3 with considerably less than a single blackwell 6000 pro, but with it I get about 22 tok/s. The host needs about 400gb ram and 720gb/s of bandwidth to achieve those speeds. Video evidence: https://youtu.be/vfi9LRJxgHs?si=FJ7NLiaDos1wOhIW","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hz6y4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can run V3 with considerably less than a single blackwell 6000 pro, but with it I get about 22 tok/s. The host needs about 400gb ram and 720gb/s of bandwidth to achieve those speeds. Video evidence: &lt;a href=\\"https://youtu.be/vfi9LRJxgHs?si=FJ7NLiaDos1wOhIW\\"&gt;https://youtu.be/vfi9LRJxgHs?si=FJ7NLiaDos1wOhIW&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hz6y4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751244135,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hll17","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MengerianMango","can_mod_post":false,"created_utc":1751239122,"send_replies":true,"parent_id":"t3_1lnsax9","score":3,"author_fullname":"t2_mcvyi","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I bought a 6000 Blackwell and I still can't even run Qwen3 235b in 4 bit. I can run in 2 bit with 64k context, even 128k context is too much.\\n\\nAnd really you want to run Deepseek v3 for real work. For that you need roughly 3 6000 Blackwells, so roughly $27k. And you'd still be disappointed vs Claude.\\n\\nI knew this going in tho. I work in quant finance and intend to use the GPU for non-llm work. I wouldnt have bought it just for llms.\\n\\nGPUs dont really make sense unless you're generating illegal porn and really dont want any trace of your crimes on other people's servers or maybe if you're discussing incredibly valuable IP. I talk with qwen3 about the internal secrets from my firm, play with new ideas, etc. Obv I dont trust OpenAI to talk with gpt about such things. I could leak million dollar secrets if they lie about not training on my chats. Other than a few very limited circumstances, just use openrouter.","edited":1751239307,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hll17","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I bought a 6000 Blackwell and I still can&amp;#39;t even run Qwen3 235b in 4 bit. I can run in 2 bit with 64k context, even 128k context is too much.&lt;/p&gt;\\n\\n&lt;p&gt;And really you want to run Deepseek v3 for real work. For that you need roughly 3 6000 Blackwells, so roughly $27k. And you&amp;#39;d still be disappointed vs Claude.&lt;/p&gt;\\n\\n&lt;p&gt;I knew this going in tho. I work in quant finance and intend to use the GPU for non-llm work. I wouldnt have bought it just for llms.&lt;/p&gt;\\n\\n&lt;p&gt;GPUs dont really make sense unless you&amp;#39;re generating illegal porn and really dont want any trace of your crimes on other people&amp;#39;s servers or maybe if you&amp;#39;re discussing incredibly valuable IP. I talk with qwen3 about the internal secrets from my firm, play with new ideas, etc. Obv I dont trust OpenAI to talk with gpt about such things. I could leak million dollar secrets if they lie about not training on my chats. Other than a few very limited circumstances, just use openrouter.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hll17/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751239122,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hthcl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"KDCreerStudios","can_mod_post":false,"created_utc":1751241986,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_1qfbu2cvzc","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"With tool callings and fine-tuned specialization it can beat out Claude in certain tasks. But a general purpose one, no, not really.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hthcl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;With tool callings and fine-tuned specialization it can beat out Claude in certain tasks. But a general purpose one, no, not really.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hthcl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751241986,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hxrlv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Syzeon","can_mod_post":false,"created_utc":1751243603,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_82nlaimd","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"rent a runpod or something, try out a few local you have in mind, evaluate yourself if local model is what you're after. Make your decision after you tried it out","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hxrlv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;rent a runpod or something, try out a few local you have in mind, evaluate yourself if local model is what you&amp;#39;re after. Make your decision after you tried it out&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hxrlv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751243603,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hy8zc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"evilbarron2","can_mod_post":false,"created_utc":1751243784,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_gr2fr79s1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"No, they can’t, but they can still do a lot. They just have far less “presence” and adaptability. I’ve been using frontier for dev and local for production - more work but I can sleep at night not worrying about a huge bill","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hy8zc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No, they can’t, but they can still do a lot. They just have far less “presence” and adaptability. I’ve been using frontier for dev and local for production - more work but I can sleep at night not worrying about a huge bill&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hy8zc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751243784,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0i30ep","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"makinggrace","can_mod_post":false,"created_utc":1751245563,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_z9vf1vr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Try before you buy. \\n\\nLocal LLM isn't (yet) IMHO a great tool for execution unless the use case is extremely specific and cannot be performed in cloud for reasons. It is too slow and too expensive to do tasks locally that can be done in cloud.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0i30ep","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Try before you buy. &lt;/p&gt;\\n\\n&lt;p&gt;Local LLM isn&amp;#39;t (yet) IMHO a great tool for execution unless the use case is extremely specific and cannot be performed in cloud for reasons. It is too slow and too expensive to do tasks locally that can be done in cloud.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0i30ep/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751245563,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0i4pyh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"robberviet","can_mod_post":false,"created_utc":1751246206,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_jxc5a","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"No, just use services. You don't get that performance selfhost.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0i4pyh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No, just use services. You don&amp;#39;t get that performance selfhost.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0i4pyh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751246206,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jgl1e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"godndiogoat","can_mod_post":false,"created_utc":1751268387,"send_replies":true,"parent_id":"t1_n0i7zlk","score":1,"author_fullname":"t2_1o8b7or53v","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Time a full day of your prompts on a rented GPU before buying one yourself. Spin up a Llama-3-8B or Mistral-7B on RunPod for a few cents, or just Ollama on CPU overnight, and log latency + token cost. If the math says you’d save more than the GPU price in six months, grab the card; if not, stay cloud-side. In my case, the break-even for a 4090 was nine months, but the local model still missed about 20% of Claude’s coding fixes, so I stuck with the API. I keep a cheap RTX 3060 only for fast unit-test stubs. I also tried Spellbook and Replicate for comparison, yet ended up tracking them through APIWrapper.ai because it lets me swap local models against cloud calls without rewriting my stack. Same idea might save you cash long-term.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jgl1e","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Time a full day of your prompts on a rented GPU before buying one yourself. Spin up a Llama-3-8B or Mistral-7B on RunPod for a few cents, or just Ollama on CPU overnight, and log latency + token cost. If the math says you’d save more than the GPU price in six months, grab the card; if not, stay cloud-side. In my case, the break-even for a 4090 was nine months, but the local model still missed about 20% of Claude’s coding fixes, so I stuck with the API. I keep a cheap RTX 3060 only for fast unit-test stubs. I also tried Spellbook and Replicate for comparison, yet ended up tracking them through APIWrapper.ai because it lets me swap local models against cloud calls without rewriting my stack. Same idea might save you cash long-term.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jgl1e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751268387,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0i7zlk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Top-Winter938","can_mod_post":false,"created_utc":1751247436,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_c1i53eiz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Try out some of the smaller models you could run locally via OpenRouter first. If they’re good enough for what you need, figure out your break-even point: is it cheaper to just keep using the API, or does it make sense to buy a GPU? Will you still be using the model after you hit that break-even? That usually gives you your answer. \\n\\nAs you’re using it for coding and used to claude, you’ll probably be frustrated by these smaller models. They are good for smaller, specific tasks","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0i7zlk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Try out some of the smaller models you could run locally via OpenRouter first. If they’re good enough for what you need, figure out your break-even point: is it cheaper to just keep using the API, or does it make sense to buy a GPU? Will you still be using the model after you hit that break-even? That usually gives you your answer. &lt;/p&gt;\\n\\n&lt;p&gt;As you’re using it for coding and used to claude, you’ll probably be frustrated by these smaller models. They are good for smaller, specific tasks&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0i7zlk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751247436,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0i90lt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kironlau","can_mod_post":false,"created_utc":1751247828,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_tb0dz2ds","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Go to openrouter, get a free trial. In field, will get you a answer.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0i90lt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Go to openrouter, get a free trial. In field, will get you a answer.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0i90lt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751247828,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0idned","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Little-Parfait-423","can_mod_post":false,"created_utc":1751249589,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_1i51wbv406","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I have a rtx 3060 and a rtx 3090. The VRAM constraint of 12GB vs 24GB between them allows me to run larger models (barely) but at significant loss in context and speed. I use it to debug code, quick scripts, look at logs, review output from tests, and prepare prompts for larger models over API. In all honestly you get decent results with 8b models but no tool use. If you prompt well and add context you get tool use (ie edit for you) at 14b models you can sometimes get tool use for small files. 24-30b models are smarter but even with 24gb VRAM there is no context window to load/edit files anyway. My advice to anyone is get a used rtx3060 12gb vram ~200-300 USD (or rtx 4060 16gb if you can find one) and run 8b/14b models to prep your work for cloud based models. Don’t mind the gap between local LLM with beefy/expensive cards it’s not worth being just as frustrated and broke on top of it :2cents:","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0idned","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have a rtx 3060 and a rtx 3090. The VRAM constraint of 12GB vs 24GB between them allows me to run larger models (barely) but at significant loss in context and speed. I use it to debug code, quick scripts, look at logs, review output from tests, and prepare prompts for larger models over API. In all honestly you get decent results with 8b models but no tool use. If you prompt well and add context you get tool use (ie edit for you) at 14b models you can sometimes get tool use for small files. 24-30b models are smarter but even with 24gb VRAM there is no context window to load/edit files anyway. My advice to anyone is get a used rtx3060 12gb vram ~200-300 USD (or rtx 4060 16gb if you can find one) and run 8b/14b models to prep your work for cloud based models. Don’t mind the gap between local LLM with beefy/expensive cards it’s not worth being just as frustrated and broke on top of it :2cents:&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0idned/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751249589,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ii0i2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Violaze27","can_mod_post":false,"created_utc":1751251311,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_fe4u4syr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For games absolutely","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ii0i2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For games absolutely&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0ii0i2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751251311,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ikgln","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ardalok","can_mod_post":false,"created_utc":1751252293,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_vgnewja","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The only locally deployable model that can compete with Claude, GPT, and similar large language models is DeepSeek, and it cannot even be hosted on a single GPU.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ikgln","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The only locally deployable model that can compete with Claude, GPT, and similar large language models is DeepSeek, and it cannot even be hosted on a single GPU.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0ikgln/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751252293,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0j2lrt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cguy1234","can_mod_post":false,"created_utc":1751260561,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_mhjuy","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Claude seems way ahead of any local LLM that I’ve tried in terms of quality of outputs and also just dragging a big PDF into it and getting quite good analysis. Even with a good GPU, I haven’t seen a local model that has the same quality.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0j2lrt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Claude seems way ahead of any local LLM that I’ve tried in terms of quality of outputs and also just dragging a big PDF into it and getting quite good analysis. Even with a good GPU, I haven’t seen a local model that has the same quality.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0j2lrt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751260561,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0j6foc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Elderberry_9132","can_mod_post":false,"created_utc":1751262600,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_byh4ysuoh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Well, here is an answer, you don’t understand how this AI magic works, mainly LLM models perform particular task of either helping to reason, or generate text, their purpose is to guess the next token a user expects, it is just guessing it with enough correct answers for you to believe it is actually intelligent. \\n\\nSo, keeping this in mind, “AI” is a huge pile of different algorithms, services, stages and pipelines. \\n\\nYou won’t be able to run it locally unless you have the time to developer it and deploy, and manage it","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0j6foc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Well, here is an answer, you don’t understand how this AI magic works, mainly LLM models perform particular task of either helping to reason, or generate text, their purpose is to guess the next token a user expects, it is just guessing it with enough correct answers for you to believe it is actually intelligent. &lt;/p&gt;\\n\\n&lt;p&gt;So, keeping this in mind, “AI” is a huge pile of different algorithms, services, stages and pipelines. &lt;/p&gt;\\n\\n&lt;p&gt;You won’t be able to run it locally unless you have the time to developer it and deploy, and manage it&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0j6foc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751262600,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jamiu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Spiritual-Spend8187","can_mod_post":false,"created_utc":1751264917,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_4aevhf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Closest you will get to cloud level on local is deepseek but that is still very expensive on consumer hardware possible but insane.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jamiu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Closest you will get to cloud level on local is deepseek but that is still very expensive on consumer hardware possible but insane.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jamiu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751264917,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jdg5y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Fast-Satisfaction482","can_mod_post":false,"created_utc":1751266535,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_9ceux4xp","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I have access to a pretty decent workstation with dual 4090 at work and while it's all fun and games as long as your company pays for it, it's by far not as good as OpenAI or Anthropic's offerings.\\n\\n\\nOf course unless your use case is against the terms of service for the big cloud services, then it's the only option. (Or cloud is against regulation at your job ) ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jdg5y","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have access to a pretty decent workstation with dual 4090 at work and while it&amp;#39;s all fun and games as long as your company pays for it, it&amp;#39;s by far not as good as OpenAI or Anthropic&amp;#39;s offerings.&lt;/p&gt;\\n\\n&lt;p&gt;Of course unless your use case is against the terms of service for the big cloud services, then it&amp;#39;s the only option. (Or cloud is against regulation at your job ) &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jdg5y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751266535,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jegxi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Liringlass","can_mod_post":false,"created_utc":1751267131,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_dfewdhav","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Not only anything local can only come close, not reach Claude for coding.\\n\\nBut anything local won't upgrade magically. What you buy today will still have the same capacity in 2 years, and while same size LLMs improve over time you won't be able to upgrade your hardware \\"for free\\".\\n\\nLike someone else said Deepseek (the big model) might be the closest one but you either need a big ram/cpu server (just a few K$) but it's gonna be very very slow or you  need multiple 10/20k$ GPUs (and even here i'm not sure how fast it will be compared to Claude on subscription services).\\n\\nPeople mention 70b models which could be fine for some help but don't expect Cursor / GHCP / Cline / RooCode agent mode level of help, it will be more basic.\\n\\nOne more thing is that subscription services seem very cheap to me compared to the cost of doing it yourself, might be due to competition? Last time I checked cursor was USD 20 a month and that's really cheap for all the work you can get done on big models.\\n\\nLastly, anything local or cloud based need work to setup and maintain. That's also something to take into account if you look at it in a ROI perspective.\\n\\nThe only cases where I would advise local LLMs are: \\n\\n\\\\- When confidentiality is a must and deserves the added cost / lower level of service. And even then you might be better off renting the equipment in the cloud, and running your own private API, rather than buying 20 or 40k USD worth of equipment.\\n\\n\\\\- When you want to DIY and learn, focusing on that journey rather than on the direct result, and added cost is not a problem","edited":1751267314,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jegxi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not only anything local can only come close, not reach Claude for coding.&lt;/p&gt;\\n\\n&lt;p&gt;But anything local won&amp;#39;t upgrade magically. What you buy today will still have the same capacity in 2 years, and while same size LLMs improve over time you won&amp;#39;t be able to upgrade your hardware &amp;quot;for free&amp;quot;.&lt;/p&gt;\\n\\n&lt;p&gt;Like someone else said Deepseek (the big model) might be the closest one but you either need a big ram/cpu server (just a few K$) but it&amp;#39;s gonna be very very slow or you  need multiple 10/20k$ GPUs (and even here i&amp;#39;m not sure how fast it will be compared to Claude on subscription services).&lt;/p&gt;\\n\\n&lt;p&gt;People mention 70b models which could be fine for some help but don&amp;#39;t expect Cursor / GHCP / Cline / RooCode agent mode level of help, it will be more basic.&lt;/p&gt;\\n\\n&lt;p&gt;One more thing is that subscription services seem very cheap to me compared to the cost of doing it yourself, might be due to competition? Last time I checked cursor was USD 20 a month and that&amp;#39;s really cheap for all the work you can get done on big models.&lt;/p&gt;\\n\\n&lt;p&gt;Lastly, anything local or cloud based need work to setup and maintain. That&amp;#39;s also something to take into account if you look at it in a ROI perspective.&lt;/p&gt;\\n\\n&lt;p&gt;The only cases where I would advise local LLMs are: &lt;/p&gt;\\n\\n&lt;p&gt;- When confidentiality is a must and deserves the added cost / lower level of service. And even then you might be better off renting the equipment in the cloud, and running your own private API, rather than buying 20 or 40k USD worth of equipment.&lt;/p&gt;\\n\\n&lt;p&gt;- When you want to DIY and learn, focusing on that journey rather than on the direct result, and added cost is not a problem&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jegxi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751267131,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jnm9c","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Cergorach","can_mod_post":false,"created_utc":1751272739,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_cs4w88d2l","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt;Can any local LLM compare with cloud models? \\n\\nWhile technically the can, when the model is available (like DeepSeek), but when you run the full model you need such ridiculous hardware that it's not realistic or practical. You can run the smaller quantized models easily, but these tend to be lobotomized models. Now, depending on your task, those lobotomized models might work perfectly fine for you...\\n\\nIt's also not just buying an expensive GPU once, it needs ridiculous amounts of power to run (possibly requiring you to upgrade your power supply as well) and even at idle (not in use) they use quite a bit of power. Almost all that power is converted into heat, so you would need to use even more power to cool that with AC... A modern 14900k + 5090 pulls as much idle as a Mac Mini pulls at full power...","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jnm9c","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Can any local LLM compare with cloud models? &lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;While technically the can, when the model is available (like DeepSeek), but when you run the full model you need such ridiculous hardware that it&amp;#39;s not realistic or practical. You can run the smaller quantized models easily, but these tend to be lobotomized models. Now, depending on your task, those lobotomized models might work perfectly fine for you...&lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s also not just buying an expensive GPU once, it needs ridiculous amounts of power to run (possibly requiring you to upgrade your power supply as well) and even at idle (not in use) they use quite a bit of power. Almost all that power is converted into heat, so you would need to use even more power to cool that with AC... A modern 14900k + 5090 pulls as much idle as a Mac Mini pulls at full power...&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jnm9c/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751272739,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jpxvk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MelodicRecognition7","can_mod_post":false,"created_utc":1751274162,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_1eex9ug5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I agree with other posters - local LLM do not and probably never will match the cloud services, you'll have to spend like $50k to come close to the cloud. You'll want to run local only if you have to - obliged by law or NDA or just don't want your horny chats get published some day lol.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jpxvk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I agree with other posters - local LLM do not and probably never will match the cloud services, you&amp;#39;ll have to spend like $50k to come close to the cloud. You&amp;#39;ll want to run local only if you have to - obliged by law or NDA or just don&amp;#39;t want your horny chats get published some day lol.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jpxvk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751274162,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0p2j7k","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"nini2352","can_mod_post":false,"created_utc":1751339300,"send_replies":true,"parent_id":"t1_n0jq4h2","score":1,"author_fullname":"t2_7s203ubo","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Good question, have people ran MCP enabled LLMs locally like quickly and fast potentially with HF transformers?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0p2j7k","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Good question, have people ran MCP enabled LLMs locally like quickly and fast potentially with HF transformers?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0p2j7k/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751339300,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0jq4h2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Round_Mixture_7541","can_mod_post":false,"created_utc":1751274273,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_114cnblv7x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"How about mistral's new model designed for tool calls and coding tasks? Has anyone set it up?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jq4h2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How about mistral&amp;#39;s new model designed for tool calls and coding tasks? Has anyone set it up?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jq4h2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751274273,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0k5w6f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ylsid","can_mod_post":false,"created_utc":1751282895,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_6lmlc","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"DeepSeek R1 will compete well with Claude, but good luck running it locally. Even Claude runs at cost","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0k5w6f","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;DeepSeek R1 will compete well with Claude, but good luck running it locally. Even Claude runs at cost&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0k5w6f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751282895,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0lv3yf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BumbleSlob","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ltzrb","score":1,"author_fullname":"t2_1j7fhlcqkp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"M2 max 64Gb\\n\\nYou can get it even faster running MLX, like 70TPS but I am fine with 50 as GGUF personally","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0lv3yf","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;M2 max 64Gb&lt;/p&gt;\\n\\n&lt;p&gt;You can get it even faster running MLX, like 70TPS but I am fine with 50 as GGUF personally&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0lv3yf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751302851,"author_flair_text":null,"treatment_tags":[],"created_utc":1751302851,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ltzrb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MLAWest","can_mod_post":false,"created_utc":1751302541,"send_replies":true,"parent_id":"t1_n0k934m","score":1,"author_fullname":"t2_12mf5l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What Apple hardware specifically are you using?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ltzrb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What Apple hardware specifically are you using?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0ltzrb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751302541,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0k934m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BumbleSlob","can_mod_post":false,"created_utc":1751284323,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_1j7fhlcqkp","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I run Apple silicon with Qwen3 30B and it’s great for most things at 50+ tps. Consumer hardware is not yet at a point where we can run the extremely large models (like Claude’s size) locally at good speeds, but it’ll probably get there in next 5 years.\\n\\nI’d recommend you get something in your budget and use it as much as you can and flip to Claude only when you need it. ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0k934m","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I run Apple silicon with Qwen3 30B and it’s great for most things at 50+ tps. Consumer hardware is not yet at a point where we can run the extremely large models (like Claude’s size) locally at good speeds, but it’ll probably get there in next 5 years.&lt;/p&gt;\\n\\n&lt;p&gt;I’d recommend you get something in your budget and use it as much as you can and flip to Claude only when you need it. &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0k934m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751284323,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0l1mjx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"protector111","can_mod_post":false,"created_utc":1751294312,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_54rrvsmz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Not even close. It will also be ridiculously slow.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0l1mjx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not even close. It will also be ridiculously slow.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0l1mjx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751294312,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0lid53","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"OmarBessa","can_mod_post":false,"created_utc":1751299189,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_guxix","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you can spare 10k, yes. If not, continue to use cloud models.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0lid53","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you can spare 10k, yes. If not, continue to use cloud models.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0lid53/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751299189,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"03eba0e8-72f2-11ee-96eb-9a14648159ce","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0lnw0a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LienniTa","can_mod_post":false,"created_utc":1751300790,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_txs45qja","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"noone mentioning samplers? if you require some cryptic sampler for your needs like DRY, XTC, or restrictive inference for SO, you dont really have a choice and have to buy hardware.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0lnw0a","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"koboldcpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;noone mentioning samplers? if you require some cryptic sampler for your needs like DRY, XTC, or restrictive inference for SO, you dont really have a choice and have to buy hardware.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0lnw0a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751300790,"author_flair_text":"koboldcpp","treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0mkop7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"e79683074","can_mod_post":false,"created_utc":1751310027,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_xj04i","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Honestly? No. None of them compares. Let alone small stuff you run on one or two GPUs.\\n\\nClosest that compares is DeepSeek 671b, and you need like 512GB of RAM there.","edited":1751326344,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0mkop7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Honestly? No. None of them compares. Let alone small stuff you run on one or two GPUs.&lt;/p&gt;\\n\\n&lt;p&gt;Closest that compares is DeepSeek 671b, and you need like 512GB of RAM there.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0mkop7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751310027,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0o5eam","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"pegarciadotcom","can_mod_post":false,"created_utc":1751327435,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_c4ceur39","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Unless you really want to play with local LLMs, or have hard requirements on privacy, you’re probably be better served by cloud models.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0o5eam","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Unless you really want to play with local LLMs, or have hard requirements on privacy, you’re probably be better served by cloud models.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0o5eam/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751327435,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0oae6q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"teddybear082","can_mod_post":false,"created_utc":1751329154,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_5275arei","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I got my GPU for playing VR, it was a nice side benefit I could try some more local AI things but absolutely despite the crazy hype I’ve yet to find any local models I could reasonably run even on a 5080 that compare at all to any of the big companies’ cloud models for cases like complex tool/function calling.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0oae6q","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I got my GPU for playing VR, it was a nice side benefit I could try some more local AI things but absolutely despite the crazy hype I’ve yet to find any local models I could reasonably run even on a 5080 that compare at all to any of the big companies’ cloud models for cases like complex tool/function calling.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0oae6q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751329154,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0p24ku","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"nini2352","can_mod_post":false,"created_utc":1751339139,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_7s203ubo","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Even with datacenter grade hardware, you’ll never beat Claude in terms of speed or accuracy \\n\\nI have 4 x A6000 Ada and running DeepSeek-R1 is a pain, I’ll get good accuracy and speed… after maybe 2 hours of tweaking… \\n\\nLike if you haven’t tried using FlashAttn on Nvidia hardware, it will be like days to weeks before you get it up and are comfortable with the performance tradeoffs you’ll end up facing \\n\\nEven on for example 5090, you won’t be happy lol… without spending lots of time quite literally doing some basic level of hardware-software codesign\\n\\nClaude’s agents aren’t actually available as open source either, so to get the exact same experience, you’d be spending API credits on top of compute and hardware costs","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0p24ku","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Even with datacenter grade hardware, you’ll never beat Claude in terms of speed or accuracy &lt;/p&gt;\\n\\n&lt;p&gt;I have 4 x A6000 Ada and running DeepSeek-R1 is a pain, I’ll get good accuracy and speed… after maybe 2 hours of tweaking… &lt;/p&gt;\\n\\n&lt;p&gt;Like if you haven’t tried using FlashAttn on Nvidia hardware, it will be like days to weeks before you get it up and are comfortable with the performance tradeoffs you’ll end up facing &lt;/p&gt;\\n\\n&lt;p&gt;Even on for example 5090, you won’t be happy lol… without spending lots of time quite literally doing some basic level of hardware-software codesign&lt;/p&gt;\\n\\n&lt;p&gt;Claude’s agents aren’t actually available as open source either, so to get the exact same experience, you’d be spending API credits on top of compute and hardware costs&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0p24ku/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751339139,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0qf37b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Impressive_Layer_634","can_mod_post":false,"created_utc":1751364771,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_7frjzqox","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Local models are good for some things, you can test a lot of them out on various services that people have mentioned here. They will never be as good as Claude. Unless you have serious privacy concerns for the work you’re doing, it’s not worth it to get a GPU just for this.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0qf37b","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Local models are good for some things, you can test a lot of them out on various services that people have mentioned here. They will never be as good as Claude. Unless you have serious privacy concerns for the work you’re doing, it’s not worth it to get a GPU just for this.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0qf37b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751364771,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hkpsv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"polandtown","can_mod_post":false,"created_utc":1751238812,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_g1ws1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Ok. Want to really learn how to use LLMs at the enterprise level? That's done though the cloud: let that be on an on-premise vlan or a 'traditional' cloud instance. You get to learn the required networking side ontop of everything else, making your portfolio stronger.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hkpsv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ok. Want to really learn how to use LLMs at the enterprise level? That&amp;#39;s done though the cloud: let that be on an on-premise vlan or a &amp;#39;traditional&amp;#39; cloud instance. You get to learn the required networking side ontop of everything else, making your portfolio stronger.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hkpsv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751238812,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hwsnb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fizzy1242","can_mod_post":false,"created_utc":1751243233,"send_replies":true,"parent_id":"t1_n0hw3au","score":3,"author_fullname":"t2_16zcsx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"waste for you, invaluable for others","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hwsnb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;waste for you, invaluable for others&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hwsnb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751243233,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hw3au","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"INtuitiveTJop","can_mod_post":false,"created_utc":1751242966,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_u16k63kl","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I paid 2k only to end up not using it. Don’t waste the money","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hw3au","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I paid 2k only to end up not using it. Don’t waste the money&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hw3au/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751242966,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hvitu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"digiwiggles","can_mod_post":false,"created_utc":1751242748,"send_replies":true,"parent_id":"t1_n0hr3lg","score":3,"author_fullname":"t2_5284u","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I've been asking around, and can't get anyone to answer.   So please forgive me, I have to try with you as well.  What have you had good results with R1-0528?  I can't get anything decent code or any work based task out of it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hvitu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve been asking around, and can&amp;#39;t get anyone to answer.   So please forgive me, I have to try with you as well.  What have you had good results with R1-0528?  I can&amp;#39;t get anything decent code or any work based task out of it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hvitu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751242748,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hr3lg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Daniel_H212","can_mod_post":false,"created_utc":1751241103,"send_replies":true,"parent_id":"t3_1lnsax9","score":-1,"author_fullname":"t2_1vi6fut","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yes, R1-0528 is on a similar level to Claude.\\n\\nNo, you won't be running it locally with any reasonable speed unless you drop enough money to buy a car.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hr3lg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, R1-0528 is on a similar level to Claude.&lt;/p&gt;\\n\\n&lt;p&gt;No, you won&amp;#39;t be running it locally with any reasonable speed unless you drop enough money to buy a car.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hr3lg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751241103,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}}]`),r=()=>e.jsx(l,{data:a});export{r as default};
