import{j as e}from"./index-BOnf-UhU.js";import{R as l}from"./RedditPostRenderer-Ce39qICS.js";import"./index-CDase6VD.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"could i pair two p100a (28gb) tenstorrent LPUs together to power an on prem AI inference model for my office of 11 people. would it be able to concurrently answer 3 people’s questions. should i look at other hardware alternatives. i’d like to be able to run something like mistral 8x7b or better on this. would love to hear any recommendations or improvements for this. would like for it to be as minimal cost as possible.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"tenstorrent for LLM inference","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1ltdrkm","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.67,"author_flair_background_color":null,"subreddit_type":"public","ups":1,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_a0q7cu31","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":1,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751840353,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;could i pair two p100a (28gb) tenstorrent LPUs together to power an on prem AI inference model for my office of 11 people. would it be able to concurrently answer 3 people’s questions. should i look at other hardware alternatives. i’d like to be able to run something like mistral 8x7b or better on this. would love to hear any recommendations or improvements for this. would like for it to be as minimal cost as possible.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1ltdrkm","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Odd_Translator_3026","discussion_type":null,"num_comments":4,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1ltdrkm/tenstorrent_for_llm_inference/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1ltdrkm/tenstorrent_for_llm_inference/","subreddit_subscribers":496034,"created_utc":1751840353,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1pqpr8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Vengoropatubus","can_mod_post":false,"created_utc":1751842553,"send_replies":true,"parent_id":"t3_1ltdrkm","score":1,"author_fullname":"t2_4vned","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I’m also curious about those cards!\\n\\nI haven’t convinced myself yet that I’d be able to successfully run the models I want to run. They seem like great value for getting enough VRAM to run a great model. I’m not sure if I’d be able to easily run quantized models though. \\n\\nI think they link a cloud option I could use to figure that out though.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1pqpr8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I’m also curious about those cards!&lt;/p&gt;\\n\\n&lt;p&gt;I haven’t convinced myself yet that I’d be able to successfully run the models I want to run. They seem like great value for getting enough VRAM to run a great model. I’m not sure if I’d be able to easily run quantized models though. &lt;/p&gt;\\n\\n&lt;p&gt;I think they link a cloud option I could use to figure that out though.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltdrkm/tenstorrent_for_llm_inference/n1pqpr8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751842553,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltdrkm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1qjf2c","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Odd_Translator_3026","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1q9mw6","score":1,"author_fullname":"t2_a0q7cu31","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"can you share these","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1qjf2c","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;can you share these&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltdrkm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltdrkm/tenstorrent_for_llm_inference/n1qjf2c/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751852932,"author_flair_text":null,"treatment_tags":[],"created_utc":1751852932,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1q9mw6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Double_Cause4609","can_mod_post":false,"created_utc":1751849355,"send_replies":true,"parent_id":"t1_n1prxgh","score":1,"author_fullname":"t2_1kubzxt2ww","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"There's a custom vLLM fork and they have a few dedicated projects for turnkey LLM servers.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1q9mw6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There&amp;#39;s a custom vLLM fork and they have a few dedicated projects for turnkey LLM servers.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltdrkm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltdrkm/tenstorrent_for_llm_inference/n1q9mw6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751849355,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1prxgh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullstackSensei","can_mod_post":false,"created_utc":1751842970,"send_replies":true,"parent_id":"t3_1ltdrkm","score":1,"author_fullname":"t2_17n3nqtj56","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Beyond the vagueness of your requirements (no mention of expected context size per user, what is deemed acceptable performance by you or your users, nor the quant of 8x7b or why can't you use a much newer model), there's the issue that the current crop of Tenstorrent cards are mainly development platforms. You have the SDKs available, but AFAIK no inference engine has integrated support for them. So, will you be writing the code to run inference of 8x7b? will you implement flash attention for it? how optimized will your code be?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1prxgh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Beyond the vagueness of your requirements (no mention of expected context size per user, what is deemed acceptable performance by you or your users, nor the quant of 8x7b or why can&amp;#39;t you use a much newer model), there&amp;#39;s the issue that the current crop of Tenstorrent cards are mainly development platforms. You have the SDKs available, but AFAIK no inference engine has integrated support for them. So, will you be writing the code to run inference of 8x7b? will you implement flash attention for it? how optimized will your code be?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltdrkm/tenstorrent_for_llm_inference/n1prxgh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751842970,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltdrkm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(l,{data:t});export{n as default};
