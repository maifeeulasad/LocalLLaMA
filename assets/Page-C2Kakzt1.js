import{j as e}from"./index-F0NXdzZX.js";import{R as t}from"./RedditPostRenderer-CoEZB1dt.js";import"./index-DrtravXm.js";const l=JSON.parse('[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"EDIT: The issue turned out to be an old version of llama.cpp. Upgrading to the latest version as of now (b5890) resulted in 3.3t/s!\\n\\nEDIT 2.1: I got this up to ~~4.5t/s~~ 5.0t/s. Details added to the bottom of the post!\\n\\nPreface: Just a disclaimer that the machine this is running on was never intended to be an inference machine. I am using it (to the dismay of its actual at-the-keyboard user!) due to it being the only machine I could fit the GPU into.\\n\\nAs per the title, I have attempted to run Qwen3-235B-A22B using `llama-server` on the machine that I felt is most capable of doing so, but I get very poor performance at 0.7t/s at most. Is anyone able to advise if I can get it up to the 5t/s I see others mentioning achieving on this machine?\\n\\nMachine specification are:\\n\\n    CPU: i3-12100F (12th Gen Intel)\\n    RAM: 128GB (4*32GB) @ 2133 MT/s (Corsair CMK128GX4M4A2666C16)\\n    Motherboard: MSI PRO B660M-A WIFI DDR4\\n    GPU: GeForce RTX 3090 24GB VRAM\\n\\n(Note: There is another GPU in this machine which is being used for the display. The 3090 is only used for inference.)\\n\\n`llama-server` launch options:\\n\\n    llama-server \\\\\\n      --host 0.0.0.0 \\\\\\n      --model unsloth/Qwen3-235B-A22B-GGUF/UD-Q2_K_XL/Qwen3-235B-A22B-UD-Q2_K_XL-00001-of-00002.gguf \\\\\\n      --ctx-size 16384 \\\\\\n      --n-gpu-layers 99 \\\\\\n      --flash-attn \\\\\\n      --threads 3 \\\\\\n      -ot \\"exps=CPU\\" \\\\\\n      --seed 3407 \\\\\\n      --prio 3 \\\\\\n      --temp 0.6 \\\\\\n      --min-p 0.0 \\\\\\n      --top-p 0.95 \\\\\\n      --top-k 20 \\\\\\n      --no-mmap \\\\\\n      --no-warmup \\\\\\n      --mlock\\n\\nAny advice is much appreciated (again, by me, maybe not so much by the user! They are very understanding though..)\\n\\n---\\n\\nManaged to achieve 5.0t/s!\\n\\n```\\nllama-server \\\\\\n  --host 0.0.0.0 \\\\\\n  --model unsloth/Qwen3-235B-A22B-GGUF/UD-Q2_K_XL/Qwen3-235B-A22B-UD-Q2_K_XL-00001-of-00002.gguf \\\\\\n  --ctx-size 16384 \\\\\\n  --n-gpu-layers 99 \\\\\\n  --flash-attn \\\\\\n  --threads 4 \\\\\\n  --seed 3407 \\\\\\n  --prio 3 \\\\\\n  --temp 0.6 \\\\\\n  --min-p 0.0 \\\\\\n  --top-p 0.95 \\\\\\n  --top-k 20 \\\\\\n  --no-warmup \\\\\\n  -ub 1 \\\\\\n  -ot \'blk\\\\.()\\\\.ffn_.*_exps\\\\.weight=CPU\' \\\\\\n  -ot \'blk\\\\.(19)\\\\.ffn_.*_exps\\\\.weight=CPU\' \\\\\\n  -ot \'blk\\\\.(2[0-9])\\\\.ffn_.*_exps\\\\.weight=CPU\' \\\\\\n  -ot \'blk\\\\.(3[0-9])\\\\.ffn_.*_exps\\\\.weight=CPU\' \\\\\\n  -ot \'blk\\\\.(4[0-9])\\\\.ffn_.*_exps\\\\.weight=CPU\' \\\\\\n  -ot \'blk\\\\.(5[0-9])\\\\.ffn_.*_exps\\\\.weight=CPU\' \\\\\\n  -ot \'blk\\\\.(6[0-9])\\\\.ffn_.*_exps\\\\.weight=CPU\' \\\\\\n  -ot \'blk\\\\.(7[0-9])\\\\.ffn_.*_exps\\\\.weight=CPU\' \\\\\\n  -ot \'blk\\\\.(8[0-9])\\\\.ffn_.*_exps\\\\.weight=CPU\' \\\\\\n  -ot \'blk\\\\.(9[0-9])\\\\.ffn_.*_exps\\\\.weight=CPU\'\\n```\\n\\nThis results in 23.76GB VRAM used and 5.0t/s.\\n\\n```\\nprompt eval time =    5383.36 ms /    29 tokens (  185.63 ms per token,     5.39 t\\n       eval time =  359004.62 ms /  1783 tokens (  201.35 ms per token,     4.97 t\\n      total time =  364387.98 ms /  1812 tokens\\n```","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Qwen3-235B-A22B @ 0.7t/s. Hardware or configuration bottleneck?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lysmo9","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.75,"author_flair_background_color":null,"subreddit_type":"public","ups":8,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_tfa1mcp1","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":8,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1752443952,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752412727,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;EDIT: The issue turned out to be an old version of llama.cpp. Upgrading to the latest version as of now (b5890) resulted in 3.3t/s!&lt;/p&gt;\\n\\n&lt;p&gt;EDIT 2.1: I got this up to &lt;del&gt;4.5t/s&lt;/del&gt; 5.0t/s. Details added to the bottom of the post!&lt;/p&gt;\\n\\n&lt;p&gt;Preface: Just a disclaimer that the machine this is running on was never intended to be an inference machine. I am using it (to the dismay of its actual at-the-keyboard user!) due to it being the only machine I could fit the GPU into.&lt;/p&gt;\\n\\n&lt;p&gt;As per the title, I have attempted to run Qwen3-235B-A22B using &lt;code&gt;llama-server&lt;/code&gt; on the machine that I felt is most capable of doing so, but I get very poor performance at 0.7t/s at most. Is anyone able to advise if I can get it up to the 5t/s I see others mentioning achieving on this machine?&lt;/p&gt;\\n\\n&lt;p&gt;Machine specification are:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;CPU: i3-12100F (12th Gen Intel)\\nRAM: 128GB (4*32GB) @ 2133 MT/s (Corsair CMK128GX4M4A2666C16)\\nMotherboard: MSI PRO B660M-A WIFI DDR4\\nGPU: GeForce RTX 3090 24GB VRAM\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;(Note: There is another GPU in this machine which is being used for the display. The 3090 is only used for inference.)&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;llama-server&lt;/code&gt; launch options:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;llama-server \\\\\\n  --host 0.0.0.0 \\\\\\n  --model unsloth/Qwen3-235B-A22B-GGUF/UD-Q2_K_XL/Qwen3-235B-A22B-UD-Q2_K_XL-00001-of-00002.gguf \\\\\\n  --ctx-size 16384 \\\\\\n  --n-gpu-layers 99 \\\\\\n  --flash-attn \\\\\\n  --threads 3 \\\\\\n  -ot &amp;quot;exps=CPU&amp;quot; \\\\\\n  --seed 3407 \\\\\\n  --prio 3 \\\\\\n  --temp 0.6 \\\\\\n  --min-p 0.0 \\\\\\n  --top-p 0.95 \\\\\\n  --top-k 20 \\\\\\n  --no-mmap \\\\\\n  --no-warmup \\\\\\n  --mlock\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;Any advice is much appreciated (again, by me, maybe not so much by the user! They are very understanding though..)&lt;/p&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;p&gt;Managed to achieve 5.0t/s!&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;\\nllama-server \\\\\\n  --host 0.0.0.0 \\\\\\n  --model unsloth/Qwen3-235B-A22B-GGUF/UD-Q2_K_XL/Qwen3-235B-A22B-UD-Q2_K_XL-00001-of-00002.gguf \\\\\\n  --ctx-size 16384 \\\\\\n  --n-gpu-layers 99 \\\\\\n  --flash-attn \\\\\\n  --threads 4 \\\\\\n  --seed 3407 \\\\\\n  --prio 3 \\\\\\n  --temp 0.6 \\\\\\n  --min-p 0.0 \\\\\\n  --top-p 0.95 \\\\\\n  --top-k 20 \\\\\\n  --no-warmup \\\\\\n  -ub 1 \\\\\\n  -ot &amp;#39;blk\\\\.()\\\\.ffn_.*_exps\\\\.weight=CPU&amp;#39; \\\\\\n  -ot &amp;#39;blk\\\\.(19)\\\\.ffn_.*_exps\\\\.weight=CPU&amp;#39; \\\\\\n  -ot &amp;#39;blk\\\\.(2[0-9])\\\\.ffn_.*_exps\\\\.weight=CPU&amp;#39; \\\\\\n  -ot &amp;#39;blk\\\\.(3[0-9])\\\\.ffn_.*_exps\\\\.weight=CPU&amp;#39; \\\\\\n  -ot &amp;#39;blk\\\\.(4[0-9])\\\\.ffn_.*_exps\\\\.weight=CPU&amp;#39; \\\\\\n  -ot &amp;#39;blk\\\\.(5[0-9])\\\\.ffn_.*_exps\\\\.weight=CPU&amp;#39; \\\\\\n  -ot &amp;#39;blk\\\\.(6[0-9])\\\\.ffn_.*_exps\\\\.weight=CPU&amp;#39; \\\\\\n  -ot &amp;#39;blk\\\\.(7[0-9])\\\\.ffn_.*_exps\\\\.weight=CPU&amp;#39; \\\\\\n  -ot &amp;#39;blk\\\\.(8[0-9])\\\\.ffn_.*_exps\\\\.weight=CPU&amp;#39; \\\\\\n  -ot &amp;#39;blk\\\\.(9[0-9])\\\\.ffn_.*_exps\\\\.weight=CPU&amp;#39;\\n&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;This results in 23.76GB VRAM used and 5.0t/s.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;\\nprompt eval time =    5383.36 ms /    29 tokens (  185.63 ms per token,     5.39 t\\n       eval time =  359004.62 ms /  1783 tokens (  201.35 ms per token,     4.97 t\\n      total time =  364387.98 ms /  1812 tokens\\n&lt;/code&gt;&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lysmo9","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"ConnectionOutside485","discussion_type":null,"num_comments":25,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lysmo9/qwen3235ba22b_07ts_hardware_or_configuration/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lysmo9/qwen3235ba22b_07ts_hardware_or_configuration/","subreddit_subscribers":498850,"created_utc":1752412727,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2wng7y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ConnectionOutside485","can_mod_post":false,"created_utc":1752418389,"send_replies":true,"parent_id":"t1_n2w7pt9","score":1,"author_fullname":"t2_tfa1mcp1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks. I will check the XMP settings when the machine is next restarted. I see how I might already be losing some performance here with 2166 vs. 2666!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2wng7y","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks. I will check the XMP settings when the machine is next restarted. I see how I might already be losing some performance here with 2166 vs. 2666!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lysmo9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lysmo9/qwen3235ba22b_07ts_hardware_or_configuration/n2wng7y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752418389,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2w7pt9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BumbleSlob","can_mod_post":false,"created_utc":1752413184,"send_replies":true,"parent_id":"t3_1lysmo9","score":3,"author_fullname":"t2_1j7fhlcqkp","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The thing that jumped out to me as the obvious bottleneck was your memory speeds. 2166Mtps is dog slow. You might be able to juice some extra performance by turning on XMP in your bios and ensuring your sticks are running at 2666Mtps.\\n\\nBut otherwise, I’d say you should target a ram bandwidth upgrade. Your mobo should support up to DDR4-4800 or so. ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2w7pt9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The thing that jumped out to me as the obvious bottleneck was your memory speeds. 2166Mtps is dog slow. You might be able to juice some extra performance by turning on XMP in your bios and ensuring your sticks are running at 2666Mtps.&lt;/p&gt;\\n\\n&lt;p&gt;But otherwise, I’d say you should target a ram bandwidth upgrade. Your mobo should support up to DDR4-4800 or so. &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lysmo9/qwen3235ba22b_07ts_hardware_or_configuration/n2w7pt9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752413184,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lysmo9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2yix52","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ConnectionOutside485","can_mod_post":false,"created_utc":1752438481,"send_replies":true,"parent_id":"t1_n2wh5l6","score":1,"author_fullname":"t2_tfa1mcp1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I tried to use a draft model, after solving the initial issue, but it\'s 0.3t/s slower.\\n\\n```\\nprompt eval time =    4483.03 ms /    29 tokens (  154.59 ms per token,     6.47 tokens per second)\\n       eval time =  238586.61 ms /   720 tokens (  331.37 ms per token,     3.02 tokens per second)\\n      total time =  243069.64 ms /   749 tokens\\nslot print_timing: id  0 | task 0 |\\ndraft acceptance rate = 0.62976 (  364 accepted /   578 generated)\\n```\\n\\nvs.\\n\\n```\\nprompt eval time =    5437.38 ms /    29 tokens (  187.50 ms per token,     5.33 tokens per second)\\n       eval time =  465704.90 ms /  1542 tokens (  302.01 ms per token,     3.31 tokens per second)\\n      total time =  471142.27 ms /  1571 tokens\\n```","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2yix52","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I tried to use a draft model, after solving the initial issue, but it&amp;#39;s 0.3t/s slower.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;\\nprompt eval time =    4483.03 ms /    29 tokens (  154.59 ms per token,     6.47 tokens per second)\\n       eval time =  238586.61 ms /   720 tokens (  331.37 ms per token,     3.02 tokens per second)\\n      total time =  243069.64 ms /   749 tokens\\nslot print_timing: id  0 | task 0 |\\ndraft acceptance rate = 0.62976 (  364 accepted /   578 generated)\\n&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;vs.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;\\nprompt eval time =    5437.38 ms /    29 tokens (  187.50 ms per token,     5.33 tokens per second)\\n       eval time =  465704.90 ms /  1542 tokens (  302.01 ms per token,     3.31 tokens per second)\\n      total time =  471142.27 ms /  1571 tokens\\n&lt;/code&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lysmo9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lysmo9/qwen3235ba22b_07ts_hardware_or_configuration/n2yix52/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752438481,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2wh5l6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Common_Heron2171","can_mod_post":false,"created_utc":1752416436,"send_replies":true,"parent_id":"t3_1lysmo9","score":2,"author_fullname":"t2_1ornc3yr5v","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"buy faster DRAMs...\\n\\nmaybe try:  -ctk q8\\\\_0 -ctv q8\\\\_0\\n\\nAlso I guess your 3090 is near idle, so you could utilize it for speculative decoding\\n\\n\\\\-md (unsloth\'s)Qwen3-8B-UD-Q8\\\\_K\\\\_XL.gguf or something --draft 12 --draft-p-min 0.9 -ngld 99 (maybe  -ctkd q8\\\\_0 -ctvd q8\\\\_0)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2wh5l6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;buy faster DRAMs...&lt;/p&gt;\\n\\n&lt;p&gt;maybe try:  -ctk q8_0 -ctv q8_0&lt;/p&gt;\\n\\n&lt;p&gt;Also I guess your 3090 is near idle, so you could utilize it for speculative decoding&lt;/p&gt;\\n\\n&lt;p&gt;-md (unsloth&amp;#39;s)Qwen3-8B-UD-Q8_K_XL.gguf or something --draft 12 --draft-p-min 0.9 -ngld 99 (maybe  -ctkd q8_0 -ctvd q8_0)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lysmo9/qwen3235ba22b_07ts_hardware_or_configuration/n2wh5l6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752416436,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lysmo9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2xgrg1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ConnectionOutside485","can_mod_post":false,"created_utc":1752427191,"send_replies":true,"parent_id":"t1_n2wldya","score":1,"author_fullname":"t2_tfa1mcp1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I do normally only run models that fit entirely into VRAM. This is my first try into offloading anything to CPU. I switched to the \\\\`-ot\\\\` parameter you suggested but there was no improvement. Thanks for the suggestions!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2xgrg1","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I do normally only run models that fit entirely into VRAM. This is my first try into offloading anything to CPU. I switched to the `-ot` parameter you suggested but there was no improvement. Thanks for the suggestions!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lysmo9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lysmo9/qwen3235ba22b_07ts_hardware_or_configuration/n2xgrg1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752427191,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2wldya","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MDT-49","can_mod_post":false,"created_utc":1752417758,"send_replies":true,"parent_id":"t3_1lysmo9","score":2,"author_fullname":"t2_h8yrica5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think the -ot regex might not be right. Maybe try `-ot \\".ffn_.*_exps.=CPU\\"`. I think your CPU has 4 cores, so try increasing `--threads` to 4. \\n\\nAs other people have already mentioned, the RAM speeds are going to be the major limitation in this setup. I feel like you might be better off switching to the best dense LLM you can find and run that fully in VRAM.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2wldya","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think the -ot regex might not be right. Maybe try &lt;code&gt;-ot &amp;quot;.ffn_.*_exps.=CPU&amp;quot;&lt;/code&gt;. I think your CPU has 4 cores, so try increasing &lt;code&gt;--threads&lt;/code&gt; to 4. &lt;/p&gt;\\n\\n&lt;p&gt;As other people have already mentioned, the RAM speeds are going to be the major limitation in this setup. I feel like you might be better off switching to the best dense LLM you can find and run that fully in VRAM.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lysmo9/qwen3235ba22b_07ts_hardware_or_configuration/n2wldya/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752417758,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lysmo9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2xeu47","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ConnectionOutside485","can_mod_post":false,"created_utc":1752426633,"send_replies":true,"parent_id":"t1_n2w8pge","score":2,"author_fullname":"t2_tfa1mcp1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"With `-ot \'.ffn_.*_exps.=CPU\'`, very little of my GPU is being used (8 of 24GB VRAM and during inference 1% GPU utilisation). It does sound like I need to leave more on the GPU.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2xeu47","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;With &lt;code&gt;-ot &amp;#39;.ffn_.*_exps.=CPU&amp;#39;&lt;/code&gt;, very little of my GPU is being used (8 of 24GB VRAM and during inference 1% GPU utilisation). It does sound like I need to leave more on the GPU.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lysmo9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lysmo9/qwen3235ba22b_07ts_hardware_or_configuration/n2xeu47/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752426633,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2w8pge","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ICanSeeYou7867","can_mod_post":false,"created_utc":1752413546,"send_replies":true,"parent_id":"t3_1lysmo9","score":1,"author_fullname":"t2_sqvpr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"How much of your gpu is being used?\\n\\nSince you have 99 layers supposedly being sent to the GPU, but its not crashing....\\n\\nI imagine your override tensor line might be pushing all the experts to your RAM and CPU?\\n\\nLooking at this post:\\nhttps://www.reddit.com/r/LocalLLaMA/s/bLOx23oaNN\\n\\nMaybe you should push specific layers to the GPU instead?\\n\\nOr remove the OT line, and adjust your gpu layers to something your GPU can handle.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2w8pge","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How much of your gpu is being used?&lt;/p&gt;\\n\\n&lt;p&gt;Since you have 99 layers supposedly being sent to the GPU, but its not crashing....&lt;/p&gt;\\n\\n&lt;p&gt;I imagine your override tensor line might be pushing all the experts to your RAM and CPU?&lt;/p&gt;\\n\\n&lt;p&gt;Looking at this post:\\n&lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/s/bLOx23oaNN\\"&gt;https://www.reddit.com/r/LocalLLaMA/s/bLOx23oaNN&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Maybe you should push specific layers to the GPU instead?&lt;/p&gt;\\n\\n&lt;p&gt;Or remove the OT line, and adjust your gpu layers to something your GPU can handle.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lysmo9/qwen3235ba22b_07ts_hardware_or_configuration/n2w8pge/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752413546,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lysmo9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2xmft0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2xfe4r","score":1,"author_fullname":"t2_h5utwre7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Your bandwidth not very good.. looks like only ~25gb/s.\\n\\nFill the rest of the GPU up with pieces of the model but I doubt you get much more than 2t/s. Tweaking the layers is free, so why not.\\n\\nThe specs for your CPU say you can get:\\n\\n    Max Memory Bandwidth \\n    76.8 GB/s\\n\\nThey likely mean with DDR5 on boards that support it. Better off using the model on OR.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2xmft0","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Your bandwidth not very good.. looks like only ~25gb/s.&lt;/p&gt;\\n\\n&lt;p&gt;Fill the rest of the GPU up with pieces of the model but I doubt you get much more than 2t/s. Tweaking the layers is free, so why not.&lt;/p&gt;\\n\\n&lt;p&gt;The specs for your CPU say you can get:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;Max Memory Bandwidth \\n76.8 GB/s\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;They likely mean with DDR5 on boards that support it. Better off using the model on OR.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lysmo9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lysmo9/qwen3235ba22b_07ts_hardware_or_configuration/n2xmft0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752428814,"author_flair_text":null,"treatment_tags":[],"created_utc":1752428814,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2xfe4r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ConnectionOutside485","can_mod_post":false,"created_utc":1752426793,"send_replies":true,"parent_id":"t1_n2w9hw3","score":2,"author_fullname":"t2_tfa1mcp1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It is not very full at all (only 8GB of 24GB VRAM used and 1% utilisation during inference).\\n\\nOutput of `mlc`;\\n```\\nIntel(R) Memory Latency Checker - v3.11b\\nMeasuring idle latencies for sequential access (in ns)...\\n                Numa node\\nNuma node            0\\n       0          69.8\\n\\nMeasuring Peak Injection Memory Bandwidths for the system\\nBandwidths are in MB/sec (1 MB/sec = 1,000,000 Bytes/sec)\\nUsing all the threads from each core if Hyper-threading is enabled\\nUsing traffic with the following read-write ratios\\nALL Reads        :      22895.3\\n3:1 Reads-Writes :      25300.8\\n2:1 Reads-Writes :      24839.4\\n1:1 Reads-Writes :      25869.6\\nStream-triad like:      24568.9\\n\\nMeasuring Memory Bandwidths between nodes within system\\nBandwidths are in MB/sec (1 MB/sec = 1,000,000 Bytes/sec)\\nUsing all the threads from each core if Hyper-threading is enabled\\nUsing Read-only traffic type\\n                Numa node\\nNuma node            0\\n       0        24107.3\\n\\nMeasuring Loaded Latencies for the system\\nUsing all the threads from each core if Hyper-threading is enabled\\nUsing Read-only traffic type\\nInject  Latency Bandwidth\\nDelay   (ns)    MB/sec\\n==========================\\n 00000  345.03    24774.1\\n 00002  283.16    25077.9\\n 00008  282.07    25067.2\\n 00015  341.81    25198.1\\n 00050  332.88    21421.0\\n 00100  190.04    20504.2\\n 00200  167.57    16924.3\\n 00300  105.40    11685.8\\n 00400  104.95     9679.3\\n 00500  107.99     8192.4\\n 00700  115.08     6310.5\\n 01000   93.58     4404.4\\n 01300  108.26     3808.5\\n 01700  112.51     2990.8\\n 02500  117.00     2052.8\\n 03500   84.59     1934.4\\n 05000   93.71     1542.0\\n 09000   94.65     1146.4\\n 20000   93.42      891.3\\n\\nMeasuring cache-to-cache transfer latency (in ns)...\\nLocal Socket L2-&gt;L2 HIT  latency        33.0\\nLocal Socket L2-&gt;L2 HITM latency        33.8\\n```","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2xfe4r","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It is not very full at all (only 8GB of 24GB VRAM used and 1% utilisation during inference).&lt;/p&gt;\\n\\n&lt;p&gt;Output of &lt;code&gt;mlc&lt;/code&gt;;\\n```\\nIntel(R) Memory Latency Checker - v3.11b\\nMeasuring idle latencies for sequential access (in ns)...\\n                Numa node\\nNuma node            0\\n       0          69.8&lt;/p&gt;\\n\\n&lt;p&gt;Measuring Peak Injection Memory Bandwidths for the system\\nBandwidths are in MB/sec (1 MB/sec = 1,000,000 Bytes/sec)\\nUsing all the threads from each core if Hyper-threading is enabled\\nUsing traffic with the following read-write ratios\\nALL Reads        :      22895.3\\n3:1 Reads-Writes :      25300.8\\n2:1 Reads-Writes :      24839.4\\n1:1 Reads-Writes :      25869.6\\nStream-triad like:      24568.9&lt;/p&gt;\\n\\n&lt;p&gt;Measuring Memory Bandwidths between nodes within system\\nBandwidths are in MB/sec (1 MB/sec = 1,000,000 Bytes/sec)\\nUsing all the threads from each core if Hyper-threading is enabled\\nUsing Read-only traffic type\\n                Numa node\\nNuma node            0\\n       0        24107.3&lt;/p&gt;\\n\\n&lt;p&gt;Measuring Loaded Latencies for the system\\nUsing all the threads from each core if Hyper-threading is enabled\\nUsing Read-only traffic type\\nInject  Latency Bandwidth&lt;/p&gt;\\n\\n&lt;h1&gt;Delay   (ns)    MB/sec&lt;/h1&gt;\\n\\n&lt;p&gt;00000  345.03    24774.1\\n 00002  283.16    25077.9\\n 00008  282.07    25067.2\\n 00015  341.81    25198.1\\n 00050  332.88    21421.0\\n 00100  190.04    20504.2\\n 00200  167.57    16924.3\\n 00300  105.40    11685.8\\n 00400  104.95     9679.3\\n 00500  107.99     8192.4\\n 00700  115.08     6310.5\\n 01000   93.58     4404.4\\n 01300  108.26     3808.5\\n 01700  112.51     2990.8\\n 02500  117.00     2052.8\\n 03500   84.59     1934.4\\n 05000   93.71     1542.0\\n 09000   94.65     1146.4\\n 20000   93.42      891.3&lt;/p&gt;\\n\\n&lt;p&gt;Measuring cache-to-cache transfer latency (in ns)...\\nLocal Socket L2-&amp;gt;L2 HIT  latency        33.0\\nLocal Socket L2-&amp;gt;L2 HITM latency        33.8\\n```&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lysmo9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lysmo9/qwen3235ba22b_07ts_hardware_or_configuration/n2xfe4r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752426793,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2w9hw3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1752413832,"send_replies":true,"parent_id":"t3_1lysmo9","score":1,"author_fullname":"t2_h5utwre7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"How full is your GPU? You put all the largest layers to CPU.\\n\\nWhat is your memory b/w with the ram? Download mlc and run a test. Probably a few more layers on the 3090 will raise it but don\'t expect a miracle judging by that memory.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2w9hw3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How full is your GPU? You put all the largest layers to CPU.&lt;/p&gt;\\n\\n&lt;p&gt;What is your memory b/w with the ram? Download mlc and run a test. Probably a few more layers on the 3090 will raise it but don&amp;#39;t expect a miracle judging by that memory.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lysmo9/qwen3235ba22b_07ts_hardware_or_configuration/n2w9hw3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752413832,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lysmo9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2y3gjg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ConnectionOutside485","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2xoj1i","score":2,"author_fullname":"t2_tfa1mcp1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think I found out why this problem is so mysterious and it is entirely my fault. **I didn\'t realise my llama.cpp installation was as old as it was.** I\'m so sorry that I overlooked this!\\n\\nI upgraded it and..\\n\\n```\\nprompt eval time =    4483.03 ms /    29 tokens (  154.59 ms per token,     6.47 tokens per second)\\n       eval time =  238586.61 ms /   720 tokens (  331.37 ms per token,     3.02 tokens per second)\\n      total time =  243069.64 ms /   749 tokens\\nslot print_timing: id  0 | task 0 |\\ndraft acceptance rate = 0.62976 (  364 accepted /   578 generated)\\n```\\nRemoving the draft model makes it go up to `~3.3 t/s` and my GPU utilisation is now 8% (which makes sense.. the CPU can keep up with it a bit more). I\'m surprised the draft model has such a negative effect. I would have thought 63% acceptance would help rather than hinder.\\n\\nI\'m going to see if I can push this further, as I have been given so many suggestions in this thread.\\n\\nThank you again!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2y3gjg","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think I found out why this problem is so mysterious and it is entirely my fault. &lt;strong&gt;I didn&amp;#39;t realise my llama.cpp installation was as old as it was.&lt;/strong&gt; I&amp;#39;m so sorry that I overlooked this!&lt;/p&gt;\\n\\n&lt;p&gt;I upgraded it and..&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;\\nprompt eval time =    4483.03 ms /    29 tokens (  154.59 ms per token,     6.47 tokens per second)\\n       eval time =  238586.61 ms /   720 tokens (  331.37 ms per token,     3.02 tokens per second)\\n      total time =  243069.64 ms /   749 tokens\\nslot print_timing: id  0 | task 0 |\\ndraft acceptance rate = 0.62976 (  364 accepted /   578 generated)\\n&lt;/code&gt;\\nRemoving the draft model makes it go up to &lt;code&gt;~3.3 t/s&lt;/code&gt; and my GPU utilisation is now 8% (which makes sense.. the CPU can keep up with it a bit more). I&amp;#39;m surprised the draft model has such a negative effect. I would have thought 63% acceptance would help rather than hinder.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m going to see if I can push this further, as I have been given so many suggestions in this thread.&lt;/p&gt;\\n\\n&lt;p&gt;Thank you again!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lysmo9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lysmo9/qwen3235ba22b_07ts_hardware_or_configuration/n2y3gjg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752433868,"author_flair_text":null,"treatment_tags":[],"created_utc":1752433868,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2xoj1i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"makistsa","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2xe70h","score":1,"author_fullname":"t2_3l1o090d","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I have no idea what\'s the problem.\\n\\n I previously set my ddr4 ram at 2133 and the cpu at 4.5ghz and used 3 threads and 16gb loaded in the gpu and i got &gt;3t/s.\\n\\nNow with my ram speed at normal i tested with -ot \'.ffn\\\\_.\\\\*\\\\_exps.=CPU\', 7.6GB used, i got 4.3t/s and the gpu was at 13%. Your gpu is twice as fast, so at 4.3t/s it would be at 6.5%. At 0.7t/s it would very well be at 1%. \\n\\nIf it was &gt;2t/s or something like this, it would be because of the ram speed and not using all the vram, but something else is going on, but i don\'t know what.\\n\\n\\n\\nI don\'t think the main problem is the ram speed or the used vram, but try fixing them.  \\n  \\nUse something like this to load more stuff in the vram.\\n\\n\\\\-ot \\"blk\\\\\\\\.(1|2|3|4|5|6|7|8|9)\\\\\\\\.ffn\\\\_.\\\\*\\\\_exps\\\\\\\\.weight=CPU\\" -ot \\"blk\\\\\\\\.(\\\\[1-9\\\\]\\\\[0-9\\\\])\\\\\\\\.ffn\\\\_.\\\\*\\\\_exps\\\\\\\\.weight=CPU\\"\\n\\ndelete the 1|2|3 etc to load those layers in vram. The second -ot is for layers 10-95. You could write it with one regex 1|2|3|4..|95.  \\n  \\nYou could also try what u/Secure_Reflection409 did instead of using your vram for more layers.\\n\\n  \\nAgain i don\'t know what the main problem might be. Test the memory read bandwidth, at 2133 with 4 ram modules i guess it would be at 30GB/s. At 2666 it would be \\\\~38. If it\'s close to those number you should have \\\\~3.5t/s at 2133","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2xoj1i","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have no idea what&amp;#39;s the problem.&lt;/p&gt;\\n\\n&lt;p&gt;I previously set my ddr4 ram at 2133 and the cpu at 4.5ghz and used 3 threads and 16gb loaded in the gpu and i got &amp;gt;3t/s.&lt;/p&gt;\\n\\n&lt;p&gt;Now with my ram speed at normal i tested with -ot &amp;#39;.ffn_.*_exps.=CPU&amp;#39;, 7.6GB used, i got 4.3t/s and the gpu was at 13%. Your gpu is twice as fast, so at 4.3t/s it would be at 6.5%. At 0.7t/s it would very well be at 1%. &lt;/p&gt;\\n\\n&lt;p&gt;If it was &amp;gt;2t/s or something like this, it would be because of the ram speed and not using all the vram, but something else is going on, but i don&amp;#39;t know what.&lt;/p&gt;\\n\\n&lt;p&gt;I don&amp;#39;t think the main problem is the ram speed or the used vram, but try fixing them.  &lt;/p&gt;\\n\\n&lt;p&gt;Use something like this to load more stuff in the vram.&lt;/p&gt;\\n\\n&lt;p&gt;-ot &amp;quot;blk\\\\.(1|2|3|4|5|6|7|8|9)\\\\.ffn_.*_exps\\\\.weight=CPU&amp;quot; -ot &amp;quot;blk\\\\.([1-9][0-9])\\\\.ffn_.*_exps\\\\.weight=CPU&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;delete the 1|2|3 etc to load those layers in vram. The second -ot is for layers 10-95. You could write it with one regex 1|2|3|4..|95.  &lt;/p&gt;\\n\\n&lt;p&gt;You could also try what &lt;a href=\\"/u/Secure_Reflection409\\"&gt;u/Secure_Reflection409&lt;/a&gt; did instead of using your vram for more layers.&lt;/p&gt;\\n\\n&lt;p&gt;Again i don&amp;#39;t know what the main problem might be. Test the memory read bandwidth, at 2133 with 4 ram modules i guess it would be at 30GB/s. At 2666 it would be ~38. If it&amp;#39;s close to those number you should have ~3.5t/s at 2133&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lysmo9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lysmo9/qwen3235ba22b_07ts_hardware_or_configuration/n2xoj1i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752429416,"author_flair_text":null,"treatment_tags":[],"created_utc":1752429416,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2xe70h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ConnectionOutside485","can_mod_post":false,"created_utc":1752426445,"send_replies":true,"parent_id":"t1_n2wo2zf","score":1,"author_fullname":"t2_tfa1mcp1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"With `-ot \'.ffn_.*_exps.=CPU\'` (as suggested by `MDT-49`) I get 8GB VRAM used and 1% utilisation during inference, so you are absolutely right that my GPU is essentially not being utilised.\\n\\nllama-server is compiled with CUDA.\\n\\nFrom what I understand, I need to adjust the `-ot` parameter to leave more on the GPU?\\n\\nEDIT: Corrected who suggested the `-ot` parameters.","edited":1752426682,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2xe70h","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;With &lt;code&gt;-ot &amp;#39;.ffn_.*_exps.=CPU&amp;#39;&lt;/code&gt; (as suggested by &lt;code&gt;MDT-49&lt;/code&gt;) I get 8GB VRAM used and 1% utilisation during inference, so you are absolutely right that my GPU is essentially not being utilised.&lt;/p&gt;\\n\\n&lt;p&gt;llama-server is compiled with CUDA.&lt;/p&gt;\\n\\n&lt;p&gt;From what I understand, I need to adjust the &lt;code&gt;-ot&lt;/code&gt; parameter to leave more on the GPU?&lt;/p&gt;\\n\\n&lt;p&gt;EDIT: Corrected who suggested the &lt;code&gt;-ot&lt;/code&gt; parameters.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lysmo9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lysmo9/qwen3235ba22b_07ts_hardware_or_configuration/n2xe70h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752426445,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2wo2zf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"makistsa","can_mod_post":false,"created_utc":1752418580,"send_replies":true,"parent_id":"t3_1lysmo9","score":1,"author_fullname":"t2_3l1o090d","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Your ram speed is slow, but even then you should have higher t/s. With 3 p-cores, 50% faster ram, and 16gb gpu i get &gt;5t/s using a q3 quant.\\n\\nYour ram seems to be able to run at least at 2666. Try to fix that.\\n\\nCheck how much vram you are using. With 24gb vram and all exps on CPU you probably have a lot of it unused\\n\\nBut the main problem should be something else. Check if the gpu is even working at all during inference\\n\\nEdit: Is the llama server compiled with cuda?\\n\\nEdit2: I did a quick test with your settings. Your gpu is definitely not doing anything.","edited":1752420099,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2wo2zf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Your ram speed is slow, but even then you should have higher t/s. With 3 p-cores, 50% faster ram, and 16gb gpu i get &amp;gt;5t/s using a q3 quant.&lt;/p&gt;\\n\\n&lt;p&gt;Your ram seems to be able to run at least at 2666. Try to fix that.&lt;/p&gt;\\n\\n&lt;p&gt;Check how much vram you are using. With 24gb vram and all exps on CPU you probably have a lot of it unused&lt;/p&gt;\\n\\n&lt;p&gt;But the main problem should be something else. Check if the gpu is even working at all during inference&lt;/p&gt;\\n\\n&lt;p&gt;Edit: Is the llama server compiled with cuda?&lt;/p&gt;\\n\\n&lt;p&gt;Edit2: I did a quick test with your settings. Your gpu is definitely not doing anything.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lysmo9/qwen3235ba22b_07ts_hardware_or_configuration/n2wo2zf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752418580,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lysmo9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2wryum","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Secure_Reflection409","can_mod_post":false,"created_utc":1752419750,"send_replies":true,"parent_id":"t3_1lysmo9","score":1,"author_fullname":"t2_by77ogdhr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Use a draft model as per the thread I made the other day.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2wryum","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Use a draft model as per the thread I made the other day.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lysmo9/qwen3235ba22b_07ts_hardware_or_configuration/n2wryum/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752419750,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lysmo9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2x98kb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"p4s2wd","can_mod_post":false,"created_utc":1752424979,"send_replies":true,"parent_id":"t3_1lysmo9","score":1,"author_fullname":"t2_3tplbw0t","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Try this one [https://github.com/ikawrakow/ik\\\\_llama.cpp/](https://github.com/ikawrakow/ik_llama.cpp/)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2x98kb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Try this one &lt;a href=\\"https://github.com/ikawrakow/ik_llama.cpp/\\"&gt;https://github.com/ikawrakow/ik_llama.cpp/&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lysmo9/qwen3235ba22b_07ts_hardware_or_configuration/n2x98kb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752424979,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lysmo9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2y06j6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ConnectionOutside485","can_mod_post":false,"created_utc":1752432890,"send_replies":true,"parent_id":"t1_n2xse1g","score":1,"author_fullname":"t2_tfa1mcp1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The other GPU is a GeForce GTX 1050 2GB VRAM. Very literally not worth mentioning in this context! :)\\n\\nThanks for the suggestion! I\'ve avoided putting 4 threads to it so far because the machine is actively being used by someone sat at it and I am trying to not interfere with their use (so some experiments such as this will wait until they\'ve retired to bed).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2y06j6","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The other GPU is a GeForce GTX 1050 2GB VRAM. Very literally not worth mentioning in this context! :)&lt;/p&gt;\\n\\n&lt;p&gt;Thanks for the suggestion! I&amp;#39;ve avoided putting 4 threads to it so far because the machine is actively being used by someone sat at it and I am trying to not interfere with their use (so some experiments such as this will wait until they&amp;#39;ve retired to bed).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lysmo9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lysmo9/qwen3235ba22b_07ts_hardware_or_configuration/n2y06j6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752432890,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2xse1g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Total_Activity_7550","can_mod_post":false,"created_utc":1752430561,"send_replies":true,"parent_id":"t3_1lysmo9","score":1,"author_fullname":"t2_nwfj64go","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Among other valuable suggestions, try setting --threads &lt;physical CPU cores&gt; --numa distribute\\n\\n\\nUsing more threads reduces performance.\\nI don\'t now what is another GPU, but maybe you can add it if it has comparable performance.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2xse1g","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Among other valuable suggestions, try setting --threads &amp;lt;physical CPU cores&amp;gt; --numa distribute&lt;/p&gt;\\n\\n&lt;p&gt;Using more threads reduces performance.\\nI don&amp;#39;t now what is another GPU, but maybe you can add it if it has comparable performance.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lysmo9/qwen3235ba22b_07ts_hardware_or_configuration/n2xse1g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752430561,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lysmo9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2yvtlj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Marksta","can_mod_post":false,"created_utc":1752442333,"send_replies":true,"parent_id":"t3_1lysmo9","score":1,"author_fullname":"t2_559a1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Is this ik_llama.cpp or llama.cpp? That first command you have \\"exps=CPU\\" is for ik_llama.cpp only I\'m pretty sure. Since you\'re doing big MoE with tons of it on CPU, you should switch to ik_llama.cpp. Then you can use that shortcut to toss all the experts onto CPU and probably pick up a lot of speed too with little change to the command.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2yvtlj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Is this ik_llama.cpp or llama.cpp? That first command you have &amp;quot;exps=CPU&amp;quot; is for ik_llama.cpp only I&amp;#39;m pretty sure. Since you&amp;#39;re doing big MoE with tons of it on CPU, you should switch to ik_llama.cpp. Then you can use that shortcut to toss all the experts onto CPU and probably pick up a lot of speed too with little change to the command.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lysmo9/qwen3235ba22b_07ts_hardware_or_configuration/n2yvtlj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752442333,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lysmo9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2y5dc0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ConnectionOutside485","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2y4alg","score":2,"author_fullname":"t2_tfa1mcp1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I added an edit to the opening post as the base issue has been discovered:\\n\\n&gt;The issue turned out to be an old version of llama.cpp. Upgrading to the latest version as of now (b5890) resulted in 3.3t/s!","edited":false,"author_flair_css_class":null,"name":"t1_n2y5dc0","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I added an edit to the opening post as the base issue has been discovered:&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;The issue turned out to be an old version of llama.cpp. Upgrading to the latest version as of now (b5890) resulted in 3.3t/s!&lt;/p&gt;\\n&lt;/blockquote&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lysmo9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lysmo9/qwen3235ba22b_07ts_hardware_or_configuration/n2y5dc0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752434450,"author_flair_text":null,"collapsed":false,"created_utc":1752434450,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2y4alg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"makistsa","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2xxrn4","score":1,"author_fullname":"t2_3l1o090d","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I run the same thing with 1 thread and 3 threads in a i5. It\'s going to bottleneck fast ddr5, but 0.7 doesn\'t make sense. With 1 thread i got 2t/s","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2y4alg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I run the same thing with 1 thread and 3 threads in a i5. It&amp;#39;s going to bottleneck fast ddr5, but 0.7 doesn&amp;#39;t make sense. With 1 thread i got 2t/s&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lysmo9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lysmo9/qwen3235ba22b_07ts_hardware_or_configuration/n2y4alg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752434123,"author_flair_text":null,"treatment_tags":[],"created_utc":1752434123,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2xxrn4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"YouDontSeemRight","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2wwul8","score":1,"author_fullname":"t2_1b7gjxtue9","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"He\'s running an MOE on a core i3 12th gen...","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2xxrn4","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;He&amp;#39;s running an MOE on a core i3 12th gen...&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lysmo9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lysmo9/qwen3235ba22b_07ts_hardware_or_configuration/n2xxrn4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752432167,"author_flair_text":null,"treatment_tags":[],"created_utc":1752432167,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2wwul8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"makistsa","can_mod_post":false,"created_utc":1752421225,"send_replies":true,"parent_id":"t1_n2wg4oo","score":2,"author_fullname":"t2_3l1o090d","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"1 thread 4.5ghz, 16gb vram, q3 quant. 2.08t/s\\n\\n3 threads 4.95t/s\\n\\nIt\'s not the cpu the bottleneck. He may not have compiled llama server with cuda or installed cuda properly.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2wwul8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;1 thread 4.5ghz, 16gb vram, q3 quant. 2.08t/s&lt;/p&gt;\\n\\n&lt;p&gt;3 threads 4.95t/s&lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s not the cpu the bottleneck. He may not have compiled llama server with cuda or installed cuda properly.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lysmo9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lysmo9/qwen3235ba22b_07ts_hardware_or_configuration/n2wwul8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752421225,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2wg4oo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"YouDontSeemRight","can_mod_post":false,"created_utc":1752416104,"send_replies":true,"parent_id":"t3_1lysmo9","score":1,"author_fullname":"t2_1b7gjxtue9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The issue is qwen 235B is an MOE. You can override the expert layers and send them to CPU while keeping the static layers in GPU. Unfortunately though your CPU is garbage and definitely the bottle neck. I also don\'t think your -ot regex is correct","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2wg4oo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The issue is qwen 235B is an MOE. You can override the expert layers and send them to CPU while keeping the static layers in GPU. Unfortunately though your CPU is garbage and definitely the bottle neck. I also don&amp;#39;t think your -ot regex is correct&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lysmo9/qwen3235ba22b_07ts_hardware_or_configuration/n2wg4oo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752416104,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lysmo9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]'),r=()=>e.jsx(t,{data:l});export{r as default};
