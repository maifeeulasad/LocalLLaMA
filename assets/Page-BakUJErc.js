import{j as e}from"./index-BpC9hjVs.js";import{R as t}from"./RedditPostRenderer-BEo6AnSR.js";import"./index-DwkJHX1_.js";const l=JSON.parse('[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Context here: WSLv2, Win11, Blackwell Pro 6000 workstation.\\n\\nI\'ve beaten my head against the wall with W8A8 FP8 support and kind of loosely eyed NVFP4 from a distance, fully expecting it to be a nightmare. Like may of you I\'ve seen on here, I went through the gauntlet and very specific hell of trying to build vllm + flash-attention + flashinfer from HEAD on nightly pytorch to get W8A8 support only to have things blow up in my face. Partial CUTLASS support, lack of Gemma-3 vision support, flash-attention version failures when combined with certain models, flashinfer failures, etc.\\n\\nSo my question to the community: has anyone gotten FP8 support working in Blackwell and lived to tell the tale? What about TensorRT-LLM w/NVFP4 support? If so - got any pointers for how to do it?\\n\\nFully acknowledging that vllm Blackwell enablement isn\'t done: [link](https://github.com/vllm-project/vllm/issues/18153), but should be done enough to work at this point?\\n\\nIdeally we could get a set of gists together on github to automate the setup of both environments that we all collaborate on to unstick this, assuming I\'m not just completely failing at something obvious.\\n\\nPart of the problem as well seems to be in model choice; I\'ve been specifically trying to get a Gemma-3-27b + Devstral-Small stack together and going for various Roo pipeline steps, and it seems like running those newer models in the TensorRT-LLM ecosystem is extra painful.\\n\\nedit: Lest I be the asshole just generally complaining and asking for things without giving back, here\'s a current(ish?) version of a script to build vllm and deps from HEAD that I\'ve been using locally below in comments. Could be augmented to calculate the correct MAX_JOBS for `flash-attention` and `vllm` builds based on available system memory; right now I have it calibrated for my ~96GB system ram I\'m allocating in WSLv2.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Blackwell FP8 W8A8 NVFP4 support discussion","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lx4zpr","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.91,"author_flair_background_color":null,"subreddit_type":"public","ups":8,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_b78412ov","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":8,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1752243703,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1752235114,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Context here: WSLv2, Win11, Blackwell Pro 6000 workstation.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ve beaten my head against the wall with W8A8 FP8 support and kind of loosely eyed NVFP4 from a distance, fully expecting it to be a nightmare. Like may of you I&amp;#39;ve seen on here, I went through the gauntlet and very specific hell of trying to build vllm + flash-attention + flashinfer from HEAD on nightly pytorch to get W8A8 support only to have things blow up in my face. Partial CUTLASS support, lack of Gemma-3 vision support, flash-attention version failures when combined with certain models, flashinfer failures, etc.&lt;/p&gt;\\n\\n&lt;p&gt;So my question to the community: has anyone gotten FP8 support working in Blackwell and lived to tell the tale? What about TensorRT-LLM w/NVFP4 support? If so - got any pointers for how to do it?&lt;/p&gt;\\n\\n&lt;p&gt;Fully acknowledging that vllm Blackwell enablement isn&amp;#39;t done: &lt;a href=\\"https://github.com/vllm-project/vllm/issues/18153\\"&gt;link&lt;/a&gt;, but should be done enough to work at this point?&lt;/p&gt;\\n\\n&lt;p&gt;Ideally we could get a set of gists together on github to automate the setup of both environments that we all collaborate on to unstick this, assuming I&amp;#39;m not just completely failing at something obvious.&lt;/p&gt;\\n\\n&lt;p&gt;Part of the problem as well seems to be in model choice; I&amp;#39;ve been specifically trying to get a Gemma-3-27b + Devstral-Small stack together and going for various Roo pipeline steps, and it seems like running those newer models in the TensorRT-LLM ecosystem is extra painful.&lt;/p&gt;\\n\\n&lt;p&gt;edit: Lest I be the asshole just generally complaining and asking for things without giving back, here&amp;#39;s a current(ish?) version of a script to build vllm and deps from HEAD that I&amp;#39;ve been using locally below in comments. Could be augmented to calculate the correct MAX_JOBS for &lt;code&gt;flash-attention&lt;/code&gt; and &lt;code&gt;vllm&lt;/code&gt; builds based on available system memory; right now I have it calibrated for my ~96GB system ram I&amp;#39;m allocating in WSLv2.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/w9EtvIrUzqo5Lu-tyiPoB9wYov3Zce-6YSW9Kr_hD50.png?auto=webp&amp;s=a8f7bf991ab8e8c135c5eb5a289e66e89c97a09b","width":1200,"height":600},"resolutions":[{"url":"https://external-preview.redd.it/w9EtvIrUzqo5Lu-tyiPoB9wYov3Zce-6YSW9Kr_hD50.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f4ce923d5f84359f99cae7bf1ae54488a8122402","width":108,"height":54},{"url":"https://external-preview.redd.it/w9EtvIrUzqo5Lu-tyiPoB9wYov3Zce-6YSW9Kr_hD50.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=23347a018f59eee8b2601cd17d3f163a314f6d41","width":216,"height":108},{"url":"https://external-preview.redd.it/w9EtvIrUzqo5Lu-tyiPoB9wYov3Zce-6YSW9Kr_hD50.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=22b0db7c43cd7eb9c413d0a9650aeaa30d754958","width":320,"height":160},{"url":"https://external-preview.redd.it/w9EtvIrUzqo5Lu-tyiPoB9wYov3Zce-6YSW9Kr_hD50.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=824d367ada5c76edcccc96f6433c6b2d58001226","width":640,"height":320},{"url":"https://external-preview.redd.it/w9EtvIrUzqo5Lu-tyiPoB9wYov3Zce-6YSW9Kr_hD50.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=889dd00ece8bf23b9817fadb20775fa36640bfbb","width":960,"height":480},{"url":"https://external-preview.redd.it/w9EtvIrUzqo5Lu-tyiPoB9wYov3Zce-6YSW9Kr_hD50.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2bec6d3d2649ff2d8c7a94a1f56359e03275dc20","width":1080,"height":540}],"variants":{},"id":"w9EtvIrUzqo5Lu-tyiPoB9wYov3Zce-6YSW9Kr_hD50"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lx4zpr","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Kitchen-Year-8434","discussion_type":null,"num_comments":13,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lx4zpr/blackwell_fp8_w8a8_nvfp4_support_discussion/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lx4zpr/blackwell_fp8_w8a8_nvfp4_support_discussion/","subreddit_subscribers":497826,"created_utc":1752235114,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ko4l3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Kitchen-Year-8434","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2khql5","score":1,"author_fullname":"t2_b78412ov","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah, TIL you can paste code in reddit if you indent a block by 4 spaces.\\n\\nBecause WTF. /sigh\\n\\nThanks for the callout on cu129 and numpy pinning; I\'ll probably need to revise w/that once I\'m done burning money on electricity with these insanely bloated flash-attention builds locally.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2ko4l3","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, TIL you can paste code in reddit if you indent a block by 4 spaces.&lt;/p&gt;\\n\\n&lt;p&gt;Because WTF. /sigh&lt;/p&gt;\\n\\n&lt;p&gt;Thanks for the callout on cu129 and numpy pinning; I&amp;#39;ll probably need to revise w/that once I&amp;#39;m done burning money on electricity with these insanely bloated flash-attention builds locally.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx4zpr","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx4zpr/blackwell_fp8_w8a8_nvfp4_support_discussion/n2ko4l3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752250836,"author_flair_text":null,"treatment_tags":[],"created_utc":1752250836,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2khql5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"lois25","can_mod_post":false,"created_utc":1752249019,"send_replies":true,"parent_id":"t1_n2jzht8","score":1,"author_fullname":"t2_5kg2cfp1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I have a very similar script but reddit is not letting me post it. For the FlashInfer, Xformers,etc.. I also mention \\\\`-extra-index-url [https://download.pytorch.org/whl/nightly/cu129](https://download.pytorch.org/whl/nightly/cu129)\\\\` so that it stops trying to downscale my torch even with --no-build-isolation . Works great on cu129 for me. I also build a custom transformers at the end to apply whisper patch on top of 4.53.1. \\n\\nAs you build those scripts and keep the environment updated over time, have to pay special attention that numpy stays at 2.2.6 and not 2.3.x as that breaks some other packages. ie. If you re-install mistral-common to run the latest devstral.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2khql5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have a very similar script but reddit is not letting me post it. For the FlashInfer, Xformers,etc.. I also mention `-extra-index-url &lt;a href=\\"https://download.pytorch.org/whl/nightly/cu129\\"&gt;https://download.pytorch.org/whl/nightly/cu129&lt;/a&gt;` so that it stops trying to downscale my torch even with --no-build-isolation . Works great on cu129 for me. I also build a custom transformers at the end to apply whisper patch on top of 4.53.1. &lt;/p&gt;\\n\\n&lt;p&gt;As you build those scripts and keep the environment updated over time, have to pay special attention that numpy stays at 2.2.6 and not 2.3.x as that breaks some other packages. ie. If you re-install mistral-common to run the latest devstral.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx4zpr","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx4zpr/blackwell_fp8_w8a8_nvfp4_support_discussion/n2khql5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752249019,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2jzht8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Kitchen-Year-8434","can_mod_post":false,"created_utc":1752243877,"send_replies":true,"parent_id":"t3_1lx4zpr","score":4,"author_fullname":"t2_b78412ov","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Assuming I can ever get reddit to format a code block correctly, here\'s the script I\'m using to build vllm locally for anyone else that\'s in the market:\\n\\n    #!/bin/bash\\n\\n    # Constrain to blackwell arch; fallback fails with missing kernel impl anyway on older\\n    export CMAKE_CUDA_ARCHITECTURES=\\"120\\"\\n    #export CMAKE_CUDA_ARCHITECTURES=\\"80;86;89;90;100;120\\"\\n    #export TORCH_CUDA_ARCH_LIST=\\"8.0;8.6;8.9;9.0;10.0;12.0+PTX\\"\\n    export TORCH_CUDA_ARCH_LIST=\\"12.0+PTX\\"\\n    \\n    # Generally will be memory constrained; these pytorch / CUDA compiles are memory hogs.\\n    # Seen anything from 5G/job to 15G.\\n    export MAX_JOBS=8\\n    \\n    # Consider mapping directly to CUDA 12.8 or 12.9 depending on what new and stupid things fail\\n    export CUDA_HOME=/usr/local/cuda\\n    \\n    resume=\\"\\"\\n    if [[ -n $1 ]]; then\\n      if [[ $1 != \\"-r\\" ]]; then\\n        echo \\"usage: build_vllm.sh [-r]\\"\\n        echo \\" -r will optionally resume a prior failed build w/out nuking local repos and build progress\\"\\n        exit 1\\n      else\\n        resume=\\"yes\\"\\n      fi\\n    fi\\n    \\n    if [[ -z $resume ]]; then\\n        echo \\"Deleting old repo checkouts\\"\\n        rm -rf xformers\\n        rm -rf flash-attention\\n        rm -rf flashinfer\\n        rm -rf vllm\\n    \\n        echo \\"Cloning new HEAD for all required dependencies\\"\\n        git clone https://github.com/facebookresearch/xformers.git\\n        git clone https://github.com/Dao-AILab/flash-attention.git\\n        git clone https://github.com/flashinfer-ai/flashinfer.git\\n        git clone https://github.com/vllm-project/vllm.git\\n    else\\n        echo \\"Resuming previous in-progress build\\"\\n    fi\\n    \\n    # Some proactive build support\\n    pip3 install packaging ninja wheel\\n    \\n    # Install PyTorch nightly with CUDA 12.8 support\\n    # At this point we could also clone and build pytorch from HEAD but then a bunch of other stupid stuff\\n    # seems to break. Guess CI on the project is less that comprehensive?\\n    pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\\n    \\n    # Build FlashAttention\\n    export MAX_JOBS=8\\n    cd flash-attention\\n    git pull\\n    pip install . --no-build-isolation\\n    # Capture SHA for later submodule version sync up (defensive posturing ftw)\\n    flash_sha=$(git rev-parse HEAD)\\n    cd ..\\n    \\n    # Build xformers\\n    cd xformers\\n    git pull\\n    git submodule update --init --recursive\\n    # Make sure our flash-attention\'s line up. This should be redundant since I don\'t think this actually _builds_ but at this point I trust nothing.\\n    $(cd third_party/flash-attention; git checkout $flash_sha)\\n    pip install . --no-build-isolation\\n    cd ..\\n    \\n    # Build FlashInfer\\n    cd flashinfer\\n    git pull\\n    pip install . --no-build-isolation\\n    cd ..\\n    \\n    # Build vLLM; this one\'s a memory hog\\n    export MAX_JOBS=8\\n    cd vllm\\n    git pull\\n    python use_existing_torch.py\\n    pip install -r requirements/build.txt --no-build-isolation\\n    pip install . --no-build-isolation\\n    cd ..\\n    \\n    echo \\"Build completed with CUDA architectures: ${CMAKE_CUDA_ARCHITECTURES}\\"\\n    echo \\"PyTorch CUDA arch list: ${TORCH_CUDA_ARCH_LIST}\\"","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2jzht8","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Assuming I can ever get reddit to format a code block correctly, here&amp;#39;s the script I&amp;#39;m using to build vllm locally for anyone else that&amp;#39;s in the market:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;#!/bin/bash\\n\\n# Constrain to blackwell arch; fallback fails with missing kernel impl anyway on older\\nexport CMAKE_CUDA_ARCHITECTURES=&amp;quot;120&amp;quot;\\n#export CMAKE_CUDA_ARCHITECTURES=&amp;quot;80;86;89;90;100;120&amp;quot;\\n#export TORCH_CUDA_ARCH_LIST=&amp;quot;8.0;8.6;8.9;9.0;10.0;12.0+PTX&amp;quot;\\nexport TORCH_CUDA_ARCH_LIST=&amp;quot;12.0+PTX&amp;quot;\\n\\n# Generally will be memory constrained; these pytorch / CUDA compiles are memory hogs.\\n# Seen anything from 5G/job to 15G.\\nexport MAX_JOBS=8\\n\\n# Consider mapping directly to CUDA 12.8 or 12.9 depending on what new and stupid things fail\\nexport CUDA_HOME=/usr/local/cuda\\n\\nresume=&amp;quot;&amp;quot;\\nif [[ -n $1 ]]; then\\n  if [[ $1 != &amp;quot;-r&amp;quot; ]]; then\\n    echo &amp;quot;usage: build_vllm.sh [-r]&amp;quot;\\n    echo &amp;quot; -r will optionally resume a prior failed build w/out nuking local repos and build progress&amp;quot;\\n    exit 1\\n  else\\n    resume=&amp;quot;yes&amp;quot;\\n  fi\\nfi\\n\\nif [[ -z $resume ]]; then\\n    echo &amp;quot;Deleting old repo checkouts&amp;quot;\\n    rm -rf xformers\\n    rm -rf flash-attention\\n    rm -rf flashinfer\\n    rm -rf vllm\\n\\n    echo &amp;quot;Cloning new HEAD for all required dependencies&amp;quot;\\n    git clone https://github.com/facebookresearch/xformers.git\\n    git clone https://github.com/Dao-AILab/flash-attention.git\\n    git clone https://github.com/flashinfer-ai/flashinfer.git\\n    git clone https://github.com/vllm-project/vllm.git\\nelse\\n    echo &amp;quot;Resuming previous in-progress build&amp;quot;\\nfi\\n\\n# Some proactive build support\\npip3 install packaging ninja wheel\\n\\n# Install PyTorch nightly with CUDA 12.8 support\\n# At this point we could also clone and build pytorch from HEAD but then a bunch of other stupid stuff\\n# seems to break. Guess CI on the project is less that comprehensive?\\npip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\\n\\n# Build FlashAttention\\nexport MAX_JOBS=8\\ncd flash-attention\\ngit pull\\npip install . --no-build-isolation\\n# Capture SHA for later submodule version sync up (defensive posturing ftw)\\nflash_sha=$(git rev-parse HEAD)\\ncd ..\\n\\n# Build xformers\\ncd xformers\\ngit pull\\ngit submodule update --init --recursive\\n# Make sure our flash-attention&amp;#39;s line up. This should be redundant since I don&amp;#39;t think this actually _builds_ but at this point I trust nothing.\\n$(cd third_party/flash-attention; git checkout $flash_sha)\\npip install . --no-build-isolation\\ncd ..\\n\\n# Build FlashInfer\\ncd flashinfer\\ngit pull\\npip install . --no-build-isolation\\ncd ..\\n\\n# Build vLLM; this one&amp;#39;s a memory hog\\nexport MAX_JOBS=8\\ncd vllm\\ngit pull\\npython use_existing_torch.py\\npip install -r requirements/build.txt --no-build-isolation\\npip install . --no-build-isolation\\ncd ..\\n\\necho &amp;quot;Build completed with CUDA architectures: ${CMAKE_CUDA_ARCHITECTURES}&amp;quot;\\necho &amp;quot;PyTorch CUDA arch list: ${TORCH_CUDA_ARCH_LIST}&amp;quot;\\n&lt;/code&gt;&lt;/pre&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx4zpr/blackwell_fp8_w8a8_nvfp4_support_discussion/n2jzht8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752243877,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lx4zpr","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2jpdjy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DAlmighty","can_mod_post":false,"created_utc":1752240812,"send_replies":true,"parent_id":"t3_1lx4zpr","score":2,"author_fullname":"t2_a04uj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I’ve failed so hard getting vLLM to work, it makes me tear up.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2jpdjy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I’ve failed so hard getting vLLM to work, it makes me tear up.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx4zpr/blackwell_fp8_w8a8_nvfp4_support_discussion/n2jpdjy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752240812,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lx4zpr","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2jdsr6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Conscious_Cut_6144","can_mod_post":false,"created_utc":1752236905,"send_replies":true,"parent_id":"t1_n2jcqu0","score":1,"author_fullname":"t2_9hl4ymvj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Could be WSL related? But in the link above I got it working on raw Ubuntu.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2jdsr6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Could be WSL related? But in the link above I got it working on raw Ubuntu.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx4zpr","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx4zpr/blackwell_fp8_w8a8_nvfp4_support_discussion/n2jdsr6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752236905,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2lczr6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Conscious_Cut_6144","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2jhfr1","score":1,"author_fullname":"t2_9hl4ymvj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"RedHatAI (previously NeuralMagic) is the primary contributor to VLLM, so their models should be well supported.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2lczr6","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;RedHatAI (previously NeuralMagic) is the primary contributor to VLLM, so their models should be well supported.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx4zpr","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx4zpr/blackwell_fp8_w8a8_nvfp4_support_discussion/n2lczr6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752257780,"author_flair_text":null,"treatment_tags":[],"created_utc":1752257780,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2jhfr1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Kitchen-Year-8434","can_mod_post":false,"created_utc":1752238190,"send_replies":true,"parent_id":"t1_n2jcqu0","score":1,"author_fullname":"t2_b78412ov","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I saw that other PR and built vllm locally post that to try it out, however ran into issues with any of the FP8 / FP8-dynamic models at least from RedHatAI [link](https://huggingface.co/RedHatAI/models?search=fp8). Don\'t recall exactly which other models I tried; might have been more Gemma-3 issues w/getting vision to work now that I think about it. It\'s been a few days which is effectively a year or two in LLM tinkering time /sigh.\\n\\nThat post you linked mentions \\"RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic\\" specifically which is part of what steered me to try out their other FP8 models. I also recall trying one or both of the Devstral-Small-2505 FP8 models (nm-testing/Devstral-Small-2505-FP8-dynamic and textgeflecht/Devstral-Small-2505-FP8-llmcompressor) and running into issues there, which is not helpful unless I were to rebuild now and retry to confirm a) that it failed, and b) how it failed.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2jhfr1","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I saw that other PR and built vllm locally post that to try it out, however ran into issues with any of the FP8 / FP8-dynamic models at least from RedHatAI &lt;a href=\\"https://huggingface.co/RedHatAI/models?search=fp8\\"&gt;link&lt;/a&gt;. Don&amp;#39;t recall exactly which other models I tried; might have been more Gemma-3 issues w/getting vision to work now that I think about it. It&amp;#39;s been a few days which is effectively a year or two in LLM tinkering time /sigh.&lt;/p&gt;\\n\\n&lt;p&gt;That post you linked mentions &amp;quot;RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic&amp;quot; specifically which is part of what steered me to try out their other FP8 models. I also recall trying one or both of the Devstral-Small-2505 FP8 models (nm-testing/Devstral-Small-2505-FP8-dynamic and textgeflecht/Devstral-Small-2505-FP8-llmcompressor) and running into issues there, which is not helpful unless I were to rebuild now and retry to confirm a) that it failed, and b) how it failed.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx4zpr","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx4zpr/blackwell_fp8_w8a8_nvfp4_support_discussion/n2jhfr1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752238190,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2jcqu0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"smahs9","can_mod_post":false,"created_utc":1752236521,"send_replies":true,"parent_id":"t3_1lx4zpr","score":1,"author_fullname":"t2_neyagc1uz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"That epic link you posted is not for Pro 6000. Here is the [w8a8 PR](https://github.com/vllm-project/vllm/pull/17280) merged last week (which was also [discussed](https://www.reddit.com/r/LocalLLaMA/comments/1lq79xx/fp8_fixed_on_vllm_for_rtx_pro_6000_and_rtx_5000/) here). Other than fp8 on vllm, tensorrt-llm [supports](https://github.com/NVIDIA/TensorRT-LLM/issues/5018) nvfp4.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2jcqu0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That epic link you posted is not for Pro 6000. Here is the &lt;a href=\\"https://github.com/vllm-project/vllm/pull/17280\\"&gt;w8a8 PR&lt;/a&gt; merged last week (which was also &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1lq79xx/fp8_fixed_on_vllm_for_rtx_pro_6000_and_rtx_5000/\\"&gt;discussed&lt;/a&gt; here). Other than fp8 on vllm, tensorrt-llm &lt;a href=\\"https://github.com/NVIDIA/TensorRT-LLM/issues/5018\\"&gt;supports&lt;/a&gt; nvfp4.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx4zpr/blackwell_fp8_w8a8_nvfp4_support_discussion/n2jcqu0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752236521,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lx4zpr","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2kcl2q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"blackwell_tart","can_mod_post":false,"created_utc":1752247590,"send_replies":true,"parent_id":"t3_1lx4zpr","score":1,"author_fullname":"t2_1t7r9dkpud","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Ah sweet synchronicity! Today we take delivery of a pair of Blackwell Workstation Pro 6000s and will be attempting to get vLLM running with them this weekend.\\n\\nYour script will be most useful, you have the thanks of my team for being kind enough to publish your work.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2kcl2q","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ah sweet synchronicity! Today we take delivery of a pair of Blackwell Workstation Pro 6000s and will be attempting to get vLLM running with them this weekend.&lt;/p&gt;\\n\\n&lt;p&gt;Your script will be most useful, you have the thanks of my team for being kind enough to publish your work.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx4zpr/blackwell_fp8_w8a8_nvfp4_support_discussion/n2kcl2q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752247590,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lx4zpr","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2l60oz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"lois25","can_mod_post":false,"created_utc":1752255817,"send_replies":true,"parent_id":"t3_1lx4zpr","score":1,"author_fullname":"t2_5kg2cfp1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I am running 2 FP8 models so far. I set up llm\\\\_compressor earlier this week but I have not had the opportunity to quantize some models down to FP8 yet.\\n\\nMistral Small FP8 build from unsloth\\n\\n\\n\\n\\\\`\\\\`\\\\`bash\\n\\nexport TORCH\\\\_CUDA\\\\_ARCH\\\\_LIST=\\"12.0\\"\\n\\nexport VLLM\\\\_ATTENTION\\\\_BACKEND=FLASHINFER\\n\\nvllm serve unsloth/Mistral-Small-3.2-24B-Instruct-2506-FP8 \\\\\\\\\\n\\n\\\\--tokenizer\\\\_mode mistral --config\\\\_format mistral --load\\\\_format mistral --tool-call-parser mistral --enable-auto-tool-choice --limit\\\\_mm\\\\_per\\\\_prompt \'image=10\'\\n\\n\\\\`\\\\`\\\\`\\n\\n\\n\\nHunyuan A13B official FP8 build (tool calling has been buggy/inconsistent for me so far but that\'s on me so far probably not configuring it properly). You need to be on a recent enough build of vllm (I think there was a patch a few days ago for reasoning). Also plenty of slow warnings/info messages for various components (tokenizer, kv\\\\_cache) but appears to be running alright for me.\\n\\n\\n\\n\\\\`\\\\`\\\\`bash\\n\\nexport TORCH\\\\_CUDA\\\\_ARCH\\\\_LIST=\\"12.0\\"\\n\\nunset VLLM\\\\_ATTENTION\\\\_BACKEND\\n\\nvllm serve tencent/Hunyuan-A13B-Instruct-FP8 \\\\\\\\\\n\\n\\\\--dtype fp8 --kv-cache-dtype fp8 --trust-remote-code \\\\\\\\\\n\\n\\\\--reasoning-parser hunyuan\\\\_a13b \\\\\\\\\\n\\n\\\\--tool-call-parser hunyuan --enable-auto-tool-choice --tool-parser-plugin \\"vllm\\\\_inference/hunyuan\\\\_tool\\\\_parser.py\\"\\n\\n\\\\`\\\\`\\\\`","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2l60oz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I am running 2 FP8 models so far. I set up llm_compressor earlier this week but I have not had the opportunity to quantize some models down to FP8 yet.&lt;/p&gt;\\n\\n&lt;p&gt;Mistral Small FP8 build from unsloth&lt;/p&gt;\\n\\n&lt;p&gt;```bash&lt;/p&gt;\\n\\n&lt;p&gt;export TORCH_CUDA_ARCH_LIST=&amp;quot;12.0&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;export VLLM_ATTENTION_BACKEND=FLASHINFER&lt;/p&gt;\\n\\n&lt;p&gt;vllm serve unsloth/Mistral-Small-3.2-24B-Instruct-2506-FP8 \\\\&lt;/p&gt;\\n\\n&lt;p&gt;--tokenizer_mode mistral --config_format mistral --load_format mistral --tool-call-parser mistral --enable-auto-tool-choice --limit_mm_per_prompt &amp;#39;image=10&amp;#39;&lt;/p&gt;\\n\\n&lt;p&gt;```&lt;/p&gt;\\n\\n&lt;p&gt;Hunyuan A13B official FP8 build (tool calling has been buggy/inconsistent for me so far but that&amp;#39;s on me so far probably not configuring it properly). You need to be on a recent enough build of vllm (I think there was a patch a few days ago for reasoning). Also plenty of slow warnings/info messages for various components (tokenizer, kv_cache) but appears to be running alright for me.&lt;/p&gt;\\n\\n&lt;p&gt;```bash&lt;/p&gt;\\n\\n&lt;p&gt;export TORCH_CUDA_ARCH_LIST=&amp;quot;12.0&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;unset VLLM_ATTENTION_BACKEND&lt;/p&gt;\\n\\n&lt;p&gt;vllm serve tencent/Hunyuan-A13B-Instruct-FP8 \\\\&lt;/p&gt;\\n\\n&lt;p&gt;--dtype fp8 --kv-cache-dtype fp8 --trust-remote-code \\\\&lt;/p&gt;\\n\\n&lt;p&gt;--reasoning-parser hunyuan_a13b \\\\&lt;/p&gt;\\n\\n&lt;p&gt;--tool-call-parser hunyuan --enable-auto-tool-choice --tool-parser-plugin &amp;quot;vllm_inference/hunyuan_tool_parser.py&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;```&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx4zpr/blackwell_fp8_w8a8_nvfp4_support_discussion/n2l60oz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752255817,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lx4zpr","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2lk64c","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Few-Welcome3297","can_mod_post":false,"created_utc":1752259865,"send_replies":true,"parent_id":"t3_1lx4zpr","score":1,"author_fullname":"t2_9f8ab953","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Able to run gemma3 fp8 with vision support on 5060Ti 16G\\n\\nhttps://preview.redd.it/d9xxhj9wjacf1.png?width=3408&amp;format=png&amp;auto=webp&amp;s=fdec3aec189bee7b7eb0b437078ec1e4db6d85b6\\n\\nUbuntu 24.04 - 575 Beta Drivers - CUDA Toolkit 12.9 and CUDNN\\n\\nFresh uv venv with python 3.12\\n\\nAdjust MAX\\\\_JOBS as per your memory and use ccache\\n\\n    uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\\n    \\n    MAX_JOBS=3 uv pip install -v --no-build-isolation -U git+https://github.com/facebookresearch/xformers.git@main#egg=xformers\\n    \\n    TORCH_CUDA_ARCH_LIST=\'12.0+PTX\' MAX_JOBS=6  python -m flashinfer.aot\\n    uv pip install --no-build-isolation --verbose .\\n    \\n    FLASHINFER_ENABLE_AOT=1 CCACHE_NOHASHDIR=\\"true\\" CCACHE_DIR=\\"$HOME/.ccache\\" USE_CUDA=1 USE_CUDNN=1 MAX_JOBS=3 TORCH_CUDA_ARCH_LIST=\'12.0+PTX\' VLLM_ATTENTION_BACKEND=FLASH_ATTN VLLM_FLASH_ATTN_VERSION=3 uv pip install -vvv --no-build-isolation -e .\\n    \\n    vllm serve RedHatAI/gemma-3-4b-it-FP8-dynamic --max-model-len 16384 --enable-prefix-caching --max_num_seqs 64 --gpu-memory-utilization 0.95","edited":1752260166,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2lk64c","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Able to run gemma3 fp8 with vision support on 5060Ti 16G&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/d9xxhj9wjacf1.png?width=3408&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fdec3aec189bee7b7eb0b437078ec1e4db6d85b6\\"&gt;https://preview.redd.it/d9xxhj9wjacf1.png?width=3408&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fdec3aec189bee7b7eb0b437078ec1e4db6d85b6&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Ubuntu 24.04 - 575 Beta Drivers - CUDA Toolkit 12.9 and CUDNN&lt;/p&gt;\\n\\n&lt;p&gt;Fresh uv venv with python 3.12&lt;/p&gt;\\n\\n&lt;p&gt;Adjust MAX_JOBS as per your memory and use ccache&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\\n\\nMAX_JOBS=3 uv pip install -v --no-build-isolation -U git+https://github.com/facebookresearch/xformers.git@main#egg=xformers\\n\\nTORCH_CUDA_ARCH_LIST=&amp;#39;12.0+PTX&amp;#39; MAX_JOBS=6  python -m flashinfer.aot\\nuv pip install --no-build-isolation --verbose .\\n\\nFLASHINFER_ENABLE_AOT=1 CCACHE_NOHASHDIR=&amp;quot;true&amp;quot; CCACHE_DIR=&amp;quot;$HOME/.ccache&amp;quot; USE_CUDA=1 USE_CUDNN=1 MAX_JOBS=3 TORCH_CUDA_ARCH_LIST=&amp;#39;12.0+PTX&amp;#39; VLLM_ATTENTION_BACKEND=FLASH_ATTN VLLM_FLASH_ATTN_VERSION=3 uv pip install -vvv --no-build-isolation -e .\\n\\nvllm serve RedHatAI/gemma-3-4b-it-FP8-dynamic --max-model-len 16384 --enable-prefix-caching --max_num_seqs 64 --gpu-memory-utilization 0.95\\n&lt;/code&gt;&lt;/pre&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx4zpr/blackwell_fp8_w8a8_nvfp4_support_discussion/n2lk64c/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752259865,"media_metadata":{"d9xxhj9wjacf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":67,"x":108,"u":"https://preview.redd.it/d9xxhj9wjacf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c7a8d24ccd180fd4cc52bb88bb464361fd1349d3"},{"y":135,"x":216,"u":"https://preview.redd.it/d9xxhj9wjacf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ee773346d75dbeafb644e98fab6041c7a3323905"},{"y":200,"x":320,"u":"https://preview.redd.it/d9xxhj9wjacf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6abd1a43483ffe7cfde8041fd856ac6525ea47b4"},{"y":400,"x":640,"u":"https://preview.redd.it/d9xxhj9wjacf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b944edce5769c14a5771b340814146a722d5eb69"},{"y":600,"x":960,"u":"https://preview.redd.it/d9xxhj9wjacf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ee4dba44f4f84497a413d7d7428fff50045f7083"},{"y":675,"x":1080,"u":"https://preview.redd.it/d9xxhj9wjacf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=60544a63249e4559dc5d9ccd29bd45d70425437d"}],"s":{"y":2130,"x":3408,"u":"https://preview.redd.it/d9xxhj9wjacf1.png?width=3408&amp;format=png&amp;auto=webp&amp;s=fdec3aec189bee7b7eb0b437078ec1e4db6d85b6"},"id":"d9xxhj9wjacf1"}},"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lx4zpr","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ppo1x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Kitchen-Year-8434","can_mod_post":false,"created_utc":1752321237,"send_replies":true,"parent_id":"t1_n2poqi2","score":1,"author_fullname":"t2_b78412ov","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The above is with:\\n&gt;pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\\n\\nGoing to see if cu129 performs any different or otherwise detonates.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ppo1x","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The above is with:&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;pip install --pre torch torchvision torchaudio --index-url &lt;a href=\\"https://download.pytorch.org/whl/nightly/cu128\\"&gt;https://download.pytorch.org/whl/nightly/cu128&lt;/a&gt;&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Going to see if cu129 performs any different or otherwise detonates.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx4zpr","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx4zpr/blackwell_fp8_w8a8_nvfp4_support_discussion/n2ppo1x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752321237,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2poqi2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Kitchen-Year-8434","can_mod_post":false,"created_utc":1752320814,"send_replies":true,"parent_id":"t3_1lx4zpr","score":1,"author_fullname":"t2_b78412ov","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Got a Devstral fp8 working locally w/recent build, looks like it\'s pushing ~ 40t/s on fp8:\\n\\nhttps://huggingface.co/stelterlab/Devstral-Small-2507-FP8\\n\\nRequired grabbing the tekken.json from: https://huggingface.co/mistralai/Devstral-Small-2507/tree/main\\n\\nLaunch script:\\n\\n    export VLLM_ATTENTION_BACKEND=FLASHINFER\\n    export VLLM_FLASH_ATTN_VERSION=2\\n    export VLLM_ALLOW_LONG_MAX_MODEL_LEN=1\\n    vllm serve /home/&lt;user&gt;/src/models/Devstral-Small-2507-FP8 \\\\\\n        --tokenizer_mode mistral \\\\\\n        --config_format mistral \\\\\\n        --load_format mistral \\\\\\n        --tokenizer_mode mistral \\\\\\n        --tool-call-parser mistral \\\\\\n        --enable-auto-tool-choice \\\\\\n        --max-model-len 128000 \\\\\\n        --calculate_kv_scales \\\\\\n        --max-num-seqs 5 \\\\\\n        --gpu-memory-utilization 0.4 \\\\\\n        --kv_cache_dtype fp8 \\\\\\n        --host 192.168.99.2 \\\\\\n        --port 8011\\n\\nNot sure I 100% trust this quant w/errors like the following:\\nWARNING 07-12 07:45:43 [kv_cache.py:130] Using uncalibrated q_scale 1.0 and/or prob_scale 1.0 with fp8 attention. This may cause accuracy issues. Please make sure q/prob scaling factors are available in the fp8 checkpoint.\\n\\nResults look reasonable though... May end up trying to llmcompressor one locally myself. But promising to see things not detonate in flames!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2poqi2","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Got a Devstral fp8 working locally w/recent build, looks like it&amp;#39;s pushing ~ 40t/s on fp8:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/stelterlab/Devstral-Small-2507-FP8\\"&gt;https://huggingface.co/stelterlab/Devstral-Small-2507-FP8&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Required grabbing the tekken.json from: &lt;a href=\\"https://huggingface.co/mistralai/Devstral-Small-2507/tree/main\\"&gt;https://huggingface.co/mistralai/Devstral-Small-2507/tree/main&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Launch script:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;export VLLM_ATTENTION_BACKEND=FLASHINFER\\nexport VLLM_FLASH_ATTN_VERSION=2\\nexport VLLM_ALLOW_LONG_MAX_MODEL_LEN=1\\nvllm serve /home/&amp;lt;user&amp;gt;/src/models/Devstral-Small-2507-FP8 \\\\\\n    --tokenizer_mode mistral \\\\\\n    --config_format mistral \\\\\\n    --load_format mistral \\\\\\n    --tokenizer_mode mistral \\\\\\n    --tool-call-parser mistral \\\\\\n    --enable-auto-tool-choice \\\\\\n    --max-model-len 128000 \\\\\\n    --calculate_kv_scales \\\\\\n    --max-num-seqs 5 \\\\\\n    --gpu-memory-utilization 0.4 \\\\\\n    --kv_cache_dtype fp8 \\\\\\n    --host 192.168.99.2 \\\\\\n    --port 8011\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;Not sure I 100% trust this quant w/errors like the following:\\nWARNING 07-12 07:45:43 [kv_cache.py:130] Using uncalibrated q_scale 1.0 and/or prob_scale 1.0 with fp8 attention. This may cause accuracy issues. Please make sure q/prob scaling factors are available in the fp8 checkpoint.&lt;/p&gt;\\n\\n&lt;p&gt;Results look reasonable though... May end up trying to llmcompressor one locally myself. But promising to see things not detonate in flames!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx4zpr/blackwell_fp8_w8a8_nvfp4_support_discussion/n2poqi2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752320814,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lx4zpr","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]'),r=()=>e.jsx(t,{data:l});export{r as default};
