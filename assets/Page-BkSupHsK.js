import{j as e}from"./index-BOnf-UhU.js";import{R as l}from"./RedditPostRenderer-Ce39qICS.js";import"./index-CDase6VD.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hey everyone,\\nIâ€™m planning to get started with machine learning. Right now, I have an M1 Mac Mini (16GB RAM, 50GB storage left). Will it be enough? \\n\\nAppreciate any advice!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Build a PC or not?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lnsgvy","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.78,"author_flair_background_color":null,"subreddit_type":"public","ups":5,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_xhdq2kf8s","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":5,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751238794,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey everyone,\\nIâ€™m planning to get started with machine learning. Right now, I have an M1 Mac Mini (16GB RAM, 50GB storage left). Will it be enough? &lt;/p&gt;\\n\\n&lt;p&gt;Appreciate any advice!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lnsgvy","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"InternetBest7599","discussion_type":null,"num_comments":15,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lnsgvy/build_a_pc_or_not/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lnsgvy/build_a_pc_or_not/","subreddit_subscribers":493240,"created_utc":1751238794,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0kohd6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Afternoon_4260","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0jie3p","score":1,"author_fullname":"t2_cj9kap4bx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Let's say that by buying a single of these gpu you enable embedding models. Which is really cool! Don't think you'll be able to build full pipeline with llms and stuff. Or it really be a research preview, not something reliable enough to use and explore.\\n\\nImo 48gb vram is a minimum for 20b -&gt; 49b, you could even try some highly quantised 70b.\\n\\nTo give you a bad analogy, today 48gb vram gives you the equivalent of last year chatgpt (kind of). But a single 16gb vram gives you enough to play with embeddings model and some chatgpt preview from 2 years ago.\\n\\nIf you want to play with RAG, where everybody starts, you really need just 16gb vram, but you'll quickly need more.\\n\\nDon't know how much the card you are aiming for is. But get a 3090 ðŸ˜…","edited":false,"author_flair_css_class":null,"name":"t1_n0kohd6","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Let&amp;#39;s say that by buying a single of these gpu you enable embedding models. Which is really cool! Don&amp;#39;t think you&amp;#39;ll be able to build full pipeline with llms and stuff. Or it really be a research preview, not something reliable enough to use and explore.&lt;/p&gt;\\n\\n&lt;p&gt;Imo 48gb vram is a minimum for 20b -&amp;gt; 49b, you could even try some highly quantised 70b.&lt;/p&gt;\\n\\n&lt;p&gt;To give you a bad analogy, today 48gb vram gives you the equivalent of last year chatgpt (kind of). But a single 16gb vram gives you enough to play with embeddings model and some chatgpt preview from 2 years ago.&lt;/p&gt;\\n\\n&lt;p&gt;If you want to play with RAG, where everybody starts, you really need just 16gb vram, but you&amp;#39;ll quickly need more.&lt;/p&gt;\\n\\n&lt;p&gt;Don&amp;#39;t know how much the card you are aiming for is. But get a 3090 ðŸ˜…&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lnsgvy","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsgvy/build_a_pc_or_not/n0kohd6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751290164,"author_flair_text":"llama.cpp","collapsed":false,"created_utc":1751290164,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0jie3p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"InternetBest7599","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0jd6k8","score":1,"author_fullname":"t2_xhdq2kf8s","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"That's why I'm thinking about building it otherwise I would have thought about renting the GPU","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jie3p","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s why I&amp;#39;m thinking about building it otherwise I would have thought about renting the GPU&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsgvy","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsgvy/build_a_pc_or_not/n0jie3p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751269507,"author_flair_text":null,"treatment_tags":[],"created_utc":1751269507,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0jd6k8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Own_Attention_3392","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0j6jag","score":1,"author_fullname":"t2_47dpmv25","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"50 GB of storage wildly inadequate for LLMs. A single quantized model will easily be anywhere from 8 to 40 GB, depending on how many parameters. It have 2 TB of storage and a full terabyte is taken up with models and associated tools/python libraries to run them.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0jd6k8","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;50 GB of storage wildly inadequate for LLMs. A single quantized model will easily be anywhere from 8 to 40 GB, depending on how many parameters. It have 2 TB of storage and a full terabyte is taken up with models and associated tools/python libraries to run them.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsgvy","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsgvy/build_a_pc_or_not/n0jd6k8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751266382,"author_flair_text":null,"treatment_tags":[],"created_utc":1751266382,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0j6jag","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"InternetBest7599","can_mod_post":false,"created_utc":1751262654,"send_replies":true,"parent_id":"t1_n0hrqda","score":1,"author_fullname":"t2_xhdq2kf8s","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"If I do decide to build one, then the GPU will be rtx 5060 ti with 16 gigs of VRAM and real concerning thing rn with my mac mini is its storage 50 gigs left\\n\\nThe end goal for me is NLP","edited":1751264227,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0j6jag","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If I do decide to build one, then the GPU will be rtx 5060 ti with 16 gigs of VRAM and real concerning thing rn with my mac mini is its storage 50 gigs left&lt;/p&gt;\\n\\n&lt;p&gt;The end goal for me is NLP&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsgvy","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsgvy/build_a_pc_or_not/n0j6jag/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751262654,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hrqda","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No_Afternoon_4260","can_mod_post":false,"created_utc":1751241335,"send_replies":true,"parent_id":"t3_1lnsgvy","score":7,"author_fullname":"t2_cj9kap4bx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Rent!!! Sadly cloud is the sensible choice price wise. Those gpu are priced for professionals that use them 90% of the time. Yours will surely be idling 80% of the time you'll never be profitable on that purchase.\\n\\nOn the other hand having some 3090's idling in my living room is the best thing I did to get into this field.\\nJust having the comfort to not think about how long have I ran the instance for or if I've shut it down is amazing. Being able to work a couple hours here and there..\\n\\nOn the other hand I can rent 8xh100 for the price of a good restaurant while I should sell a kidney to buy such rig.\\n\\nChoice is yours my friend\\n\\nIf you want to buy hardware imo 3090 is the only choice if you don't want to buy a 5090 or a rtx pro","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hrqda","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Rent!!! Sadly cloud is the sensible choice price wise. Those gpu are priced for professionals that use them 90% of the time. Yours will surely be idling 80% of the time you&amp;#39;ll never be profitable on that purchase.&lt;/p&gt;\\n\\n&lt;p&gt;On the other hand having some 3090&amp;#39;s idling in my living room is the best thing I did to get into this field.\\nJust having the comfort to not think about how long have I ran the instance for or if I&amp;#39;ve shut it down is amazing. Being able to work a couple hours here and there..&lt;/p&gt;\\n\\n&lt;p&gt;On the other hand I can rent 8xh100 for the price of a good restaurant while I should sell a kidney to buy such rig.&lt;/p&gt;\\n\\n&lt;p&gt;Choice is yours my friend&lt;/p&gt;\\n\\n&lt;p&gt;If you want to buy hardware imo 3090 is the only choice if you don&amp;#39;t want to buy a 5090 or a rtx pro&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsgvy/build_a_pc_or_not/n0hrqda/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751241335,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lnsgvy","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hn474","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Snipedzoi","can_mod_post":false,"created_utc":1751239666,"send_replies":true,"parent_id":"t3_1lnsgvy","score":1,"author_fullname":"t2_6civse82","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"idk if you'll run anything big locally but you could code with online models","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hn474","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;idk if you&amp;#39;ll run anything big locally but you could code with online models&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsgvy/build_a_pc_or_not/n0hn474/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751239666,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsgvy","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hoyer","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SM8085","can_mod_post":false,"created_utc":1751240328,"send_replies":true,"parent_id":"t3_1lnsgvy","score":1,"author_fullname":"t2_14vikjao97","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"16GB RAM can hold some smaller models.  For instance, Gemma3 4B &amp; Qwen3 4B should be loadable.  I bought an old workstation with as much RAM as I could get for an LLM rig because I wanted to open Chrome on my main PC again.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hoyer","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;16GB RAM can hold some smaller models.  For instance, Gemma3 4B &amp;amp; Qwen3 4B should be loadable.  I bought an old workstation with as much RAM as I could get for an LLM rig because I wanted to open Chrome on my main PC again.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsgvy/build_a_pc_or_not/n0hoyer/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751240328,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsgvy","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0i87pr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Thick-Protection-458","can_mod_post":false,"created_utc":1751247523,"send_replies":true,"parent_id":"t3_1lnsgvy","score":1,"author_fullname":"t2_abr7phdd","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"To get started with classic ML to understand basics? Fine. Deep learning? I would recommend better machine (rent or your own). Playing with LLMs, even small ones? Same here.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0i87pr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;To get started with classic ML to understand basics? Fine. Deep learning? I would recommend better machine (rent or your own). Playing with LLMs, even small ones? Same here.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsgvy/build_a_pc_or_not/n0i87pr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751247523,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsgvy","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jn1k2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"InternetBest7599","can_mod_post":false,"created_utc":1751272378,"send_replies":true,"parent_id":"t1_n0jlsms","score":1,"author_fullname":"t2_xhdq2kf8s","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"yes!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jn1k2","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;yes!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsgvy","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsgvy/build_a_pc_or_not/n0jn1k2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751272378,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0jlsms","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"s101c","can_mod_post":false,"created_utc":1751271593,"send_replies":true,"parent_id":"t3_1lnsgvy","score":1,"author_fullname":"t2_rg6hb6my5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"To build or not to build? That is the question.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jlsms","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;To build or not to build? That is the question.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsgvy/build_a_pc_or_not/n0jlsms/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751271593,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsgvy","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0kczpz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"960be6dde311","can_mod_post":false,"created_utc":1751285948,"send_replies":true,"parent_id":"t3_1lnsgvy","score":1,"author_fullname":"t2_1mf6icgwm4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I would recommend building a Windows or Linux system with an NVIDIA GPU. You'll get the best support for most machine learning frameworks that way.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0kczpz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I would recommend building a Windows or Linux system with an NVIDIA GPU. You&amp;#39;ll get the best support for most machine learning frameworks that way.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsgvy/build_a_pc_or_not/n0kczpz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751285948,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsgvy","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0opidr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Herr_Drosselmeyer","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ldbba","score":1,"author_fullname":"t2_1zr9gwsn","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"There are rumours of a 5070ti Super with 24GB of VRAM, you might want to wait and see if they're true, since it might end up being a better choice.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0opidr","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There are rumours of a 5070ti Super with 24GB of VRAM, you might want to wait and see if they&amp;#39;re true, since it might end up being a better choice.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsgvy","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsgvy/build_a_pc_or_not/n0opidr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751334510,"author_flair_text":null,"treatment_tags":[],"created_utc":1751334510,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ldbba","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"InternetBest7599","can_mod_post":false,"created_utc":1751297737,"send_replies":true,"parent_id":"t1_n0lbtne","score":1,"author_fullname":"t2_xhdq2kf8s","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah screw apple, and will 5060 ti with 16 gigs of VRAM be enough for the long term?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ldbba","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah screw apple, and will 5060 ti with 16 gigs of VRAM be enough for the long term?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsgvy","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsgvy/build_a_pc_or_not/n0ldbba/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751297737,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0lbtne","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Herr_Drosselmeyer","can_mod_post":false,"created_utc":1751297302,"send_replies":true,"parent_id":"t3_1lnsgvy","score":0,"author_fullname":"t2_1zr9gwsn","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"50GB storage left?Â \\n\\n\\nYeah, that ain't gonna work. Large language models are called that for a reason. You'll run out of space very quickly.\\n\\n\\nApple's price policy on storage is such a joke, leaving their customers with borderline unusable rigs.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0lbtne","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;50GB storage left?Â &lt;/p&gt;\\n\\n&lt;p&gt;Yeah, that ain&amp;#39;t gonna work. Large language models are called that for a reason. You&amp;#39;ll run out of space very quickly.&lt;/p&gt;\\n\\n&lt;p&gt;Apple&amp;#39;s price policy on storage is such a joke, leaving their customers with borderline unusable rigs.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsgvy/build_a_pc_or_not/n0lbtne/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751297302,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsgvy","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}}]`),o=()=>e.jsx(l,{data:a});export{o as default};
