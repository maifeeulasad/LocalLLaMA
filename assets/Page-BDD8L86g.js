import{j as e}from"./index-F0NXdzZX.js";import{R as t}from"./RedditPostRenderer-CoEZB1dt.js";import"./index-DrtravXm.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"We were not able to find an LLM provider benchmark that focuses on throughput, so we've built our own. [ArtificialAnalysis](https://artificialanalysis.ai/leaderboards/providers) only tests up to 10 RPS, which is too low for most applications. Additionally, it's not open-source, so it doesn’t help much if you’re self-hosting or running your own LLM inference service (like we are).\\n\\nThe main takeaway is that **throughput varies dramatically across providers under concurrent load**, and the primary cause is usually strict rate limits. These are often hard to bypass—even if you pay. Some providers require a $100 deposit to lift limits, but the actual performance gain is negligible.\\n\\n[https://medium.com/data-science-collective/choosing-your-llm-powerhouse-a-comprehensive-comparison-of-inference-providers-192cdb0b9f17](https://medium.com/data-science-collective/choosing-your-llm-powerhouse-a-comprehensive-comparison-of-inference-providers-192cdb0b9f17)","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Comparing LLM Providers Throughput","link_flair_richtext":[{"e":"text","t":"Resources"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1luy711","quarantine":false,"link_flair_text_color":"light","upvote_ratio":1,"author_flair_background_color":null,"subreddit_type":"public","ups":8,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1neapdttam","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Resources","can_mod_post":false,"score":8,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":true,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1752003703,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;We were not able to find an LLM provider benchmark that focuses on throughput, so we&amp;#39;ve built our own. &lt;a href=\\"https://artificialanalysis.ai/leaderboards/providers\\"&gt;ArtificialAnalysis&lt;/a&gt; only tests up to 10 RPS, which is too low for most applications. Additionally, it&amp;#39;s not open-source, so it doesn’t help much if you’re self-hosting or running your own LLM inference service (like we are).&lt;/p&gt;\\n\\n&lt;p&gt;The main takeaway is that &lt;strong&gt;throughput varies dramatically across providers under concurrent load&lt;/strong&gt;, and the primary cause is usually strict rate limits. These are often hard to bypass—even if you pay. Some providers require a $100 deposit to lift limits, but the actual performance gain is negligible.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://medium.com/data-science-collective/choosing-your-llm-powerhouse-a-comprehensive-comparison-of-inference-providers-192cdb0b9f17\\"&gt;https://medium.com/data-science-collective/choosing-your-llm-powerhouse-a-comprehensive-comparison-of-inference-providers-192cdb0b9f17&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?auto=webp&amp;s=efc17c9f241b4403d22cbacfe5d71900ee1cf85a","width":1260,"height":700},"resolutions":[{"url":"https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=700f91dbca11e5a7030b915550ae877ef725a0d4","width":108,"height":60},{"url":"https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b97954336b79c1390848d0e44fa056a85de68672","width":216,"height":120},{"url":"https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=65f53b80ab9674ee645013e3e8eeac4f953d657e","width":320,"height":177},{"url":"https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=47f397e4a22ed5ec7e82aad070eb446319603abc","width":640,"height":355},{"url":"https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0f4359d47b78f5c1aa35de8804dbe36a749fc11a","width":960,"height":533},{"url":"https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=62eb4b7216f41af6600fc4df79cfa67425c19442","width":1080,"height":600}],"variants":{},"id":"RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#ccac2b","id":"1luy711","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"NoVibeCoding","discussion_type":null,"num_comments":4,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1luy711/comparing_llm_providers_throughput/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1luy711/comparing_llm_providers_throughput/","subreddit_subscribers":496592,"created_utc":1752003703,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n22pxpx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"NoVibeCoding","can_mod_post":false,"send_replies":true,"parent_id":"t1_n22oko6","score":1,"author_fullname":"t2_1neapdttam","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Fair. Regardless of the underlying measurement mechanism, OpenRouter doesn't provide numbers for concurrent requests. TPS for 200 concurrent requests is in the thousands for most providers due to batching and horizontal scaling.","edited":1752016547,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n22pxpx","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Fair. Regardless of the underlying measurement mechanism, OpenRouter doesn&amp;#39;t provide numbers for concurrent requests. TPS for 200 concurrent requests is in the thousands for most providers due to batching and horizontal scaling.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"link_id":"t3_1luy711","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luy711/comparing_llm_providers_throughput/n22pxpx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752015576,"author_flair_text":null,"treatment_tags":[],"created_utc":1752015576,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n22oko6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"vasileer","can_mod_post":false,"send_replies":true,"parent_id":"t1_n223uzk","score":1,"author_fullname":"t2_730bgdulm","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"openrouter is not \\"testing\\": it shows uptime and stats of the traffic that is going through their service","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n22oko6","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;openrouter is not &amp;quot;testing&amp;quot;: it shows uptime and stats of the traffic that is going through their service&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1luy711","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luy711/comparing_llm_providers_throughput/n22oko6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752015147,"author_flair_text":null,"treatment_tags":[],"created_utc":1752015147,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n223uzk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"NoVibeCoding","can_mod_post":false,"created_utc":1752008907,"send_replies":true,"parent_id":"t1_n221jaf","score":1,"author_fullname":"t2_1neapdttam","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Just like the AA, OpenRouter is testing throughout for one request at a time which is not particularly useful for assessing performance on real use cases. Most likely you’ll be sending dozens, if not hundreds requests at a time. Thus, testing concurrent throughput is important.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n223uzk","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just like the AA, OpenRouter is testing throughout for one request at a time which is not particularly useful for assessing performance on real use cases. Most likely you’ll be sending dozens, if not hundreds requests at a time. Thus, testing concurrent throughput is important.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"link_id":"t3_1luy711","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luy711/comparing_llm_providers_throughput/n223uzk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752008907,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n221jaf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"vasileer","can_mod_post":false,"created_utc":1752008268,"send_replies":true,"parent_id":"t3_1luy711","score":1,"author_fullname":"t2_730bgdulm","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I check both ArtificialAnalysis leaderboard, but also OpenRouter for inference speed, but also for cost\\n\\nhttps://preview.redd.it/oot33k33spbf1.png?width=1475&amp;format=png&amp;auto=webp&amp;s=2baaff8aa0db9d86af31f7ace80f0f836f0638d5","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n221jaf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I check both ArtificialAnalysis leaderboard, but also OpenRouter for inference speed, but also for cost&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/oot33k33spbf1.png?width=1475&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2baaff8aa0db9d86af31f7ace80f0f836f0638d5\\"&gt;https://preview.redd.it/oot33k33spbf1.png?width=1475&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2baaff8aa0db9d86af31f7ace80f0f836f0638d5&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luy711/comparing_llm_providers_throughput/n221jaf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752008268,"media_metadata":{"oot33k33spbf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":65,"x":108,"u":"https://preview.redd.it/oot33k33spbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a541a2161bd1c6ae7dfff5b283a066f46f285987"},{"y":131,"x":216,"u":"https://preview.redd.it/oot33k33spbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4e67fd09377678e6cb3470513ec602b95ba222ef"},{"y":194,"x":320,"u":"https://preview.redd.it/oot33k33spbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=47804b6c9ace3e050e2aba1515a73101796daff9"},{"y":388,"x":640,"u":"https://preview.redd.it/oot33k33spbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=13b8b0c4c8ad0787f43e22612b6f6bcfe96b5453"},{"y":582,"x":960,"u":"https://preview.redd.it/oot33k33spbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=da482ee63f55ed8272f38a2d8db04b75330672ad"},{"y":655,"x":1080,"u":"https://preview.redd.it/oot33k33spbf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a88c78f79d89bc2b1e9f5e3aaccc76858dbea294"}],"s":{"y":895,"x":1475,"u":"https://preview.redd.it/oot33k33spbf1.png?width=1475&amp;format=png&amp;auto=webp&amp;s=2baaff8aa0db9d86af31f7ace80f0f836f0638d5"},"id":"oot33k33spbf1"}},"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1luy711","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),s=()=>e.jsx(t,{data:a});export{s as default};
