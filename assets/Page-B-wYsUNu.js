import{j as e}from"./index-F0NXdzZX.js";import{R as l}from"./RedditPostRenderer-CoEZB1dt.js";import"./index-DrtravXm.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hi everyone,\\n\\nLast year I posted about 2x MI60 performance. Since then, I bought more cards and PCIE riser cables to build a rack with 8x AMD MI50 32GB cards. My motherboard (Asus rog dark hero viii with AMD 5950x CPU and 96GB 3200Mhz RAM) had stability issues with 8x MI50 (does not boot), so I connected four (or sometimes six) of those cards. I bought these cards on eBay when one seller sold them for around $150 (I started seeing MI50 32GB cards again on eBay).\\n\\nI connected 4x MI50 cards using ASUS Hyper M.2 x16 Gen5 Card (PCIE4.0 x16 to 4xM.2 card then I used M.2 to PCIE4.0 cables to connect 4 GPUs) through the first PCIE4.0 x16 slot on the motherboard that supports 4x4 bifurcation. I set the PCIE to use PCIE3.0 so that I don't get occasional freezing issues in my system. Each card was running at PCIE3.0 x4 (later I also tested 2x MI50s with PCIE4.0 x8 speed and did not see any PP/TG speed difference).\\n\\nI am using 1.2A blower fans to cool these cards which are a bit noisy at max speed but I adjusted their speeds to be acceptable.\\n\\nI have tested both llama.cpp (ROCm 6.3.4 and vulkan backend) and vLLM v0.9.2 in Ubuntu 24.04.02. Below are some results.\\n\\nNote that MI50/60 cards do not have matrix or tensor cores and that is why their Prompt Processing (PP) speed is not great. But Text Generation (TG) speeds are great!\\n\\nLlama.cpp (build: 247e5c6e (5606)) with ROCm 6.3.4. All of the runs use one MI50 (I will note the ones that use 2x or 4x MI50 in the model column). Note that MI50/60 cards perform best with Q4\\\\_0 and Q4\\\\_1 quantizations (that is why I ran larger models with those Quants).\\n\\n|Model|size|test|t/s|\\n|:-|:-|:-|:-|\\n|qwen3 0.6B Q8\\\\_0|604.15 MiB|pp1024|3014.18 ± 1.71|\\n|qwen3 0.6B Q8\\\\_0|604.15 MiB|tg128|191.63 ± 0.38|\\n|llama 7B Q4\\\\_0|3.56 GiB|pp512|1289.11 ± 0.62|\\n|llama 7B Q4\\\\_0|3.56 GiB|tg128|91.46 ± 0.13|\\n|qwen3 8B Q8\\\\_0|8.11 GiB|pp512|357.71 ± 0.04|\\n|qwen3 8B Q8\\\\_0|8.11 GiB|tg128|48.09 ± 0.04|\\n|qwen2 14B Q8\\\\_0|14.62 GiB|pp512|249.45 ± 0.08|\\n|qwen2 14B Q8\\\\_0|14.62 GiB|tg128|29.24 ± 0.03|\\n|qwen2 32B Q4\\\\_0|17.42 GiB|pp512|300.02 ± 0.52|\\n|qwen2 32B Q4\\\\_0|17.42 GiB|tg128|20.39 ± 0.37|\\n|qwen2 70B Q5\\\\_K - Medium|50.70 GiB|pp512|48.92 ± 0.02|\\n|qwen2 70B Q5\\\\_K - Medium|50.70 GiB|tg128|9.05 ± 0.10|\\n|qwen2vl 70B Q4\\\\_1 (4x MI50 row split)|42.55 GiB|pp512|56.33 ± 0.09|\\n|qwen2vl 70B Q4\\\\_1 (4x MI50 row split)|42.55 GiB|tg128|16.00 ± 0.01|\\n|qwen3moe 30B.A3B Q4\\\\_1|17.87 GiB|pp1024|1023.81 ± 3.76|\\n|qwen3moe 30B.A3B Q4\\\\_1|17.87 GiB|tg128|63.87 ± 0.06|\\n|qwen3 32B Q4\\\\_1 (2x MI50)|19.21 GiB|pp1024|238.17 ± 0.30|\\n|qwen3 32B Q4\\\\_1 (2x MI50)|19.21 GiB|tg128|25.17 ± 0.01|\\n|qwen3moe 235B.A22B Q4\\\\_1 (5x MI50)|137.11 GiB|pp1024|202.50 ± 0.32|\\n|qwen3moe 235B.A22B Q4\\\\_1 (5x MI50) (4x mi50 with some expert offloading should give around 16t/s)|137.11 GiB|tg128|19.17 ± 0.04|\\n\\nPP is not great but TG is very good for most use cases. \\n\\nBy the way, I also tested Deepseek R1 IQ2-XXS (although it was running with 6x MI50) and I was getting \\\\~9 t/s for TG with a few experts offloaded to CPU RAM.\\n\\nNow, let's look at vllm (version 0.9.2.dev1+g5273453b6. Fork used: https://github.com/nlzy/vllm-gfx906).\\n\\nAWQ and GPTQ quants are supported. For gptq models, desc\\\\_act=false quants are used to get a better performance. Max concurrency is set to 1.\\n\\n|Model|Output token throughput (tok/s) (256)|Prompt processing  t/s (4096)|\\n|:-|:-|:-|\\n|Mistral-Large-Instruct-2407-AWQ 123B (4x MI50)|19.68|80|\\n|Qwen2.5-72B-Instruct-GPTQ-Int4 (2x MI50)|19.76|130|\\n|Qwen2.5-72B-Instruct-GPTQ-Int4 (4x MI50)|25.96|130|\\n|Llama-3.3-70B-Instruct-AWQ (4x MI50)|27.26|130|\\n|Qwen3-32B-GPTQ-Int8 (4x MI50)|32.3|230|\\n|Qwen3-32B-autoround-4bit-gptq (4x MI50)|38.55|230|\\n|gemma-3-27b-it-int4-awq (4x MI50)|36.96|350|\\n\\n  \\nTensor parallelism (TP) gives MI50s extra performance in Text Generation (TG). Overall, great performance for the price. And I am sure we will not get 128GB VRAM with such TG speeds any time soon for \\\\~$600.\\n\\nPower consumption is around 900W for the system when using vllm with TP during text generation. Llama.cpp does not use TP so I did not see it using above 500W. Each GPU runs at around 18W when idle.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"128GB VRAM for ~$600. Qwen3 MOE 235B.A22B reaching 20 t/s. 4x AMD MI50 32GB.","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lspzn3","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.96,"author_flair_background_color":null,"subreddit_type":"public","ups":370,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_3zy7pnf1","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":370,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1751767150,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\\n\\n&lt;p&gt;Last year I posted about 2x MI60 performance. Since then, I bought more cards and PCIE riser cables to build a rack with 8x AMD MI50 32GB cards. My motherboard (Asus rog dark hero viii with AMD 5950x CPU and 96GB 3200Mhz RAM) had stability issues with 8x MI50 (does not boot), so I connected four (or sometimes six) of those cards. I bought these cards on eBay when one seller sold them for around $150 (I started seeing MI50 32GB cards again on eBay).&lt;/p&gt;\\n\\n&lt;p&gt;I connected 4x MI50 cards using ASUS Hyper M.2 x16 Gen5 Card (PCIE4.0 x16 to 4xM.2 card then I used M.2 to PCIE4.0 cables to connect 4 GPUs) through the first PCIE4.0 x16 slot on the motherboard that supports 4x4 bifurcation. I set the PCIE to use PCIE3.0 so that I don&amp;#39;t get occasional freezing issues in my system. Each card was running at PCIE3.0 x4 (later I also tested 2x MI50s with PCIE4.0 x8 speed and did not see any PP/TG speed difference).&lt;/p&gt;\\n\\n&lt;p&gt;I am using 1.2A blower fans to cool these cards which are a bit noisy at max speed but I adjusted their speeds to be acceptable.&lt;/p&gt;\\n\\n&lt;p&gt;I have tested both llama.cpp (ROCm 6.3.4 and vulkan backend) and vLLM v0.9.2 in Ubuntu 24.04.02. Below are some results.&lt;/p&gt;\\n\\n&lt;p&gt;Note that MI50/60 cards do not have matrix or tensor cores and that is why their Prompt Processing (PP) speed is not great. But Text Generation (TG) speeds are great!&lt;/p&gt;\\n\\n&lt;p&gt;Llama.cpp (build: 247e5c6e (5606)) with ROCm 6.3.4. All of the runs use one MI50 (I will note the ones that use 2x or 4x MI50 in the model column). Note that MI50/60 cards perform best with Q4_0 and Q4_1 quantizations (that is why I ran larger models with those Quants).&lt;/p&gt;\\n\\n&lt;table&gt;&lt;thead&gt;\\n&lt;tr&gt;\\n&lt;th align=\\"left\\"&gt;Model&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;size&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;test&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;t/s&lt;/th&gt;\\n&lt;/tr&gt;\\n&lt;/thead&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;qwen3 0.6B Q8_0&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;604.15 MiB&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;pp1024&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;3014.18 ± 1.71&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;qwen3 0.6B Q8_0&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;604.15 MiB&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;tg128&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;191.63 ± 0.38&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;llama 7B Q4_0&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;3.56 GiB&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;pp512&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;1289.11 ± 0.62&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;llama 7B Q4_0&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;3.56 GiB&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;tg128&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;91.46 ± 0.13&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;qwen3 8B Q8_0&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;8.11 GiB&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;pp512&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;357.71 ± 0.04&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;qwen3 8B Q8_0&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;8.11 GiB&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;tg128&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;48.09 ± 0.04&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;qwen2 14B Q8_0&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;14.62 GiB&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;pp512&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;249.45 ± 0.08&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;qwen2 14B Q8_0&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;14.62 GiB&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;tg128&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;29.24 ± 0.03&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;qwen2 32B Q4_0&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;17.42 GiB&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;pp512&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;300.02 ± 0.52&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;qwen2 32B Q4_0&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;17.42 GiB&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;tg128&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;20.39 ± 0.37&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;qwen2 70B Q5_K - Medium&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;50.70 GiB&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;pp512&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;48.92 ± 0.02&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;qwen2 70B Q5_K - Medium&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;50.70 GiB&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;tg128&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;9.05 ± 0.10&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;qwen2vl 70B Q4_1 (4x MI50 row split)&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;42.55 GiB&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;pp512&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;56.33 ± 0.09&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;qwen2vl 70B Q4_1 (4x MI50 row split)&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;42.55 GiB&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;tg128&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;16.00 ± 0.01&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;qwen3moe 30B.A3B Q4_1&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;17.87 GiB&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;pp1024&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;1023.81 ± 3.76&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;qwen3moe 30B.A3B Q4_1&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;17.87 GiB&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;tg128&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;63.87 ± 0.06&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;qwen3 32B Q4_1 (2x MI50)&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;19.21 GiB&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;pp1024&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;238.17 ± 0.30&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;qwen3 32B Q4_1 (2x MI50)&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;19.21 GiB&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;tg128&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;25.17 ± 0.01&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;qwen3moe 235B.A22B Q4_1 (5x MI50)&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;137.11 GiB&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;pp1024&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;202.50 ± 0.32&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;qwen3moe 235B.A22B Q4_1 (5x MI50) (4x mi50 with some expert offloading should give around 16t/s)&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;137.11 GiB&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;tg128&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;19.17 ± 0.04&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;/tbody&gt;&lt;/table&gt;\\n\\n&lt;p&gt;PP is not great but TG is very good for most use cases. &lt;/p&gt;\\n\\n&lt;p&gt;By the way, I also tested Deepseek R1 IQ2-XXS (although it was running with 6x MI50) and I was getting ~9 t/s for TG with a few experts offloaded to CPU RAM.&lt;/p&gt;\\n\\n&lt;p&gt;Now, let&amp;#39;s look at vllm (version 0.9.2.dev1+g5273453b6. Fork used: &lt;a href=\\"https://github.com/nlzy/vllm-gfx906\\"&gt;https://github.com/nlzy/vllm-gfx906&lt;/a&gt;).&lt;/p&gt;\\n\\n&lt;p&gt;AWQ and GPTQ quants are supported. For gptq models, desc_act=false quants are used to get a better performance. Max concurrency is set to 1.&lt;/p&gt;\\n\\n&lt;table&gt;&lt;thead&gt;\\n&lt;tr&gt;\\n&lt;th align=\\"left\\"&gt;Model&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;Output token throughput (tok/s) (256)&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;Prompt processing  t/s (4096)&lt;/th&gt;\\n&lt;/tr&gt;\\n&lt;/thead&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;Mistral-Large-Instruct-2407-AWQ 123B (4x MI50)&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;19.68&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;80&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;Qwen2.5-72B-Instruct-GPTQ-Int4 (2x MI50)&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;19.76&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;130&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;Qwen2.5-72B-Instruct-GPTQ-Int4 (4x MI50)&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;25.96&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;130&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;Llama-3.3-70B-Instruct-AWQ (4x MI50)&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;27.26&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;130&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;Qwen3-32B-GPTQ-Int8 (4x MI50)&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;32.3&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;230&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;Qwen3-32B-autoround-4bit-gptq (4x MI50)&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;38.55&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;230&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;gemma-3-27b-it-int4-awq (4x MI50)&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;36.96&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;350&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;/tbody&gt;&lt;/table&gt;\\n\\n&lt;p&gt;Tensor parallelism (TP) gives MI50s extra performance in Text Generation (TG). Overall, great performance for the price. And I am sure we will not get 128GB VRAM with such TG speeds any time soon for ~$600.&lt;/p&gt;\\n\\n&lt;p&gt;Power consumption is around 900W for the system when using vllm with TP during text generation. Llama.cpp does not use TP so I did not see it using above 500W. Each GPU runs at around 18W when idle.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/hNPAr3eIAChvCruA_30RyOLRAM__-hwPLVex8tW4YLU.png?auto=webp&amp;s=8c340ab5ae3eebd3a1f3a8e634fa1bb0cf891fee","width":1200,"height":600},"resolutions":[{"url":"https://external-preview.redd.it/hNPAr3eIAChvCruA_30RyOLRAM__-hwPLVex8tW4YLU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5e4c9f1d82654452ab9abf4c2dfaa69dd9495bbf","width":108,"height":54},{"url":"https://external-preview.redd.it/hNPAr3eIAChvCruA_30RyOLRAM__-hwPLVex8tW4YLU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=87b01812b7fbad8b7970e973412d609dc1ebcd54","width":216,"height":108},{"url":"https://external-preview.redd.it/hNPAr3eIAChvCruA_30RyOLRAM__-hwPLVex8tW4YLU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3dfcdc7e43c5456819743e8f71d12c77ef8db87a","width":320,"height":160},{"url":"https://external-preview.redd.it/hNPAr3eIAChvCruA_30RyOLRAM__-hwPLVex8tW4YLU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6166a63e8cec75d08489356905b0d102369f198e","width":640,"height":320},{"url":"https://external-preview.redd.it/hNPAr3eIAChvCruA_30RyOLRAM__-hwPLVex8tW4YLU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d1c7db1463f9c600c11ea411ad404650ada2e07e","width":960,"height":480},{"url":"https://external-preview.redd.it/hNPAr3eIAChvCruA_30RyOLRAM__-hwPLVex8tW4YLU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b47148bbb6b2e8e0e6fb909a8a087abbf287326e","width":1080,"height":540}],"variants":{},"id":"hNPAr3eIAChvCruA_30RyOLRAM__-hwPLVex8tW4YLU"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lspzn3","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"MLDataScientist","discussion_type":null,"num_comments":104,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/","subreddit_subscribers":496033,"created_utc":1751767150,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1p7ekd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MLDataScientist","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1oxvfy","score":3,"author_fullname":"t2_3zy7pnf1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"[https://github.com/ikawrakow/ik\\\\_llama.cpp](https://github.com/ikawrakow/ik_llama.cpp) \\\\- optimized for GPU and CPU offloading for models like Deepseek R1 that gives you better prefill speed than llama.cpp.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1p7ekd","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://github.com/ikawrakow/ik_llama.cpp\\"&gt;https://github.com/ikawrakow/ik_llama.cpp&lt;/a&gt; - optimized for GPU and CPU offloading for models like Deepseek R1 that gives you better prefill speed than llama.cpp.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1p7ekd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751836077,"author_flair_text":null,"treatment_tags":[],"created_utc":1751836077,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n1oxvfy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Caffdy","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1kzb6e","score":1,"author_fullname":"t2_ql2vu0wz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"what is ik_llama? what's the difference with normal llama?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1oxvfy","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;what is ik_llama? what&amp;#39;s the difference with normal llama?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1oxvfy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751833159,"author_flair_text":null,"treatment_tags":[],"created_utc":1751833159,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1kzb6e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MLDataScientist","can_mod_post":false,"created_utc":1751775079,"send_replies":true,"parent_id":"t1_n1khzoo","score":10,"author_fullname":"t2_3zy7pnf1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks for sharing! Unfortunately, ik-llama does not support AMD GPUs. But they are working on vulkan support. So, there is hope for that in the future.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1kzb6e","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for sharing! Unfortunately, ik-llama does not support AMD GPUs. But they are working on vulkan support. So, there is hope for that in the future.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1kzb6e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751775079,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1nur4j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1nppiu","score":2,"author_fullname":"t2_h5utwre7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"AWQ for sure, you can juice GPTQ with group size. Haven't used Q4_1 or Q4_0 in like 2 years. \\n\\nI used to have only 3x ampere GPU and VLLM didn't want to do TP in that config. Plus they only support FP8 CTX. Exllama and L.cpp let you do 4/6/8 bit. Since I keep running models where memory use is 98%, VLLM doesn't play nice. \\n\\nSay I want to run mistral-large, exllama gets it done on 3 GPUs and leaves the other one for tts/image gen/image captioning. Single user performance in my case isn't very far off. Bringing back cards like P40s or getting AMD, I might sing a different tune.","edited":false,"author_flair_css_class":null,"name":"t1_n1nur4j","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;AWQ for sure, you can juice GPTQ with group size. Haven&amp;#39;t used Q4_1 or Q4_0 in like 2 years. &lt;/p&gt;\\n\\n&lt;p&gt;I used to have only 3x ampere GPU and VLLM didn&amp;#39;t want to do TP in that config. Plus they only support FP8 CTX. Exllama and L.cpp let you do 4/6/8 bit. Since I keep running models where memory use is 98%, VLLM doesn&amp;#39;t play nice. &lt;/p&gt;\\n\\n&lt;p&gt;Say I want to run mistral-large, exllama gets it done on 3 GPUs and leaves the other one for tts/image gen/image captioning. Single user performance in my case isn&amp;#39;t very far off. Bringing back cards like P40s or getting AMD, I might sing a different tune.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lspzn3","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1nur4j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751821151,"author_flair_text":null,"collapsed":false,"created_utc":1751821151,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1nppiu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MLDataScientist","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1maeb7","score":2,"author_fullname":"t2_3zy7pnf1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think gptq and awq 4bit quants are better than Q4\\\\_1. Additionally, MI50s get better performance with vLLM. So, for larger models, vLLM is a good option while keeping both speed and quality of models high.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1nppiu","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think gptq and awq 4bit quants are better than Q4_1. Additionally, MI50s get better performance with vLLM. So, for larger models, vLLM is a good option while keeping both speed and quality of models high.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1nppiu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751819611,"author_flair_text":null,"treatment_tags":[],"created_utc":1751819611,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1maeb7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1llkm9","score":3,"author_fullname":"t2_h5utwre7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Quality of those quants ain't great, even if they're fast.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1maeb7","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Quality of those quants ain&amp;#39;t great, even if they&amp;#39;re fast.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1maeb7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751801520,"author_flair_text":null,"treatment_tags":[],"created_utc":1751801520,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1lrq5n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No-Refrigerator-1672","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1lpvyc","score":3,"author_fullname":"t2_baavelp5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I've got mines for $130 a piece from Alibaba (plus shipping plus tax), which turned out to be 350 Eur total for a pair. I don't believe that the prices will get lower than this in any time soon, but they are already low enough to be a compelling option.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1lrq5n","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve got mines for $130 a piece from Alibaba (plus shipping plus tax), which turned out to be 350 Eur total for a pair. I don&amp;#39;t believe that the prices will get lower than this in any time soon, but they are already low enough to be a compelling option.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1lrq5n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751790682,"author_flair_text":null,"treatment_tags":[],"created_utc":1751790682,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n1lpvyc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"My_Unbiased_Opinion","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1llkm9","score":2,"author_fullname":"t2_esiyl0yb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Interesting. Good to know. Im considering a Mi50 rn. \\n\\n\\nDo you think prices will go lower?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1lpvyc","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Interesting. Good to know. Im considering a Mi50 rn. &lt;/p&gt;\\n\\n&lt;p&gt;Do you think prices will go lower?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1lpvyc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751789568,"author_flair_text":null,"treatment_tags":[],"created_utc":1751789568,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1llkm9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No-Refrigerator-1672","can_mod_post":false,"created_utc":1751787035,"send_replies":true,"parent_id":"t1_n1khzoo","score":5,"author_fullname":"t2_baavelp5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"In llama.cpp, Mi50 is incompatible with Q4\\\\_0 quants (I believe it's due to them requiring BF16), but with Q4\\\\_1 quants you get roughly 10-15% performance uplift against Unsloth Dynamic Q4 quants.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1llkm9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;In llama.cpp, Mi50 is incompatible with Q4_0 quants (I believe it&amp;#39;s due to them requiring BF16), but with Q4_1 quants you get roughly 10-15% performance uplift against Unsloth Dynamic Q4 quants.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1llkm9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751787035,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1kpdmg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"My_Unbiased_Opinion","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1klbbr","score":2,"author_fullname":"t2_esiyl0yb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks. Fixed it as well. ","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1kpdmg","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks. Fixed it as well. &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1kpdmg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751770638,"author_flair_text":null,"treatment_tags":[],"created_utc":1751770638,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1klbbr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"natufian","can_mod_post":false,"created_utc":1751768925,"send_replies":true,"parent_id":"t1_n1khzoo","score":3,"author_fullname":"t2_65uig","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"[Fixed Linky](https://www.reddit.com/r/LocalLLaMA/comments/1eqfok2/overclocked_m40_24gb_vs_p40_benchmark_results/)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1klbbr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1eqfok2/overclocked_m40_24gb_vs_p40_benchmark_results/\\"&gt;Fixed Linky&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1klbbr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751768925,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n1khzoo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"My_Unbiased_Opinion","can_mod_post":false,"created_utc":1751767562,"send_replies":true,"parent_id":"t3_1lspzn3","score":35,"author_fullname":"t2_esiyl0yb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Nice dude. I was about to recommend Q4_0 with older cards. I've done some testing with P40s and M40s as well \\n\\n\\nhttps://www.reddit.com/r/LocalLLaMA/comments/1eqfok2/overclocked_m40_24gb_vs_p40_benchmark_results/\\n\\n\\nHave you tried ik-llama.cpp with a 4_0 quant? I havent (old GPUs are in storage) but there might be some more gains to be had. ","edited":1751770628,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1khzoo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nice dude. I was about to recommend Q4_0 with older cards. I&amp;#39;ve done some testing with P40s and M40s as well &lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1eqfok2/overclocked_m40_24gb_vs_p40_benchmark_results/\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1eqfok2/overclocked_m40_24gb_vs_p40_benchmark_results/&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Have you tried ik-llama.cpp with a 4_0 quant? I havent (old GPUs are in storage) but there might be some more gains to be had. &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1khzoo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751767562,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lspzn3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":35}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1l62h4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1l0n6q","score":4,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Actually, my understanding is there's a software issue with the 395 and MOEs and that's why the PP is so low. Hopefully that gets fixed.\\n\\nAnyways, here's a dense model. Small, but still dense. I picked the llama 7b because I have another GPU that I already ran that model on to post too.\\n\\nMi50\\n\\n\\"llama 7B Q4_0 \\t|   3.56 GiB \\t|   pp512 \\t|   1289.11 ± 0.62\\n\\nllama 7B Q4_0 \\t|   3.56 GiB \\t|   tg128 \\t|   91.46 ± 0.13\\"\\n\\nMax+ 395\\n\\n\\"llama 7B Q4_0                  |   3.56 GiB |    pp512 |        937.33 ± 5.67\\n\\nllama 7B Q4_0                  |   3.56 GiB |    tg128 |         48.47 ± 0.72\\"\\n\\nAlso, here's from a $50 V340.\\n\\n\\"llama 7B Q4_0                  |   3.56 GiB |     pp512 |       1247.83 ± 3.78\\n\\nllama 7B Q4_0                  |   3.56 GiB |     tg128 |         47.73 ± 0.09\\"","edited":1751778668,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1l62h4","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Actually, my understanding is there&amp;#39;s a software issue with the 395 and MOEs and that&amp;#39;s why the PP is so low. Hopefully that gets fixed.&lt;/p&gt;\\n\\n&lt;p&gt;Anyways, here&amp;#39;s a dense model. Small, but still dense. I picked the llama 7b because I have another GPU that I already ran that model on to post too.&lt;/p&gt;\\n\\n&lt;p&gt;Mi50&lt;/p&gt;\\n\\n&lt;p&gt;&amp;quot;llama 7B Q4_0  |   3.56 GiB    |   pp512   |   1289.11 ± 0.62&lt;/p&gt;\\n\\n&lt;p&gt;llama 7B Q4_0   |   3.56 GiB    |   tg128   |   91.46 ± 0.13&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;Max+ 395&lt;/p&gt;\\n\\n&lt;p&gt;&amp;quot;llama 7B Q4_0                  |   3.56 GiB |    pp512 |        937.33 ± 5.67&lt;/p&gt;\\n\\n&lt;p&gt;llama 7B Q4_0                  |   3.56 GiB |    tg128 |         48.47 ± 0.72&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;Also, here&amp;#39;s from a $50 V340.&lt;/p&gt;\\n\\n&lt;p&gt;&amp;quot;llama 7B Q4_0                  |   3.56 GiB |     pp512 |       1247.83 ± 3.78&lt;/p&gt;\\n\\n&lt;p&gt;llama 7B Q4_0                  |   3.56 GiB |     tg128 |         47.73 ± 0.09&amp;quot;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1l62h4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751778375,"author_flair_text":null,"treatment_tags":[],"created_utc":1751778375,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n1l0n6q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MLDataScientist","can_mod_post":false,"created_utc":1751775705,"send_replies":true,"parent_id":"t1_n1kzt03","score":11,"author_fullname":"t2_3zy7pnf1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I see. But you have to also consider dense models. Mistral Large is 123B parameter model and int4 quant runs at ~20t/s with 4x MI50. I doubt that you will get even 5 t/s TG with Max+.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1l0n6q","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I see. But you have to also consider dense models. Mistral Large is 123B parameter model and int4 quant runs at ~20t/s with 4x MI50. I doubt that you will get even 5 t/s TG with Max+.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1l0n6q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751775705,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1nq6f6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MLDataScientist","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1ls0kg","score":1,"author_fullname":"t2_3zy7pnf1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"yes, that is exactly what I did with vLLM.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1nq6f6","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;yes, that is exactly what I did with vLLM.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1nq6f6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751819755,"author_flair_text":null,"treatment_tags":[],"created_utc":1751819755,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1om6h7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1ls0kg","score":1,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I already have.\\n\\nhttps://www.reddit.com/r/LocalLLaMA/comments/1le951x/gmk_x2amd_max_395_w128gb_first_impressions/","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1om6h7","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I already have.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1le951x/gmk_x2amd_max_395_w128gb_first_impressions/\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1le951x/gmk_x2amd_max_395_w128gb_first_impressions/&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1om6h7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751829475,"author_flair_text":null,"treatment_tags":[],"created_utc":1751829475,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1ls0kg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"COBECT","can_mod_post":false,"created_utc":1751790862,"send_replies":true,"parent_id":"t1_n1kzt03","score":6,"author_fullname":"t2_1umam7ln","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Please run large models 20+B, nobody cares about rather speed for small models since it almost everywhere insane.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ls0kg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Please run large models 20+B, nobody cares about rather speed for small models since it almost everywhere insane.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1ls0kg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751790862,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n1kzt03","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"fallingdowndizzyvr","can_mod_post":false,"created_utc":1751775308,"send_replies":true,"parent_id":"t3_1lspzn3","score":18,"author_fullname":"t2_o65i6kx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For comparison. It blows the Max+ 395 away for PP. But is about comparable in TG. Yes, I know it's not the same quant, but it's close enough for a hand wave comparison.\\n\\nMi50\\n\\n\\"qwen3moe 30B.A3B Q4_1 |  \\t17.87 GiB |  \\tpp1024 |  \\t1023.81 ± 3.76\\n\\nqwen3moe 30B.A3B Q4_1 |  \\t17.87 GiB |  \\ttg128 |  \\t63.87 ± 0.06\\"\\n\\nMax+ 395\\n\\n\\"qwen3moe 30B.A3B Q4_K - Medium |  16.49 GiB |          pp1024 |         66.64 ± 0.25\\n\\nqwen3moe 30B.A3B Q4_K - Medium |  16.49 GiB |           tg128 |         71.29 ± 0.07\\"","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1kzt03","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For comparison. It blows the Max+ 395 away for PP. But is about comparable in TG. Yes, I know it&amp;#39;s not the same quant, but it&amp;#39;s close enough for a hand wave comparison.&lt;/p&gt;\\n\\n&lt;p&gt;Mi50&lt;/p&gt;\\n\\n&lt;p&gt;&amp;quot;qwen3moe 30B.A3B Q4_1 |    17.87 GiB |     pp1024 |    1023.81 ± 3.76&lt;/p&gt;\\n\\n&lt;p&gt;qwen3moe 30B.A3B Q4_1 |     17.87 GiB |     tg128 |     63.87 ± 0.06&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;Max+ 395&lt;/p&gt;\\n\\n&lt;p&gt;&amp;quot;qwen3moe 30B.A3B Q4_K - Medium |  16.49 GiB |          pp1024 |         66.64 ± 0.25&lt;/p&gt;\\n\\n&lt;p&gt;qwen3moe 30B.A3B Q4_K - Medium |  16.49 GiB |           tg128 |         71.29 ± 0.07&amp;quot;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1kzt03/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751775308,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lspzn3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":18}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1nd8vp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MLDataScientist","can_mod_post":false,"created_utc":1751815877,"send_replies":true,"parent_id":"t1_n1m0yja","score":1,"author_fullname":"t2_3zy7pnf1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thank you! Never tried command-A since there was no much interest in that model in this community. But I can give it a try.\\n\\n\\nI just checked it. It is a 111B dense model. So, I think it would perform slightly faster than Mistral Large.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1nd8vp","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you! Never tried command-A since there was no much interest in that model in this community. But I can give it a try.&lt;/p&gt;\\n\\n&lt;p&gt;I just checked it. It is a 111B dense model. So, I think it would perform slightly faster than Mistral Large.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1nd8vp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751815877,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1m0yja","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"CheatCodesOfLife","can_mod_post":false,"created_utc":1751796313,"send_replies":true,"parent_id":"t3_1lspzn3","score":4,"author_fullname":"t2_32el727b","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Have you tried Command-A in AWQ quant with VLLM? I'd be curious about the prompt processing and generation speeds.\\n\\nI get 32t/s with 4x3090.\\n\\nIf you can get similar speeds to ML2407, that'd be a great model to run locally, and 128GB of VRAM would let you take advantage of it's coherence at long contexts!\\n\\nThanks for you extremely details post btw, you covered everything clearly.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1m0yja","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Have you tried Command-A in AWQ quant with VLLM? I&amp;#39;d be curious about the prompt processing and generation speeds.&lt;/p&gt;\\n\\n&lt;p&gt;I get 32t/s with 4x3090.&lt;/p&gt;\\n\\n&lt;p&gt;If you can get similar speeds to ML2407, that&amp;#39;d be a great model to run locally, and 128GB of VRAM would let you take advantage of it&amp;#39;s coherence at long contexts!&lt;/p&gt;\\n\\n&lt;p&gt;Thanks for you extremely details post btw, you covered everything clearly.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1m0yja/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751796313,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lspzn3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1mz8lz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HilLiedTroopsDied","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1ldw11","score":2,"author_fullname":"t2_1snfn3ui","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"dang reddit taught me something for once.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1mz8lz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;dang reddit taught me something for once.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1mz8lz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751811497,"author_flair_text":null,"treatment_tags":[],"created_utc":1751811497,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1roi7a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ahjorth","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1ldw11","score":2,"author_fullname":"t2_sc1eb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"No joke, I am writing out a plain language description of a research project and I just wrote this:\\n\\n&gt;  \\n**LLMs are differentiable as ML models and we can (and do) use gradient descent to train them. \\\\[...\\\\] More specifically, we can use the chain rule to get gradient descent over all dimensions and identify parameter(s) to change so we get “the most close” to the desired output vector for the smallest (set of) change(s) to parameter(s).**\\n\\nI don't think I totally appreciated just how much I do this. Hahah.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1roi7a","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No joke, I am writing out a plain language description of a research project and I just wrote this:&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;&lt;strong&gt;LLMs are differentiable as ML models and we can (and do) use gradient descent to train them. [...] More specifically, we can use the chain rule to get gradient descent over all dimensions and identify parameter(s) to change so we get “the most close” to the desired output vector for the smallest (set of) change(s) to parameter(s).&lt;/strong&gt;&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I don&amp;#39;t think I totally appreciated just how much I do this. Hahah.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1roi7a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751871858,"author_flair_text":null,"treatment_tags":[],"created_utc":1751871858,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1ldw11","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ahjorth","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1lcoo1","score":21,"author_fullname":"t2_sc1eb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I have an (official) diagnosis, can relate (100%).","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1ldw11","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have an (official) diagnosis, can relate (100%).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1ldw11/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751782597,"author_flair_text":null,"treatment_tags":[],"created_utc":1751782597,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":21}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1s4i1x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"orinoco_w","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1lcoo1","score":1,"author_fullname":"t2_glr2w","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks for this observation.\\n\\nAnd thanks OP for the awesome investment of time to do and write up these tests!\\n\\nI'm waiting on a mobo to be able to run both 7900xtx and mi100 at the same time for my aged AM4 with 5900x and 128gb of 3200mHz ram (yeah all 4 sticks are stable at 3200mhz.. ECC Udimms).\\n\\nBeen waiting to test with mi100 before deciding whether to spend on some mi50/60s.\\n\\nAlso love the m.2 idea for bifurcating mobos.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1s4i1x","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for this observation.&lt;/p&gt;\\n\\n&lt;p&gt;And thanks OP for the awesome investment of time to do and write up these tests!&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m waiting on a mobo to be able to run both 7900xtx and mi100 at the same time for my aged AM4 with 5900x and 128gb of 3200mHz ram (yeah all 4 sticks are stable at 3200mhz.. ECC Udimms).&lt;/p&gt;\\n\\n&lt;p&gt;Been waiting to test with mi100 before deciding whether to spend on some mi50/60s.&lt;/p&gt;\\n\\n&lt;p&gt;Also love the m.2 idea for bifurcating mobos.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1s4i1x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751881450,"author_flair_text":null,"treatment_tags":[],"created_utc":1751881450,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1pmqz1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cubixy2k","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1lcoo1","score":0,"author_fullname":"t2_399rf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"TikTok (brain)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1pmqz1","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;TikTok (brain)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1pmqz1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751841165,"author_flair_text":null,"treatment_tags":[],"created_utc":1751841165,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n1lcoo1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"beryugyo619","can_mod_post":false,"created_utc":1751781921,"send_replies":true,"parent_id":"t1_n1l6nnt","score":30,"author_fullname":"t2_v8wruy0k","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I've seen people describing it as ADHD brains(working (only sporadically) extra hard) giving out bonus contents(like in movie Blu-rays) like those were free candies for sentences","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1lcoo1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve seen people describing it as ADHD brains(working (only sporadically) extra hard) giving out bonus contents(like in movie Blu-rays) like those were free candies for sentences&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1lcoo1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751781921,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":30}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1n7cgd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"fullouterjoin","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1lxpu4","score":6,"author_fullname":"t2_406sj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Please only use TeX with citations.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1n7cgd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Please only use TeX with citations.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1n7cgd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751814058,"author_flair_text":null,"treatment_tags":[],"created_utc":1751814058,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n1lxpu4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"jrherita","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1l7vad","score":14,"author_fullname":"t2_skayi","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"fwiw I found your parentheses easy to read.  They're useful for breaking up walls of text.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1lxpu4","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;fwiw I found your parentheses easy to read.  They&amp;#39;re useful for breaking up walls of text.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1lxpu4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751794366,"author_flair_text":null,"treatment_tags":[],"created_utc":1751794366,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":14}}],"before":null}},"user_reports":[],"saved":false,"id":"n1l7vad","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MLDataScientist","can_mod_post":false,"created_utc":1751779309,"send_replies":true,"parent_id":"t1_n1l6nnt","score":16,"author_fullname":"t2_3zy7pnf1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Roger that. I was in a rush, but good point.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1l7vad","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Roger that. I was in a rush, but good point.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1l7vad/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751779309,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":16}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1lk9j5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FunnyAsparagus1253","can_mod_post":false,"created_utc":1751786261,"send_replies":true,"parent_id":"t1_n1l6nnt","score":6,"author_fullname":"t2_i6c8tay3w","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I can read the first one fine. Your version does flow a little better for reading but loses a little info imo (the last sentence seems disconnected, for example). Both are fine though! 😅🫶","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1lk9j5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I can read the first one fine. Your version does flow a little better for reading but loses a little info imo (the last sentence seems disconnected, for example). Both are fine though! 😅🫶&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1lk9j5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751786261,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1owack","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"fallingdowndizzyvr","can_mod_post":false,"created_utc":1751832660,"send_replies":true,"parent_id":"t1_n1l6nnt","score":4,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; You are using parentheses all over the place, like every sentence.\\n\\nDude, what do you have against LISP?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1owack","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;You are using parentheses all over the place, like every sentence.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Dude, what do you have against LISP?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1owack/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751832660,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1lrjr5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1751790576,"send_replies":true,"parent_id":"t1_n1l6nnt","score":5,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I like with parens more.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1lrjr5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I like with parens more.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1lrjr5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751790576,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"d2642412-d9ce-11ed-ae30-32b11309f5bd","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1lbfyc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Everlier","can_mod_post":false,"created_utc":1751781237,"send_replies":true,"parent_id":"t1_n1l6nnt","score":5,"author_fullname":"t2_o7p5m","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I also needed this advice, thanks","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1lbfyc","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Alpaca"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I also needed this advice, thanks&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1lbfyc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751781237,"author_flair_text":"Alpaca","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bd9e9e","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1lzskc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"arakinas","can_mod_post":false,"created_utc":1751795627,"send_replies":true,"parent_id":"t1_n1l6nnt","score":4,"author_fullname":"t2_2q8t0rpr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I prefer it the other way. It reads way better to me","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1lzskc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I prefer it the other way. It reads way better to me&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1lzskc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751795627,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1o5nw2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Brilliant-Silver-111","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1mlyby","score":2,"author_fullname":"t2_let35yog","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Actually, not having an inner voice would allow for more abstract structures as it doesn't need to be spoken. The same with Aphantasia.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1o5nw2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Actually, not having an inner voice would allow for more abstract structures as it doesn&amp;#39;t need to be spoken. The same with Aphantasia.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1o5nw2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751824425,"author_flair_text":null,"treatment_tags":[],"created_utc":1751824425,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1mlyby","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"randylush","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1mdjsd","score":1,"author_fullname":"t2_111wufdu01","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This is a good question. If you didn’t have an inner voice while you read then maybe you’d want your text as structured as possible. At that point maybe just use chat GPT bullets everywhere","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1mlyby","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is a good question. If you didn’t have an inner voice while you read then maybe you’d want your text as structured as possible. At that point maybe just use chat GPT bullets everywhere&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1mlyby/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751806643,"author_flair_text":null,"treatment_tags":[],"created_utc":1751806643,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1s7w4b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Equivalent-Poem-6356","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1mdjsd","score":1,"author_fullname":"t2_w3e7ovh3e","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes, I don't get it  \\nHow's that helpful or not?. I'm intrigued with this question","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1s7w4b","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, I don&amp;#39;t get it&lt;br/&gt;\\nHow&amp;#39;s that helpful or not?. I&amp;#39;m intrigued with this question&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1s7w4b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751883367,"author_flair_text":null,"treatment_tags":[],"created_utc":1751883367,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1mdjsd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Brilliant-Silver-111","can_mod_post":false,"created_utc":1751803040,"send_replies":true,"parent_id":"t1_n1l6nnt","score":3,"author_fullname":"t2_let35yog","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"For those in the comments preferring the parentheses, do you have an inner voice and monologue when you read?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1mdjsd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For those in the comments preferring the parentheses, do you have an inner voice and monologue when you read?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1mdjsd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751803040,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1mg93t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"-Hakuryu-","can_mod_post":false,"created_utc":1751804259,"send_replies":true,"parent_id":"t1_n1l6nnt","score":3,"author_fullname":"t2_z2iyq61","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"sorry but no，compartmentalized info reads just better, and leaves room for additional context should the writer thinks necessary","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1mg93t","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;sorry but no，compartmentalized info reads just better, and leaves room for additional context should the writer thinks necessary&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1mg93t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751804259,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n1l6nnt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"randylush","can_mod_post":false,"created_utc":1751778677,"send_replies":true,"parent_id":"t3_1lspzn3","score":14,"author_fullname":"t2_111wufdu01","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt; My motherboard (Asus rog dark hero viii with AMD 5950x CPU and 96GB 3200Mhz RAM) had stability issues with 8x MI50 (does not boot), so I connected four (or sometimes six) of those cards. I bought these cards on eBay when one seller sold them for around $150 (I started seeing MI50 32GB cards again on eBay).\\n\\nCan I give you a minor language tip. You are using parentheses all over the place, like every sentence. It makes it slightly harder to read. When people read parentheses it’s usually in a different tone of voice, so if you use it too much the language can sound chaotic. I’m not saying don’t use parentheses, just don’t use it every single sentence. \\n\\nThis, for example, would flow better and would be slightly easier to read: \\n\\n&gt; My motherboard, an Asus rog dark hero viii with AMD 5950x CPU and 96GB 3200Mhz RAM, had stability issues with 8x MI50; it wouldn’t boot. so I connected four (or sometimes six) of those cards. I bought these cards on eBay when one seller sold them for around $150. I started seeing MI50 32GB cards again on eBay.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1l6nnt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;My motherboard (Asus rog dark hero viii with AMD 5950x CPU and 96GB 3200Mhz RAM) had stability issues with 8x MI50 (does not boot), so I connected four (or sometimes six) of those cards. I bought these cards on eBay when one seller sold them for around $150 (I started seeing MI50 32GB cards again on eBay).&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Can I give you a minor language tip. You are using parentheses all over the place, like every sentence. It makes it slightly harder to read. When people read parentheses it’s usually in a different tone of voice, so if you use it too much the language can sound chaotic. I’m not saying don’t use parentheses, just don’t use it every single sentence. &lt;/p&gt;\\n\\n&lt;p&gt;This, for example, would flow better and would be slightly easier to read: &lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;My motherboard, an Asus rog dark hero viii with AMD 5950x CPU and 96GB 3200Mhz RAM, had stability issues with 8x MI50; it wouldn’t boot. so I connected four (or sometimes six) of those cards. I bought these cards on eBay when one seller sold them for around $150. I started seeing MI50 32GB cards again on eBay.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1l6nnt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751778677,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lspzn3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":14}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1lfkbg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"segmond","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1l7d2p","score":3,"author_fullname":"t2_ah13x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I have seen folks suggest it, but I haven't personally done so.  \\n  Perhaps using -mg to select the rtx 3090 as the main GPU?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1lfkbg","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have seen folks suggest it, but I haven&amp;#39;t personally done so.&lt;br/&gt;\\n  Perhaps using -mg to select the rtx 3090 as the main GPU?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1lfkbg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751783553,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1751783553,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1ncdzr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MLDataScientist","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1lzzyh","score":1,"author_fullname":"t2_3zy7pnf1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thanks! Good point. I will try -ts and -ot for llama3.3 70B soon.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ncdzr","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks! Good point. I will try -ts and -ot for llama3.3 70B soon.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1ncdzr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751815615,"author_flair_text":null,"treatment_tags":[],"created_utc":1751815615,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1lzzyh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CheatCodesOfLife","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1l7d2p","score":3,"author_fullname":"t2_32el727b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You can certainly achieve this with the -ts and -ot flags (my Deepseek-R1 on 5x3090 + CPU setup does this, prompt processing is all on GPU0 which is PCIe bandwidth bound at PCIe4.0 x16).\\n\\nBut there may be a simpler, I remember reading something about setting the \\"main\\" gpu","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1lzzyh","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can certainly achieve this with the -ts and -ot flags (my Deepseek-R1 on 5x3090 + CPU setup does this, prompt processing is all on GPU0 which is PCIe bandwidth bound at PCIe4.0 x16).&lt;/p&gt;\\n\\n&lt;p&gt;But there may be a simpler, I remember reading something about setting the &amp;quot;main&amp;quot; gpu&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1lzzyh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751795749,"author_flair_text":null,"treatment_tags":[],"created_utc":1751795749,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1nbzqg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1nbgda","score":2,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"you need to use -ts switch like -ts 24/10 tweak the ratio in a way that the as many as possible amount of weights end up in 3090, while still being able to load model.","edited":false,"author_flair_css_class":null,"name":"t1_n1nbzqg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;you need to use -ts switch like -ts 24/10 tweak the ratio in a way that the as many as possible amount of weights end up in 3090, while still being able to load model.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lspzn3","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1nbzqg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751815490,"author_flair_text":null,"collapsed":false,"created_utc":1751815490,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1nbgda","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MLDataScientist","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1lrq1w","score":1,"author_fullname":"t2_3zy7pnf1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What is the command for tensor split in llama cpp? I tried using -sm row and main gpu as RTX 3090 but that Didi not improve the PP.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1nbgda","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What is the command for tensor split in llama cpp? I tried using -sm row and main gpu as RTX 3090 but that Didi not improve the PP.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1nbgda/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751815326,"author_flair_text":null,"treatment_tags":[],"created_utc":1751815326,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1lrq1w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1l7d2p","score":2,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You need tensor split to have most of tensors in 3090, and only whatever dose not fit into AMD. Disabling/enabling flash attention may help too.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1lrq1w","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You need tensor split to have most of tensors in 3090, and only whatever dose not fit into AMD. Disabling/enabling flash attention may help too.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1lrq1w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751790680,"author_flair_text":null,"treatment_tags":[],"created_utc":1751790680,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1l7d2p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MLDataScientist","can_mod_post":false,"created_utc":1751779044,"send_replies":true,"parent_id":"t1_n1l5ac2","score":2,"author_fullname":"t2_3zy7pnf1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You mean using vulkan backend in llama.cpp? I tried adding RTX 3090 to MI50s but could not get better PP. Not sure what argument in llama cpp allows me to run PP in RTX 3090 only and other operations in MI50s. Let me know if there is a way.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1l7d2p","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You mean using vulkan backend in llama.cpp? I tried adding RTX 3090 to MI50s but could not get better PP. Not sure what argument in llama cpp allows me to run PP in RTX 3090 only and other operations in MI50s. Let me know if there is a way.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1l7d2p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751779044,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1l5ac2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"segmond","can_mod_post":false,"created_utc":1751777975,"send_replies":true,"parent_id":"t3_1lspzn3","score":3,"author_fullname":"t2_ah13x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Have you thought of sticking in 1 nvidia card in there and having that for PP?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1l5ac2","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Have you thought of sticking in 1 nvidia card in there and having that for PP?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1l5ac2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751777975,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lspzn3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1nb0hk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MLDataScientist","can_mod_post":false,"created_utc":1751815192,"send_replies":true,"parent_id":"t1_n1lmex4","score":2,"author_fullname":"t2_3zy7pnf1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes. These cards still have some juice left to run bigger models.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1nb0hk","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes. These cards still have some juice left to run bigger models.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1nb0hk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751815192,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1lmex4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"coolestmage","can_mod_post":false,"created_utc":1751787540,"send_replies":true,"parent_id":"t3_1lspzn3","score":3,"author_fullname":"t2_6dtdz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I also have some MI50s and I didn't realize they performed so much better on Q4_0 and Q4_1. I've been using a lot of Q4_XS and _K_M. I just tested and several models are running more than 2x faster for inference. Thanks for the pointer!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1lmex4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I also have some MI50s and I didn&amp;#39;t realize they performed so much better on Q4_0 and Q4_1. I&amp;#39;ve been using a lot of Q4_XS and _K_M. I just tested and several models are running more than 2x faster for inference. Thanks for the pointer!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1lmex4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751787540,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lspzn3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1m4trv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Afternoon_4260","can_mod_post":false,"created_utc":1751798595,"send_replies":true,"parent_id":"t3_1lspzn3","score":3,"author_fullname":"t2_cj9kap4bx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Vllm for the win as usual","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1m4trv","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Vllm for the win as usual&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1m4trv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751798595,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lspzn3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1kzvn6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MLDataScientist","can_mod_post":false,"created_utc":1751775342,"send_replies":true,"parent_id":"t1_n1kpdz7","score":5,"author_fullname":"t2_3zy7pnf1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I always use no-mmap so that the CPU doesn't get filled with the model that is bigger than my CPU RAM.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1kzvn6","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I always use no-mmap so that the CPU doesn&amp;#39;t get filled with the model that is bigger than my CPU RAM.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1kzvn6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751775342,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n1kpdz7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DinoAmino","can_mod_post":false,"created_utc":1751770642,"send_replies":true,"parent_id":"t3_1lspzn3","score":2,"author_fullname":"t2_j1v7f","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Curious to know when running this (the 235B) model like this ... is there no RAM available to run anything else?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1kpdz7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Curious to know when running this (the 235B) model like this ... is there no RAM available to run anything else?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1kpdz7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751770642,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lspzn3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1mgm6n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"gtek_engineer66","can_mod_post":false,"created_utc":1751804421,"send_replies":true,"parent_id":"t3_1lspzn3","score":2,"author_fullname":"t2_wp10tzju4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You legend! Lovely statistics","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1mgm6n","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You legend! Lovely statistics&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1mgm6n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751804421,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lspzn3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1mm4z2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ke5han","can_mod_post":false,"created_utc":1751806717,"send_replies":true,"parent_id":"t3_1lspzn3","score":2,"author_fullname":"t2_2e4w9jin","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Great, i am about to pull the trigger for a few of them, I was looking for more info regarding the inference performance and the power consumption.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1mm4z2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Great, i am about to pull the trigger for a few of them, I was looking for more info regarding the inference performance and the power consumption.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1mm4z2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751806717,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lspzn3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1nch9k","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Hanthunius","can_mod_post":false,"created_utc":1751815643,"send_replies":true,"parent_id":"t3_1lspzn3","score":2,"author_fullname":"t2_d2gb9jhgg","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This is pretty cool! Thank you for the complete table. We need more experimentations like this. It makes a lot of sense especially for sporadic use where high energy consumption is not so impactful to the bottomline.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1nch9k","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is pretty cool! Thank you for the complete table. We need more experimentations like this. It makes a lot of sense especially for sporadic use where high energy consumption is not so impactful to the bottomline.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1nch9k/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751815643,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lspzn3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1op7ix","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Threatening-Silence-","can_mod_post":false,"created_utc":1751830424,"send_replies":true,"parent_id":"t1_n1ombkb","score":3,"author_fullname":"t2_15wqsifdjf","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I used this one\\n\\nhttps://www.alibaba.com/x/B03rEE?ck=pdp","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n1op7ix","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I used this one&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.alibaba.com/x/B03rEE?ck=pdp\\"&gt;https://www.alibaba.com/x/B03rEE?ck=pdp&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lspzn3","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1op7ix/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751830424,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n1ombkb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MLDataScientist","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1mch32","score":1,"author_fullname":"t2_3zy7pnf1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"is this alibaba?  can you please share the link to this product?","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n1ombkb","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;is this alibaba?  can you please share the link to this product?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lspzn3","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1ombkb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751829520,"author_flair_text":null,"treatment_tags":[],"created_utc":1751829520,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1pxiwt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"donald-bro","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1mch32","score":1,"author_fullname":"t2_scsy7b81","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Can these be plugged in same machine?  Please share when it works. These vram may afford R1.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n1pxiwt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Can these be plugged in same machine?  Please share when it works. These vram may afford R1.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lspzn3","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1pxiwt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751844935,"author_flair_text":null,"treatment_tags":[],"created_utc":1751844935,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1mch32","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Threatening-Silence-","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1lm4a8","score":5,"author_fullname":"t2_15wqsifdjf","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Just ordered 11 cards for shipping into UK. Good price I think.\\n\\nhttps://preview.redd.it/30rtd6bhs8bf1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=a357b13a744346d0eea0605d6af68b012f8482d8\\n\\nThat's 352gb of vram for the same price as 2.5 3090s. Sick.","edited":1751830556,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n1mch32","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just ordered 11 cards for shipping into UK. Good price I think.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/30rtd6bhs8bf1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a357b13a744346d0eea0605d6af68b012f8482d8\\"&gt;https://preview.redd.it/30rtd6bhs8bf1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a357b13a744346d0eea0605d6af68b012f8482d8&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;That&amp;#39;s 352gb of vram for the same price as 2.5 3090s. Sick.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lspzn3","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1mch32/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751802538,"media_metadata":{"30rtd6bhs8bf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":84,"x":108,"u":"https://preview.redd.it/30rtd6bhs8bf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=acd7901515a7f518b66984b490c38dbbcee0e28c"},{"y":169,"x":216,"u":"https://preview.redd.it/30rtd6bhs8bf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5e6002f9c3400de5b37b159247453cf285353de2"},{"y":250,"x":320,"u":"https://preview.redd.it/30rtd6bhs8bf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=40988d4f1615e888c1dd3d84a191e85cb181fda1"},{"y":501,"x":640,"u":"https://preview.redd.it/30rtd6bhs8bf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b468efe655a450dab35e0957154b586b7f756f51"},{"y":752,"x":960,"u":"https://preview.redd.it/30rtd6bhs8bf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c2df2527b70f75a24a769f4f7dfcd7c0a353a81d"},{"y":846,"x":1080,"u":"https://preview.redd.it/30rtd6bhs8bf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=029edd79018e827512a0512bae489967c1c6991c"}],"s":{"y":846,"x":1080,"u":"https://preview.redd.it/30rtd6bhs8bf1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=a357b13a744346d0eea0605d6af68b012f8482d8"},"id":"30rtd6bhs8bf1"}},"author_flair_text":null,"treatment_tags":[],"created_utc":1751802538,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n1lm4a8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No-Refrigerator-1672","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1ky9ng","score":2,"author_fullname":"t2_baavelp5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I've got a pair of 32GBs Mi50s with DHL shipping for just under 300 euro into EU from Alibaba (tax excluded, everything else included). Leaving it there in case anybody from EU will also consider this.","edited":false,"author_flair_css_class":null,"name":"t1_n1lm4a8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve got a pair of 32GBs Mi50s with DHL shipping for just under 300 euro into EU from Alibaba (tax excluded, everything else included). Leaving it there in case anybody from EU will also consider this.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lspzn3","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1lm4a8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751787364,"author_flair_text":null,"collapsed":false,"created_utc":1751787364,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1ky9ng","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"terminoid_","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1kwb5b","score":9,"author_fullname":"t2_1iu07dnz2i","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"you can find em for ~$130 on alibaba, but then shipping is $60, and you have to factor in customs fees. there's a ~$40 processing fee, and either $100 fee from your carrier, or a percentage of the declared value. (thx Trump)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ky9ng","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;you can find em for ~$130 on alibaba, but then shipping is $60, and you have to factor in customs fees. there&amp;#39;s a ~$40 processing fee, and either $100 fee from your carrier, or a percentage of the declared value. (thx Trump)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1ky9ng/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751774591,"author_flair_text":null,"treatment_tags":[],"created_utc":1751774591,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1kz5ke","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"beryugyo619","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1kwb5b","score":2,"author_fullname":"t2_v8wruy0k","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"They sell at that kind of prices on Chinese equivalents of eBay, but they don't really speak or think in English and aren't interested in setting up 1-click international sales. Those of them who do speak English just scalp them at double prices on actual eBay","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1kz5ke","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They sell at that kind of prices on Chinese equivalents of eBay, but they don&amp;#39;t really speak or think in English and aren&amp;#39;t interested in setting up 1-click international sales. Those of them who do speak English just scalp them at double prices on actual eBay&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1kz5ke/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751775007,"author_flair_text":null,"treatment_tags":[],"created_utc":1751775007,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1kwb5b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"--dany--","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1kqnw0","score":5,"author_fullname":"t2_bjeo1gwy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It seems this the price has inflated a lot. No more MI50 32GB at your price any more.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1kwb5b","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It seems this the price has inflated a lot. No more MI50 32GB at your price any more.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1kwb5b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751773696,"author_flair_text":null,"treatment_tags":[],"created_utc":1751773696,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n1kqnw0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"fallingdowndizzyvr","can_mod_post":false,"created_utc":1751771198,"send_replies":true,"parent_id":"t1_n1kl8oe","score":11,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"\\"I bought these cards on eBay when one seller sold them for around $150 \\"","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1kqnw0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&amp;quot;I bought these cards on eBay when one seller sold them for around $150 &amp;quot;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1kqnw0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751771198,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1kzr4b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MLDataScientist","can_mod_post":false,"created_utc":1751775283,"send_replies":true,"parent_id":"t1_n1kl8oe","score":2,"author_fullname":"t2_3zy7pnf1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I was lucky to find these 3 months ago for that price. Note that the prices never were $150. I bought 4 of them and the seller was initially selling them for $230. I negotiated by sending messages on eBay. E.g. \\"there is no warranty after 30 day return window, so I am also taking a risk buying 4\\". So, these GPUs have not failed.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1kzr4b","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I was lucky to find these 3 months ago for that price. Note that the prices never were $150. I bought 4 of them and the seller was initially selling them for $230. I negotiated by sending messages on eBay. E.g. &amp;quot;there is no warranty after 30 day return window, so I am also taking a risk buying 4&amp;quot;. So, these GPUs have not failed.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1kzr4b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751775283,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1kl8oe","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"--dany--","can_mod_post":false,"created_utc":1751768894,"send_replies":true,"parent_id":"t3_1lspzn3","score":4,"author_fullname":"t2_bjeo1gwy","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Where did you get those cards at $150? Are you buying from china directly?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1kl8oe","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Where did you get those cards at $150? Are you buying from china directly?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1kl8oe/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751768894,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lspzn3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1l01zh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MLDataScientist","can_mod_post":false,"created_utc":1751775425,"send_replies":true,"parent_id":"t1_n1ku05g","score":1,"author_fullname":"t2_3zy7pnf1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks! Yes, that fork of vLLM will work fine with 6.3.4.  ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1l01zh","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks! Yes, that fork of vLLM will work fine with 6.3.4.  &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1l01zh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751775425,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1ku05g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"EmPips","can_mod_post":false,"created_utc":1751772665,"send_replies":true,"parent_id":"t3_1lspzn3","score":1,"author_fullname":"t2_w2gxqd6i2","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"vLLM supports 6.3? I checked a few weeks ago and it wasn't happy with any installation above 6.2 .\\n\\nAmazing work though and thanks so much for documenting all of this!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ku05g","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;vLLM supports 6.3? I checked a few weeks ago and it wasn&amp;#39;t happy with any installation above 6.2 .&lt;/p&gt;\\n\\n&lt;p&gt;Amazing work though and thanks so much for documenting all of this!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1ku05g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751772665,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lspzn3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1na98z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MLDataScientist","can_mod_post":false,"created_utc":1751814956,"send_replies":true,"parent_id":"t1_n1lg7ei","score":2,"author_fullname":"t2_3zy7pnf1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes, I installed amdgpus. Did you enable resizable bar? These cards require that.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1na98z","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, I installed amdgpus. Did you enable resizable bar? These cards require that.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1na98z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751814956,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1p210i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"xanduonc","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1owg6l","score":1,"author_fullname":"t2_10n3b6gg97","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Wow, i didn't know  community drivers for gpu exist.  \\n  \\nAnd it actually does work with my cards! Thank you!","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1p210i","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Wow, i didn&amp;#39;t know  community drivers for gpu exist.  &lt;/p&gt;\\n\\n&lt;p&gt;And it actually does work with my cards! Thank you!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1p210i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751834429,"author_flair_text":null,"treatment_tags":[],"created_utc":1751834429,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1owg6l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"created_utc":1751832711,"send_replies":true,"parent_id":"t1_n1lg7ei","score":2,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; Windows does not have any working drivers that accept them\\n\\nHave you tried R.ID?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1owg6l","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Windows does not have any working drivers that accept them&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Have you tried R.ID?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1owg6l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751832711,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1lg7ei","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"xanduonc","can_mod_post":false,"created_utc":1751783920,"send_replies":true,"parent_id":"t3_1lspzn3","score":1,"author_fullname":"t2_10n3b6gg97","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Did you install amdgpu drivers in addition to rocm?  \\n  \\nI bought 2 of these cards and sadly could not get them to work yet. Windows does not have any working drivers that accept them and Linux either crashes at boot time either gets \\"error -12\\" and rocm sees nothing.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1lg7ei","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Did you install amdgpu drivers in addition to rocm?  &lt;/p&gt;\\n\\n&lt;p&gt;I bought 2 of these cards and sadly could not get them to work yet. Windows does not have any working drivers that accept them and Linux either crashes at boot time either gets &amp;quot;error -12&amp;quot; and rocm sees nothing.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1lg7ei/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751783920,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lspzn3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1nnm7h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FunnyAsparagus1253","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1nasd2","score":2,"author_fullname":"t2_i6c8tay3w","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah it’s the driver breaking I’m scared of. Still though, good to know P40 has a true successor! 🤘","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1nnm7h","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah it’s the driver breaking I’m scared of. Still though, good to know P40 has a true successor! 🤘&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1nnm7h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751818970,"author_flair_text":null,"treatment_tags":[],"created_utc":1751818970,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1nasd2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MLDataScientist","can_mod_post":false,"created_utc":1751815123,"send_replies":true,"parent_id":"t1_n1lkq1l","score":1,"author_fullname":"t2_3zy7pnf1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I have RTX 3090 along with these cards. Only vulkan backend in llama cpp supports splitting models across amd and Nvidia gpus but the performance is not great. So, you can in practice do image gen in Nvidia and llms in amd gpus. But you have to be good with Linux commands to not break drivers on both gpus.","edited":1751819282,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1nasd2","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have RTX 3090 along with these cards. Only vulkan backend in llama cpp supports splitting models across amd and Nvidia gpus but the performance is not great. So, you can in practice do image gen in Nvidia and llms in amd gpus. But you have to be good with Linux commands to not break drivers on both gpus.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1nasd2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751815123,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1lkq1l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FunnyAsparagus1253","can_mod_post":false,"created_utc":1751786531,"send_replies":true,"parent_id":"t3_1lspzn3","score":1,"author_fullname":"t2_i6c8tay3w","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If I were to add one of these to my P40 setup, would they a) play well together, split models across cards etc, b) they’d work but I’d have to treat them as separate things (image gen on nvidia, LLMs on AMD for example) or c) trying to set up drivers will destroy my whole system, don’t bother. ? Asking for myself.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1lkq1l","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If I were to add one of these to my P40 setup, would they a) play well together, split models across cards etc, b) they’d work but I’d have to treat them as separate things (image gen on nvidia, LLMs on AMD for example) or c) trying to set up drivers will destroy my whole system, don’t bother. ? Asking for myself.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1lkq1l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751786531,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lspzn3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1ott13","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1omkew","score":2,"author_fullname":"t2_h5utwre7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"PP on exl3 is still better. Despite t/g being lower. So reprocessing for rag is not great, etc.\\n\\n     |    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\\n     |-------|--------|--------|----------|----------|----------|----------|\\n     |  1024 |    256 |      0 |    5.432 |   188.50 |   13.878 |    18.45 |\\n     |  1024 |    256 |   1024 |    5.402 |   189.55 |   14.069 |    18.20 |\\n     |  1024 |    256 |   2048 |    5.434 |   188.43 |   14.268 |    17.94 |\\n     |  1024 |    256 |  16384 |    6.139 |   166.80 |   17.983 |    14.24 |\\n     |  1024 |    256 |  22528 |    6.421 |   159.49 |   19.196 |    13.34 |\\n\\n\\nDeepseek IQ1_S not as good:\\n\\n\\n|    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\\n|-------|--------|--------|----------|----------|----------|----------|\\n|  4096 |   1024 |      0 |   24.428 |   167.68 |   97.109 |    10.54 |","edited":false,"author_flair_css_class":null,"name":"t1_n1ott13","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;PP on exl3 is still better. Despite t/g being lower. So reprocessing for rag is not great, etc.&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt; |    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\\n |-------|--------|--------|----------|----------|----------|----------|\\n |  1024 |    256 |      0 |    5.432 |   188.50 |   13.878 |    18.45 |\\n |  1024 |    256 |   1024 |    5.402 |   189.55 |   14.069 |    18.20 |\\n |  1024 |    256 |   2048 |    5.434 |   188.43 |   14.268 |    17.94 |\\n |  1024 |    256 |  16384 |    6.139 |   166.80 |   17.983 |    14.24 |\\n |  1024 |    256 |  22528 |    6.421 |   159.49 |   19.196 |    13.34 |\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;Deepseek IQ1_S not as good:&lt;/p&gt;\\n\\n&lt;table&gt;&lt;thead&gt;\\n&lt;tr&gt;\\n&lt;th&gt;PP&lt;/th&gt;\\n&lt;th&gt;TG&lt;/th&gt;\\n&lt;th&gt;N_KV&lt;/th&gt;\\n&lt;th&gt;T_PP s&lt;/th&gt;\\n&lt;th&gt;S_PP t/s&lt;/th&gt;\\n&lt;th&gt;T_TG s&lt;/th&gt;\\n&lt;th&gt;S_TG t/s&lt;/th&gt;\\n&lt;/tr&gt;\\n&lt;/thead&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;td&gt;4096&lt;/td&gt;\\n&lt;td&gt;1024&lt;/td&gt;\\n&lt;td&gt;0&lt;/td&gt;\\n&lt;td&gt;24.428&lt;/td&gt;\\n&lt;td&gt;167.68&lt;/td&gt;\\n&lt;td&gt;97.109&lt;/td&gt;\\n&lt;td&gt;10.54&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;/tbody&gt;&lt;/table&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lspzn3","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1ott13/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751831873,"author_flair_text":null,"collapsed":false,"created_utc":1751831873,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1omkew","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MLDataScientist","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1nngrz","score":1,"author_fullname":"t2_3zy7pnf1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"nice metrics! what PP do you get for 4x3090 with mistral large iq4\\\\_xs at 32k context?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1omkew","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;nice metrics! what PP do you get for 4x3090 with mistral large iq4_xs at 32k context?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1omkew/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751829597,"author_flair_text":null,"treatment_tags":[],"created_utc":1751829597,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1nngrz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1ndwm6","score":2,"author_fullname":"t2_h5utwre7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I run it at 32k.. I think the regular version tops out around ~40k anyway per the config files. If I wanted more, I'd have to trade speed for CTX on gpu.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1nngrz","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I run it at 32k.. I think the regular version tops out around ~40k anyway per the config files. If I wanted more, I&amp;#39;d have to trade speed for CTX on gpu.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1nngrz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751818924,"author_flair_text":null,"treatment_tags":[],"created_utc":1751818924,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1ndwm6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MLDataScientist","can_mod_post":false,"created_utc":1751816072,"send_replies":true,"parent_id":"t1_n1maov2","score":2,"author_fullname":"t2_3zy7pnf1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Interesting. Are you referring to Qwen3moe 235B.A22B? What context can you fit with iq4_xs?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ndwm6","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Interesting. Are you referring to Qwen3moe 235B.A22B? What context can you fit with iq4_xs?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1ndwm6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751816072,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1maov2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1751801665,"send_replies":true,"parent_id":"t3_1lspzn3","score":1,"author_fullname":"t2_h5utwre7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"4x3090 gets about 18 with iq4_xs and ik_llama for several times the price and some offloading. I'd call it a good deal.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1maov2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;4x3090 gets about 18 with iq4_xs and ik_llama for several times the price and some offloading. I&amp;#39;d call it a good deal.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1maov2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751801665,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lspzn3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1ned65","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MLDataScientist","can_mod_post":false,"created_utc":1751816209,"send_replies":true,"parent_id":"t1_n1mayhw","score":1,"author_fullname":"t2_3zy7pnf1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Tests column in llama cpp table and columns in vLLM table show the size of test tokens. Text generation is mostly 128 toekns for llama cpp and 256 for vLLM. ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ned65","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Tests column in llama cpp table and columns in vLLM table show the size of test tokens. Text generation is mostly 128 toekns for llama cpp and 256 for vLLM. &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1ned65/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751816209,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1mayhw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cantgetthistowork","can_mod_post":false,"created_utc":1751801796,"send_replies":true,"parent_id":"t3_1lspzn3","score":1,"author_fullname":"t2_j1i0o","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Context size?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1mayhw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Context size?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1mayhw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751801796,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lspzn3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1nf7dq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MLDataScientist","can_mod_post":false,"created_utc":1751816462,"send_replies":true,"parent_id":"t1_n1mhfc2","score":6,"author_fullname":"t2_3zy7pnf1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It is PP - prompt processing speed. If you have large text data e.g. several pages of text, the LLM needs to read that text and that's called prompt processing. For large text data, you may have 10k+ tokens and when you send that text to LLM, it will read all that text at some PP speed. If that PP is low, say 100 t/s then you will need to wait 10k/100 = 100 seconds for the model to process it. Meanwhile, if you have a model with 1k t/s PP, your model will process the same text in 10 seconds. Lots of time saved!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1nf7dq","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It is PP - prompt processing speed. If you have large text data e.g. several pages of text, the LLM needs to read that text and that&amp;#39;s called prompt processing. For large text data, you may have 10k+ tokens and when you send that text to LLM, it will read all that text at some PP speed. If that PP is low, say 100 t/s then you will need to wait 10k/100 = 100 seconds for the model to process it. Meanwhile, if you have a model with 1k t/s PP, your model will process the same text in 10 seconds. Lots of time saved!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1nf7dq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751816462,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n1mhfc2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"gtek_engineer66","can_mod_post":false,"created_utc":1751804773,"send_replies":true,"parent_id":"t3_1lspzn3","score":1,"author_fullname":"t2_wp10tzju4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You got over 1023 tokens second on qwen30 MOE??","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1mhfc2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You got over 1023 tokens second on qwen30 MOE??&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1mhfc2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751804773,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lspzn3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1nfv73","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MLDataScientist","can_mod_post":false,"created_utc":1751816655,"send_replies":true,"parent_id":"t1_n1mshaz","score":2,"author_fullname":"t2_3zy7pnf1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It is just an experiment. I don't have real use case for LLMs as of now. I like tinkering with hardware and software to fix them. Whenever there is a new model, I try to run it with my system to see if I can run it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1nfv73","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It is just an experiment. I don&amp;#39;t have real use case for LLMs as of now. I like tinkering with hardware and software to fix them. Whenever there is a new model, I try to run it with my system to see if I can run it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1nfv73/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751816655,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1mshaz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Safe-Wasabi","can_mod_post":false,"created_utc":1751809136,"send_replies":true,"parent_id":"t3_1lspzn3","score":1,"author_fullname":"t2_5679drnx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What are you actually doing with these big models locally? Do you need it or is it just to experiment to see if it can be done? Thanks","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1mshaz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What are you actually doing with these big models locally? Do you need it or is it just to experiment to see if it can be done? Thanks&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1mshaz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751809136,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lspzn3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1pmay4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MLDataScientist","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1p8i7x","score":1,"author_fullname":"t2_3zy7pnf1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I used 'GLOTRENDS 300mm M.2 Key M to PCIe 4.0 X16 Riser' (around $30).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1pmay4","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I used &amp;#39;GLOTRENDS 300mm M.2 Key M to PCIe 4.0 X16 Riser&amp;#39; (around $30).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1pmay4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751841007,"author_flair_text":null,"treatment_tags":[],"created_utc":1751841007,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1p8i7x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"gnad","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1p7su6","score":1,"author_fullname":"t2_giuiz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"My mobo support x4x4x4x4 bifurcation, so i guess it could work. What m2 to pcie cable are you using?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1p8i7x","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;My mobo support x4x4x4x4 bifurcation, so i guess it could work. What m2 to pcie cable are you using?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1p8i7x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751836419,"author_flair_text":null,"treatment_tags":[],"created_utc":1751836419,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1p7su6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MLDataScientist","can_mod_post":false,"created_utc":1751836200,"send_replies":true,"parent_id":"t1_n1p0mpk","score":1,"author_fullname":"t2_3zy7pnf1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"these cards will run any quant that llama.cpp supports. You can use PCIE 4x4 bifurcation only if your motherboard supports it. Otherwise, the splitter will not help (it will only show 1 or 2 devices). Check your motherboard specs.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1p7su6","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;these cards will run any quant that llama.cpp supports. You can use PCIE 4x4 bifurcation only if your motherboard supports it. Otherwise, the splitter will not help (it will only show 1 or 2 devices). Check your motherboard specs.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1p7su6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751836200,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1p0mpk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"gnad","can_mod_post":false,"created_utc":1751834006,"send_replies":true,"parent_id":"t3_1lspzn3","score":1,"author_fullname":"t2_giuiz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Im looking for a similar setup, already have 96GB RAM. Can this run unsloth UD quant or just regular Q4? Also my mobo only have 1x pcie x16, i guess i can run 4x card on pcie riser splitter + 1 more card on m2 using m2 to pcie adapter?","edited":1751834198,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1p0mpk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Im looking for a similar setup, already have 96GB RAM. Can this run unsloth UD quant or just regular Q4? Also my mobo only have 1x pcie x16, i guess i can run 4x card on pcie riser splitter + 1 more card on m2 using m2 to pcie adapter?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1p0mpk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751834006,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lspzn3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1tmc1g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MLDataScientist","can_mod_post":false,"created_utc":1751902162,"send_replies":true,"parent_id":"t1_n1pws1x","score":1,"author_fullname":"t2_3zy7pnf1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I have not tried it. That should be possible with pytorch. However, note that AMD MI50s do not have matrix/tensor cores, so the training will be slower than, say, rtx 3090.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1tmc1g","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have not tried it. That should be possible with pytorch. However, note that AMD MI50s do not have matrix/tensor cores, so the training will be slower than, say, rtx 3090.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1tmc1g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751902162,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1pws1x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"donald-bro","can_mod_post":false,"created_utc":1751844672,"send_replies":true,"parent_id":"t3_1lspzn3","score":1,"author_fullname":"t2_scsy7b81","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Can we do some fine tune or RL with this config ?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1pws1x","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Can we do some fine tune or RL with this config ?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1pws1x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751844672,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lspzn3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1tlil5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MLDataScientist","can_mod_post":false,"created_utc":1751901923,"send_replies":true,"parent_id":"t1_n1t3da3","score":1,"author_fullname":"t2_3zy7pnf1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"concurrency set to 1 in vllm. \\n\\nllama-3-1-8B-Instruct-GPTQ-Int4:\\n\\nMean TTFT (ms): 65.21 \\n\\nMedian TTFT (ms): 65.14\\n\\nP99 TTFT (ms)**:** 66.3\\n\\n  \\nQwen3-32B-AWQ:\\n\\nMean TTFT (ms): 92.84\\n\\nMedian TTFT (ms): 92.28\\n\\nP99 TTFT (ms)**:** 95.81","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1tlil5","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;concurrency set to 1 in vllm. &lt;/p&gt;\\n\\n&lt;p&gt;llama-3-1-8B-Instruct-GPTQ-Int4:&lt;/p&gt;\\n\\n&lt;p&gt;Mean TTFT (ms): 65.21 &lt;/p&gt;\\n\\n&lt;p&gt;Median TTFT (ms): 65.14&lt;/p&gt;\\n\\n&lt;p&gt;P99 TTFT (ms)&lt;strong&gt;:&lt;/strong&gt; 66.3&lt;/p&gt;\\n\\n&lt;p&gt;Qwen3-32B-AWQ:&lt;/p&gt;\\n\\n&lt;p&gt;Mean TTFT (ms): 92.84&lt;/p&gt;\\n\\n&lt;p&gt;Median TTFT (ms): 92.28&lt;/p&gt;\\n\\n&lt;p&gt;P99 TTFT (ms)&lt;strong&gt;:&lt;/strong&gt; 95.81&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1tlil5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751901923,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1t3da3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ThatsFluke","can_mod_post":false,"created_utc":1751896470,"send_replies":true,"parent_id":"t3_1lspzn3","score":1,"author_fullname":"t2_4l7n4xc6","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What is your time to first token?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1t3da3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What is your time to first token?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1t3da3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751896470,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lspzn3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1ubxhr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ThatsFluke","can_mod_post":false,"created_utc":1751909536,"send_replies":true,"parent_id":"t3_1lspzn3","score":1,"author_fullname":"t2_4l7n4xc6","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"May I ask also where you got 4 MI50s from for $600?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ubxhr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;May I ask also where you got 4 MI50s from for $600?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1ubxhr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751909536,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lspzn3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1xe8qv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CheatCodesOfLife","can_mod_post":false,"created_utc":1751946588,"send_replies":true,"parent_id":"t3_1lspzn3","score":1,"author_fullname":"t2_32el727b","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"hey mate, is this \`llama 7B Q4_0\` llama 1?\\n\\nI don't suppose you know how fast the MI50 can run llama3.2-**3b** at Q8_0 with llama.cpp?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1xe8qv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;hey mate, is this &lt;code&gt;llama 7B Q4_0&lt;/code&gt; llama 1?&lt;/p&gt;\\n\\n&lt;p&gt;I don&amp;#39;t suppose you know how fast the MI50 can run llama3.2-&lt;strong&gt;3b&lt;/strong&gt; at Q8_0 with llama.cpp?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1xe8qv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751946588,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lspzn3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1lzr0i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CheatCodesOfLife","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1l70ps","score":1,"author_fullname":"t2_32el727b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; I know Mac uses system RAM for GPU as well. In PCs, system RAM is separate from GPU VRAM. \\n\\nGood answer! I actually didn't consider that there would be people who only know Mac / Silicon and wouldn't understand the concept of separate system ram + video ram!","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1lzr0i","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;I know Mac uses system RAM for GPU as well. In PCs, system RAM is separate from GPU VRAM. &lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Good answer! I actually didn&amp;#39;t consider that there would be people who only know Mac / Silicon and wouldn&amp;#39;t understand the concept of separate system ram + video ram!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1lzr0i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751795601,"author_flair_text":null,"treatment_tags":[],"created_utc":1751795601,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1l70ps","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MLDataScientist","can_mod_post":false,"created_utc":1751778863,"send_replies":true,"parent_id":"t1_n1l3kaq","score":4,"author_fullname":"t2_3zy7pnf1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I don't have a Mac. But I know Mac uses system RAM for GPU as well. In PCs, system RAM is separate from GPU VRAM. I have 128 VRAM and 96GB RAM.\\n\\n\\n Also, for MoE - mixture of experts - models like qwen3 235B.A22B has 22B active parameters for each token generation. So, remaining parameters are not used for that token generation. Due to this architecture, we can offload some experts to system RAM if you don't have enough VRAM.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1l70ps","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t have a Mac. But I know Mac uses system RAM for GPU as well. In PCs, system RAM is separate from GPU VRAM. I have 128 VRAM and 96GB RAM.&lt;/p&gt;\\n\\n&lt;p&gt; Also, for MoE - mixture of experts - models like qwen3 235B.A22B has 22B active parameters for each token generation. So, remaining parameters are not used for that token generation. Due to this architecture, we can offload some experts to system RAM if you don&amp;#39;t have enough VRAM.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1l70ps/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751778863,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1l6zg7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"created_utc":1751778845,"send_replies":true,"parent_id":"t1_n1l3kaq","score":2,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; also how are you loading &gt;130G on a 128G VRAM?\\n\\n\\"qwen3moe 235B.A22B Q4_1 (**5x** MI50)\\"\\n\\n5x32 = 160. 160 &gt; 130.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1l6zg7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;also how are you loading &amp;gt;130G on a 128G VRAM?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;&amp;quot;qwen3moe 235B.A22B Q4_1 (&lt;strong&gt;5x&lt;/strong&gt; MI50)&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;5x32 = 160. 160 &amp;gt; 130.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1l6zg7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751778845,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1l3kaq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"davikrehalt","can_mod_post":false,"created_utc":1751777114,"send_replies":true,"parent_id":"t3_1lspzn3","score":-1,"author_fullname":"t2_6okc6","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"is there a Mac guide for this? also how are you loading &gt;130G on a 128G VRAM? sorry I'm dumb","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1l3kaq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;is there a Mac guide for this? also how are you loading &amp;gt;130G on a 128G VRAM? sorry I&amp;#39;m dumb&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1l3kaq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751777114,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lspzn3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}},{"kind":"t1","data":{"total_awards_received":0,"approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"ups":-5,"removal_reason":null,"link_id":"t3_1lspzn3","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1m6ntu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Subject_Ratio6842","can_mod_post":false,"created_utc":1751799597,"send_replies":true,"parent_id":"t1_n1lwnbc","score":1,"author_fullname":"t2_y8qz0s2mk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks for sharing. I'll check it out\\n\\n(Many of us like exploring the local llms because we might need solutions dealing with private or sensitive information relating to businesses and we don't want to send our data to other companies)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1m6ntu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for sharing. I&amp;#39;ll check it out&lt;/p&gt;\\n\\n&lt;p&gt;(Many of us like exploring the local llms because we might need solutions dealing with private or sensitive information relating to businesses and we don&amp;#39;t want to send our data to other companies)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lspzn3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1m6ntu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751799597,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1lwnbc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"DELETED","no_follow":true,"author":"[deleted]","can_mod_post":false,"send_replies":true,"parent_id":"t3_1lspzn3","score":-5,"approved_by":null,"report_reasons":null,"all_awardings":[],"subreddit_id":"t5_81eyvm","body":"[removed]","edited":false,"downs":0,"author_flair_css_class":null,"collapsed":true,"is_submitter":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;[removed]&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"associated_award":null,"stickied":false,"subreddit_type":"public","can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/n1lwnbc/","num_reports":null,"locked":false,"name":"t1_n1lwnbc","created":1751793698,"subreddit":"LocalLLaMA","author_flair_text":null,"treatment_tags":[],"created_utc":1751793698,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"","collapsed_because_crowd_control":null,"mod_reports":[],"mod_note":null,"distinguished":null}}],"before":null}}]`),o=()=>e.jsx(l,{data:t});export{o as default};
