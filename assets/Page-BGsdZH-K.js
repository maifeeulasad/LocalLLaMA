import{j as e}from"./index-DACS7Nh6.js";import{R as l}from"./RedditPostRenderer-Dqa1NZuX.js";import"./index-DiMIVQx4.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Every month a new LLM is releasing, beating others in every benchmark, but is it actually better for day to day use?\\n\\nWell, yes, they are smarter, that's for sure, at least on paper, benchmarks don't show the full thing. Thing is, I don't feel like they have actually improved that much, even getting worse, I remember when GPT-3.0 came out on the OpenAI Playground, it was mindblowing, of course I was trying to use it to chat with it, it wasn't pretty, but it worked, then ChatGPT came out, I tried it, and wow, that was amazing, buuuut, only for a while, then after every update it felt less and less useful, one day, I was trying to code with it and it would send the whole code I asked for, then the next day, after an update, it would simply add placeholders where code that I asked it to write had to go.\\n\\nThen GPT-4o came out, sure it was faster, it could do more stuff, but I feel like it was mostly because of the updated knowdelge that comes from the training data more than anything.\\n\\nThis also could apply to some open LLM models, Gemma 1 was horrible, subsequent versions (where are we now, Gemma 3? Will have to check) were much better, but I think we've hit a plateau.\\n\\nWhat do you guys think?\\n\\ntl;dr: LLMs peaked at GPT-3.5 and have been downhill since, being lobotomized every \\"update\\"","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Have LLMs really improved for actual use?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1ls5qjv","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.33,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_wv40imo","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751706778,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Every month a new LLM is releasing, beating others in every benchmark, but is it actually better for day to day use?&lt;/p&gt;\\n\\n&lt;p&gt;Well, yes, they are smarter, that&amp;#39;s for sure, at least on paper, benchmarks don&amp;#39;t show the full thing. Thing is, I don&amp;#39;t feel like they have actually improved that much, even getting worse, I remember when GPT-3.0 came out on the OpenAI Playground, it was mindblowing, of course I was trying to use it to chat with it, it wasn&amp;#39;t pretty, but it worked, then ChatGPT came out, I tried it, and wow, that was amazing, buuuut, only for a while, then after every update it felt less and less useful, one day, I was trying to code with it and it would send the whole code I asked for, then the next day, after an update, it would simply add placeholders where code that I asked it to write had to go.&lt;/p&gt;\\n\\n&lt;p&gt;Then GPT-4o came out, sure it was faster, it could do more stuff, but I feel like it was mostly because of the updated knowdelge that comes from the training data more than anything.&lt;/p&gt;\\n\\n&lt;p&gt;This also could apply to some open LLM models, Gemma 1 was horrible, subsequent versions (where are we now, Gemma 3? Will have to check) were much better, but I think we&amp;#39;ve hit a plateau.&lt;/p&gt;\\n\\n&lt;p&gt;What do you guys think?&lt;/p&gt;\\n\\n&lt;p&gt;tl;dr: LLMs peaked at GPT-3.5 and have been downhill since, being lobotomized every &amp;quot;update&amp;quot;&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1ls5qjv","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Xpl0it_U","discussion_type":null,"num_comments":24,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1ls5qjv/have_llms_really_improved_for_actual_use/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1ls5qjv/have_llms_really_improved_for_actual_use/","subreddit_subscribers":494986,"created_utc":1751706778,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1gzqa2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Firm-Fix-5946","can_mod_post":false,"created_utc":1751724356,"send_replies":true,"parent_id":"t1_n1fzvca","score":2,"author_fullname":"t2_1jwybnbql3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"considering they went right from 3.5 to 4o and pretended 4 never happened, I suspect you're right. if it's not rage bait that level of cherry picking makes this post worth completely ignoring anyway","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1gzqa2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;considering they went right from 3.5 to 4o and pretended 4 never happened, I suspect you&amp;#39;re right. if it&amp;#39;s not rage bait that level of cherry picking makes this post worth completely ignoring anyway&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ls5qjv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ls5qjv/have_llms_really_improved_for_actual_use/n1gzqa2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751724356,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1hfhta","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DinoAmino","can_mod_post":false,"created_utc":1751729504,"send_replies":true,"parent_id":"t1_n1fzvca","score":2,"author_fullname":"t2_j1v7f","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"And it doesn't belong here. OP should go whine about it in r/OpenAI instead.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1hfhta","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;And it doesn&amp;#39;t belong here. OP should go whine about it in &lt;a href=\\"/r/OpenAI\\"&gt;r/OpenAI&lt;/a&gt; instead.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ls5qjv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ls5qjv/have_llms_really_improved_for_actual_use/n1hfhta/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751729504,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1ge6re","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AtomicDouche","can_mod_post":false,"created_utc":1751715244,"send_replies":true,"parent_id":"t1_n1fzvca","score":1,"author_fullname":"t2_79h0p","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Are you having an exponentially better experience compared to the first few models? Linearly? Like, ChatGPT 3.5, ChatGPT 4, now 3 years later in 2025, is the experience significantly different in practise?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ge6re","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Are you having an exponentially better experience compared to the first few models? Linearly? Like, ChatGPT 3.5, ChatGPT 4, now 3 years later in 2025, is the experience significantly different in practise?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ls5qjv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ls5qjv/have_llms_really_improved_for_actual_use/n1ge6re/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751715244,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1fzvca","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"T2WIN","can_mod_post":false,"created_utc":1751706890,"send_replies":true,"parent_id":"t3_1ls5qjv","score":23,"author_fullname":"t2_38ilynpn","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I feel like this is ragebait","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1fzvca","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I feel like this is ragebait&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ls5qjv/have_llms_really_improved_for_actual_use/n1fzvca/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751706890,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ls5qjv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":23}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1gvg70","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RogueZero123","can_mod_post":false,"created_utc":1751722764,"send_replies":true,"parent_id":"t1_n1g033j","score":1,"author_fullname":"t2_12ixxi6q7j","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Agreed. The thinking mode improves outputs on smaller models for logical answers, like coding. But if they don't have the information packed in there, they will still get the wrong answer.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1gvg70","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Agreed. The thinking mode improves outputs on smaller models for logical answers, like coding. But if they don&amp;#39;t have the information packed in there, they will still get the wrong answer.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ls5qjv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ls5qjv/have_llms_really_improved_for_actual_use/n1gvg70/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751722764,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1g033j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"b3081a","can_mod_post":false,"created_utc":1751707027,"send_replies":true,"parent_id":"t3_1ls5qjv","score":10,"author_fullname":"t2_17n5yh7l","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"They've been constantly improving in key areas like coding and tool/agentic use cases, but in most other areas that haven't received much attention, the size of the model still matters a lot.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1g033j","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They&amp;#39;ve been constantly improving in key areas like coding and tool/agentic use cases, but in most other areas that haven&amp;#39;t received much attention, the size of the model still matters a lot.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ls5qjv/have_llms_really_improved_for_actual_use/n1g033j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751707027,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1ls5qjv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1g0t8n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"sersoniko","can_mod_post":false,"created_utc":1751707496,"send_replies":true,"parent_id":"t3_1ls5qjv","score":5,"author_fullname":"t2_22pmb26r","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Gemma 3 also has vision, thinking models like Qwen and DeepSeek have been improving a lot the type of reasoning, but thereâ€™s diminishing return for sure\\n\\nI think the future will focus more and more on agent architectures, where multiple smaller models have access to different tools and collaborate for more complex tasks, at that point what models are used underneath will not be as important","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1g0t8n","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Gemma 3 also has vision, thinking models like Qwen and DeepSeek have been improving a lot the type of reasoning, but thereâ€™s diminishing return for sure&lt;/p&gt;\\n\\n&lt;p&gt;I think the future will focus more and more on agent architectures, where multiple smaller models have access to different tools and collaborate for more complex tasks, at that point what models are used underneath will not be as important&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ls5qjv/have_llms_really_improved_for_actual_use/n1g0t8n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751707496,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ls5qjv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1g0b0f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"catgirl_liker","can_mod_post":false,"created_utc":1751707169,"send_replies":true,"parent_id":"t3_1ls5qjv","score":3,"author_fullname":"t2_w7qdzn48","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Nah, we've peaked at original character.ai","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1g0b0f","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nah, we&amp;#39;ve peaked at original character.ai&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ls5qjv/have_llms_really_improved_for_actual_use/n1g0b0f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751707169,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ls5qjv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1lqzjf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"zoupishness7","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1l9vpl","score":1,"author_fullname":"t2_disot","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I don't remember where I heard it originally, but I just  found out it's called Cunningham's law.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1lqzjf","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t remember where I heard it originally, but I just  found out it&amp;#39;s called Cunningham&amp;#39;s law.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ls5qjv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ls5qjv/have_llms_really_improved_for_actual_use/n1lqzjf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751790236,"author_flair_text":null,"treatment_tags":[],"created_utc":1751790236,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1l9vpl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RobinRelique","can_mod_post":false,"created_utc":1751780382,"send_replies":true,"parent_id":"t1_n1g0f5z","score":1,"author_fullname":"t2_1c34hury96","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Someone remembers this! ðŸ«‚","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1l9vpl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Someone remembers this! ðŸ«‚&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ls5qjv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ls5qjv/have_llms_really_improved_for_actual_use/n1l9vpl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751780382,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1g0f5z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"zoupishness7","can_mod_post":false,"created_utc":1751707244,"send_replies":true,"parent_id":"t3_1ls5qjv","score":2,"author_fullname":"t2_disot","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yeah yeah, if you want a response on the internet, post something mostly wrong, I get it.\\n\\n[https://aistudio.google.com/apps](https://aistudio.google.com/apps)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1g0f5z","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah yeah, if you want a response on the internet, post something mostly wrong, I get it.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://aistudio.google.com/apps\\"&gt;https://aistudio.google.com/apps&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ls5qjv/have_llms_really_improved_for_actual_use/n1g0f5z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751707244,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ls5qjv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1g2v92","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok-Pipe-5151","can_mod_post":false,"created_utc":1751708798,"send_replies":true,"parent_id":"t3_1ls5qjv","score":1,"author_fullname":"t2_uxbdufm8b","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"They are improving, but not significantly.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1g2v92","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They are improving, but not significantly.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ls5qjv/have_llms_really_improved_for_actual_use/n1g2v92/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751708798,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ls5qjv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1kob7q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Lissanro","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1kbhbi","score":1,"author_fullname":"t2_fpfao9g","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I am running R1 0528 locally, to be more specific its IQ4\\\\_K\\\\_M quant - it is just 355 GB, so not that big (compared to the full FP8 version). I previously sharedÂ [how to setup and run it](https://www.reddit.com/r/LocalLLaMA/comments/1jtx05j/comment/mlyf0ux/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button) with ik\\\\_llama.cpp (the linked comment also includes info about my rig and performance).","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1kob7q","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I am running R1 0528 locally, to be more specific its IQ4_K_M quant - it is just 355 GB, so not that big (compared to the full FP8 version). I previously sharedÂ &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1jtx05j/comment/mlyf0ux/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button\\"&gt;how to setup and run it&lt;/a&gt; with ik_llama.cpp (the linked comment also includes info about my rig and performance).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ls5qjv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ls5qjv/have_llms_really_improved_for_actual_use/n1kob7q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751770183,"author_flair_text":null,"treatment_tags":[],"created_utc":1751770183,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1kbhbi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GrungeWerX","can_mod_post":false,"created_utc":1751764896,"send_replies":true,"parent_id":"t1_n1h9qe0","score":1,"author_fullname":"t2_7qduc583w","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Iâ€™m assuming youâ€™re not using R1 0528 locally?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1kbhbi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Iâ€™m assuming youâ€™re not using R1 0528 locally?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ls5qjv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ls5qjv/have_llms_really_improved_for_actual_use/n1kbhbi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751764896,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1h9qe0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Lissanro","can_mod_post":false,"created_utc":1751727682,"send_replies":true,"parent_id":"t3_1ls5qjv","score":1,"author_fullname":"t2_fpfao9g","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For me, it was just improvement after improvement:\\n\\n\\\\- First time I tried an LLM, there were just GPT-2, with the biggest version not released yet, then there also was GPT-J 6B and some others, all interesting but practically useless.\\n\\n\\\\- When ChatGPT was released in Beta, this was first time LLM became actually useful for day to day tasks for me, but it wasn't local - ClosedAI made changes multiple times without my explicit permissions, they can take it down for maintenance without asking me, etc. It actually broke my workflows multiple times (like prompt that used to return useful result started to return just an explanation or partial results) and few times they were down when I needed LLM. And I am not even mentioning privacy issue, which limited me greatly in what projects I can use it with.\\n\\n\\\\- Obviously, I was highly motivated to look for local options. The very first local models that were useful for me, were Llama 2 70B models and some smaller models in 30B-34B range that were available at the time, I was also interested in larger models but none were available - Goliath 120B and the like were cool, but only worked for creative writing and even then had issues. Base Llama 2 wasn't very good so I was mostly using fine-tunes. I could not create any complex programs, context length was very limited, even with context extension tricks going to 12K-16K resulted in noticeable degradation.\\n\\n\\\\- Then, Mixtral 8x7B came out, becoming my daily driver for a while: fast and small, it was great MoE, the very first popular open weight MoE. If I remember right, somewhere at the time, Micu came out too, but it had too many issues, so I barely used it.\\n\\n\\\\- My next daily driver was Mixtral 8x22B, following by WizardLM 8x22B and later WizardLM Beige 8x22B (which had higher MMLU Pro scores than either Mixtral or original WizardLM, and was less prone to unnecessary verbosity or repetition).\\n\\n\\\\- When Llama 3 came out, I was at first looking forward to 405B model, but the very next day Mistral Large 123B cameout, and it easily fit on just four 3090 GPUs, so I ended up using it for a while. For many months it was my main model.\\n\\n\\\\- Eventually, R1 came out (not the recent one, but the old one) - it was huge leap forward for sure, but at the same time wasn't very practical in its raw form. However, soon enough updated V3 came out, which I started to use actively, and after it [R1T](https://huggingface.co/tngtech/DeepSeek-R1T-Chimera) became my daily driver.\\n\\n\\\\- Recent R1 0528 was a big step forward - it manages for me to both avoid thinking too much when I do not need it (well, at least in most cases) and can think a lot on harder tasks, also its Web UI capabilities improved, tool calling was added, etc. I think it is still the best local model, and I can use both for normal chat in SillyTaver or as an agent, for example, with Cline - thanks to fast prompt processing with ik\\\\_llama.cpp, even with old 3090 GPUs it is still practical for daily use. So, R1 0528 is what I use today.\\n\\nThe above is only about text models. For vision, I still use Qwen2.5-VL and look forward for their next update.\\n\\nSo no, your statement \\"LLMs peaked at GPT-3.5 and have been downhill since\\" is clearly not correct. The above is just my personal experience, so it is focused on models that I mainly used at each period of time, and does not even come close to encompass all improvements made in LLMs, I even did not mention in my story Qwen3 models, their both smaller and larger models were an excellent update compared to their previous ones.","edited":1751727982,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1h9qe0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For me, it was just improvement after improvement:&lt;/p&gt;\\n\\n&lt;p&gt;- First time I tried an LLM, there were just GPT-2, with the biggest version not released yet, then there also was GPT-J 6B and some others, all interesting but practically useless.&lt;/p&gt;\\n\\n&lt;p&gt;- When ChatGPT was released in Beta, this was first time LLM became actually useful for day to day tasks for me, but it wasn&amp;#39;t local - ClosedAI made changes multiple times without my explicit permissions, they can take it down for maintenance without asking me, etc. It actually broke my workflows multiple times (like prompt that used to return useful result started to return just an explanation or partial results) and few times they were down when I needed LLM. And I am not even mentioning privacy issue, which limited me greatly in what projects I can use it with.&lt;/p&gt;\\n\\n&lt;p&gt;- Obviously, I was highly motivated to look for local options. The very first local models that were useful for me, were Llama 2 70B models and some smaller models in 30B-34B range that were available at the time, I was also interested in larger models but none were available - Goliath 120B and the like were cool, but only worked for creative writing and even then had issues. Base Llama 2 wasn&amp;#39;t very good so I was mostly using fine-tunes. I could not create any complex programs, context length was very limited, even with context extension tricks going to 12K-16K resulted in noticeable degradation.&lt;/p&gt;\\n\\n&lt;p&gt;- Then, Mixtral 8x7B came out, becoming my daily driver for a while: fast and small, it was great MoE, the very first popular open weight MoE. If I remember right, somewhere at the time, Micu came out too, but it had too many issues, so I barely used it.&lt;/p&gt;\\n\\n&lt;p&gt;- My next daily driver was Mixtral 8x22B, following by WizardLM 8x22B and later WizardLM Beige 8x22B (which had higher MMLU Pro scores than either Mixtral or original WizardLM, and was less prone to unnecessary verbosity or repetition).&lt;/p&gt;\\n\\n&lt;p&gt;- When Llama 3 came out, I was at first looking forward to 405B model, but the very next day Mistral Large 123B cameout, and it easily fit on just four 3090 GPUs, so I ended up using it for a while. For many months it was my main model.&lt;/p&gt;\\n\\n&lt;p&gt;- Eventually, R1 came out (not the recent one, but the old one) - it was huge leap forward for sure, but at the same time wasn&amp;#39;t very practical in its raw form. However, soon enough updated V3 came out, which I started to use actively, and after it &lt;a href=\\"https://huggingface.co/tngtech/DeepSeek-R1T-Chimera\\"&gt;R1T&lt;/a&gt; became my daily driver.&lt;/p&gt;\\n\\n&lt;p&gt;- Recent R1 0528 was a big step forward - it manages for me to both avoid thinking too much when I do not need it (well, at least in most cases) and can think a lot on harder tasks, also its Web UI capabilities improved, tool calling was added, etc. I think it is still the best local model, and I can use both for normal chat in SillyTaver or as an agent, for example, with Cline - thanks to fast prompt processing with ik_llama.cpp, even with old 3090 GPUs it is still practical for daily use. So, R1 0528 is what I use today.&lt;/p&gt;\\n\\n&lt;p&gt;The above is only about text models. For vision, I still use Qwen2.5-VL and look forward for their next update.&lt;/p&gt;\\n\\n&lt;p&gt;So no, your statement &amp;quot;LLMs peaked at GPT-3.5 and have been downhill since&amp;quot; is clearly not correct. The above is just my personal experience, so it is focused on models that I mainly used at each period of time, and does not even come close to encompass all improvements made in LLMs, I even did not mention in my story Qwen3 models, their both smaller and larger models were an excellent update compared to their previous ones.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ls5qjv/have_llms_really_improved_for_actual_use/n1h9qe0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751727682,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ls5qjv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1g1cv9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"stoppableDissolution","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1g0zo2","score":2,"author_fullname":"t2_1n0su21k4z","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I dont have the hardware to run the big one, but I felt like new 32 is way stiffer (and, oddly enough, worse at following instructions, especially with regards to formatting) than old 32. Old 32 was not particularly stellar either, but eva-qwen was at least usable.\\n\\nIt just feels like I'm coercing it to do something it really does not want to do, even if its a perfectly wholesome sfw, idk.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1g1cv9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I dont have the hardware to run the big one, but I felt like new 32 is way stiffer (and, oddly enough, worse at following instructions, especially with regards to formatting) than old 32. Old 32 was not particularly stellar either, but eva-qwen was at least usable.&lt;/p&gt;\\n\\n&lt;p&gt;It just feels like I&amp;#39;m coercing it to do something it really does not want to do, even if its a perfectly wholesome sfw, idk.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ls5qjv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ls5qjv/have_llms_really_improved_for_actual_use/n1g1cv9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751707849,"author_flair_text":null,"treatment_tags":[],"created_utc":1751707849,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1g0zo2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1g0tmh","score":1,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I disagree. Check eqbench. The big Qwen 252 is pretty good, Qwen 3 32b is not terrible either, compared to absolutely unusable for fiction Qwen 2.5 32b.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1g0zo2","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I disagree. Check eqbench. The big Qwen 252 is pretty good, Qwen 3 32b is not terrible either, compared to absolutely unusable for fiction Qwen 2.5 32b.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ls5qjv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ls5qjv/have_llms_really_improved_for_actual_use/n1g0zo2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751707611,"author_flair_text":null,"treatment_tags":[],"created_utc":1751707611,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1g0tmh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"stoppableDissolution","can_mod_post":false,"created_utc":1751707503,"send_replies":true,"parent_id":"t1_n1g0dq3","score":3,"author_fullname":"t2_1n0su21k4z","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Idk, qwen3 is way, WAY more rigid and dry than 2.5. They have really overcooked it on math and benchmark-related things.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1g0tmh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Idk, qwen3 is way, WAY more rigid and dry than 2.5. They have really overcooked it on math and benchmark-related things.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ls5qjv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ls5qjv/have_llms_really_improved_for_actual_use/n1g0tmh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751707503,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n1g0dq3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1751707218,"send_replies":true,"parent_id":"t3_1ls5qjv","score":1,"author_fullname":"t2_uz37qfx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Models seem to have started getting better at creative writing. Qwen 3 and Mistral Small 3.2 are good examples of becoming less stiff and sloppy compared to the Qwen 2.5 and Small 3 correspondingly. Context recall seems to have improved a bit too.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1g0dq3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Models seem to have started getting better at creative writing. Qwen 3 and Mistral Small 3.2 are good examples of becoming less stiff and sloppy compared to the Qwen 2.5 and Small 3 correspondingly. Context recall seems to have improved a bit too.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ls5qjv/have_llms_really_improved_for_actual_use/n1g0dq3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751707218,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ls5qjv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1g25qc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"custodiam99","can_mod_post":false,"created_utc":1751708355,"send_replies":true,"parent_id":"t3_1ls5qjv","score":1,"author_fullname":"t2_nqnhgqqf5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"They are improving. Qwen3 14b can summarize large texts AND create working xml mind maps from them. That would have been impossible a year ago.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1g25qc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They are improving. Qwen3 14b can summarize large texts AND create working xml mind maps from them. That would have been impossible a year ago.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ls5qjv/have_llms_really_improved_for_actual_use/n1g25qc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751708355,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ls5qjv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1g530y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LevianMcBirdo","can_mod_post":false,"created_utc":1751710162,"send_replies":true,"parent_id":"t3_1ls5qjv","score":1,"author_fullname":"t2_cw9f6o0r","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"A lot of people probably just lost the scale of time we are moving in. 3.5 was released 2 1/2 years ago. o1 last year. R1 beginning of this year. Qwen3 2 months ago. The beginning of the reasoning models solved a lot that wasn't solved (reliably) before. AlphaEvolve came out mid may, while not strictly an LLM it shows that maybe standard chatbot isn't perfect for all problems.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1g530y","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;A lot of people probably just lost the scale of time we are moving in. 3.5 was released 2 1/2 years ago. o1 last year. R1 beginning of this year. Qwen3 2 months ago. The beginning of the reasoning models solved a lot that wasn&amp;#39;t solved (reliably) before. AlphaEvolve came out mid may, while not strictly an LLM it shows that maybe standard chatbot isn&amp;#39;t perfect for all problems.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ls5qjv/have_llms_really_improved_for_actual_use/n1g530y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751710162,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ls5qjv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(l,{data:t});export{n as default};
