import{j as e}from"./index-CWmJdUH_.js";import{R as l}from"./RedditPostRenderer-D2iunoQ9.js";import"./index-BCg9RP6g.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I don't get the obsession with llama.cpp. It's completely unusable for any real work. The token generation speed collapses as soon as you add any meaningful context, and the prompt processing is painfully slow. With these fatal flaws, what is anyone actually using this for besides running toy demos? It's fundamentally broken for any serious application.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"llama.cpp is unusable for real work","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m6skm6","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.14,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_qhk9kpc","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753224220,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t get the obsession with llama.cpp. It&amp;#39;s completely unusable for any real work. The token generation speed collapses as soon as you add any meaningful context, and the prompt processing is painfully slow. With these fatal flaws, what is anyone actually using this for besides running toy demos? It&amp;#39;s fundamentally broken for any serious application.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m6skm6","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"d00m_sayer","discussion_type":null,"num_comments":18,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m6skm6/llamacpp_is_unusable_for_real_work/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m6skm6/llamacpp_is_unusable_for_real_work/","subreddit_subscribers":503254,"created_utc":1753224220,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4m33xm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Toooooool","can_mod_post":false,"created_utc":1753224650,"send_replies":true,"parent_id":"t3_1m6skm6","score":10,"author_fullname":"t2_8llornh4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The token generation speed dropping sounds like an out-of-memory issue, where the overflow is shared to the regular RAM and thus the output speed drops drastically.\\n\\nYou'll want to make sure that you're running a language model that's small enough to be stored entirely on your GPU's VRAM, as well as to limit the KV cache to an amount that also fits within the VRAM.  \\n(Hint: if you don't limit the KV it will use whatever the model's maximum is, which can be a lot)\\n\\nAs for why people use it, it scales ridiculously well when it comes to multi-client handling.  \\nMy PNY A4000 might perform 70T/s with 1 client but a collective 280T/s with 64 clients.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4m33xm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The token generation speed dropping sounds like an out-of-memory issue, where the overflow is shared to the regular RAM and thus the output speed drops drastically.&lt;/p&gt;\\n\\n&lt;p&gt;You&amp;#39;ll want to make sure that you&amp;#39;re running a language model that&amp;#39;s small enough to be stored entirely on your GPU&amp;#39;s VRAM, as well as to limit the KV cache to an amount that also fits within the VRAM.&lt;br/&gt;\\n(Hint: if you don&amp;#39;t limit the KV it will use whatever the model&amp;#39;s maximum is, which can be a lot)&lt;/p&gt;\\n\\n&lt;p&gt;As for why people use it, it scales ridiculously well when it comes to multi-client handling.&lt;br/&gt;\\nMy PNY A4000 might perform 70T/s with 1 client but a collective 280T/s with 64 clients.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6skm6/llamacpp_is_unusable_for_real_work/n4m33xm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753224650,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6skm6","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4m6czx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"NNN_Throwaway2","can_mod_post":false,"created_utc":1753225701,"send_replies":true,"parent_id":"t3_1m6skm6","score":10,"author_fullname":"t2_8rrihts9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Maybe you should share your hardware before jumping to such strong assertions.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4m6czx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Maybe you should share your hardware before jumping to such strong assertions.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6skm6/llamacpp_is_unusable_for_real_work/n4m6czx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753225701,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6skm6","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4momx1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DinoAmino","can_mod_post":false,"created_utc":1753231759,"send_replies":true,"parent_id":"t1_n4m7h6r","score":3,"author_fullname":"t2_j1v7f","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Like you said, it's pebkac. OP probably has no GPU. Has posted here before but doesn't interact with the comments. Comments a lot on r/singularity though, if that means anything.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4momx1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Like you said, it&amp;#39;s pebkac. OP probably has no GPU. Has posted here before but doesn&amp;#39;t interact with the comments. Comments a lot on &lt;a href=\\"/r/singularity\\"&gt;r/singularity&lt;/a&gt; though, if that means anything.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6skm6","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6skm6/llamacpp_is_unusable_for_real_work/n4momx1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753231759,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n4m7h6r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"fdg_avid","can_mod_post":false,"created_utc":1753226062,"send_replies":true,"parent_id":"t3_1m6skm6","score":9,"author_fullname":"t2_7vr0myfd","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"As always with posts like this, it is almost certainly a “you” problem. Yes, it’s slower than vLLM and sglang, but not that much slower if used correctly. Stop being a coward and post your hardware specs and precise details of your use case so we can pinpoint the problem with how you are using it. Don’t vague post your complaints and clutter the feed.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4m7h6r","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;As always with posts like this, it is almost certainly a “you” problem. Yes, it’s slower than vLLM and sglang, but not that much slower if used correctly. Stop being a coward and post your hardware specs and precise details of your use case so we can pinpoint the problem with how you are using it. Don’t vague post your complaints and clutter the feed.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6skm6/llamacpp_is_unusable_for_real_work/n4m7h6r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753226062,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6skm6","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4m78k0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Minute_Attempt3063","can_mod_post":false,"created_utc":1753225985,"send_replies":true,"parent_id":"t3_1m6skm6","score":4,"author_fullname":"t2_t6m6d9my","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"No specs, no model mentioned, no real examples where it breaks. Honestly, how do you think people can help?\\n\\nThere are companies fully running this, and i fully believe that Openai is using it in some capacity as well, perhaps only internally.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4m78k0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No specs, no model mentioned, no real examples where it breaks. Honestly, how do you think people can help?&lt;/p&gt;\\n\\n&lt;p&gt;There are companies fully running this, and i fully believe that Openai is using it in some capacity as well, perhaps only internally.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6skm6/llamacpp_is_unusable_for_real_work/n4m78k0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753225985,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6skm6","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4mh2kf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1753229176,"send_replies":true,"parent_id":"t3_1m6skm6","score":5,"author_fullname":"t2_h5utwre7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"So many details, you left nothing out.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4mh2kf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So many details, you left nothing out.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6skm6/llamacpp_is_unusable_for_real_work/n4mh2kf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753229176,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6skm6","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4mii4q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"sleepy_roger","can_mod_post":false,"created_utc":1753229658,"send_replies":true,"parent_id":"t3_1m6skm6","score":5,"author_fullname":"t2_usojvms","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Swap out that Voodoo 3 for something with more vram my dude.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4mii4q","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Swap out that Voodoo 3 for something with more vram my dude.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6skm6/llamacpp_is_unusable_for_real_work/n4mii4q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753229658,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6skm6","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4m44ue","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Interesting8547","can_mod_post":false,"created_utc":1753224980,"send_replies":true,"parent_id":"t3_1m6skm6","score":3,"author_fullname":"t2_d82aa036","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Either you get out of memory (i.e. out of VRAM) or your GPU  can't process prompts fast which means it's probably not Nvidia.  If you still get slow generation and you're with an Nvidia GPU, it means your config is probably wrong.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4m44ue","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Either you get out of memory (i.e. out of VRAM) or your GPU  can&amp;#39;t process prompts fast which means it&amp;#39;s probably not Nvidia.  If you still get slow generation and you&amp;#39;re with an Nvidia GPU, it means your config is probably wrong.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6skm6/llamacpp_is_unusable_for_real_work/n4m44ue/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753224980,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6skm6","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4momoz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"robberviet","can_mod_post":false,"created_utc":1753231757,"send_replies":true,"parent_id":"t3_1m6skm6","score":2,"author_fullname":"t2_jxc5a","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Maybe you just don't know how to use it?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4momoz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Maybe you just don&amp;#39;t know how to use it?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6skm6/llamacpp_is_unusable_for_real_work/n4momoz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753231757,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6skm6","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4m2sbt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"thebadslime","can_mod_post":false,"created_utc":1753224545,"send_replies":true,"parent_id":"t3_1m6skm6","score":1,"author_fullname":"t2_i5os0v0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It works fine for my usecases. I only run small models though.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4m2sbt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It works fine for my usecases. I only run small models though.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6skm6/llamacpp_is_unusable_for_real_work/n4m2sbt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753224545,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6skm6","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4m3yuq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Weary_Long3409","can_mod_post":false,"created_utc":1753224926,"send_replies":true,"parent_id":"t3_1m6skm6","score":1,"author_fullname":"t2_k7w90vyh8","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I left llama.cpp, exllamav2, and vllm for lmdeploy. Turbomind engine is great with 8 bit cache.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4m3yuq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I left llama.cpp, exllamav2, and vllm for lmdeploy. Turbomind engine is great with 8 bit cache.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6skm6/llamacpp_is_unusable_for_real_work/n4m3yuq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753224926,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6skm6","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4m7k4i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fp4guru","can_mod_post":false,"created_utc":1753226089,"send_replies":true,"parent_id":"t3_1m6skm6","score":1,"author_fullname":"t2_1tp8zldw5g","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"We used text generation webui multi-user mode for 3 months for a group of 5 to 10 users. No real issue. Llamacpp is the backend. We switched to vllm after we had to serve up to 40 users.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4m7k4i","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;We used text generation webui multi-user mode for 3 months for a group of 5 to 10 users. No real issue. Llamacpp is the backend. We switched to vllm after we had to serve up to 40 users.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6skm6/llamacpp_is_unusable_for_real_work/n4m7k4i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753226089,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6skm6","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4m7xah","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No-Mountain3817","can_mod_post":false,"created_utc":1753226204,"send_replies":true,"parent_id":"t3_1m6skm6","score":1,"author_fullname":"t2_hylfch6q5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"short answer, check out vllm","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4m7xah","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;short answer, check out vllm&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6skm6/llamacpp_is_unusable_for_real_work/n4m7xah/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753226204,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6skm6","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4mkxjt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"chibop1","can_mod_post":false,"created_utc":1753230488,"send_replies":true,"parent_id":"t3_1m6skm6","score":1,"author_fullname":"t2_e9jh97s","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It sounds like you are trying to use it on Raspberry Pi!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4mkxjt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It sounds like you are trying to use it on Raspberry Pi!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6skm6/llamacpp_is_unusable_for_real_work/n4mkxjt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753230488,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6skm6","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ngii2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ttkciar","can_mod_post":false,"created_utc":1753241821,"send_replies":true,"parent_id":"t3_1m6skm6","score":1,"author_fullname":"t2_cpegz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think your setup might be faulty.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ngii2","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think your setup might be faulty.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6skm6/llamacpp_is_unusable_for_real_work/n4ngii2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753241821,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m6skm6","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4nqebh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DeepWisdomGuy","can_mod_post":false,"created_utc":1753246053,"send_replies":true,"parent_id":"t3_1m6skm6","score":1,"author_fullname":"t2_lznk2wv8h","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Speaking of meaningful context. Task(of \\"real work\\" or \\"for any serious application\\")? Hardware? Configuration? You're not being specific, you're being specifically vague. Is this just hit and run slop?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4nqebh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Speaking of meaningful context. Task(of &amp;quot;real work&amp;quot; or &amp;quot;for any serious application&amp;quot;)? Hardware? Configuration? You&amp;#39;re not being specific, you&amp;#39;re being specifically vague. Is this just hit and run slop?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6skm6/llamacpp_is_unusable_for_real_work/n4nqebh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753246053,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6skm6","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4nvtyx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Efficiency_1144","can_mod_post":false,"created_utc":1753248659,"send_replies":true,"parent_id":"t3_1m6skm6","score":1,"author_fullname":"t2_1nkj9l14b0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The industry standard is mostly vLLM, TensorRT-LLM and Slang along with a framework like Nvidia Dynamo or its predecessor Triton Inference Server. It’s partly because this is the actual setup the clouds, and Nvidia themselves do their testing, tuning and optimising for, so on some level it makes sense to use the same software as Nvidia and the clouds. In particular TensorRT is very flexible it essentially works around a computational graph which you can compile to, optimise or add extensions to. It is more of a universal toolkit than pytorch in that sense.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4nvtyx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The industry standard is mostly vLLM, TensorRT-LLM and Slang along with a framework like Nvidia Dynamo or its predecessor Triton Inference Server. It’s partly because this is the actual setup the clouds, and Nvidia themselves do their testing, tuning and optimising for, so on some level it makes sense to use the same software as Nvidia and the clouds. In particular TensorRT is very flexible it essentially works around a computational graph which you can compile to, optimise or add extensions to. It is more of a universal toolkit than pytorch in that sense.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6skm6/llamacpp_is_unusable_for_real_work/n4nvtyx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753248659,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6skm6","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4o0tjy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Mammoth_Cut_1525","can_mod_post":false,"created_utc":1753251219,"send_replies":true,"parent_id":"t3_1m6skm6","score":1,"author_fullname":"t2_jyjntlyrl","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"L, skill issue, go make your own","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4o0tjy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;L, skill issue, go make your own&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6skm6/llamacpp_is_unusable_for_real_work/n4o0tjy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753251219,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6skm6","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),s=()=>e.jsx(l,{data:a});export{s as default};
