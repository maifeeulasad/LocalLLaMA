import{j as e}from"./index-CWmJdUH_.js";import{R as t}from"./RedditPostRenderer-D2iunoQ9.js";import"./index-BCg9RP6g.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I want to get up to speed with current LLM architecture (in a deep technical way), and in particular understand the major breakthroughs / milestones that got us here, to help give me the intuition to better grasp the context for evolution ahead.\\n\\n**What sequence of technical papers (top 5) do you recommend I read to build this understanding**\\n\\nHere's ChatGPT's recommendations:\\n\\n1. Attention Is All You Need (2017)\\n2. Language Models are Few-Shot Learners (GPT-3, 2020)\\n3. Switch Transformers (2021)\\n4. Training Compute-Optimal LLMs (Chinchilla, 2022)\\n5. LLaMA 3 Technical Report (2025)\\n\\nThanks!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Best sequence of papers to understand evolution of LLMs","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lltmig","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.91,"author_flair_background_color":null,"subreddit_type":"public","ups":9,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_16sbex5btt","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":9,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751030170,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I want to get up to speed with current LLM architecture (in a deep technical way), and in particular understand the major breakthroughs / milestones that got us here, to help give me the intuition to better grasp the context for evolution ahead.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;What sequence of technical papers (top 5) do you recommend I read to build this understanding&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Here&amp;#39;s ChatGPT&amp;#39;s recommendations:&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;Attention Is All You Need (2017)&lt;/li&gt;\\n&lt;li&gt;Language Models are Few-Shot Learners (GPT-3, 2020)&lt;/li&gt;\\n&lt;li&gt;Switch Transformers (2021)&lt;/li&gt;\\n&lt;li&gt;Training Compute-Optimal LLMs (Chinchilla, 2022)&lt;/li&gt;\\n&lt;li&gt;LLaMA 3 Technical Report (2025)&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;Thanks!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lltmig","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"lucaducca","discussion_type":null,"num_comments":5,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lltmig/best_sequence_of_papers_to_understand_evolution/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lltmig/best_sequence_of_papers_to_understand_evolution/","subreddit_subscribers":492233,"created_utc":1751030170,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n034ife","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Amgadoz","can_mod_post":false,"send_replies":true,"parent_id":"t1_n02hnp2","score":1,"author_fullname":"t2_3el21u3z","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Because it's an architecture paper, it isn't exactly about language modeling.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n034ife","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Because it&amp;#39;s an architecture paper, it isn&amp;#39;t exactly about language modeling.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lltmig","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lltmig/best_sequence_of_papers_to_understand_evolution/n034ife/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751040315,"author_flair_text":null,"treatment_tags":[],"created_utc":1751040315,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n02hnp2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"lucaducca","can_mod_post":false,"created_utc":1751033786,"send_replies":true,"parent_id":"t1_n029g87","score":3,"author_fullname":"t2_16sbex5btt","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Amazing thank you - curious why not the attention paper?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n02hnp2","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Amazing thank you - curious why not the attention paper?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lltmig","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lltmig/best_sequence_of_papers_to_understand_evolution/n02hnp2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751033786,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n029g87","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Amgadoz","can_mod_post":false,"created_utc":1751031252,"send_replies":true,"parent_id":"t3_1lltmig","score":6,"author_fullname":"t2_3el21u3z","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Here's my list:\\n\\n1. ULMFit: Universal Language Model Fine-tuning for Text Classification (2017)\\n2. GPT-1: Improving Language Understanding by Generative Pre-Training\\n3. GPT-2: Language Models are Unsupervised Multitask Learners\\n4. GPT-3\\n5. InstructGPT\\n6. FLAN: Finetuned Language Models Are Zero-Shot Learners\\n7. Scaling Laws for Neural Language Models\\n8. Llama3 technical report\\n9. GRPO and DeepSeek math papers","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n029g87","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Here&amp;#39;s my list:&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;ULMFit: Universal Language Model Fine-tuning for Text Classification (2017)&lt;/li&gt;\\n&lt;li&gt;GPT-1: Improving Language Understanding by Generative Pre-Training&lt;/li&gt;\\n&lt;li&gt;GPT-2: Language Models are Unsupervised Multitask Learners&lt;/li&gt;\\n&lt;li&gt;GPT-3&lt;/li&gt;\\n&lt;li&gt;InstructGPT&lt;/li&gt;\\n&lt;li&gt;FLAN: Finetuned Language Models Are Zero-Shot Learners&lt;/li&gt;\\n&lt;li&gt;Scaling Laws for Neural Language Models&lt;/li&gt;\\n&lt;li&gt;Llama3 technical report&lt;/li&gt;\\n&lt;li&gt;GRPO and DeepSeek math papers&lt;/li&gt;\\n&lt;/ol&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lltmig/best_sequence_of_papers_to_understand_evolution/n029g87/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751031252,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lltmig","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n03tjpl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"lompocus","can_mod_post":false,"created_utc":1751047321,"send_replies":true,"parent_id":"t3_1lltmig","score":1,"author_fullname":"t2_f4boq","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"the alexnet paper is well-written, try implementing it yourself with llvm mlir, setting up the tools will be the biggest challenge, afterward it is very easy. afterward cnn details related to invariance on this or that detail. afterward attention. then study state-space models you will eventually find a paper that mathematically subsumes attention. there's more but that should be enough to occupy you, on the diffusion area there is an electromagnetics mathematical subsumption analogous to the state space stuff.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n03tjpl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;the alexnet paper is well-written, try implementing it yourself with llvm mlir, setting up the tools will be the biggest challenge, afterward it is very easy. afterward cnn details related to invariance on this or that detail. afterward attention. then study state-space models you will eventually find a paper that mathematically subsumes attention. there&amp;#39;s more but that should be enough to occupy you, on the diffusion area there is an electromagnetics mathematical subsumption analogous to the state space stuff.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lltmig/best_sequence_of_papers_to_understand_evolution/n03tjpl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751047321,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lltmig","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),s=()=>e.jsx(t,{data:l});export{s as default};
