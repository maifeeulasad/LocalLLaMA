import{j as e}from"./index-xfnGEtuL.js";import{R as t}from"./RedditPostRenderer-DAZRIZZK.js";import"./index-BaNn5-RR.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"New here, new to AI in general — i’m trying to finetune a 7b/8b/9b model for writing in a very specific style, and i have a few questions i could really use some help with :)\\n\\ni’ll be using lora on a cloud service (not local), and the model won’t need to do anything general — it’s only going to be used for this one use case. basically, i’ll give it bullet points or key ideas, and it should expand on them in the target style. so consistency in tone/writing is the main thing that matters to me.\\n\\n* does it make more sense to go with an older model (like mistral 7b, qwen2 7b, gemma2 9b) since newer ones seem more “assistant-y”? or are newer ones still fine if i’m just going to finetune them anyway?\\n* i have about 1.2 million tokens in my dataset right now — is that enough to start with? i can add more if needed.\\n* should i just do supervised finetuning, or would continued pretraining + sft give better results for this kind of task?\\n\\nalso open to any model recommendations if anyone’s done something similar — thanks in advance!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Trying to finetune my first model for writing — need some beginner advice :)","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m7sbb0","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":1,"author_flair_background_color":null,"subreddit_type":"public","ups":7,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_188iniuukq","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":7,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1753325982,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753323428,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;New here, new to AI in general — i’m trying to finetune a 7b/8b/9b model for writing in a very specific style, and i have a few questions i could really use some help with :)&lt;/p&gt;\\n\\n&lt;p&gt;i’ll be using lora on a cloud service (not local), and the model won’t need to do anything general — it’s only going to be used for this one use case. basically, i’ll give it bullet points or key ideas, and it should expand on them in the target style. so consistency in tone/writing is the main thing that matters to me.&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;does it make more sense to go with an older model (like mistral 7b, qwen2 7b, gemma2 9b) since newer ones seem more “assistant-y”? or are newer ones still fine if i’m just going to finetune them anyway?&lt;/li&gt;\\n&lt;li&gt;i have about 1.2 million tokens in my dataset right now — is that enough to start with? i can add more if needed.&lt;/li&gt;\\n&lt;li&gt;should i just do supervised finetuning, or would continued pretraining + sft give better results for this kind of task?&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;also open to any model recommendations if anyone’s done something similar — thanks in advance!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m7sbb0","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Lonely_Original4730","discussion_type":null,"num_comments":1,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m7sbb0/trying_to_finetune_my_first_model_for_writing/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m7sbb0/trying_to_finetune_my_first_model_for_writing/","subreddit_subscribers":503759,"created_utc":1753323428,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ubg44","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rnosov","can_mod_post":false,"created_utc":1753330055,"send_replies":true,"parent_id":"t3_1m7sbb0","score":1,"author_fullname":"t2_18x6fa","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"* It doesn't, go for the newer models\\n* 1.2m will even fit into a context of some models. You'd better off with a (much) bigger dataset.\\n* SFT alone should be fine\\n\\nI'd recommend using a bigger model","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ubg44","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;ul&gt;\\n&lt;li&gt;It doesn&amp;#39;t, go for the newer models&lt;/li&gt;\\n&lt;li&gt;1.2m will even fit into a context of some models. You&amp;#39;d better off with a (much) bigger dataset.&lt;/li&gt;\\n&lt;li&gt;SFT alone should be fine&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;I&amp;#39;d recommend using a bigger model&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7sbb0/trying_to_finetune_my_first_model_for_writing/n4ubg44/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753330055,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7sbb0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),o=()=>e.jsx(t,{data:l});export{o as default};
