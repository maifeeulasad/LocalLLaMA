import{j as e}from"./index-DACS7Nh6.js";import{R as a}from"./RedditPostRenderer-Dqa1NZuX.js";import"./index-DiMIVQx4.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I am new to AI/ML. We are trying to generate captions for images. I tested various versions of Qwen 2.5 VL.   \\n  \\nI was able to run these models in Google Enterprise Colab with g2-standard-8 (8 vCPU, 32GB) and L4 (24 GB GDDR6) GPU. \\n\\nQwen 2.5 VL 3B  \\nCaption generation - average time taken for max pixel 768\\\\*768 - 1.62s  \\nCaption generation - average time taken for max pixel 1024\\\\*1024 - 2.02s  \\nCaption generation - average time taken for max pixel 1280\\\\*1280 - 2.79s\\n\\nQwen 2.5 VL 7B  \\nCaption generation - average time taken for max pixel 768\\\\*768 - 2.21s  \\nCaption generation - average time taken for max pixel 1024\\\\*1024 - 2.73s  \\nCaption generation - average time taken for max pixel 1280\\\\*1280 - 3.64s  \\n  \\nQwen 2.5 VL 7B AWQ  \\nCaption generation - average time taken for max pixel 768\\\\*768 - 2.84s  \\nCaption generation - average time taken for max pixel 1024\\\\*1024 - 2.94s  \\nCaption generation - average time taken for max pixel 1280\\\\*1280 - 3.85s  \\n\\n\\n1. Why 7B AWQ is slower than 7B?  \\n2. What other better Image caption/VQA model exists that runs in less or similar resource requirments?\\n\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Suggestions/Alternatives for Image captions with efficient system requirements","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lzkrwg","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.6,"author_flair_background_color":null,"subreddit_type":"public","ups":1,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_7doe6lck","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":1,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752495232,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I am new to AI/ML. We are trying to generate captions for images. I tested various versions of Qwen 2.5 VL.   &lt;/p&gt;\\n\\n&lt;p&gt;I was able to run these models in Google Enterprise Colab with g2-standard-8 (8 vCPU, 32GB) and L4 (24 GB GDDR6) GPU. &lt;/p&gt;\\n\\n&lt;p&gt;Qwen 2.5 VL 3B&lt;br/&gt;\\nCaption generation - average time taken for max pixel 768*768 - 1.62s&lt;br/&gt;\\nCaption generation - average time taken for max pixel 1024*1024 - 2.02s&lt;br/&gt;\\nCaption generation - average time taken for max pixel 1280*1280 - 2.79s&lt;/p&gt;\\n\\n&lt;p&gt;Qwen 2.5 VL 7B&lt;br/&gt;\\nCaption generation - average time taken for max pixel 768*768 - 2.21s&lt;br/&gt;\\nCaption generation - average time taken for max pixel 1024*1024 - 2.73s&lt;br/&gt;\\nCaption generation - average time taken for max pixel 1280*1280 - 3.64s  &lt;/p&gt;\\n\\n&lt;p&gt;Qwen 2.5 VL 7B AWQ&lt;br/&gt;\\nCaption generation - average time taken for max pixel 768*768 - 2.84s&lt;br/&gt;\\nCaption generation - average time taken for max pixel 1024*1024 - 2.94s&lt;br/&gt;\\nCaption generation - average time taken for max pixel 1280*1280 - 3.85s  &lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;Why 7B AWQ is slower than 7B?&lt;br/&gt;&lt;/li&gt;\\n&lt;li&gt;What other better Image caption/VQA model exists that runs in less or similar resource requirments?&lt;/li&gt;\\n&lt;/ol&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lzkrwg","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"palaniappan_05","discussion_type":null,"num_comments":8,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lzkrwg/suggestionsalternatives_for_image_captions_with/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lzkrwg/suggestionsalternatives_for_image_captions_with/","subreddit_subscribers":499296,"created_utc":1752495232,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n36vmds","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"palaniappan_05","can_mod_post":false,"created_utc":1752545777,"send_replies":true,"parent_id":"t1_n32j7w6","score":1,"author_fullname":"t2_7doe6lck","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks a the explanation. I did above benchmark directly in notebook using transformer library. Now for hosting it and serving via an API, that’s the best approach?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n36vmds","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks a the explanation. I did above benchmark directly in notebook using transformer library. Now for hosting it and serving via an API, that’s the best approach?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzkrwg","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzkrwg/suggestionsalternatives_for_image_captions_with/n36vmds/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752545777,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n32j7w6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"lly0571","can_mod_post":false,"created_utc":1752498556,"send_replies":true,"parent_id":"t3_1lzkrwg","score":2,"author_fullname":"t2_70vzcleel","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"VLMs require some preprocessing such as scaling, normalization, etc before sending the image to the model. These preprocessing steps incur similar overheads for both larger and smaller models.  \\n\\nBesides, W4A16 quantization like AWQ cannot reduce the overhead of the Prefill stage, which might explain the above phenomenon why 7B-AWQ performs close to 7B w/o quantization. If your caption is short, this task is primarily based on the Prefill stage. For example, for Qwen2.5-VL, a 1024×1024 image is encoded as approximately (1024/28)²= 1369 tokens, while the tokens you want to generate might significantly fewer than that.  \\n\\nI believe Qwen2.5-VL is already one of the most efficient open-weight VLMs. Gemma3 or Minicpm-o-2.6 might have better token density which means they can encode more pixels into each visual token. However, Gemma3 uses a fixed 896×896 resolution, which may underperform compared to Qwen-VL in scenarios requiring high resolution or needs to processing non-square aspect ratios. Minicpm-o, on the other hand, may not perform as good as Qwen2.5-VL.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n32j7w6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;VLMs require some preprocessing such as scaling, normalization, etc before sending the image to the model. These preprocessing steps incur similar overheads for both larger and smaller models.  &lt;/p&gt;\\n\\n&lt;p&gt;Besides, W4A16 quantization like AWQ cannot reduce the overhead of the Prefill stage, which might explain the above phenomenon why 7B-AWQ performs close to 7B w/o quantization. If your caption is short, this task is primarily based on the Prefill stage. For example, for Qwen2.5-VL, a 1024×1024 image is encoded as approximately (1024/28)²= 1369 tokens, while the tokens you want to generate might significantly fewer than that.  &lt;/p&gt;\\n\\n&lt;p&gt;I believe Qwen2.5-VL is already one of the most efficient open-weight VLMs. Gemma3 or Minicpm-o-2.6 might have better token density which means they can encode more pixels into each visual token. However, Gemma3 uses a fixed 896×896 resolution, which may underperform compared to Qwen-VL in scenarios requiring high resolution or needs to processing non-square aspect ratios. Minicpm-o, on the other hand, may not perform as good as Qwen2.5-VL.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzkrwg/suggestionsalternatives_for_image_captions_with/n32j7w6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752498556,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzkrwg","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n32us29","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"umtksa","can_mod_post":false,"created_utc":1752502308,"send_replies":true,"parent_id":"t3_1lzkrwg","score":2,"author_fullname":"t2_ut19jt","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"moondream2 can generate really good captions and its really fast in response","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n32us29","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;moondream2 can generate really good captions and its really fast in response&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzkrwg/suggestionsalternatives_for_image_captions_with/n32us29/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752502308,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzkrwg","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n36vwos","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"palaniappan_05","can_mod_post":false,"created_utc":1752545881,"send_replies":true,"parent_id":"t1_n32bugm","score":1,"author_fullname":"t2_7doe6lck","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"We are looking at generating nearly 5000 captions per day. I don’t think pay per API makes sense. So thought of hosting an open source model ourselves for better ROI. Or Am I missing something?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n36vwos","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;We are looking at generating nearly 5000 captions per day. I don’t think pay per API makes sense. So thought of hosting an open source model ourselves for better ROI. Or Am I missing something?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzkrwg","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzkrwg/suggestionsalternatives_for_image_captions_with/n36vwos/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752545881,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n32bugm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MHTMakerspace","can_mod_post":false,"created_utc":1752495896,"send_replies":true,"parent_id":"t3_1lzkrwg","score":1,"author_fullname":"t2_dv0swue7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Is your long-term goal to run this locally, or in Google Enterprise Colab or another cloud?\\n\\nIf a cloud-based service is acceptable, there are several vendors offering an image captioning API.   We've been using   \\\\*Azure AI Vision\\\\*, it doesn't seem to offer a SLA,   works great for batch and near-realtime captioning.\\n\\n&gt;What other better Image caption/VQA model exists that runs in less or similar resource requirments?\\n\\nIn our limited testing on a local GPU, Qwen 2.5 was the fastest Ollama-supported \\"Vision\\" model for a given memory footprint.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n32bugm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Is your long-term goal to run this locally, or in Google Enterprise Colab or another cloud?&lt;/p&gt;\\n\\n&lt;p&gt;If a cloud-based service is acceptable, there are several vendors offering an image captioning API.   We&amp;#39;ve been using   *Azure AI Vision*, it doesn&amp;#39;t seem to offer a SLA,   works great for batch and near-realtime captioning.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;What other better Image caption/VQA model exists that runs in less or similar resource requirments?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;In our limited testing on a local GPU, Qwen 2.5 was the fastest Ollama-supported &amp;quot;Vision&amp;quot; model for a given memory footprint.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzkrwg/suggestionsalternatives_for_image_captions_with/n32bugm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752495896,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzkrwg","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"609bf7d4-01f3-11f0-9760-5611c8333bee","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n36w1lk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"palaniappan_05","can_mod_post":false,"created_utc":1752545929,"send_replies":true,"parent_id":"t1_n32j6de","score":1,"author_fullname":"t2_7doe6lck","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Currently the benchmark is done in Notebook using transformer library. What’s the best approach for hosting it as an API for realtime inference?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n36w1lk","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Currently the benchmark is done in Notebook using transformer library. What’s the best approach for hosting it as an API for realtime inference?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzkrwg","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzkrwg/suggestionsalternatives_for_image_captions_with/n36w1lk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752545929,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n32j6de","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"You_Wen_AzzHu","can_mod_post":false,"created_utc":1752498541,"send_replies":true,"parent_id":"t3_1lzkrwg","score":1,"author_fullname":"t2_p4oxcufl","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Are you using vllm to serve as backend?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n32j6de","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"exllama"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Are you using vllm to serve as backend?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzkrwg/suggestionsalternatives_for_image_captions_with/n32j6de/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752498541,"author_flair_text":"exllama","treatment_tags":[],"link_id":"t3_1lzkrwg","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n33dxwq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Capable-Ad-7494","can_mod_post":false,"created_utc":1752507869,"send_replies":true,"parent_id":"t3_1lzkrwg","score":1,"author_fullname":"t2_9so78ol2","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"use joycaption\\n\\n7b awq, if it’s vllm awq is just slow for whatever reason,.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n33dxwq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;use joycaption&lt;/p&gt;\\n\\n&lt;p&gt;7b awq, if it’s vllm awq is just slow for whatever reason,.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzkrwg/suggestionsalternatives_for_image_captions_with/n33dxwq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752507869,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzkrwg","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),o=()=>e.jsx(a,{data:l});export{o as default};
