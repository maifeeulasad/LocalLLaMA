import{j as e}from"./index-DQXiEb7D.js";import{R as l}from"./RedditPostRenderer-BjndLgq8.js";import"./index-B-ILyjT1.js";const t=JSON.parse('[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hello!\\n\\nI have a server on my network housing the RTX Pro 6000. I\'d like to run a few models so that I can 1. Generate video (open to the interface used, but it seems like comfyui works well) and 2. Run a chat (likely with openwebui).\\n\\nMy question is, what is the most efficient way to run the models? Openllama? I prefer to run it dockerized, but it seems you can really fine tune things using pytorch? openllama i have used, but pytorch i am not familiar with. I am willing to run the models baremetal if it is significantly more efficient/performant.\\n\\nIt would also be beneficial if the program would automatically load/unload models based on their usage as it would be someone non-technical utilizing them and likely not always at the same time with long periods of non-use.\\n\\nAny tips would be appreciated.  Feel free to roast me as long as I can learn something from it ;)","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Best way to run dockerized linux LLM server?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lzggo2","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.33,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_7xyy4","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752479561,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\\n\\n&lt;p&gt;I have a server on my network housing the RTX Pro 6000. I&amp;#39;d like to run a few models so that I can 1. Generate video (open to the interface used, but it seems like comfyui works well) and 2. Run a chat (likely with openwebui).&lt;/p&gt;\\n\\n&lt;p&gt;My question is, what is the most efficient way to run the models? Openllama? I prefer to run it dockerized, but it seems you can really fine tune things using pytorch? openllama i have used, but pytorch i am not familiar with. I am willing to run the models baremetal if it is significantly more efficient/performant.&lt;/p&gt;\\n\\n&lt;p&gt;It would also be beneficial if the program would automatically load/unload models based on their usage as it would be someone non-technical utilizing them and likely not always at the same time with long periods of non-use.&lt;/p&gt;\\n\\n&lt;p&gt;Any tips would be appreciated.  Feel free to roast me as long as I can learn something from it ;)&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lzggo2","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"a_40oz_of_Mickeys","discussion_type":null,"num_comments":1,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lzggo2/best_way_to_run_dockerized_linux_llm_server/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lzggo2/best_way_to_run_dockerized_linux_llm_server/","subreddit_subscribers":499296,"created_utc":1752479561,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n31gih7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MaxKruse96","can_mod_post":false,"created_utc":1752479946,"send_replies":true,"parent_id":"t3_1lzggo2","score":1,"author_fullname":"t2_pfi81","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"for LLM serving i\'d say go for [https://docs.vllm.ai/en/stable/deployment/docker.html](https://docs.vllm.ai/en/stable/deployment/docker.html)   \\nIf you dont need multi-user or want something simpler, [https://github.com/mostlygeek/llama-swap](https://github.com/mostlygeek/llama-swap) offers a docker image, and it contains \\\\`/app/llama-server\\\\` inside the docker image for you to serve models with that (feel free to tinker with the cli flags!), and does auto loading and unloading depending on your config file\\n\\nfor video, comfyui should be the go-to, swarm-ui perhaps","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n31gih7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;for LLM serving i&amp;#39;d say go for &lt;a href=\\"https://docs.vllm.ai/en/stable/deployment/docker.html\\"&gt;https://docs.vllm.ai/en/stable/deployment/docker.html&lt;/a&gt;&lt;br/&gt;\\nIf you dont need multi-user or want something simpler, &lt;a href=\\"https://github.com/mostlygeek/llama-swap\\"&gt;https://github.com/mostlygeek/llama-swap&lt;/a&gt; offers a docker image, and it contains `/app/llama-server` inside the docker image for you to serve models with that (feel free to tinker with the cli flags!), and does auto loading and unloading depending on your config file&lt;/p&gt;\\n\\n&lt;p&gt;for video, comfyui should be the go-to, swarm-ui perhaps&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzggo2/best_way_to_run_dockerized_linux_llm_server/n31gih7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752479946,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzggo2","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]'),n=()=>e.jsx(l,{data:t});export{n as default};
