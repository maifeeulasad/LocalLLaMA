import{j as e}from"./index-M4edQi1P.js";import{R as t}from"./RedditPostRenderer-CESBGIIy.js";import"./index-DFpL1mt4.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"While some models (Gemini, MiniMax, Llama4) claim context lengths in the 1M+ token range, performance beyond ~100K tokens is usually quite poor. Beyond those lengths is it is usually [better to do RAG](https://www.databricks.com/blog/long-context-rag-performance-llms).\\n\\nWhy is that? Does the limit come from architecture or training data?\\n\\nI could see one problem being too much noise/distraction in the attention scores (like in [this paper](https://arxiv.org/pdf/2410.05258)).\\n\\nHowever, I could also see it being from a lack of long-context training data. A novel is around 100K tokens, so it lines up that performance beyond that degrades due to lack of examples. I believe the creators of Fiction.liveBench have also mentioned the difficulty of creating extremely long context benchmarks.\\n\\nWhat is the consensus, and how long might it be until the problem is solved?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"What Causes Poor Long-Context Performance?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lykf92","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.92,"author_flair_background_color":null,"subreddit_type":"public","ups":47,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1bvkixdlpc","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":47,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1752382484,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;While some models (Gemini, MiniMax, Llama4) claim context lengths in the 1M+ token range, performance beyond ~100K tokens is usually quite poor. Beyond those lengths is it is usually &lt;a href=\\"https://www.databricks.com/blog/long-context-rag-performance-llms\\"&gt;better to do RAG&lt;/a&gt;.&lt;/p&gt;\\n\\n&lt;p&gt;Why is that? Does the limit come from architecture or training data?&lt;/p&gt;\\n\\n&lt;p&gt;I could see one problem being too much noise/distraction in the attention scores (like in &lt;a href=\\"https://arxiv.org/pdf/2410.05258\\"&gt;this paper&lt;/a&gt;).&lt;/p&gt;\\n\\n&lt;p&gt;However, I could also see it being from a lack of long-context training data. A novel is around 100K tokens, so it lines up that performance beyond that degrades due to lack of examples. I believe the creators of Fiction.liveBench have also mentioned the difficulty of creating extremely long context benchmarks.&lt;/p&gt;\\n\\n&lt;p&gt;What is the consensus, and how long might it be until the problem is solved?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/88QOjYTKpMZIM86KNi6b42N407jrxbaPAjbmnwm1Z0A.png?auto=webp&amp;s=446351fd930a6a3c2288234f2b8553cdffe12e5b","width":3047,"height":1600},"resolutions":[{"url":"https://external-preview.redd.it/88QOjYTKpMZIM86KNi6b42N407jrxbaPAjbmnwm1Z0A.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=afbb01428ea71e1677bafb9d67ec3dca25236c74","width":108,"height":56},{"url":"https://external-preview.redd.it/88QOjYTKpMZIM86KNi6b42N407jrxbaPAjbmnwm1Z0A.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=66d49f2ee6b1a122d5ca0ad7a49746262dbd0110","width":216,"height":113},{"url":"https://external-preview.redd.it/88QOjYTKpMZIM86KNi6b42N407jrxbaPAjbmnwm1Z0A.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=78d808914c4c69536a9936bd8a7d16f7632e83e8","width":320,"height":168},{"url":"https://external-preview.redd.it/88QOjYTKpMZIM86KNi6b42N407jrxbaPAjbmnwm1Z0A.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=06d504ba6e45772c5e08586046db3251ad0af53b","width":640,"height":336},{"url":"https://external-preview.redd.it/88QOjYTKpMZIM86KNi6b42N407jrxbaPAjbmnwm1Z0A.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5d3882037f7df9fd68eaf79f4448fa60bccbb01b","width":960,"height":504},{"url":"https://external-preview.redd.it/88QOjYTKpMZIM86KNi6b42N407jrxbaPAjbmnwm1Z0A.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5854508c74dcb490f587749a0bee1621d5c15b2e","width":1080,"height":567}],"variants":{},"id":"88QOjYTKpMZIM86KNi6b42N407jrxbaPAjbmnwm1Z0A"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lykf92","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"simulated-souls","discussion_type":null,"num_comments":17,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lykf92/what_causes_poor_longcontext_performance/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lykf92/what_causes_poor_longcontext_performance/","subreddit_subscribers":498344,"created_utc":1752382484,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ulpxr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"SlowFail2433","can_mod_post":false,"created_utc":1752383160,"send_replies":true,"parent_id":"t3_1lykf92","score":20,"author_fullname":"t2_131eezppgs","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Attention is fundamentally a form of message passing on implicit graphs.\\n\\n\\nIt is not necessarily always the optimal message passing algorithm or graph structure for the task.\\n\\n\\nIt is an extremely good fit for our hardware which is why it is used so much though.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ulpxr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Attention is fundamentally a form of message passing on implicit graphs.&lt;/p&gt;\\n\\n&lt;p&gt;It is not necessarily always the optimal message passing algorithm or graph structure for the task.&lt;/p&gt;\\n\\n&lt;p&gt;It is an extremely good fit for our hardware which is why it is used so much though.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lykf92/what_causes_poor_longcontext_performance/n2ulpxr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752383160,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lykf92","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":20}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2un8vc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Koksny","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2umjed","score":3,"author_fullname":"t2_olk3n","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I think it was Raven year or two ago with extremely good benchmarks for long context, but i have no idea how it was implemented, or how it compares to something like modern Gemini.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2un8vc","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think it was Raven year or two ago with extremely good benchmarks for long context, but i have no idea how it was implemented, or how it compares to something like modern Gemini.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lykf92","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lykf92/what_causes_poor_longcontext_performance/n2un8vc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752383912,"author_flair_text":null,"treatment_tags":[],"created_utc":1752383912,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2umjed","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"simulated-souls","can_mod_post":false,"created_utc":1752383564,"send_replies":true,"parent_id":"t1_n2ulzz6","score":7,"author_fullname":"t2_1bvkixdlpc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Aren't RNNs generally worse about it though, since they need to compress the entire context into a fixed-size state?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2umjed","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Aren&amp;#39;t RNNs generally worse about it though, since they need to compress the entire context into a fixed-size state?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lykf92","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lykf92/what_causes_poor_longcontext_performance/n2umjed/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752383564,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2vrdpj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SouvikMandal","can_mod_post":false,"created_utc":1752406366,"send_replies":true,"parent_id":"t1_n2ulzz6","score":1,"author_fullname":"t2_45o2l0mg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Depth has correlation with context length. Deep LLMs might solve this?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2vrdpj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Depth has correlation with context length. Deep LLMs might solve this?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lykf92","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lykf92/what_causes_poor_longcontext_performance/n2vrdpj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752406366,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ulzz6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Koksny","can_mod_post":false,"created_utc":1752383296,"send_replies":true,"parent_id":"t3_1lykf92","score":19,"author_fullname":"t2_olk3n","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt;I could see one problem being too much noise/distraction in the attention scores (like in [this paper](https://arxiv.org/pdf/2410.05258)).\\n\\nPretty much this. Unless we start using RNN's, the issue of noise increasing with context is inevitable.\\n\\n&gt;What is the consensus, and how long might it be until the problem is solved?\\n\\nAs soon as we can scale the models horizontally, run multiple summarizations in background, etc. Essentially with the architecture used across all SOTA models, there is nothing more that can be done, other than to limit the context length.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ulzz6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;I could see one problem being too much noise/distraction in the attention scores (like in &lt;a href=\\"https://arxiv.org/pdf/2410.05258\\"&gt;this paper&lt;/a&gt;).&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Pretty much this. Unless we start using RNN&amp;#39;s, the issue of noise increasing with context is inevitable.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;What is the consensus, and how long might it be until the problem is solved?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;As soon as we can scale the models horizontally, run multiple summarizations in background, etc. Essentially with the architecture used across all SOTA models, there is nothing more that can be done, other than to limit the context length.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lykf92/what_causes_poor_longcontext_performance/n2ulzz6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752383296,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lykf92","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":19}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2upmke","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"simulated-souls","can_mod_post":false,"created_utc":1752385126,"send_replies":true,"parent_id":"t1_n2uoma3","score":13,"author_fullname":"t2_1bvkixdlpc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"My understanding is that the reason MAMBA hasn't seen adoption is because it didn't solve the issue.\\n\\nIt looks good on toy problems and can even get better loss/perplexity in some cases, but it just doesn't match transformers on real-world tasks.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2upmke","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;My understanding is that the reason MAMBA hasn&amp;#39;t seen adoption is because it didn&amp;#39;t solve the issue.&lt;/p&gt;\\n\\n&lt;p&gt;It looks good on toy problems and can even get better loss/perplexity in some cases, but it just doesn&amp;#39;t match transformers on real-world tasks.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lykf92","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lykf92/what_causes_poor_longcontext_performance/n2upmke/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752385126,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":13}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2upq5y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"SlowFail2433","can_mod_post":false,"created_utc":1752385177,"send_replies":true,"parent_id":"t1_n2uoma3","score":5,"author_fullname":"t2_131eezppgs","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It’s fair to call it mainstream now. It was in some Nemotron models recently but also vision/image mamba models are common.\\n\\n\\nThere are significant downsides so it is a trade-off. It also is competing with various linearised, windowed, striding, hierarchical and frequency/fourier/wavelet-space attention setups as well as simply traditional RNN/LSTM/GRU.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2upq5y","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It’s fair to call it mainstream now. It was in some Nemotron models recently but also vision/image mamba models are common.&lt;/p&gt;\\n\\n&lt;p&gt;There are significant downsides so it is a trade-off. It also is competing with various linearised, windowed, striding, hierarchical and frequency/fourier/wavelet-space attention setups as well as simply traditional RNN/LSTM/GRU.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lykf92","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lykf92/what_causes_poor_longcontext_performance/n2upq5y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752385177,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n2uoma3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"BABA_yaaGa","can_mod_post":false,"created_utc":1752384607,"send_replies":true,"parent_id":"t3_1lykf92","score":5,"author_fullname":"t2_meae4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"MAMBA sort of solved this issue but not sure why it hasn't seen mainstream adoption.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2uoma3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;MAMBA sort of solved this issue but not sure why it hasn&amp;#39;t seen mainstream adoption.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lykf92/what_causes_poor_longcontext_performance/n2uoma3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752384607,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lykf92","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2wtnv5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"logicchains","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2ws3bh","score":2,"author_fullname":"t2_chol6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Gemini 2.5 came out within a couple months after that paper was published, and was a huge improvement over Gemini 2.0, especially WRT long context. The paper said the authors (who work at Google) were planning to open source the model, but they never did. Around that time DeepMind adopted a 6 month publishing embargo on competitive ideas: https://www.reddit.com/r/LocalLLaMA/comments/1jp1555/deepmind_will_delay_sharing_research_to_remain/ . And the paper itself demonstrated a strong empirical improvement over transformers at long context, and the approach it used was extremely theoretically clean (using surprisal to determine what new information to memorise), so it'd be surprising if Google didn't try incorporating something like that into Gemini.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2wtnv5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Gemini 2.5 came out within a couple months after that paper was published, and was a huge improvement over Gemini 2.0, especially WRT long context. The paper said the authors (who work at Google) were planning to open source the model, but they never did. Around that time DeepMind adopted a 6 month publishing embargo on competitive ideas: &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1jp1555/deepmind_will_delay_sharing_research_to_remain/\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1jp1555/deepmind_will_delay_sharing_research_to_remain/&lt;/a&gt; . And the paper itself demonstrated a strong empirical improvement over transformers at long context, and the approach it used was extremely theoretically clean (using surprisal to determine what new information to memorise), so it&amp;#39;d be surprising if Google didn&amp;#39;t try incorporating something like that into Gemini.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lykf92","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lykf92/what_causes_poor_longcontext_performance/n2wtnv5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752420253,"author_flair_text":null,"treatment_tags":[],"created_utc":1752420253,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ws3bh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Howard_banister","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2w87xu","score":2,"author_fullname":"t2_27jg5ljp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What evidence leads you to believe that Gemini 2.5 is built on the Titan architecture?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2ws3bh","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What evidence leads you to believe that Gemini 2.5 is built on the Titan architecture?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lykf92","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lykf92/what_causes_poor_longcontext_performance/n2ws3bh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752419786,"author_flair_text":null,"treatment_tags":[],"created_utc":1752419786,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2w87xu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"logicchains","can_mod_post":false,"created_utc":1752413368,"send_replies":true,"parent_id":"t1_n2vhi7n","score":1,"author_fullname":"t2_chol6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Google's pretty much already solved the problem with Gemini 2.5, likely based on ideas from their Titans paper, it's just matter of other labs finding a way to replicate it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2w87xu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Google&amp;#39;s pretty much already solved the problem with Gemini 2.5, likely based on ideas from their Titans paper, it&amp;#39;s just matter of other labs finding a way to replicate it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lykf92","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lykf92/what_causes_poor_longcontext_performance/n2w87xu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752413368,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2vhi7n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"onil_gova","can_mod_post":false,"created_utc":1752401041,"send_replies":true,"parent_id":"t3_1lykf92","score":8,"author_fullname":"t2_vcawomd","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Feels like we’ve hit the same wall we hit with RNNs before Transformers, except this time, we don’t really understand the limitations. Transformers scaled far beyond what anyone imagined, but now long-context failures feel like we’re probing in the dark rather than addressing clearly defined bottlenecks. Maybe the next breakthrough isn’t a new architecture but a deeper scientific understanding of where Transformers break down, so we can make informed design choices instead of empirical hacks.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2vhi7n","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Feels like we’ve hit the same wall we hit with RNNs before Transformers, except this time, we don’t really understand the limitations. Transformers scaled far beyond what anyone imagined, but now long-context failures feel like we’re probing in the dark rather than addressing clearly defined bottlenecks. Maybe the next breakthrough isn’t a new architecture but a deeper scientific understanding of where Transformers break down, so we can make informed design choices instead of empirical hacks.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lykf92/what_causes_poor_longcontext_performance/n2vhi7n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752401041,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lykf92","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2wnr6y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"nomorebuttsplz","can_mod_post":false,"created_utc":1752418481,"send_replies":true,"parent_id":"t1_n2um9m6","score":1,"author_fullname":"t2_syq52","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"But I think the training argument might make sense because it’s trained on context and answer pairs. You can throw a large context into the training set, but what you might need is a long context and all the various types of answers which might be answerable from reading that context. Which could be thousands. It’s not like a math problem where one question always leads to one answer. Qwen have had some success in training in ultra long context model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2wnr6y","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;But I think the training argument might make sense because it’s trained on context and answer pairs. You can throw a large context into the training set, but what you might need is a long context and all the various types of answers which might be answerable from reading that context. Which could be thousands. It’s not like a math problem where one question always leads to one answer. Qwen have had some success in training in ultra long context model.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lykf92","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lykf92/what_causes_poor_longcontext_performance/n2wnr6y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752418481,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2um9m6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"z_3454_pfk","can_mod_post":false,"created_utc":1752383429,"send_replies":true,"parent_id":"t3_1lykf92","score":6,"author_fullname":"t2_askwa","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Main issues are:\\n-positional bias (favours start and end of context)\\n-informational retrieval issues (knows where the information is but can’t access it or encodes it but doesn’t use it)\\n-transformer attention mechanism limitations\\n-poor information management (can’t determine what’s important and what’s not)\\n-noise interference (irrelevant info becomes a distraction)\\n-contradictions (large contexts have contradicting info, confusing the model)\\n-training limitations (bs though because if you chuck in a few studies the context is easily 100k+)\\n-extending long range usually worsens short range performance","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2um9m6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Main issues are:\\n-positional bias (favours start and end of context)\\n-informational retrieval issues (knows where the information is but can’t access it or encodes it but doesn’t use it)\\n-transformer attention mechanism limitations\\n-poor information management (can’t determine what’s important and what’s not)\\n-noise interference (irrelevant info becomes a distraction)\\n-contradictions (large contexts have contradicting info, confusing the model)\\n-training limitations (bs though because if you chuck in a few studies the context is easily 100k+)\\n-extending long range usually worsens short range performance&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lykf92/what_causes_poor_longcontext_performance/n2um9m6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752383429,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lykf92","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2wbtyp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"plankalkul-z1","can_mod_post":false,"created_utc":1752414657,"send_replies":true,"parent_id":"t1_n2vx2ef","score":1,"author_fullname":"t2_w73n3yrsx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; A simple example: \\"The quick brown fox jumps over the lazy dog.\\" An efficient context should not keep \\"the\\", and, depending on the situation, it might even be unimportant to remember the color of the fox.\\n\\n\\nAll that you say here makes sense *to me*.\\n\\n\\nBut, you know, should this be implemented, the \\"BrownFox\\" benchmark will appear in no time, and [majority] of reviewers will argue that model X sucks because it failed to memorize the color of the fox.\\n\\n\\nWhich invariably begs the question: are long-context models of *today* really as bad as we're led to believe?\\n\\n\\nI for one have no answer to that. And, like with almost everything else, the conclusion that I draw for myself is that I have to test it on *my tasks* to find out...","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2wbtyp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;A simple example: &amp;quot;The quick brown fox jumps over the lazy dog.&amp;quot; An efficient context should not keep &amp;quot;the&amp;quot;, and, depending on the situation, it might even be unimportant to remember the color of the fox.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;All that you say here makes sense &lt;em&gt;to me&lt;/em&gt;.&lt;/p&gt;\\n\\n&lt;p&gt;But, you know, should this be implemented, the &amp;quot;BrownFox&amp;quot; benchmark will appear in no time, and [majority] of reviewers will argue that model X sucks because it failed to memorize the color of the fox.&lt;/p&gt;\\n\\n&lt;p&gt;Which invariably begs the question: are long-context models of &lt;em&gt;today&lt;/em&gt; really as bad as we&amp;#39;re led to believe?&lt;/p&gt;\\n\\n&lt;p&gt;I for one have no answer to that. And, like with almost everything else, the conclusion that I draw for myself is that I have to test it on &lt;em&gt;my tasks&lt;/em&gt; to find out...&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lykf92","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lykf92/what_causes_poor_longcontext_performance/n2wbtyp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752414657,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2vx2ef","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"martinerous","can_mod_post":false,"created_utc":1752408976,"send_replies":true,"parent_id":"t3_1lykf92","score":4,"author_fullname":"t2_5tp54ey","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Just speculating here (although have heard some other LLM experts talking about this). \\n\\nA possible approach to improve long context handling would be to create an efficient auto-summarization mechanism that works similarly to our memory. When reading a long text, we do not clutter our memory with the exact replica of the entire text but we are efficient with picking up the key concepts. Determining what is a \\"key concept\\" - that's an issue. For humans, we have this psychological feature of prioritizing memories that have caused intense emotions (surprise effect). We don't care about grammar and language when dealing with memories - we work with concepts directly, which is so much more efficient way to store memories.\\n\\nA simple example: \\"The quick brown fox jumps over the lazy dog.\\" An efficient context should not keep \\"the\\", and, depending on the situation, it might even be unimportant to remember the color of the fox. An efficient context should be dynamic, an LLM should be given instructions first for what's more important and then it would know what to ignore when loading a long text into the \\"context memory\\".","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2vx2ef","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just speculating here (although have heard some other LLM experts talking about this). &lt;/p&gt;\\n\\n&lt;p&gt;A possible approach to improve long context handling would be to create an efficient auto-summarization mechanism that works similarly to our memory. When reading a long text, we do not clutter our memory with the exact replica of the entire text but we are efficient with picking up the key concepts. Determining what is a &amp;quot;key concept&amp;quot; - that&amp;#39;s an issue. For humans, we have this psychological feature of prioritizing memories that have caused intense emotions (surprise effect). We don&amp;#39;t care about grammar and language when dealing with memories - we work with concepts directly, which is so much more efficient way to store memories.&lt;/p&gt;\\n\\n&lt;p&gt;A simple example: &amp;quot;The quick brown fox jumps over the lazy dog.&amp;quot; An efficient context should not keep &amp;quot;the&amp;quot;, and, depending on the situation, it might even be unimportant to remember the color of the fox. An efficient context should be dynamic, an LLM should be given instructions first for what&amp;#39;s more important and then it would know what to ignore when loading a long text into the &amp;quot;context memory&amp;quot;.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lykf92/what_causes_poor_longcontext_performance/n2vx2ef/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752408976,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lykf92","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2w8js1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"logicchains","can_mod_post":false,"created_utc":1752413489,"send_replies":true,"parent_id":"t3_1lykf92","score":3,"author_fullname":"t2_chol6","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Google's pretty much solved it based on something like https://arxiv.org/html/2501.00663v1 , that's why Gemini 2.5 is so much better at long context than other LLMs (it can reliably work with a 500k codebase as context). Other labs are just slow to copy/replicate Google's approach.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2w8js1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Google&amp;#39;s pretty much solved it based on something like &lt;a href=\\"https://arxiv.org/html/2501.00663v1\\"&gt;https://arxiv.org/html/2501.00663v1&lt;/a&gt; , that&amp;#39;s why Gemini 2.5 is so much better at long context than other LLMs (it can reliably work with a 500k codebase as context). Other labs are just slow to copy/replicate Google&amp;#39;s approach.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lykf92/what_causes_poor_longcontext_performance/n2w8js1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752413489,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lykf92","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}}]`),r=()=>e.jsx(t,{data:l});export{r as default};
