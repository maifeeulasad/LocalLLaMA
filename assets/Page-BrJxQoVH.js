import{j as e}from"./index-DLSqWzaI.js";import{R as t}from"./RedditPostRenderer-CysRo2D_.js";import"./index-COXiL3Lo.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I’m trying to build an AI application that transcribes long audio recordings (around hundreds of thousands of tokens) and allows interaction with an LLM. However, every answer I get from searches and inquiries tells me that I need to chunk and vectorize the long text.\\n\\nBut with LLMs like Gemini that support 1M-token context, isn’t building a RAG system somewhat extra?\\n\\nThanks a lot!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"With a 1M context Gemini, does it still make sense to do embedding or use RAG for long texts?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lx10ja","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.92,"author_flair_background_color":null,"subreddit_type":"public","ups":40,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_vw0gx7hg","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":40,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752220264,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I’m trying to build an AI application that transcribes long audio recordings (around hundreds of thousands of tokens) and allows interaction with an LLM. However, every answer I get from searches and inquiries tells me that I need to chunk and vectorize the long text.&lt;/p&gt;\\n\\n&lt;p&gt;But with LLMs like Gemini that support 1M-token context, isn’t building a RAG system somewhat extra?&lt;/p&gt;\\n\\n&lt;p&gt;Thanks a lot!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lx10ja","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"GyozaHoop","discussion_type":null,"num_comments":26,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lx10ja/with_a_1m_context_gemini_does_it_still_make_sense/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lx10ja/with_a_1m_context_gemini_does_it_still_make_sense/","subreddit_subscribers":497503,"created_utc":1752220264,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2jci1m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"damiangorlami","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2j7e30","score":8,"author_fullname":"t2_fkmblwx4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I have similair experience here. I found Gemini to be excellent at simple tasks dumping over 700-800k context to organise, classify and summarize things.\\n\\nI do agree that for coding there is a bit of diminishing returns after 200k context.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2jci1m","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have similair experience here. I found Gemini to be excellent at simple tasks dumping over 700-800k context to organise, classify and summarize things.&lt;/p&gt;\\n\\n&lt;p&gt;I do agree that for coding there is a bit of diminishing returns after 200k context.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx10ja","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx10ja/with_a_1m_context_gemini_does_it_still_make_sense/n2jci1m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752236431,"author_flair_text":null,"treatment_tags":[],"created_utc":1752236431,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}}],"before":null}},"user_reports":[],"saved":false,"id":"n2j7e30","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Echo9Zulu-","can_mod_post":false,"created_utc":1752234458,"send_replies":true,"parent_id":"t1_n2ihyx3","score":9,"author_fullname":"t2_pw77g8dq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I had a task last week with 2.5 Pro that decoded model numbers from rendered html in ai studio that worked out to 750k tokens. This was done manually over the course of several hours and was pretty hard as a needle in haystack challenge, though not part of any formal eval.\\n\\nGemini was even able to adapt to formatting instructions given as edge cases when the input contained those same edge cases at different points while I was working. Few show learning across hundreds of thousands of tokens was very impressive. \\n\\nCrunching raw data vs code/text are very different challenges though, and not dumping context in one shot probably helped as well. Still very impressive.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2j7e30","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I had a task last week with 2.5 Pro that decoded model numbers from rendered html in ai studio that worked out to 750k tokens. This was done manually over the course of several hours and was pretty hard as a needle in haystack challenge, though not part of any formal eval.&lt;/p&gt;\\n\\n&lt;p&gt;Gemini was even able to adapt to formatting instructions given as edge cases when the input contained those same edge cases at different points while I was working. Few show learning across hundreds of thousands of tokens was very impressive. &lt;/p&gt;\\n\\n&lt;p&gt;Crunching raw data vs code/text are very different challenges though, and not dumping context in one shot probably helped as well. Still very impressive.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx10ja","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx10ja/with_a_1m_context_gemini_does_it_still_make_sense/n2j7e30/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752234458,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2jccst","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CYTR_","can_mod_post":false,"created_utc":1752236377,"send_replies":true,"parent_id":"t1_n2ihyx3","score":1,"author_fullname":"t2_qfghd56x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I find that context is best exploited when doing one-shots. Otherwise, it's better to stick with a few thousand tokens.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2jccst","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I find that context is best exploited when doing one-shots. Otherwise, it&amp;#39;s better to stick with a few thousand tokens.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx10ja","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx10ja/with_a_1m_context_gemini_does_it_still_make_sense/n2jccst/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752236377,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2l6t4w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"colbyshores","can_mod_post":false,"created_utc":1752256033,"send_replies":true,"parent_id":"t1_n2ihyx3","score":1,"author_fullname":"t2_e8x2jqef","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I am throwing entire ansible and terraform logs in to gemini to fix my code and it doesn't skip a beat.  This back and forth happens all day under the same chat window.  It actually does retain context over a million tokens","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2l6t4w","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I am throwing entire ansible and terraform logs in to gemini to fix my code and it doesn&amp;#39;t skip a beat.  This back and forth happens all day under the same chat window.  It actually does retain context over a million tokens&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx10ja","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx10ja/with_a_1m_context_gemini_does_it_still_make_sense/n2l6t4w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752256033,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ihyx3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"indicava","can_mod_post":false,"created_utc":1752221350,"send_replies":true,"parent_id":"t3_1lx10ja","score":71,"author_fullname":"t2_4dvff","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The 1M context window is misleading, it’s much more of a technical metric that it is a practical limit. \\n\\nI have yet to see an LLM, including the (very good) Gemini 2.5 Pro that doesn’t collapse or at least deteriorates exponentially after 32k-128k tokens.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ihyx3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The 1M context window is misleading, it’s much more of a technical metric that it is a practical limit. &lt;/p&gt;\\n\\n&lt;p&gt;I have yet to see an LLM, including the (very good) Gemini 2.5 Pro that doesn’t collapse or at least deteriorates exponentially after 32k-128k tokens.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx10ja/with_a_1m_context_gemini_does_it_still_make_sense/n2ihyx3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752221350,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lx10ja","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":71}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ioasq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ttkciar","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2im1q3","score":9,"author_fullname":"t2_cpegz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"None whatsoever.  It's solely from my experience, hence why it's a \\"rule of thumb\\" and not anything more formal :-)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2ioasq","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;None whatsoever.  It&amp;#39;s solely from my experience, hence why it&amp;#39;s a &amp;quot;rule of thumb&amp;quot; and not anything more formal :-)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx10ja","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx10ja/with_a_1m_context_gemini_does_it_still_make_sense/n2ioasq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752225031,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752225031,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2j3j04","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Willdudes","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2im1q3","score":2,"author_fullname":"t2_14v7k3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Fiction.live has a benchmark.\\n\\nhttps://fiction.live/stories/Fiction-liveBench-Mar-25-2025/oQdzQvKHw8JyXbN87\\n\\nRemember that if you add multi-turn there is also degradation.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2j3j04","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Fiction.live has a benchmark.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://fiction.live/stories/Fiction-liveBench-Mar-25-2025/oQdzQvKHw8JyXbN87\\"&gt;https://fiction.live/stories/Fiction-liveBench-Mar-25-2025/oQdzQvKHw8JyXbN87&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Remember that if you add multi-turn there is also degradation.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx10ja","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx10ja/with_a_1m_context_gemini_does_it_still_make_sense/n2j3j04/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752232822,"author_flair_text":null,"treatment_tags":[],"created_utc":1752232822,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2im1q3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Powerful_Survey5044","can_mod_post":false,"created_utc":1752223724,"send_replies":true,"parent_id":"t1_n2iipuk","score":2,"author_fullname":"t2_17bsa02gju","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"do you have any scientific based of that number one-third? or it just base on your experience? can you please share, thank you!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2im1q3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;do you have any scientific based of that number one-third? or it just base on your experience? can you please share, thank you!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx10ja","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx10ja/with_a_1m_context_gemini_does_it_still_make_sense/n2im1q3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752223724,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2l73a7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"colbyshores","can_mod_post":false,"created_utc":1752256111,"send_replies":true,"parent_id":"t1_n2iipuk","score":1,"author_fullname":"t2_e8x2jqef","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That's not true at all or hasn't been for the last several months","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2l73a7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s not true at all or hasn&amp;#39;t been for the last several months&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx10ja","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx10ja/with_a_1m_context_gemini_does_it_still_make_sense/n2l73a7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752256111,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2iipuk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ttkciar","can_mod_post":false,"created_utc":1752221783,"send_replies":true,"parent_id":"t3_1lx10ja","score":20,"author_fullname":"t2_cpegz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Gemini and other long-context models are not great at dealing with long contexts.  Their inference quality gets worse and worse the more of their context you use, and hallucinations become more frequent.  My rule of thumb is that long context models are only useful up to about a third of their claimed context.\\n\\nIMO you're better off using RAG, and then tuning how much retrieved content you load into context, to find the \\"sweet spot\\" where you get the most advantage from it without degrading inference quality too much.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2iipuk","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Gemini and other long-context models are not great at dealing with long contexts.  Their inference quality gets worse and worse the more of their context you use, and hallucinations become more frequent.  My rule of thumb is that long context models are only useful up to about a third of their claimed context.&lt;/p&gt;\\n\\n&lt;p&gt;IMO you&amp;#39;re better off using RAG, and then tuning how much retrieved content you load into context, to find the &amp;quot;sweet spot&amp;quot; where you get the most advantage from it without degrading inference quality too much.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx10ja/with_a_1m_context_gemini_does_it_still_make_sense/n2iipuk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752221783,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lx10ja","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":20}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ixrgz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Afternoon_4260","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2ihh4a","score":2,"author_fullname":"t2_cj9kap4bx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Idk about the new one, but the last one had performance degradation pass that point.\\nBeside price also note that time is a factor, it's not the same time/price passing 30k or 500k ctx to a model, in the long run for multiple interactions that stacks up","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2ixrgz","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Idk about the new one, but the last one had performance degradation pass that point.\\nBeside price also note that time is a factor, it&amp;#39;s not the same time/price passing 30k or 500k ctx to a model, in the long run for multiple interactions that stacks up&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx10ja","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx10ja/with_a_1m_context_gemini_does_it_still_make_sense/n2ixrgz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752230148,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752230148,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2iky0l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok_Warning2146","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2ihh4a","score":1,"author_fullname":"t2_s6sfw4yy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"VRAM wise, RAG is not that costly. I find that embedding models around 130m size works very well already.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2iky0l","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;VRAM wise, RAG is not that costly. I find that embedding models around 130m size works very well already.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx10ja","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx10ja/with_a_1m_context_gemini_does_it_still_make_sense/n2iky0l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752223082,"author_flair_text":null,"treatment_tags":[],"created_utc":1752223082,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ihh4a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GyozaHoop","can_mod_post":false,"created_utc":1752221070,"send_replies":true,"parent_id":"t1_n2igw9d","score":0,"author_fullname":"t2_vw0gx7hg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks, I think your point makes a lot of sense.\\n\\nRight now, building a RAG system is more costly for me LOL(mainly in terms of learning and time).\\n\\nSo I’d rather build an MVP first to see if my idea works. \\n\\nSo from your perspective, using Gemini to handle over 100k tokens shouldn’t cause performance or hallucination issues, right?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ihh4a","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks, I think your point makes a lot of sense.&lt;/p&gt;\\n\\n&lt;p&gt;Right now, building a RAG system is more costly for me LOL(mainly in terms of learning and time).&lt;/p&gt;\\n\\n&lt;p&gt;So I’d rather build an MVP first to see if my idea works. &lt;/p&gt;\\n\\n&lt;p&gt;So from your perspective, using Gemini to handle over 100k tokens shouldn’t cause performance or hallucination issues, right?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx10ja","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx10ja/with_a_1m_context_gemini_does_it_still_make_sense/n2ihh4a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752221070,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n2igw9d","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"solidsnakeblue","can_mod_post":false,"created_utc":1752220740,"send_replies":true,"parent_id":"t3_1lx10ja","score":7,"author_fullname":"t2_7zh6fslk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think the answer is it depends.  If you need the full audio recording in context at once so the AI can really understand the nuance of it then you should use Gemini.  But this is expensive in the long run.  If you only need to query small parts of the recording at a time RAG will probably work just fine and save $$.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2igw9d","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think the answer is it depends.  If you need the full audio recording in context at once so the AI can really understand the nuance of it then you should use Gemini.  But this is expensive in the long run.  If you only need to query small parts of the recording at a time RAG will probably work just fine and save $$.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx10ja/with_a_1m_context_gemini_does_it_still_make_sense/n2igw9d/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752220740,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lx10ja","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ime7g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"KernQ","can_mod_post":false,"created_utc":1752223923,"send_replies":true,"parent_id":"t3_1lx10ja","score":5,"author_fullname":"t2_1pozn81kn1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Test test test!\\n\\nDepending on what you mean by \\"interact with\\" you may find traditional search is better and faster. Eg - something like \\"How many times is the word _frog_ mentioned?\\" may run into the Rs in Strawberry problem and would be faster and more accurate/reliable without using the model itself (eg by providing a \\"word count\\" UI element or giving the model a WordCount tool).\\n\\nConsider your token costs as well. Depending on the API each question may require sending the entire context (hundreds of thousands of tokens each time). Gemini allows explicit caching, but they still charge you ($0.6 per cache entry and $4.50 per 1M tokens per hour 😳💶🔥).\\n\\nThink about making a suite of analysis tools and using the AI as the UI to call them (with an ID for the content in your backend, not the actual content). Use the AI to create vector embeddings that can be stored and queried \\"for free\\" in your backend.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ime7g","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Test test test!&lt;/p&gt;\\n\\n&lt;p&gt;Depending on what you mean by &amp;quot;interact with&amp;quot; you may find traditional search is better and faster. Eg - something like &amp;quot;How many times is the word &lt;em&gt;frog&lt;/em&gt; mentioned?&amp;quot; may run into the Rs in Strawberry problem and would be faster and more accurate/reliable without using the model itself (eg by providing a &amp;quot;word count&amp;quot; UI element or giving the model a WordCount tool).&lt;/p&gt;\\n\\n&lt;p&gt;Consider your token costs as well. Depending on the API each question may require sending the entire context (hundreds of thousands of tokens each time). Gemini allows explicit caching, but they still charge you ($0.6 per cache entry and $4.50 per 1M tokens per hour 😳💶🔥).&lt;/p&gt;\\n\\n&lt;p&gt;Think about making a suite of analysis tools and using the AI as the UI to call them (with an ID for the content in your backend, not the actual content). Use the AI to create vector embeddings that can be stored and queried &amp;quot;for free&amp;quot; in your backend.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx10ja/with_a_1m_context_gemini_does_it_still_make_sense/n2ime7g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752223923,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lx10ja","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2io19g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"mags0ft","can_mod_post":false,"created_utc":1752224876,"send_replies":true,"parent_id":"t3_1lx10ja","score":5,"author_fullname":"t2_1tbbr8pu4s","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"As far as I'm aware, the technology behind Gemini &amp; co. doesn't really work as well as soon as you go above 64k-128k tokens. Performance just degrades from there, even though your content is technically still in the context window. Might be a limitation of the Transformer architecture, but only time and the smart people at Google etc. will tell whether RAG is going to become obsolete. I don't think that'll happen soon.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2io19g","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;As far as I&amp;#39;m aware, the technology behind Gemini &amp;amp; co. doesn&amp;#39;t really work as well as soon as you go above 64k-128k tokens. Performance just degrades from there, even though your content is technically still in the context window. Might be a limitation of the Transformer architecture, but only time and the smart people at Google etc. will tell whether RAG is going to become obsolete. I don&amp;#39;t think that&amp;#39;ll happen soon.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx10ja/with_a_1m_context_gemini_does_it_still_make_sense/n2io19g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752224876,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lx10ja","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2is4i6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Remarkable-Law9287","can_mod_post":false,"created_utc":1752227208,"send_replies":true,"parent_id":"t3_1lx10ja","score":4,"author_fullname":"t2_95rp229c","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"gemini cant answer without hallucinating after 100k tokens.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2is4i6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;gemini cant answer without hallucinating after 100k tokens.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx10ja/with_a_1m_context_gemini_does_it_still_make_sense/n2is4i6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752227208,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lx10ja","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2j5y4c","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Lesser-than","can_mod_post":false,"created_utc":1752233863,"send_replies":true,"parent_id":"t3_1lx10ja","score":2,"author_fullname":"t2_98d256k","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If only 100 tokens of your 1000 tokens you input are relevent, then you just poisoned the context window with 900 tokens of garbage you did not want, this doesnt seem like a big deal but if you need large context for discussion room you dont want those 900 poisened tokens to keep coming back and filling it up. That and Quadratic scaling context with transformer models means the advertised supported context length is hypothetical and becomes largly unusable at a certain point anyway.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2j5y4c","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If only 100 tokens of your 1000 tokens you input are relevent, then you just poisoned the context window with 900 tokens of garbage you did not want, this doesnt seem like a big deal but if you need large context for discussion room you dont want those 900 poisened tokens to keep coming back and filling it up. That and Quadratic scaling context with transformer models means the advertised supported context length is hypothetical and becomes largly unusable at a certain point anyway.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx10ja/with_a_1m_context_gemini_does_it_still_make_sense/n2j5y4c/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752233863,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lx10ja","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2jc5um","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"damiangorlami","can_mod_post":false,"created_utc":1752236307,"send_replies":true,"parent_id":"t3_1lx10ja","score":2,"author_fullname":"t2_fkmblwx4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"RAG will always be needed because some companies have at least over 20M tokens in code, documents, data, etc.\\n\\nEven if we would scale the current architecture 10x it would still not be viable.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2jc5um","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;RAG will always be needed because some companies have at least over 20M tokens in code, documents, data, etc.&lt;/p&gt;\\n\\n&lt;p&gt;Even if we would scale the current architecture 10x it would still not be viable.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx10ja/with_a_1m_context_gemini_does_it_still_make_sense/n2jc5um/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752236307,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lx10ja","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2j7sd9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"night0x63","can_mod_post":false,"created_utc":1752234619,"send_replies":true,"parent_id":"t3_1lx10ja","score":1,"author_fullname":"t2_3h2irqtz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"As another user said, test it.\\n\\n\\nIf you have less than 128k tokens test llama3.3. for 128k to 10m test with llama4.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2j7sd9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;As another user said, test it.&lt;/p&gt;\\n\\n&lt;p&gt;If you have less than 128k tokens test llama3.3. for 128k to 10m test with llama4.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx10ja/with_a_1m_context_gemini_does_it_still_make_sense/n2j7sd9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752234619,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lx10ja","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2jg4lp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"joey2scoops","can_mod_post":false,"created_utc":1752237738,"send_replies":true,"parent_id":"t3_1lx10ja","score":1,"author_fullname":"t2_16wxcj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"As others have said, I've found Gemini context to be useless past say 300k on a good day. I guess it says 1M on the tin but I would not push it anywhere near that.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2jg4lp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;As others have said, I&amp;#39;ve found Gemini context to be useless past say 300k on a good day. I guess it says 1M on the tin but I would not push it anywhere near that.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx10ja/with_a_1m_context_gemini_does_it_still_make_sense/n2jg4lp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752237738,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lx10ja","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2jyic2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No-Source-9920","can_mod_post":false,"created_utc":1752243593,"send_replies":true,"parent_id":"t3_1lx10ja","score":1,"author_fullname":"t2_1gew47j6vy","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"after 10k to 16k context ALL llms deteriorate significantly.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2jyic2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;after 10k to 16k context ALL llms deteriorate significantly.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx10ja/with_a_1m_context_gemini_does_it_still_make_sense/n2jyic2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752243593,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lx10ja","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2kou3o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"PsychohistorySeldon","can_mod_post":false,"created_utc":1752251039,"send_replies":true,"parent_id":"t3_1lx10ja","score":1,"author_fullname":"t2_bk621","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I've actually tested this in production at scale. Yes, it's true that generally recall goes down the larger the context window. However, Gemini (at least 2.5 Pro) still performs incredibly well at around 700-900k input tokens, being able to both recall the full content exhaustively as well as \\"needle in the haystack\\" problems. \\n\\nSo to answer your question: no, it doesn't. Not for most applications. RAG was never really meant to be used for this type of use case; it was just a patch. If you're interested in building memory/knowledge graphs, yes by all means, RAG is the way to go for now. \\n\\nAlso, Gemini's frontend application uses RAG under the hood when you go above 1M tokens. (Similar to how custom GPTs pull from knowledge base). If you have imperative language in the fetched chunks, it will very frequently conflict with the overall purpose of the prompt.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2kou3o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve actually tested this in production at scale. Yes, it&amp;#39;s true that generally recall goes down the larger the context window. However, Gemini (at least 2.5 Pro) still performs incredibly well at around 700-900k input tokens, being able to both recall the full content exhaustively as well as &amp;quot;needle in the haystack&amp;quot; problems. &lt;/p&gt;\\n\\n&lt;p&gt;So to answer your question: no, it doesn&amp;#39;t. Not for most applications. RAG was never really meant to be used for this type of use case; it was just a patch. If you&amp;#39;re interested in building memory/knowledge graphs, yes by all means, RAG is the way to go for now. &lt;/p&gt;\\n\\n&lt;p&gt;Also, Gemini&amp;#39;s frontend application uses RAG under the hood when you go above 1M tokens. (Similar to how custom GPTs pull from knowledge base). If you have imperative language in the fetched chunks, it will very frequently conflict with the overall purpose of the prompt.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx10ja/with_a_1m_context_gemini_does_it_still_make_sense/n2kou3o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752251039,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lx10ja","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2kp7u1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"premium0","can_mod_post":false,"created_utc":1752251149,"send_replies":true,"parent_id":"t3_1lx10ja","score":1,"author_fullname":"t2_crkio","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Everyone’s talking about the drop in performance after x amount of tokens, but the biggest issue would be the latency for anything requiring realtime interactiveness.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2kp7u1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Everyone’s talking about the drop in performance after x amount of tokens, but the biggest issue would be the latency for anything requiring realtime interactiveness.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx10ja/with_a_1m_context_gemini_does_it_still_make_sense/n2kp7u1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752251149,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lx10ja","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2l0v1s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Xamanthas","can_mod_post":false,"created_utc":1752254402,"send_replies":true,"parent_id":"t3_1lx10ja","score":1,"author_fullname":"t2_e6bnx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You have no business building paid products for people if this is a serious question.\\n\\nI agree with /u/joey2scoops","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2l0v1s","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You have no business building paid products for people if this is a serious question.&lt;/p&gt;\\n\\n&lt;p&gt;I agree with &lt;a href=\\"/u/joey2scoops\\"&gt;/u/joey2scoops&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx10ja/with_a_1m_context_gemini_does_it_still_make_sense/n2l0v1s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752254402,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lx10ja","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2j3it3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"Physical-Ad-7770","can_mod_post":false,"created_utc":1752232819,"send_replies":true,"parent_id":"t3_1lx10ja","score":-5,"author_fullname":"t2_9fppyyst","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":true,"body":"Great question — and it’s a common one now that we have these huge context windows.\\n\\n\\n\\nEven if your transcribed text fits inside 1M tokens, RAG still brings real advantages:\\n\\n✅ Keeps the prompt leaner → faster responses and lower cost per query.\\n\\n✅ Lets you update or correct data dynamically (without re-prompting or re-embedding the whole block).\\n\\n✅ Adds grounding &amp; citation → so answers can explicitly reference the retrieved chunk, not just blend it in.\\n\\n\\n\\nWe’re actually building Lumine: an independent API to add powerful RAG (Retrieval-Augmented Generation) to your app, SaaS, or AI agent — exactly to solve this problem when you want both large context and smart, targeted retrieval.\\n\\n\\n\\nWe're in soft launch if you'd like to explore or share feedback:\\n\\nlumine","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2j3it3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Great question — and it’s a common one now that we have these huge context windows.&lt;/p&gt;\\n\\n&lt;p&gt;Even if your transcribed text fits inside 1M tokens, RAG still brings real advantages:&lt;/p&gt;\\n\\n&lt;p&gt;✅ Keeps the prompt leaner → faster responses and lower cost per query.&lt;/p&gt;\\n\\n&lt;p&gt;✅ Lets you update or correct data dynamically (without re-prompting or re-embedding the whole block).&lt;/p&gt;\\n\\n&lt;p&gt;✅ Adds grounding &amp;amp; citation → so answers can explicitly reference the retrieved chunk, not just blend it in.&lt;/p&gt;\\n\\n&lt;p&gt;We’re actually building Lumine: an independent API to add powerful RAG (Retrieval-Augmented Generation) to your app, SaaS, or AI agent — exactly to solve this problem when you want both large context and smart, targeted retrieval.&lt;/p&gt;\\n\\n&lt;p&gt;We&amp;#39;re in soft launch if you&amp;#39;d like to explore or share feedback:&lt;/p&gt;\\n\\n&lt;p&gt;lumine&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx10ja/with_a_1m_context_gemini_does_it_still_make_sense/n2j3it3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752232819,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lx10ja","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-5}}],"before":null}}]`),r=()=>e.jsx(t,{data:l});export{r as default};
