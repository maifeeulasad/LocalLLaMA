import{j as e}from"./index-CNyNkRpk.js";import{R as l}from"./RedditPostRenderer-Dza0u9i2.js";import"./index-BUchu_-K.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"A project to bring CUDA to non-Nvidia GPUs is making major progress","link_flair_richtext":[{"e":"text","t":"News"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":78,"top_awarded_type":null,"hide_score":false,"name":"t3_1lqvovt","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.97,"author_flair_background_color":null,"ups":442,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1ipy2mlwcz","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"News","can_mod_post":false,"score":442,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://external-preview.redd.it/ADNOrzyLUcH9GZiKV8wujT8yD5FQYGEatyugJVGa73E.jpeg?width=140&amp;height=78&amp;crop=140:78,smart&amp;auto=webp&amp;s=f8513a13ec05e844f00af50b1129d68270d1a0e3","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"link","content_categories":null,"is_self":false,"subreddit_type":"public","created":1751564116,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"tomshardware.com","allow_live_comments":false,"selftext_html":null,"likes":null,"suggested_sort":null,"banned_at_utc":null,"url_overridden_by_dest":"https://www.tomshardware.com/software/a-project-to-bring-cuda-to-non-nvidia-gpus-is-making-major-progress-zluda-update-now-has-two-full-time-developers-working-on-32-bit-physx-support-and-llms-amongst-other-things","view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/ADNOrzyLUcH9GZiKV8wujT8yD5FQYGEatyugJVGa73E.jpeg?auto=webp&amp;s=89597e27a33fccf11c3d8cb4bba4680fd3482911","width":1920,"height":1080},"resolutions":[{"url":"https://external-preview.redd.it/ADNOrzyLUcH9GZiKV8wujT8yD5FQYGEatyugJVGa73E.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9e37ec99fec8c2fe6a73b6748741209a5f1f5c4a","width":108,"height":60},{"url":"https://external-preview.redd.it/ADNOrzyLUcH9GZiKV8wujT8yD5FQYGEatyugJVGa73E.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f2192a2dee3ef234ceea5f28dcbbe00f2badf7a6","width":216,"height":121},{"url":"https://external-preview.redd.it/ADNOrzyLUcH9GZiKV8wujT8yD5FQYGEatyugJVGa73E.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=53021c4c074ef2e8cc80c4a45e8ac25404af8e4f","width":320,"height":180},{"url":"https://external-preview.redd.it/ADNOrzyLUcH9GZiKV8wujT8yD5FQYGEatyugJVGa73E.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0b1a092718ebc960a0b1d79e4d2f8bc6ea6c934f","width":640,"height":360},{"url":"https://external-preview.redd.it/ADNOrzyLUcH9GZiKV8wujT8yD5FQYGEatyugJVGa73E.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d00e1c1ada20cf48e84e3b3fdba5ba81608a97ff","width":960,"height":540},{"url":"https://external-preview.redd.it/ADNOrzyLUcH9GZiKV8wujT8yD5FQYGEatyugJVGa73E.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=19e499d6f1f0b3a98a8c99312501005dacb10260","width":1080,"height":607}],"variants":{},"id":"ADNOrzyLUcH9GZiKV8wujT8yD5FQYGEatyugJVGa73E"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"mod_note":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"num_reports":null,"removal_reason":null,"link_flair_background_color":"#cc3600","id":"1lqvovt","is_robot_indexable":true,"num_duplicates":2,"report_reasons":null,"author":"OwnWitness2836","discussion_type":null,"num_comments":52,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/","stickied":false,"url":"https://www.tomshardware.com/software/a-project-to-bring-cuda-to-non-nvidia-gpus-is-making-major-progress-zluda-update-now-has-two-full-time-developers-working-on-32-bit-physx-support-and-llms-amongst-other-things","subreddit_subscribers":494198,"created_utc":1751564116,"num_crossposts":1,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n16oz0r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"hilldog4lyfe","can_mod_post":false,"created_utc":1751572619,"send_replies":true,"parent_id":"t1_n15ycsr","score":46,"author_fullname":"t2_1jweeypdhp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Tinygrad is a neural network framework \\n\\nCUDA is general purpose GPU software. It includes cuDNN which is a CUDA neural network library.\\n\\nThese are not the same thing at all.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n16oz0r","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Tinygrad is a neural network framework &lt;/p&gt;\\n\\n&lt;p&gt;CUDA is general purpose GPU software. It includes cuDNN which is a CUDA neural network library.&lt;/p&gt;\\n\\n&lt;p&gt;These are not the same thing at all.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqvovt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n16oz0r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751572619,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":46}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"50c36eba-fdca-11ee-9735-92a88d7e3b87","distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n18a62x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"tat_tvam_asshole","can_mod_post":false,"created_utc":1751591616,"send_replies":true,"parent_id":"t1_n17kfkr","score":9,"author_fullname":"t2_jxuvgdyj","approved_by":null,"mod_note":null,"all_awardings":[],"body":"literal cousins","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n18a62x","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;literal cousins&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lqvovt","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n18a62x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751591616,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n18wjiw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"llamabott","can_mod_post":false,"created_utc":1751600244,"send_replies":true,"parent_id":"t1_n17kfkr","score":3,"author_fullname":"t2_t0a1a","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I mean, you call it sus, plural, but between the two of them there is only one Su.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n18wjiw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I mean, you call it sus, plural, but between the two of them there is only one Su.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lqvovt","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n18wjiw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751600244,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n17kfkr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"OrangeESP32x99","can_mod_post":false,"send_replies":true,"parent_id":"t1_n17jxn9","score":17,"author_fullname":"t2_1a6iu2trts","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I mean the CEOs are also related.\\n\\nMaybe itâ€™s nothing, but itâ€™s kind of sus to me,","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n17kfkr","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Ollama"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I mean the CEOs are also related.&lt;/p&gt;\\n\\n&lt;p&gt;Maybe itâ€™s nothing, but itâ€™s kind of sus to me,&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lqvovt","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n17kfkr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751582414,"author_flair_text":"Ollama","treatment_tags":[],"created_utc":1751582414,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":17}}],"before":null}},"user_reports":[],"saved":false,"id":"n17jxn9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"featherless_fiend","can_mod_post":false,"send_replies":true,"parent_id":"t1_n176p4g","score":19,"author_fullname":"t2_5kjq1v0e","approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt;I don't believe that\\n\\nAMD recently said that most gamers don't need more than 8GB vram. This is NOT the approach a competitive company would be taking. This is **the** battleground to fight on but they don't want to.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n17jxn9","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;I don&amp;#39;t believe that&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;AMD recently said that most gamers don&amp;#39;t need more than 8GB vram. This is NOT the approach a competitive company would be taking. This is &lt;strong&gt;the&lt;/strong&gt; battleground to fight on but they don&amp;#39;t want to.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lqvovt","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n17jxn9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751582245,"author_flair_text":null,"treatment_tags":[],"created_utc":1751582245,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":19}}],"before":null}},"user_reports":[],"saved":false,"id":"n176p4g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ForsookComparison","can_mod_post":false,"send_replies":true,"parent_id":"t1_n16ot1c","score":17,"author_fullname":"t2_on5es7pe3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I don't believe that as much as I'd believe that their engineering to leadership talent ratio got way off track.\\n\\nI'm baffled at how the last two years haven't been Lisa Su up in front of a roadmap of TinyGrad or something similar every keynote. Instead it's *\\"we are now even moreso the leader in single-GPU inference\\"* , which is phenomenal, Instinct cards are monsters, but I would still get laughed out of my company's decision making circles if I even suggested it.","edited":false,"author_flair_css_class":null,"name":"t1_n176p4g","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t believe that as much as I&amp;#39;d believe that their engineering to leadership talent ratio got way off track.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m baffled at how the last two years haven&amp;#39;t been Lisa Su up in front of a roadmap of TinyGrad or something similar every keynote. Instead it&amp;#39;s &lt;em&gt;&amp;quot;we are now even moreso the leader in single-GPU inference&amp;quot;&lt;/em&gt; , which is phenomenal, Instinct cards are monsters, but I would still get laughed out of my company&amp;#39;s decision making circles if I even suggested it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lqvovt","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n176p4g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751577932,"author_flair_text":"llama.cpp","collapsed":false,"created_utc":1751577932,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":17}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n17mrxn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"doomdayx","can_mod_post":false,"send_replies":true,"parent_id":"t1_n16ot1c","score":5,"author_fullname":"t2_90b6q","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Their CEOs are related ðŸ˜‚","edited":false,"author_flair_css_class":null,"name":"t1_n17mrxn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Their CEOs are related ðŸ˜‚&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lqvovt","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n17mrxn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751583202,"author_flair_text":null,"collapsed":false,"created_utc":1751583202,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1774an","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Yellow_The_White","can_mod_post":false,"send_replies":true,"parent_id":"t1_n16pg7w","score":19,"author_fullname":"t2_m085q","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I can really only believe a coincidence of incompetence so many times before \\"minor conspiracy between the wealthy elite\\" starts winning Occam's Razor.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n1774an","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I can really only believe a coincidence of incompetence so many times before &amp;quot;minor conspiracy between the wealthy elite&amp;quot; starts winning Occam&amp;#39;s Razor.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lqvovt","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n1774an/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751578065,"author_flair_text":null,"treatment_tags":[],"created_utc":1751578065,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":19}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n17cva7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"yobo9193","can_mod_post":false,"send_replies":true,"parent_id":"t1_n16x6iq","score":7,"author_fullname":"t2_1092tt","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Idk why youâ€™re getting downvoted, thatâ€™s certainly relevant; why would the Board of Directors for either company choose a CEO thatâ€™s related to the CEO of their main competitor, unless they didnâ€™t care about potential information leaks?","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n17cva7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Idk why youâ€™re getting downvoted, thatâ€™s certainly relevant; why would the Board of Directors for either company choose a CEO thatâ€™s related to the CEO of their main competitor, unless they didnâ€™t care about potential information leaks?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lqvovt","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n17cva7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751579887,"author_flair_text":null,"treatment_tags":[],"created_utc":1751579887,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n17izys","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"TennesseeGenesis","can_mod_post":false,"created_utc":1751581929,"send_replies":true,"parent_id":"t1_n17bbvq","score":13,"author_fullname":"t2_4bnvkrq","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Because it's a lie, Lisa Su's grandfather is actually Jen-Hsun Huang's uncle. They are related by blood, just distantly.\\n\\nhttps://finance.yahoo.com/news/amd-ceo-lisa-su-says-014208942.html\\n\\nAnd if this is somehow not true I wonder why neither nVidia nor AMD and neither of the two people involved ever publicly denied it. Should be a simple thing to say, no?","edited":1751582325,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n17izys","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Because it&amp;#39;s a lie, Lisa Su&amp;#39;s grandfather is actually Jen-Hsun Huang&amp;#39;s uncle. They are related by blood, just distantly.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://finance.yahoo.com/news/amd-ceo-lisa-su-says-014208942.html\\"&gt;https://finance.yahoo.com/news/amd-ceo-lisa-su-says-014208942.html&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;And if this is somehow not true I wonder why neither nVidia nor AMD and neither of the two people involved ever publicly denied it. Should be a simple thing to say, no?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n17izys/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751581929,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqvovt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":8,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":13}}],"before":null}},"user_reports":[],"saved":false,"id":"n17bbvq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Allseeing_Argos","can_mod_post":false,"created_utc":1751579399,"send_replies":true,"parent_id":"t1_n177u6s","score":5,"author_fullname":"t2_14t0qoupfu","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Oh, thanks for clearing up that it's only a little collusion, not major collusion.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n17bbvq","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Oh, thanks for clearing up that it&amp;#39;s only a little collusion, not major collusion.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lqvovt","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n17bbvq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751579399,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n177u6s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"gjallerhorns_only","can_mod_post":false,"send_replies":true,"parent_id":"t1_n16x6iq","score":-1,"author_fullname":"t2_97ebkrd1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"They're not actually related. I forget but it's either their parents or grandparents that are part of the same group of friends. So it's just their families being friends not relation via blood or marriage.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n177u6s","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They&amp;#39;re not actually related. I forget but it&amp;#39;s either their parents or grandparents that are part of the same group of friends. So it&amp;#39;s just their families being friends not relation via blood or marriage.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lqvovt","associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n177u6s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751578292,"author_flair_text":null,"treatment_tags":[],"created_utc":1751578292,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n16x6iq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No_Afternoon_4260","can_mod_post":false,"send_replies":true,"parent_id":"t1_n16pg7w","score":5,"author_fullname":"t2_cj9kap4bx","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Do you know amd and nvidia ceo are cousins?","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n16x6iq","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Do you know amd and nvidia ceo are cousins?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lqvovt","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n16x6iq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751575053,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1751575053,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":5,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n16pg7w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Jumper775-2","can_mod_post":false,"send_replies":true,"parent_id":"t1_n16ot1c","score":15,"author_fullname":"t2_qk5xr04yp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"No way lol. NVidia just invested in the right stuff early on and AMD didnâ€™t. CUDA was huge before AI and they already had work on dedicated hardware for it. AMDs division is playing catchup with less money and less rate of expansion. Their MI instinct cards are competitive though.","edited":false,"author_flair_css_class":null,"name":"t1_n16pg7w","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No way lol. NVidia just invested in the right stuff early on and AMD didnâ€™t. CUDA was huge before AI and they already had work on dedicated hardware for it. AMDs division is playing catchup with less money and less rate of expansion. Their MI instinct cards are competitive though.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lqvovt","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n16pg7w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751572761,"author_flair_text":null,"collapsed":false,"created_utc":1751572761,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":15}}],"before":null}},"user_reports":[],"saved":false,"id":"n16ot1c","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"TheRealMasonMac","can_mod_post":false,"send_replies":true,"parent_id":"t1_n161op7","score":46,"author_fullname":"t2_101haj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"AMD is effectively controlled opposition for NVIDIA to pretend they aren't a monopoly.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n16ot1c","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;AMD is effectively controlled opposition for NVIDIA to pretend they aren&amp;#39;t a monopoly.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqvovt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n16ot1c/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751572569,"author_flair_text":null,"treatment_tags":[],"created_utc":1751572569,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":46}}],"before":null}},"user_reports":[],"saved":false,"id":"n161op7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Jumper775-2","can_mod_post":false,"send_replies":true,"parent_id":"t1_n160fmi","score":64,"author_fullname":"t2_qk5xr04yp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"AMD funded it originally but cut funding and made them revert all the released code.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n161op7","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;AMD funded it originally but cut funding and made them revert all the released code.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqvovt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n161op7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751565759,"author_flair_text":null,"treatment_tags":[],"created_utc":1751565759,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":64}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n17vv0q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Unlikely_Track_5154","can_mod_post":false,"send_replies":true,"parent_id":"t1_n160fmi","score":4,"author_fullname":"t2_r783n0ey","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Unfortunately it seems like both those companies hired too many phoenix.edu MBAs to be effective.\\n\\nIntel seems like they have too many boomers making decisions, which good God, can the geriatrics just leave already.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n17vv0q","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Unfortunately it seems like both those companies hired too many phoenix.edu MBAs to be effective.&lt;/p&gt;\\n\\n&lt;p&gt;Intel seems like they have too many boomers making decisions, which good God, can the geriatrics just leave already.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqvovt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n17vv0q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751586308,"author_flair_text":null,"treatment_tags":[],"created_utc":1751586308,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n168kld","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"Limp_Classroom_2645","can_mod_post":false,"send_replies":true,"parent_id":"t1_n160fmi","score":-6,"author_fullname":"t2_1lwf5vg68e","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"read the article, they probably are","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n168kld","is_submitter":false,"collapsed":true,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;read the article, they probably are&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqvovt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n168kld/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751567784,"author_flair_text":null,"treatment_tags":[],"created_utc":1751567784,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n16pwwp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DepthHour1669","can_mod_post":false,"send_replies":true,"parent_id":"t1_n16ngk8","score":15,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"? Go read the article. \\n\\nHow much do you think the 1 extra developer they hired cost? Theyâ€™re definitely not paying him $1mil/year. $1 mil would do a LOT, itâ€™d allow them to double the team from 2 to 4 for 2 years, which is probably how long itâ€™ll take for them to get to a minimum viable product.\\n\\nHow much investment do you think 2 person startups get? You do know that a Ycombinator seed round for a startup is a mere $500k?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n16pwwp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;? Go read the article. &lt;/p&gt;\\n\\n&lt;p&gt;How much do you think the 1 extra developer they hired cost? Theyâ€™re definitely not paying him $1mil/year. $1 mil would do a LOT, itâ€™d allow them to double the team from 2 to 4 for 2 years, which is probably how long itâ€™ll take for them to get to a minimum viable product.&lt;/p&gt;\\n\\n&lt;p&gt;How much investment do you think 2 person startups get? You do know that a Ycombinator seed round for a startup is a mere $500k?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqvovt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n16pwwp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751572900,"author_flair_text":null,"treatment_tags":[],"created_utc":1751572900,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":15}}],"before":null}},"user_reports":[],"saved":false,"id":"n16ngk8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"hilldog4lyfe","can_mod_post":false,"send_replies":true,"parent_id":"t1_n160fmi","score":-7,"author_fullname":"t2_1jweeypdhp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"lmfao you think $1mil is gonna do shit","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n16ngk8","is_submitter":false,"collapsed":true,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;lmfao you think $1mil is gonna do shit&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqvovt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n16ngk8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751572171,"author_flair_text":null,"treatment_tags":[],"created_utc":1751572171,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-7}}],"before":null}},"user_reports":[],"saved":false,"id":"n160fmi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DepthHour1669","can_mod_post":false,"created_utc":1751565402,"send_replies":true,"parent_id":"t1_n15ycsr","score":64,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Intel or AMD should throw $1mil at it. Itâ€™d be the best $1mil they can invest strategically","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n160fmi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Intel or AMD should throw $1mil at it. Itâ€™d be the best $1mil they can invest strategically&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqvovt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n160fmi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751565402,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":64}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n16wpfb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"teleprint-me","can_mod_post":false,"created_utc":1751574911,"send_replies":true,"parent_id":"t1_n15ycsr","score":15,"author_fullname":"t2_slcrtxpr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The firmware for hardware is usually propiretary. Any attempts to reverse engineer it are consider legally questionable.\\n\\n\\nYou could probably do what wine did for firmware and just take the inputs and outputs of the interface and implement the details there.\\n\\n\\nBut you'd better be prepared because hardware vendors are hell bent on owning that aspect.\\n\\n\\n- Sony\\n- AMD\\n- Nvidia\\n- MS\\n- Nintendo\\n\\n\\nThe list goes on and on. This is why the right to repair, modify, and share improvements for consumer end products has so much value to regular end users.\\n\\n\\nThe way I see it is that I bought the hardware and I should have the right do what I want with it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n16wpfb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The firmware for hardware is usually propiretary. Any attempts to reverse engineer it are consider legally questionable.&lt;/p&gt;\\n\\n&lt;p&gt;You could probably do what wine did for firmware and just take the inputs and outputs of the interface and implement the details there.&lt;/p&gt;\\n\\n&lt;p&gt;But you&amp;#39;d better be prepared because hardware vendors are hell bent on owning that aspect.&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Sony&lt;/li&gt;\\n&lt;li&gt;AMD&lt;/li&gt;\\n&lt;li&gt;Nvidia&lt;/li&gt;\\n&lt;li&gt;MS&lt;/li&gt;\\n&lt;li&gt;Nintendo&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;The list goes on and on. This is why the right to repair, modify, and share improvements for consumer end products has so much value to regular end users.&lt;/p&gt;\\n\\n&lt;p&gt;The way I see it is that I bought the hardware and I should have the right do what I want with it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqvovt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n16wpfb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751574911,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":15}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n16dx1w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"nowybulubator","can_mod_post":false,"send_replies":true,"parent_id":"t1_n16b2n5","score":8,"author_fullname":"t2_1pmese5c5c","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes, but now thanks to Oracle we know that APIs are not copyrightable.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n16dx1w","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, but now thanks to Oracle we know that APIs are not copyrightable.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqvovt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n16dx1w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751569342,"author_flair_text":null,"treatment_tags":[],"created_utc":1751569342,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}}],"before":null}},"user_reports":[],"saved":false,"id":"n16b2n5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"gtek_engineer66","can_mod_post":false,"created_utc":1751568515,"send_replies":true,"parent_id":"t1_n15ycsr","score":18,"author_fullname":"t2_wp10tzju4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You would be surprised what one man yolo'ing can do that entire empires have failed!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n16b2n5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You would be surprised what one man yolo&amp;#39;ing can do that entire empires have failed!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqvovt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n16b2n5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751568515,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":18}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"e75d59de-c910-11ed-9258-8e03d7ee129a","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n16w97l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"minnsoup","can_mod_post":false,"send_replies":true,"parent_id":"t1_n16jska","score":1,"author_fullname":"t2_ba9bm","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I think they are doing some kernel work too. They got a external AMD GPU running with mac studio which is awesome and that definitely isn't using CUDA.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n16w97l","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think they are doing some kernel work too. They got a external AMD GPU running with mac studio which is awesome and that definitely isn&amp;#39;t using CUDA.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqvovt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n16w97l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751574776,"author_flair_text":null,"treatment_tags":[],"created_utc":1751574776,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n16jska","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DigThatData","can_mod_post":false,"created_utc":1751571077,"send_replies":true,"parent_id":"t1_n15ycsr","score":4,"author_fullname":"t2_jozy285b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"is tinygrad even an alternative to cuda? I think it's more like an alternative to pytorch.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n16jska","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 7B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;is tinygrad even an alternative to cuda? I think it&amp;#39;s more like an alternative to pytorch.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqvovt","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n16jska/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751571077,"author_flair_text":"Llama 7B","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n16ax1r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"YouDontSeemRight","can_mod_post":false,"created_utc":1751568470,"send_replies":true,"parent_id":"t1_n15ycsr","score":2,"author_fullname":"t2_1b7gjxtue9","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"One guy swapping out LLM's hoping one gives him more coverage. Automate it. Automate Nvidia reverse engineering","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n16ax1r","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;One guy swapping out LLM&amp;#39;s hoping one gives him more coverage. Automate it. Automate Nvidia reverse engineering&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqvovt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n16ax1r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751568470,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n174b5a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"TheThoccnessMonster","can_mod_post":false,"created_utc":1751577192,"send_replies":true,"parent_id":"t1_n15ycsr","score":-1,"author_fullname":"t2_5nwuqw4y","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"And written by none other than Geohot.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n174b5a","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;And written by none other than Geohot.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqvovt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n174b5a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751577192,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n15ycsr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Temporary_Exam_3620","can_mod_post":false,"created_utc":1751564827,"send_replies":true,"parent_id":"t3_1lqvovt","score":155,"author_fullname":"t2_1rg0qz5eci","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"ZLUDA has a solo developer, but they hired another for a grand total of two. This is a BIG undertaking any accelerator company would be dedicating considerably sized teams to. But given the resource constraints i wouldn't be expecting anything substantial mid-term or short-term unless mainstream LLMs become great at doing firmware. \\n\\nTinygrad is another stack worth looking into - better funded for that matter.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n15ycsr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;ZLUDA has a solo developer, but they hired another for a grand total of two. This is a BIG undertaking any accelerator company would be dedicating considerably sized teams to. But given the resource constraints i wouldn&amp;#39;t be expecting anything substantial mid-term or short-term unless mainstream LLMs become great at doing firmware. &lt;/p&gt;\\n\\n&lt;p&gt;Tinygrad is another stack worth looking into - better funded for that matter.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n15ycsr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751564827,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqvovt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":155}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n188f7u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"HistorianPotential48","can_mod_post":false,"created_utc":1751590957,"send_replies":true,"parent_id":"t1_n1727a0","score":10,"author_fullname":"t2_4dzthia7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"fair point but just wanna say AMD supported ZLUDA, had a deal, and then years later suddenly sent a cease and decease letter to the maintainer saying no you can't do this anymore delete the code and the repo needed to be cleaned up. through out these months, everything was rewritten from a very early state.\\n\\ni'd warn against working with AMD, who knows, their legal department might sue you once you spent a few years down in their drain.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n188f7u","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;fair point but just wanna say AMD supported ZLUDA, had a deal, and then years later suddenly sent a cease and decease letter to the maintainer saying no you can&amp;#39;t do this anymore delete the code and the repo needed to be cleaned up. through out these months, everything was rewritten from a very early state.&lt;/p&gt;\\n\\n&lt;p&gt;i&amp;#39;d warn against working with AMD, who knows, their legal department might sue you once you spent a few years down in their drain.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqvovt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n188f7u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751590957,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n186kms","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CompromisedToolchain","can_mod_post":false,"created_utc":1751590266,"send_replies":true,"parent_id":"t1_n1727a0","score":2,"author_fullname":"t2_p6uw690c","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Havenâ€™t seen this before, thanks. Have you used this?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n186kms","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Havenâ€™t seen this before, thanks. Have you used this?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqvovt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n186kms/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751590266,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1727a0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"CatalyticDragon","can_mod_post":false,"created_utc":1751576541,"send_replies":true,"parent_id":"t3_1lqvovt","score":22,"author_fullname":"t2_3h1nb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Instead of entering a legal minefield with NVIDIA after you, it would be nice if developers would port to HIP which is an open source clone of the CUDA API.\\n\\nThen you can build and run for either AMD or NVIDIA.\\n\\nhttps://rocm.docs.amd.com/projects/HIP/en/docs-develop/what_is_hip.html \\n\\nFor legacy and unmaintained software though this is a great project.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1727a0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Instead of entering a legal minefield with NVIDIA after you, it would be nice if developers would port to HIP which is an open source clone of the CUDA API.&lt;/p&gt;\\n\\n&lt;p&gt;Then you can build and run for either AMD or NVIDIA.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://rocm.docs.amd.com/projects/HIP/en/docs-develop/what_is_hip.html\\"&gt;https://rocm.docs.amd.com/projects/HIP/en/docs-develop/what_is_hip.html&lt;/a&gt; &lt;/p&gt;\\n\\n&lt;p&gt;For legacy and unmaintained software though this is a great project.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n1727a0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751576541,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqvovt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":22}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n17yqac","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"xrailgun","can_mod_post":false,"created_utc":1751587333,"send_replies":true,"parent_id":"t1_n164u20","score":9,"author_fullname":"t2_kggm5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It was actually AMD who threatened to sue. Nvidia never officially acknowledged Zluda's existence.","edited":1751587810,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n17yqac","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It was actually AMD who threatened to sue. Nvidia never officially acknowledged Zluda&amp;#39;s existence.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqvovt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n17yqac/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751587333,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}}],"before":null}},"user_reports":[],"saved":false,"id":"n164u20","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"One-Employment3759","can_mod_post":false,"created_utc":1751566678,"send_replies":true,"parent_id":"t3_1lqvovt","score":52,"author_fullname":"t2_1f6wnmakwr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"We actually had this years ago already but Nvidia sued them to oblivionÂ ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n164u20","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;We actually had this years ago already but Nvidia sued them to oblivionÂ &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n164u20/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751566678,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqvovt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":52}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n184eu5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"kyuubi840","can_mod_post":false,"created_utc":1751589449,"send_replies":true,"parent_id":"t1_n16ryzc","score":7,"author_fullname":"t2_l9c1l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"On Oracle v Google, wasn't that decision overturned? In the end the usage of the APIs was considered fair use IIRC (of course, there was still a long legal battle before that, which companies still want to avoid)Â ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n184eu5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;On Oracle v Google, wasn&amp;#39;t that decision overturned? In the end the usage of the APIs was considered fair use IIRC (of course, there was still a long legal battle before that, which companies still want to avoid)Â &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqvovt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n184eu5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751589449,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n18iw8w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Veastli","can_mod_post":false,"created_utc":1751594882,"send_replies":true,"parent_id":"t1_n16ryzc","score":5,"author_fullname":"t2_8xedytci","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; Oracle successfully sued Google\\n\\nNo... Oracle *lost* to Google.\\n\\n&gt; The Court issued its decision on April 5, 2021. In a 6â€“2 majority, the (US Supreme) Court ruled that Google's use of the Java APIs was within the bounds of fair use...\\n\\nhttps://en.wikipedia.org/wiki/Google_LLC_v._Oracle_America,_Inc.#Decision","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n18iw8w","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Oracle successfully sued Google&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;No... Oracle &lt;em&gt;lost&lt;/em&gt; to Google.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;The Court issued its decision on April 5, 2021. In a 6â€“2 majority, the (US Supreme) Court ruled that Google&amp;#39;s use of the Java APIs was within the bounds of fair use...&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://en.wikipedia.org/wiki/Google_LLC_v._Oracle_America,_Inc.#Decision\\"&gt;https://en.wikipedia.org/wiki/Google_LLC_v._Oracle_America,_Inc.#Decision&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqvovt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n18iw8w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751594882,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n16ryzc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"loudmax","can_mod_post":false,"created_utc":1751573517,"send_replies":true,"parent_id":"t3_1lqvovt","score":29,"author_fullname":"t2_61k60","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Oracle successfully sued Google for shipping a Java-compatible runtime that wasn't Java.  AMD might see the same risk here: if they support a CUDA-compatible runtime that isn't actually CUDA, they might open themselves to being sued by Nvidia.  IMHO that court ruling was a disaster for a competitive free marketplace, but here we are.\\n\\nThe good news is that ROCm and other projects are making serious progress, even if there's a long way to go.  I'm also interested to see what comes of the Mojo programming language (https://www.modular.com/mojo), if it ever becomes fully open source as promised.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n16ryzc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Oracle successfully sued Google for shipping a Java-compatible runtime that wasn&amp;#39;t Java.  AMD might see the same risk here: if they support a CUDA-compatible runtime that isn&amp;#39;t actually CUDA, they might open themselves to being sued by Nvidia.  IMHO that court ruling was a disaster for a competitive free marketplace, but here we are.&lt;/p&gt;\\n\\n&lt;p&gt;The good news is that ROCm and other projects are making serious progress, even if there&amp;#39;s a long way to go.  I&amp;#39;m also interested to see what comes of the Mojo programming language (&lt;a href=\\"https://www.modular.com/mojo\\"&gt;https://www.modular.com/mojo&lt;/a&gt;), if it ever becomes fully open source as promised.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n16ryzc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751573517,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqvovt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":29}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n16c657","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"thomthehound","can_mod_post":false,"created_utc":1751568838,"send_replies":true,"parent_id":"t3_1lqvovt","score":14,"author_fullname":"t2_vxbs7cf4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This is great and all, and I salute it, but AMD's own ROCm is also making pretty big strides these days. The Windows release is still scheduled for August, last I heard.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n16c657","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is great and all, and I salute it, but AMD&amp;#39;s own ROCm is also making pretty big strides these days. The Windows release is still scheduled for August, last I heard.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n16c657/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751568838,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqvovt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":14}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n176amz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AvidCyclist250","can_mod_post":false,"created_utc":1751577806,"send_replies":true,"parent_id":"t1_n16aznf","score":2,"author_fullname":"t2_lmkezzo6j","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It's why we can't have nice things. It hurts corporate feelings.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n176amz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s why we can&amp;#39;t have nice things. It hurts corporate feelings.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqvovt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n176amz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751577806,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n16aznf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ykoech","can_mod_post":false,"created_utc":1751568491,"send_replies":true,"parent_id":"t3_1lqvovt","score":7,"author_fullname":"t2_vuh1jbn","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"We've seen this before ðŸ¥¹","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n16aznf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;We&amp;#39;ve seen this before ðŸ¥¹&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n16aznf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751568491,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqvovt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n17jq3m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"sibylrouge","can_mod_post":false,"created_utc":1751582175,"send_replies":true,"parent_id":"t1_n16v43a","score":6,"author_fullname":"t2_vgve9vryk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah this is exactly what China has to do at this moment","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n17jq3m","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah this is exactly what China has to do at this moment&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqvovt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n17jq3m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751582175,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n18iw81","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DraconPern","can_mod_post":false,"created_utc":1751594882,"send_replies":true,"parent_id":"t1_n16v43a","score":3,"author_fullname":"t2_6q3s4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Why would they?  They made an entire stack from the ground up, so there's no need to fix someone else's issue.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n18iw81","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why would they?  They made an entire stack from the ground up, so there&amp;#39;s no need to fix someone else&amp;#39;s issue.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqvovt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n18iw81/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751594882,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n16v43a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Commercial-Celery769","can_mod_post":false,"created_utc":1751574440,"send_replies":true,"parent_id":"t3_1lqvovt","score":7,"author_fullname":"t2_zws5yqyow","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Why cant china hop on this? Don't have to worry about the lawsuits from Nvidia and could get rid of the monopoly they have.Â ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n16v43a","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why cant china hop on this? Don&amp;#39;t have to worry about the lawsuits from Nvidia and could get rid of the monopoly they have.Â &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n16v43a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751574440,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqvovt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n16iu89","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"created_utc":1751570791,"send_replies":true,"parent_id":"t3_1lqvovt","score":3,"author_fullname":"t2_o65i6kx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"These things while interesting novelties, never really take off. Look at HIP for ROCm. Which also lets you run CUDA on AMD. Sure, it's useful but it's not exactly convincing people to buy AMD GPUs when they need to run CUDA code. That's probably why AMD passed on supporting Zluda. Since they already have HIP.","edited":1751572223,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n16iu89","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;These things while interesting novelties, never really take off. Look at HIP for ROCm. Which also lets you run CUDA on AMD. Sure, it&amp;#39;s useful but it&amp;#39;s not exactly convincing people to buy AMD GPUs when they need to run CUDA code. That&amp;#39;s probably why AMD passed on supporting Zluda. Since they already have HIP.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n16iu89/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751570791,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqvovt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n161y6k","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Buey","can_mod_post":false,"created_utc":1751565835,"send_replies":true,"parent_id":"t3_1lqvovt","score":1,"author_fullname":"t2_6nde1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"From my trials with ZLUDA, the dev(s) aren't able to keep up with AMD driver updates.  Hopefully they can get more resources, because ROCm support is really spotty.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n161y6k","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;From my trials with ZLUDA, the dev(s) aren&amp;#39;t able to keep up with AMD driver updates.  Hopefully they can get more resources, because ROCm support is really spotty.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n161y6k/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751565835,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqvovt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n17omdn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fogonthebarrow-downs","can_mod_post":false,"created_utc":1751583826,"send_replies":true,"parent_id":"t3_1lqvovt","score":1,"author_fullname":"t2_13jy44","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Asking as someone who has no idea about this: why not move towards something like OpenCL? Is CUDA that far ahead? And if so, is this down to adoption or features?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n17omdn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Asking as someone who has no idea about this: why not move towards something like OpenCL? Is CUDA that far ahead? And if so, is this down to adoption or features?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n17omdn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751583826,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqvovt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n17t5br","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RIP26770","can_mod_post":false,"created_utc":1751585370,"send_replies":true,"parent_id":"t3_1lqvovt","score":1,"author_fullname":"t2_9k6j4pos","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"That would be a game changer.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n17t5br","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That would be a game changer.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n17t5br/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751585370,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqvovt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n187nbi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HistorianPotential48","can_mod_post":false,"created_utc":1751590669,"send_replies":true,"parent_id":"t3_1lqvovt","score":1,"author_fullname":"t2_4dzthia7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"finally i can generate my illustrious women images faster soon","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n187nbi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;finally i can generate my illustrious women images faster soon&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n187nbi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751590669,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqvovt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n18icqv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"6969its_a_great_time","can_mod_post":false,"created_utc":1751594685,"send_replies":true,"parent_id":"t3_1lqvovt","score":1,"author_fullname":"t2_1gcr8621k5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Mojo and Max have made good progress lately. Curious what benefits this would provide.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n18icqv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Mojo and Max have made good progress lately. Curious what benefits this would provide.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n18icqv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751594685,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqvovt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n18m5qz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Trysem","can_mod_post":false,"created_utc":1751596103,"send_replies":true,"parent_id":"t3_1lqvovt","score":1,"author_fullname":"t2_9phou5uh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"A dumb question, can nvidia sue for developing ZLUDA? As it is a translation layer of their CUDA?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n18m5qz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;A dumb question, can nvidia sue for developing ZLUDA? As it is a translation layer of their CUDA?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n18m5qz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751596103,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqvovt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n17fqtn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"tangoshukudai","can_mod_post":false,"created_utc":1751580837,"send_replies":true,"parent_id":"t3_1lqvovt","score":0,"author_fullname":"t2_46xop","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I so wish CUDA would just die.  Please developers just use standard compute shaders.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n17fqtn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I so wish CUDA would just die.  Please developers just use standard compute shaders.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n17fqtn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751580837,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqvovt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1739vt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DocFail","can_mod_post":false,"created_utc":1751576872,"send_replies":true,"parent_id":"t3_1lqvovt","score":0,"author_fullname":"t2_l34ih","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Iâ€™mÂ ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1739vt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Iâ€™mÂ &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/n1739vt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751576872,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqvovt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}}]`),n=()=>e.jsx(l,{data:a});export{n as default};
