import{j as e}from"./index-Bqs-ekb2.js";import{R as t}from"./RedditPostRenderer-DUVdf0-i.js";import"./index-D52ORTDm.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I'm using OpenRouter to manage multiple LLM subscriptions in one place for a research project where I need to benchmark responses across different models. However, I've noticed some discrepancies between responses when calling the same model (like GPT-4) through OpenRouter's API versus OpenAI's native API.\\n\\nI've verified that:\\n\\n* temperature and top\\\\_p parameters are identical\\n* No caching is occurring on either side\\n* Same prompts are being used\\n\\nThe differences aren't huge, but they're noticeable enough to potentially affect my benchmark results.\\n\\nHas anyone else run into this issue? I'm wondering if:\\n\\n1. OpenRouter adds any middleware processing that could affect outputs\\n2. There are default parameters being set differently\\n3. There's some other configuration I'm missing\\n\\nAny insights would be appreciated - trying to determine if this is expected behavior or if there's something I can adjust to get more consistent results.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Inconsistent responses between OpenRouter API and native OpenAI API","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lm4s6i","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.38,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_14isxf","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751057481,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m using OpenRouter to manage multiple LLM subscriptions in one place for a research project where I need to benchmark responses across different models. However, I&amp;#39;ve noticed some discrepancies between responses when calling the same model (like GPT-4) through OpenRouter&amp;#39;s API versus OpenAI&amp;#39;s native API.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ve verified that:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;temperature and top_p parameters are identical&lt;/li&gt;\\n&lt;li&gt;No caching is occurring on either side&lt;/li&gt;\\n&lt;li&gt;Same prompts are being used&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;The differences aren&amp;#39;t huge, but they&amp;#39;re noticeable enough to potentially affect my benchmark results.&lt;/p&gt;\\n\\n&lt;p&gt;Has anyone else run into this issue? I&amp;#39;m wondering if:&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;OpenRouter adds any middleware processing that could affect outputs&lt;/li&gt;\\n&lt;li&gt;There are default parameters being set differently&lt;/li&gt;\\n&lt;li&gt;There&amp;#39;s some other configuration I&amp;#39;m missing&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;Any insights would be appreciated - trying to determine if this is expected behavior or if there&amp;#39;s something I can adjust to get more consistent results.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lm4s6i","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Anada01","discussion_type":null,"num_comments":9,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lm4s6i/inconsistent_responses_between_openrouter_api_and/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lm4s6i/inconsistent_responses_between_openrouter_api_and/","subreddit_subscribers":492572,"created_utc":1751057481,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n06npec","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Anada01","can_mod_post":false,"created_utc":1751081964,"send_replies":true,"parent_id":"t1_n05q2lp","score":2,"author_fullname":"t2_14isxf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I have set the temperature to 0, and it appears that even when I provide the exact same prompt to the OpenAI API, I consistently receive the same result, which is also true for Openrouter. However, the results from Openrouter and OpenAI differ.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n06npec","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have set the temperature to 0, and it appears that even when I provide the exact same prompt to the OpenAI API, I consistently receive the same result, which is also true for Openrouter. However, the results from Openrouter and OpenAI differ.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm4s6i","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm4s6i/inconsistent_responses_between_openrouter_api_and/n06npec/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751081964,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n05q2lp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SomeOddCodeGuy","can_mod_post":false,"created_utc":1751068905,"send_replies":true,"parent_id":"t3_1lm4s6i","score":1,"author_fullname":"t2_kle75fbd6","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It's entirely possible that openrouter either has an additional system prompt in the background that you aren't aware of, or that it unpacks the payload your front end sends to it, and repackages it in a slightly different way.\\n\\nI do want to specify one thing in your post those:\\n\\n&gt;temperature and top\\\\_p parameters are identical\\n\\nThe temperature is the same, but are they both 0-0.1? Because anything higher than that is going to produce differences. Essentially, to really test if there are differences you want to be able to successfully generate identical responses with the same model no matter how many times you sent the prompt. Temp of 0 should do that. So rather than comparing OpenAI to openrouter first, make sure you can send openai the same prompt twice and get the exact same response, verbatim, twice. Then try the same setup on openrouter and see what happens.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n05q2lp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s entirely possible that openrouter either has an additional system prompt in the background that you aren&amp;#39;t aware of, or that it unpacks the payload your front end sends to it, and repackages it in a slightly different way.&lt;/p&gt;\\n\\n&lt;p&gt;I do want to specify one thing in your post those:&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;temperature and top_p parameters are identical&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;The temperature is the same, but are they both 0-0.1? Because anything higher than that is going to produce differences. Essentially, to really test if there are differences you want to be able to successfully generate identical responses with the same model no matter how many times you sent the prompt. Temp of 0 should do that. So rather than comparing OpenAI to openrouter first, make sure you can send openai the same prompt twice and get the exact same response, verbatim, twice. Then try the same setup on openrouter and see what happens.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm4s6i/inconsistent_responses_between_openrouter_api_and/n05q2lp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751068905,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lm4s6i","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n07b89v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"godndiogoat","can_mod_post":false,"send_replies":true,"parent_id":"t1_n06zsav","score":1,"author_fullname":"t2_1o8b7or53v","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Push a tougher case: hit both APIs with a multi-turn prompt that forces structured output, capture logprobs, and diff the streaming token order. Add a fixed seed plus logit_bias to clamp YES=2, NO=-2 so any middleware nudge stands out. New API keys with zero chat history rule out personalization. If responses still line up over 30-40 tokens, variance sits in OP’s own post-processing or rate-limit retries. If they diverge, piping curl -v will expose extra headers showing safety or cache layers. Main takeaway: deeper, seeded multi-turn tests isolate the culprit.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n07b89v","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Push a tougher case: hit both APIs with a multi-turn prompt that forces structured output, capture logprobs, and diff the streaming token order. Add a fixed seed plus logit_bias to clamp YES=2, NO=-2 so any middleware nudge stands out. New API keys with zero chat history rule out personalization. If responses still line up over 30-40 tokens, variance sits in OP’s own post-processing or rate-limit retries. If they diverge, piping curl -v will expose extra headers showing safety or cache layers. Main takeaway: deeper, seeded multi-turn tests isolate the culprit.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm4s6i","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm4s6i/inconsistent_responses_between_openrouter_api_and/n07b89v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751093732,"author_flair_text":null,"treatment_tags":[],"created_utc":1751093732,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n06zsav","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"llmentry","can_mod_post":false,"send_replies":true,"parent_id":"t1_n06x9cw","score":1,"author_fullname":"t2_1lufy6yx6z","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That's a fairly low-bar setup, but there certainly are no issues with OpenRouter's implementation at that level (all YES, as you would expect, with GPT-4.1).\\n\\nIf that failed, though, I'd be seriously concerned!\\n\\nWithout knowing what the OP's exact scenario is, it's hard to say more other than that I can't reproduce any noticeable difference at first glance between the two API endpoints.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n06zsav","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s a fairly low-bar setup, but there certainly are no issues with OpenRouter&amp;#39;s implementation at that level (all YES, as you would expect, with GPT-4.1).&lt;/p&gt;\\n\\n&lt;p&gt;If that failed, though, I&amp;#39;d be seriously concerned!&lt;/p&gt;\\n\\n&lt;p&gt;Without knowing what the OP&amp;#39;s exact scenario is, it&amp;#39;s hard to say more other than that I can&amp;#39;t reproduce any noticeable difference at first glance between the two API endpoints.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm4s6i","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm4s6i/inconsistent_responses_between_openrouter_api_and/n06zsav/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751087710,"author_flair_text":null,"treatment_tags":[],"created_utc":1751087710,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n06x9cw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"godndiogoat","can_mod_post":false,"created_utc":1751086419,"send_replies":true,"parent_id":"t1_n06wwib","score":1,"author_fullname":"t2_1o8b7or53v","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Quick way to spot drift: hit both endpoints with a bare-bones, deterministic prompt and compare tokens. Try this chat payload:\\nsystem: \\"You are a binary oracle. Reply with exactly YES or NO-no other text.\\"\\nuser: \\"Is water wet?\\"\\n\\nRun it at temp 0, topp 0, n 5. GPT-4 from OpenAI should return five identical YES strings; if the OpenRouter shows mixed casing, hidden punctuation, or occasional NO, something upstream is nudging logits. Two likely culprits: account-level embeddings (OpenAI may personalize slightly off your key) and OpenRouter’s safety middleware that rewrites or embeds metadata before forwarding. To rule those out, also set seed and log response headers. I’ve bounced the same test through LangChain’s proxy and Together AI, but APIWrapper.ai gave the cleanest request/response diff, which helped trace a rogue logitbias field I’d forgotten about. Once everything matches, any remaining variance is just model nondeterminism at the 1e-5 level.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n06x9cw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Quick way to spot drift: hit both endpoints with a bare-bones, deterministic prompt and compare tokens. Try this chat payload:\\nsystem: &amp;quot;You are a binary oracle. Reply with exactly YES or NO-no other text.&amp;quot;\\nuser: &amp;quot;Is water wet?&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;Run it at temp 0, topp 0, n 5. GPT-4 from OpenAI should return five identical YES strings; if the OpenRouter shows mixed casing, hidden punctuation, or occasional NO, something upstream is nudging logits. Two likely culprits: account-level embeddings (OpenAI may personalize slightly off your key) and OpenRouter’s safety middleware that rewrites or embeds metadata before forwarding. To rule those out, also set seed and log response headers. I’ve bounced the same test through LangChain’s proxy and Together AI, but APIWrapper.ai gave the cleanest request/response diff, which helped trace a rogue logitbias field I’d forgotten about. Once everything matches, any remaining variance is just model nondeterminism at the 1e-5 level.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm4s6i","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm4s6i/inconsistent_responses_between_openrouter_api_and/n06x9cw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751086419,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n06wwib","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"llmentry","can_mod_post":false,"created_utc":1751086243,"send_replies":true,"parent_id":"t3_1lm4s6i","score":1,"author_fullname":"t2_1lufy6yx6z","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I just tried it out.  I can't see any difference, although I'm also finding that GPT 4.1's response even at temp=0, top\\\\_p=0 is still surprisingly non-deterministic (whether using the OpenAI API or OpenRouter's API).\\n\\nDo you have a sample prompt to illustrate?  I'm happy to test it out myself.\\n\\nOne other possible explanation, if there really is a difference, is that OpenRouter sends prompts anonymously to the API, whereas OpenAI has your account linked to your API key (so there's a history associated with the key).  I'd hate to think that's a potential reason for any discrepancy, but ... just putting it out there.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n06wwib","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I just tried it out.  I can&amp;#39;t see any difference, although I&amp;#39;m also finding that GPT 4.1&amp;#39;s response even at temp=0, top_p=0 is still surprisingly non-deterministic (whether using the OpenAI API or OpenRouter&amp;#39;s API).&lt;/p&gt;\\n\\n&lt;p&gt;Do you have a sample prompt to illustrate?  I&amp;#39;m happy to test it out myself.&lt;/p&gt;\\n\\n&lt;p&gt;One other possible explanation, if there really is a difference, is that OpenRouter sends prompts anonymously to the API, whereas OpenAI has your account linked to your API key (so there&amp;#39;s a history associated with the key).  I&amp;#39;d hate to think that&amp;#39;s a potential reason for any discrepancy, but ... just putting it out there.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm4s6i/inconsistent_responses_between_openrouter_api_and/n06wwib/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751086243,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lm4s6i","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n07wwda","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SnooPaintings8639","can_mod_post":false,"created_utc":1751106411,"send_replies":true,"parent_id":"t3_1lm4s6i","score":1,"author_fullname":"t2_6etmz7p2","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Check model providers on open router. If it's openai, it definitely should be the same. But others, like Microsoft Azure, have different checkpoint deployed.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n07wwda","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Check model providers on open router. If it&amp;#39;s openai, it definitely should be the same. But others, like Microsoft Azure, have different checkpoint deployed.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm4s6i/inconsistent_responses_between_openrouter_api_and/n07wwda/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751106411,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lm4s6i","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n04uekl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Anada01","can_mod_post":false,"created_utc":1751058261,"send_replies":true,"parent_id":"t1_n04sm7x","score":1,"author_fullname":"t2_14isxf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I initially thought the same thing, but when I looked closer at the model specifications - for example, with gpt-4o-mini - there appears to be only one model with that exact name, so it should be the same version being called.\\n\\nI've also tested this with gemini-2.0-flash, and I'm seeing similar inconsistencies there as well. This makes me think something might be happening on OpenRouter's backend when they process the API requests, rather than it being a model version issue.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n04uekl","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I initially thought the same thing, but when I looked closer at the model specifications - for example, with gpt-4o-mini - there appears to be only one model with that exact name, so it should be the same version being called.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ve also tested this with gemini-2.0-flash, and I&amp;#39;m seeing similar inconsistencies there as well. This makes me think something might be happening on OpenRouter&amp;#39;s backend when they process the API requests, rather than it being a model version issue.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm4s6i","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm4s6i/inconsistent_responses_between_openrouter_api_and/n04uekl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751058261,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n04sm7x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"captin_Zenux","can_mod_post":false,"created_utc":1751057714,"send_replies":true,"parent_id":"t3_1lm4s6i","score":0,"author_fullname":"t2_7ryd4nn1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Only speculating\\nBut openai got different variations of gpt-4 so open router’s gpt-4 could simply be connecting you to a different gpt-4 than openai’s apis\\nYou could verify this by checking available versions of gpt-4 and trying them out and comparing\\nHaven’t used Open AI in a long while hence the costs so i dont have much insight..","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n04sm7x","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Only speculating\\nBut openai got different variations of gpt-4 so open router’s gpt-4 could simply be connecting you to a different gpt-4 than openai’s apis\\nYou could verify this by checking available versions of gpt-4 and trying them out and comparing\\nHaven’t used Open AI in a long while hence the costs so i dont have much insight..&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm4s6i/inconsistent_responses_between_openrouter_api_and/n04sm7x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751057714,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lm4s6i","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}}]`),r=()=>e.jsx(t,{data:a});export{r as default};
