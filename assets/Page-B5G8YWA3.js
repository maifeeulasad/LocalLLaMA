import{j as e}from"./index-Bu7qcPAU.js";import{R as t}from"./RedditPostRenderer-CbHA7O5q.js";import"./index-BKgbfxhf.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Before I fiddle with this, I wanted to see if anyone else has tried deactivating all but the shared expert in a MoE model to evaluate whether its output is coherent ... or if it can be trivially trained to be useful.\\n\\nMore broadly, I'm very interested in the potential of training a single model to work with different inferencing resources (Google's MatFormer work with Gemma 3n is the obvious other approach).\\n\\nI'd love to see models that can yield coherent output from just using the shared expert FFN (squeeze a little more memory efficiency by skipping all the router parameters also), from a small set of experts, and of course from the full set.\\n\\nYes, this was inspired by the absolutely wild setup in Kimi K2: 384(!) shared FFN experts, with 8 activated during inference plus one shared expert... What can just that one shared expert do?\\n\\n**Clarifying a point from the thread:**\\n\\nThe end goal here isn't to distill a crappy small dense model from an MOE, it's to get a sense of how far the expert is from a small dense LLM. If it's not too far, then we plausibly could train, in one go, an MOE that works reasonably at one expert scale, better with 2 out of 8 experts, and Kimi 2K level with 8 out or 384 experts. i.e. MOEs that usefully scale to different available infrastructures.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Is the output of only the shared expert(s) in a MOE model coherent?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lzu9e8","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.75,"author_flair_background_color":null,"subreddit_type":"public","ups":2,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_2roqrw5l","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":2,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1752520423,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752517544,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Before I fiddle with this, I wanted to see if anyone else has tried deactivating all but the shared expert in a MoE model to evaluate whether its output is coherent ... or if it can be trivially trained to be useful.&lt;/p&gt;\\n\\n&lt;p&gt;More broadly, I&amp;#39;m very interested in the potential of training a single model to work with different inferencing resources (Google&amp;#39;s MatFormer work with Gemma 3n is the obvious other approach).&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;d love to see models that can yield coherent output from just using the shared expert FFN (squeeze a little more memory efficiency by skipping all the router parameters also), from a small set of experts, and of course from the full set.&lt;/p&gt;\\n\\n&lt;p&gt;Yes, this was inspired by the absolutely wild setup in Kimi K2: 384(!) shared FFN experts, with 8 activated during inference plus one shared expert... What can just that one shared expert do?&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Clarifying a point from the thread:&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;The end goal here isn&amp;#39;t to distill a crappy small dense model from an MOE, it&amp;#39;s to get a sense of how far the expert is from a small dense LLM. If it&amp;#39;s not too far, then we plausibly could train, in one go, an MOE that works reasonably at one expert scale, better with 2 out of 8 experts, and Kimi 2K level with 8 out or 384 experts. i.e. MOEs that usefully scale to different available infrastructures.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lzu9e8","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"gofiend","discussion_type":null,"num_comments":2,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lzu9e8/is_the_output_of_only_the_shared_experts_in_a_moe/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lzu9e8/is_the_output_of_only_the_shared_experts_in_a_moe/","subreddit_subscribers":499295,"created_utc":1752517544,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n34hlne","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Double_Cause4609","can_mod_post":false,"created_utc":1752518875,"send_replies":true,"parent_id":"t3_1lzu9e8","score":2,"author_fullname":"t2_1kubzxt2ww","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Short answer:\\n\\nThis doesn't do what you want it to do. If you want a dense model, just train a dense model.\\n\\nMoE is not some magical alternative formulation. The experts are not domain experts skilled in a specific area cleanly delineated by a human like hierarchical analysis of different compartmentalized subjects.\\n\\nMoE is just an approximation of a dense neural network (see: Approximating Two-Layer Feedforward Networks for Efficient Transformers, \\\\[csordas et al\\\\]).\\n\\nCan you take a row out of a typical FFN's matrix and use just that to get a coherent response? That's effectively what you're asking.\\n\\nA better approach, IMO, if you really must continue on something like this, is to take a dense model, do an SVD or PCA operation, and take the top-k entries (the most relevant ones) and save those as a new network. In theory you'll have preserved the most important components of the model in a now much smaller model.\\n\\nIn the same way, treating an MoE as an approximation of a dense network, you can do the same operation, treating \\\\*all\\\\* of the experts together as a single \\"unit\\" and you can probably extract the majority of the model's internal representations. This requires some additional tricks to account for the explicit block-sparsity in an MoE model.\\n\\nNote that while PCA is good, it's not perfect, so you may need to do some level of retraining or self distillation.\\n\\nI will never understand people's obsession with compressing MoE models into dense networks. The entire point (in the context of local usage for end-consumers) is that they give extra quality by trading off memory capacity, meaning that relatively affordable solutions like system DRAM become viable for scaling to quite large and performant models, and this tradeoff is much more desirable than just stacking VRAM endlessly.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n34hlne","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Short answer:&lt;/p&gt;\\n\\n&lt;p&gt;This doesn&amp;#39;t do what you want it to do. If you want a dense model, just train a dense model.&lt;/p&gt;\\n\\n&lt;p&gt;MoE is not some magical alternative formulation. The experts are not domain experts skilled in a specific area cleanly delineated by a human like hierarchical analysis of different compartmentalized subjects.&lt;/p&gt;\\n\\n&lt;p&gt;MoE is just an approximation of a dense neural network (see: Approximating Two-Layer Feedforward Networks for Efficient Transformers, [csordas et al]).&lt;/p&gt;\\n\\n&lt;p&gt;Can you take a row out of a typical FFN&amp;#39;s matrix and use just that to get a coherent response? That&amp;#39;s effectively what you&amp;#39;re asking.&lt;/p&gt;\\n\\n&lt;p&gt;A better approach, IMO, if you really must continue on something like this, is to take a dense model, do an SVD or PCA operation, and take the top-k entries (the most relevant ones) and save those as a new network. In theory you&amp;#39;ll have preserved the most important components of the model in a now much smaller model.&lt;/p&gt;\\n\\n&lt;p&gt;In the same way, treating an MoE as an approximation of a dense network, you can do the same operation, treating *all* of the experts together as a single &amp;quot;unit&amp;quot; and you can probably extract the majority of the model&amp;#39;s internal representations. This requires some additional tricks to account for the explicit block-sparsity in an MoE model.&lt;/p&gt;\\n\\n&lt;p&gt;Note that while PCA is good, it&amp;#39;s not perfect, so you may need to do some level of retraining or self distillation.&lt;/p&gt;\\n\\n&lt;p&gt;I will never understand people&amp;#39;s obsession with compressing MoE models into dense networks. The entire point (in the context of local usage for end-consumers) is that they give extra quality by trading off memory capacity, meaning that relatively affordable solutions like system DRAM become viable for scaling to quite large and performant models, and this tradeoff is much more desirable than just stacking VRAM endlessly.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzu9e8/is_the_output_of_only_the_shared_experts_in_a_moe/n34hlne/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752518875,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzu9e8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),s=()=>e.jsx(t,{data:a});export{s as default};
