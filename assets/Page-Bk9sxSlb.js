import{j as t}from"./index-CWmJdUH_.js";import{R as e}from"./RedditPostRenderer-D2iunoQ9.js";import"./index-BCg9RP6g.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Anyone with an Epyc 9015 or better able to test Qwen3 235B Q8 for prompt processing and token generation?  Ideally with a 3090 or better for prompt processing.\\n\\nI've been looking at Kimi, but I've been discouraged by results, and thinking about settling on a system to run 235B Q8 for now.\\n\\nWas wondering if a 9015 256GB+ system would be enough, or would need the higher end CPUs with more CCDs.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Epyc Qwen3 235B Q8 speed?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m6h67y","quarantine":false,"link_flair_text_color":"light","upvote_ratio":1,"author_flair_background_color":null,"subreddit_type":"public","ups":13,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_ijzb7","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":13,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753198166,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Anyone with an Epyc 9015 or better able to test Qwen3 235B Q8 for prompt processing and token generation?  Ideally with a 3090 or better for prompt processing.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ve been looking at Kimi, but I&amp;#39;ve been discouraged by results, and thinking about settling on a system to run 235B Q8 for now.&lt;/p&gt;\\n\\n&lt;p&gt;Was wondering if a 9015 256GB+ system would be enough, or would need the higher end CPUs with more CCDs.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1m6h67y","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"MidnightProgrammer","discussion_type":null,"num_comments":16,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m6h67y/epyc_qwen3_235b_q8_speed/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m6h67y/epyc_qwen3_235b_q8_speed/","subreddit_subscribers":503254,"created_utc":1753198166,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4l28tu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"eloquentemu","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4k7ovl","score":1,"author_fullname":"t2_lpdsy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Looking at it again, I think the 32/36 comes from xGMI vs GMI - the former is for socket-socket comms while the latter is CCD-IO comms.  I think I missed this given things like 4x GMI vs 4 xGMI and they refer to both interchangeably as \\"infinity fabric\\".  The xGMI link speed is \\"easy\\" because it's just a 32GT/s SERDES repurposed from PCIe5.\\n\\nThe 36 is still confusing though as they definitely say \\"Gbps\\" quite consistently and also used the same value for Genoa.  My Genoa _definitely_ gets 48-52GBps (big B) per link which has like, nothing to do with 36 :).  AMD has some tuning docs that claim the FCLK for Genoa will go to 2400MHz to match it's nominal DDR5-4800.  But I'm not sure how to get 36 from 2.4, nor how to reconcile the observed ~50GBps to either.\\n\\ntl;dr I'm not sure how to reconcile the numbers, but Turin GMI links definitely benchmark at ~60GBps","edited":false,"author_flair_css_class":null,"name":"t1_n4l28tu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Looking at it again, I think the 32/36 comes from xGMI vs GMI - the former is for socket-socket comms while the latter is CCD-IO comms.  I think I missed this given things like 4x GMI vs 4 xGMI and they refer to both interchangeably as &amp;quot;infinity fabric&amp;quot;.  The xGMI link speed is &amp;quot;easy&amp;quot; because it&amp;#39;s just a 32GT/s SERDES repurposed from PCIe5.&lt;/p&gt;\\n\\n&lt;p&gt;The 36 is still confusing though as they definitely say &amp;quot;Gbps&amp;quot; quite consistently and also used the same value for Genoa.  My Genoa &lt;em&gt;definitely&lt;/em&gt; gets 48-52GBps (big B) per link which has like, nothing to do with 36 :).  AMD has some tuning docs that claim the FCLK for Genoa will go to 2400MHz to match it&amp;#39;s nominal DDR5-4800.  But I&amp;#39;m not sure how to get 36 from 2.4, nor how to reconcile the observed ~50GBps to either.&lt;/p&gt;\\n\\n&lt;p&gt;tl;dr I&amp;#39;m not sure how to reconcile the numbers, but Turin GMI links definitely benchmark at ~60GBps&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m6h67y","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6h67y/epyc_qwen3_235b_q8_speed/n4l28tu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753213818,"author_flair_text":null,"collapsed":false,"created_utc":1753213818,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4k7ovl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"101m4n","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4k0mg2","score":1,"author_fullname":"t2_p7nc2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Just deduction. The links are 256 bits wide and (I presume) run at the same frequency as the IO die (1/2 the ddr transfer rate). For 6000MHz DDR5 that would be 3000MHz. That's where I got the numbers from. Slower memory would result in lower values, which I guess could explain the variability in quoted speeds? But for 6000MHz ddr5, that puts the theoretical peak bandwidth at 96GB/s per link.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4k7ovl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just deduction. The links are 256 bits wide and (I presume) run at the same frequency as the IO die (1/2 the ddr transfer rate). For 6000MHz DDR5 that would be 3000MHz. That&amp;#39;s where I got the numbers from. Slower memory would result in lower values, which I guess could explain the variability in quoted speeds? But for 6000MHz ddr5, that puts the theoretical peak bandwidth at 96GB/s per link.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6h67y","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6h67y/epyc_qwen3_235b_q8_speed/n4k7ovl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753205252,"author_flair_text":null,"treatment_tags":[],"created_utc":1753205252,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4k0mg2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"eloquentemu","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4jw8n8","score":1,"author_fullname":"t2_lpdsy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm curious your source, or maybe it's just a misunderstanding?  The dual-link Turins benchmark at ~100GBps, but as I note in my edit, 8 CCD Turins are still dual-link (unlike Genoa) so most are effectively that ~100GBps until you reach the super density chips.\\n\\nFWIW I think theoretical is 2x64GBps, coming from a link being 32Gbps that is 16b wide. One AMD doc lists the link speed as \\"up to 36Gbps\\" but the rest say 32.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4k0mg2","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m curious your source, or maybe it&amp;#39;s just a misunderstanding?  The dual-link Turins benchmark at ~100GBps, but as I note in my edit, 8 CCD Turins are still dual-link (unlike Genoa) so most are effectively that ~100GBps until you reach the super density chips.&lt;/p&gt;\\n\\n&lt;p&gt;FWIW I think theoretical is 2x64GBps, coming from a link being 32Gbps that is 16b wide. One AMD doc lists the link speed as &amp;quot;up to 36Gbps&amp;quot; but the rest say 32.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6h67y","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6h67y/epyc_qwen3_235b_q8_speed/n4k0mg2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753203358,"author_flair_text":null,"treatment_tags":[],"created_utc":1753203358,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4jw8n8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"101m4n","can_mod_post":false,"created_utc":1753202152,"send_replies":true,"parent_id":"t1_n4jsw73","score":1,"author_fullname":"t2_p7nc2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"For zen4 and up, the links are 96GB/s read and 48 write. But still yeah, OP should still avoid 2ccd chips.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4jw8n8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For zen4 and up, the links are 96GB/s read and 48 write. But still yeah, OP should still avoid 2ccd chips.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6h67y","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6h67y/epyc_qwen3_235b_q8_speed/n4jw8n8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753202152,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4m0kms","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Afternoon_4260","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4ltyhc","score":1,"author_fullname":"t2_cj9kap4bx","approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt;it is only ~50% efficient in terms of bandwidth\\n\\nYou are right there is some optimisations to chase, but i think you'll still be compute bound from all these matmul, may be intel has a chance with AMX I don't know really\\n\\nI understand it's the bare workable minimum, the thing when using these tools all day long is that speed is what allows you to iterate quickly and not losing the thread of thought.\\n\\nWhen you run the numbers, for a turin with warranty count around 15k euros, around 8.5k if you want a rtx pro 6000, or 10k euros if you want 4 5090.\\nThat brings you a \\"sample\\" of the future at 96 or 128gb of vram at 1.7tb/s (mind the parralel with 4 5090).\\n\\nOn the other hand for 5k more you have 144gb of ~5tb/s in a gh200. Mind a arm architecture and 480gb system ram (rather slow at ~500gb/s). And a 900gb/s link between cpu and gpu (I'm wet dreaming swapping some weights in ram at these speeds\\nFrom what I read ikllama should support arm cpu (because iirc for mac it uses arm neon instructions)\\nBut then the software stack to use these at their full potential aren't llama.cpp and comfyui lol\\n\\nWhat do you think about that?","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n4m0kms","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;it is only ~50% efficient in terms of bandwidth&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;You are right there is some optimisations to chase, but i think you&amp;#39;ll still be compute bound from all these matmul, may be intel has a chance with AMX I don&amp;#39;t know really&lt;/p&gt;\\n\\n&lt;p&gt;I understand it&amp;#39;s the bare workable minimum, the thing when using these tools all day long is that speed is what allows you to iterate quickly and not losing the thread of thought.&lt;/p&gt;\\n\\n&lt;p&gt;When you run the numbers, for a turin with warranty count around 15k euros, around 8.5k if you want a rtx pro 6000, or 10k euros if you want 4 5090.\\nThat brings you a &amp;quot;sample&amp;quot; of the future at 96 or 128gb of vram at 1.7tb/s (mind the parralel with 4 5090).&lt;/p&gt;\\n\\n&lt;p&gt;On the other hand for 5k more you have 144gb of ~5tb/s in a gh200. Mind a arm architecture and 480gb system ram (rather slow at ~500gb/s). And a 900gb/s link between cpu and gpu (I&amp;#39;m wet dreaming swapping some weights in ram at these speeds\\nFrom what I read ikllama should support arm cpu (because iirc for mac it uses arm neon instructions)\\nBut then the software stack to use these at their full potential aren&amp;#39;t llama.cpp and comfyui lol&lt;/p&gt;\\n\\n&lt;p&gt;What do you think about that?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m6h67y","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6h67y/epyc_qwen3_235b_q8_speed/n4m0kms/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753223827,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1753223827,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4ltyhc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"eloquentemu","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4lk4h2","score":1,"author_fullname":"t2_lpdsy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; But honestly what do you think about cpu inference? I mean no flash attention, limited to slow inference with batch 1 anyway. That's only good for moe, dense models and other diffusion models are out of the question\\n\\nI mean, currently it's actually great.  Yeah, it's limited but at the same time I _can_ run anything on CPU even if it's mediocre.  Like Llama-405B?  No problem!  I mean, okay, not if 1.5t/s is a problem but it runs. I can run 70B dense at 6t/s @ Q4 CPU-only though it's not like that can't offload ~half to a GPU.  You're right, of course, that it's mostly for batch 1 MoE but for local LLMs that's a really hot capability right now.  And it gives you a \\"free\\" a server platform with a bunch of I/O if you want to drop in 3090s or Pro6000s or whatever for high batch dense inference jobs.  \\n\\nIf you do the math, it is only ~50% efficient in terms of bandwidth, but I think it gets better with Q8 (it's ~60% so a bit ~~machine is in use so can't test right now, maybe I'll update later~~).  But the 80% vs 90% probably isn't too meaningful regardless.","edited":1753222934,"author_flair_css_class":null,"name":"t1_n4ltyhc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;But honestly what do you think about cpu inference? I mean no flash attention, limited to slow inference with batch 1 anyway. That&amp;#39;s only good for moe, dense models and other diffusion models are out of the question&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I mean, currently it&amp;#39;s actually great.  Yeah, it&amp;#39;s limited but at the same time I &lt;em&gt;can&lt;/em&gt; run anything on CPU even if it&amp;#39;s mediocre.  Like Llama-405B?  No problem!  I mean, okay, not if 1.5t/s is a problem but it runs. I can run 70B dense at 6t/s @ Q4 CPU-only though it&amp;#39;s not like that can&amp;#39;t offload ~half to a GPU.  You&amp;#39;re right, of course, that it&amp;#39;s mostly for batch 1 MoE but for local LLMs that&amp;#39;s a really hot capability right now.  And it gives you a &amp;quot;free&amp;quot; a server platform with a bunch of I/O if you want to drop in 3090s or Pro6000s or whatever for high batch dense inference jobs.  &lt;/p&gt;\\n\\n&lt;p&gt;If you do the math, it is only ~50% efficient in terms of bandwidth, but I think it gets better with Q8 (it&amp;#39;s ~60% so a bit &lt;del&gt;machine is in use so can&amp;#39;t test right now, maybe I&amp;#39;ll update later&lt;/del&gt;).  But the 80% vs 90% probably isn&amp;#39;t too meaningful regardless.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m6h67y","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6h67y/epyc_qwen3_235b_q8_speed/n4ltyhc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753221712,"author_flair_text":null,"collapsed":false,"created_utc":1753221712,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4lk4h2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Afternoon_4260","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4l9wrd","score":1,"author_fullname":"t2_cj9kap4bx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt;it's on ebay for $1700 right now, and broadly Genoa is &lt;=$2k.\\n\\nYou're absolutely right, I was thinking about a new epyc turin. Used genoa is a very sensible choice if you don't care about warranty.  \\niirc from fairydreaming's work, you should expect 80% theoretical ram bw for genoa and 90% for turin. Iirc that was for a synthetic workload (not llm inference) and that was for comparable sku with 8CCDs iirc.\\n\\nA used genoa should bring you most of the way for a fair discount\\n\\nBut honestly what do you think about cpu inference? I mean no flash attention, limited to slow inference with batch 1 anyway. That's only good for moe, dense models and other diffusion models are out of the question 🤷\\n\\nOn the other hand, from fairydreaming's experiment he ran deepseek (q4m?) at around 380w with a 9374F [here](https://www.reddit.com/r/test/s/wBtRXK7kgQ)\\n[here](https://www.reddit.com/r/LocalLLaMA/s/3aScHly4E2) you have some more up to date speeds","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4lk4h2","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;it&amp;#39;s on ebay for $1700 right now, and broadly Genoa is &amp;lt;=$2k.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;You&amp;#39;re absolutely right, I was thinking about a new epyc turin. Used genoa is a very sensible choice if you don&amp;#39;t care about warranty.&lt;br/&gt;\\niirc from fairydreaming&amp;#39;s work, you should expect 80% theoretical ram bw for genoa and 90% for turin. Iirc that was for a synthetic workload (not llm inference) and that was for comparable sku with 8CCDs iirc.&lt;/p&gt;\\n\\n&lt;p&gt;A used genoa should bring you most of the way for a fair discount&lt;/p&gt;\\n\\n&lt;p&gt;But honestly what do you think about cpu inference? I mean no flash attention, limited to slow inference with batch 1 anyway. That&amp;#39;s only good for moe, dense models and other diffusion models are out of the question 🤷&lt;/p&gt;\\n\\n&lt;p&gt;On the other hand, from fairydreaming&amp;#39;s experiment he ran deepseek (q4m?) at around 380w with a 9374F &lt;a href=\\"https://www.reddit.com/r/test/s/wBtRXK7kgQ\\"&gt;here&lt;/a&gt;\\n&lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/s/3aScHly4E2\\"&gt;here&lt;/a&gt; you have some more up to date speeds&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6h67y","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6h67y/epyc_qwen3_235b_q8_speed/n4lk4h2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753218790,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1753218790,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4l9wrd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"eloquentemu","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4l3ml6","score":1,"author_fullname":"t2_lpdsy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The 9175F is a neat chip that actually has 16 CCDs rather than 12 (and 16 cores).  They're pretty specialized and really good in some applications but not great in general due to lack of shared caches and only having 16c.  The single core boosts fast enough that you _could_ use almost all of the CCD-IO bandwidth but for LLMs you'll indeed probably be compute bound.\\n\\n&gt; i know you need at least 2 or 3K more to get a decent cpu \\n\\nI mean, it's all about how you define decent.  My 9B14 is a 96 core Genoa that can run 400W and DDR5-5200 for a nice little boost and it's on ebay for $1700 right now, and broadly Genoa is &lt;=$2k.  So, sure, if you want high performance at the bleeding edge you'll need to pay for it, but Genoa is more reasonably priced, very performant (esp for LLMs), and most systems can upgrade to Turin once it becomes last-gen and costs go down.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4l9wrd","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The 9175F is a neat chip that actually has 16 CCDs rather than 12 (and 16 cores).  They&amp;#39;re pretty specialized and really good in some applications but not great in general due to lack of shared caches and only having 16c.  The single core boosts fast enough that you &lt;em&gt;could&lt;/em&gt; use almost all of the CCD-IO bandwidth but for LLMs you&amp;#39;ll indeed probably be compute bound.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;i know you need at least 2 or 3K more to get a decent cpu &lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I mean, it&amp;#39;s all about how you define decent.  My 9B14 is a 96 core Genoa that can run 400W and DDR5-5200 for a nice little boost and it&amp;#39;s on ebay for $1700 right now, and broadly Genoa is &amp;lt;=$2k.  So, sure, if you want high performance at the bleeding edge you&amp;#39;ll need to pay for it, but Genoa is more reasonably priced, very performant (esp for LLMs), and most systems can upgrade to Turin once it becomes last-gen and costs go down.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6h67y","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6h67y/epyc_qwen3_235b_q8_speed/n4l9wrd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753215946,"author_flair_text":null,"treatment_tags":[],"created_utc":1753215946,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4l3ml6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Afternoon_4260","can_mod_post":false,"created_utc":1753214201,"send_replies":true,"parent_id":"t1_n4jsw73","score":1,"author_fullname":"t2_cj9kap4bx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Not an expert nor my personal experiment but I understood that you need compute power to hope to saturate the ram bandwidth your max theoretical ram bandwidth. There is a 9175F with 16 cores 12CCDs and fast clock.. it was meuh.. i know you need at least 2 or 3K more to get a decent cpu but you also get the full epyc experience","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4l3ml6","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not an expert nor my personal experiment but I understood that you need compute power to hope to saturate the ram bandwidth your max theoretical ram bandwidth. There is a 9175F with 16 cores 12CCDs and fast clock.. it was meuh.. i know you need at least 2 or 3K more to get a decent cpu but you also get the full epyc experience&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6h67y","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6h67y/epyc_qwen3_235b_q8_speed/n4l3ml6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753214201,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4jsw73","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"eloquentemu","can_mod_post":false,"created_utc":1753201224,"send_replies":true,"parent_id":"t3_1m6h67y","score":11,"author_fullname":"t2_lpdsy","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Nobody building for LLMs should get a Epyc 9015 (or 9115 or 9135).  It has 2 CCDs so will only be able to use about 6 channels of DDR5 worth on bandwidth, as the CCD-IO link is limited to about 120GBps (60GBps per link, with &lt;=4 CCD designs using 2 links).  Cores can matter too, but the GPU offload mitigates that a lot.  I guess if you only plan on populating 6 channels maybe it's fair though?  Still seems a waste.\\n\\nI have an Epyc 9B14, 3.7GHz, 12ch DDR5 5200, so not quite the same as Turin, but should be an okay comparison.  I have SMT turned off, which you probably wouldn't for the 9015 though I don't expect it would make a huge difference on a heavy compute workload like this.  I _did_ limit my benchmark to 4 CDDs with 2 cores each, which should emulate the 9015 (it should have 2x2 links so I'm using 4x1 links).  This offloads to a 4090:\\n\\n| model                     |       size |     params | backend   | ngl | ot        |            test |                  t/s |\\n| ------------------------- | ---------: | ---------: | --------- | --: | --------- | --------------: | -------------------: |\\n| qwen3moe 235B.A22B Q8_0   | 232.77 GiB |   235.09 B | CUDA      |  99 | exps=CPU  |           pp512 |         44.55 ± 0.00 |\\n| qwen3moe 235B.A22B Q8_0   | 232.77 GiB |   235.09 B | CUDA      |  99 | exps=CPU  |           tg128 |          7.21 ± 0.00 |\\n| qwen3moe 235B.A22B Q8_0   | 232.77 GiB |   235.09 B | CUDA      |  99 | exps=CPU  |   pp512 @ d2000 |         43.89 ± 0.00 |\\n| qwen3moe 235B.A22B Q8_0   | 232.77 GiB |   235.09 B | CUDA      |  99 | exps=CPU  |   tg128 @ d2000 |          6.89 ± 0.00 |\\n\\nIf I use 8 CCDs and 32 threads like a Epyc 9355 I get:\\n\\n| model                     |       size |     params | backend    | ngl | ot        |            test |                  t/s |\\n| ------------------------- | ---------: | ---------: | ---------- | --: | --------- | --------------: | -------------------: |\\n| qwen3moe 235B.A22B Q8_0   | 232.77 GiB |   235.09 B | CUDA       |  99 | exps=CPU  |           pp512 |         45.91 ± 0.00 |\\n| qwen3moe 235B.A22B Q8_0   | 232.77 GiB |   235.09 B | CUDA       |  99 | exps=CPU  |           tg128 |         12.64 ± 0.00 |\\n| qwen3moe 235B.A22B Q8_0   | 232.77 GiB |   235.09 B | CUDA       |  99 | exps=CPU  |   pp512 @ d2000 |         45.28 ± 0.00 |\\n| qwen3moe 235B.A22B Q8_0   | 232.77 GiB |   235.09 B | CUDA       |  99 | exps=CPU  |   tg128 @ d2000 |         11.42 ± 0.00 |\\n\\nEDIT: As a fun fact, Turin supports 16 max links to Genoa's 12 so some (all?) of the Turin 8 CCD models will have dual-link architectures making them a better option than the 12 CCD, though you lose out a bit on L3.  I would be curious about a genuine 9015 benchmark because there's one document that might imply that a CCD could have 4 links to the IO, but I suspect that's not true and $600 is a little more than I want to spend to test it :D.\\n\\nEDIT2: Just for completeness, here's my normal execution parameters (48c with 4c x 12ccd) with a few different quants.  I do this to note that for whatever reason Qwen-235B is actually somewhat inefficient and not entirely memory bound at lower quants so you don't lose as much performance as one might expect running Q8_0.  I noticed this because I was also testing ERNIE-4.5-300B-A47B yesterday and found that to run shockingly fast and double checked I wasn't still running Qwen-235B-A22B since you'd expect that ERNIE having 2x the active parameters would mean running 1/2 the speed, but it's only about 30% slower at Q4!?  So yeah, if you're worried about quantization and have the RAM I guess just run the Q8.\\n  \\n| model                         |       size |     params | backend    | ngl | ot        |   test |            t/s |\\n| ----------------------------- | ---------: | ---------: | ---------- | --: | --------- | -----: | -------------: |\\n| qwen3moe 235B.A22B Q4_K_M     | 132.39 GiB |   235.09 B | CUDA       |  99 | exps=CPU  |  pp512 |   77.07 ± 0.02 |\\n| qwen3moe 235B.A22B Q4_K_M     | 132.39 GiB |   235.09 B | CUDA       |  99 | exps=CPU  |  tg128 |   18.69 ± 0.11 |\\n| qwen3moe 235B.A22B Q6_K       | 179.75 GiB |   235.09 B | CUDA       |  99 | exps=CPU  |  pp512 |   57.96 ± 0.02 |\\n| qwen3moe 235B.A22B Q6_K       | 179.75 GiB |   235.09 B | CUDA       |  99 | exps=CPU  |  tg128 |   15.78 ± 0.01 |\\n| qwen3moe 235B.A22B Q8_0       | 232.77 GiB |   235.09 B | CUDA       |  99 | exps=CPU  |  pp512 |   45.61 ± 0.02 |\\n| qwen3moe 235B.A22B Q8_0       | 232.77 GiB |   235.09 B | CUDA       |  99 | exps=CPU  |  tg128 |   14.18 ± 0.09 |","edited":1753215075,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4jsw73","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nobody building for LLMs should get a Epyc 9015 (or 9115 or 9135).  It has 2 CCDs so will only be able to use about 6 channels of DDR5 worth on bandwidth, as the CCD-IO link is limited to about 120GBps (60GBps per link, with &amp;lt;=4 CCD designs using 2 links).  Cores can matter too, but the GPU offload mitigates that a lot.  I guess if you only plan on populating 6 channels maybe it&amp;#39;s fair though?  Still seems a waste.&lt;/p&gt;\\n\\n&lt;p&gt;I have an Epyc 9B14, 3.7GHz, 12ch DDR5 5200, so not quite the same as Turin, but should be an okay comparison.  I have SMT turned off, which you probably wouldn&amp;#39;t for the 9015 though I don&amp;#39;t expect it would make a huge difference on a heavy compute workload like this.  I &lt;em&gt;did&lt;/em&gt; limit my benchmark to 4 CDDs with 2 cores each, which should emulate the 9015 (it should have 2x2 links so I&amp;#39;m using 4x1 links).  This offloads to a 4090:&lt;/p&gt;\\n\\n&lt;table&gt;&lt;thead&gt;\\n&lt;tr&gt;\\n&lt;th&gt;model&lt;/th&gt;\\n&lt;th align=\\"right\\"&gt;size&lt;/th&gt;\\n&lt;th align=\\"right\\"&gt;params&lt;/th&gt;\\n&lt;th&gt;backend&lt;/th&gt;\\n&lt;th align=\\"right\\"&gt;ngl&lt;/th&gt;\\n&lt;th&gt;ot&lt;/th&gt;\\n&lt;th align=\\"right\\"&gt;test&lt;/th&gt;\\n&lt;th align=\\"right\\"&gt;t/s&lt;/th&gt;\\n&lt;/tr&gt;\\n&lt;/thead&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;td&gt;qwen3moe 235B.A22B Q8_0&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;232.77 GiB&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;235.09 B&lt;/td&gt;\\n&lt;td&gt;CUDA&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;99&lt;/td&gt;\\n&lt;td&gt;exps=CPU&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;pp512&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;44.55 ± 0.00&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td&gt;qwen3moe 235B.A22B Q8_0&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;232.77 GiB&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;235.09 B&lt;/td&gt;\\n&lt;td&gt;CUDA&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;99&lt;/td&gt;\\n&lt;td&gt;exps=CPU&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;tg128&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;7.21 ± 0.00&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td&gt;qwen3moe 235B.A22B Q8_0&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;232.77 GiB&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;235.09 B&lt;/td&gt;\\n&lt;td&gt;CUDA&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;99&lt;/td&gt;\\n&lt;td&gt;exps=CPU&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;pp512 @ d2000&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;43.89 ± 0.00&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td&gt;qwen3moe 235B.A22B Q8_0&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;232.77 GiB&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;235.09 B&lt;/td&gt;\\n&lt;td&gt;CUDA&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;99&lt;/td&gt;\\n&lt;td&gt;exps=CPU&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;tg128 @ d2000&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;6.89 ± 0.00&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;/tbody&gt;&lt;/table&gt;\\n\\n&lt;p&gt;If I use 8 CCDs and 32 threads like a Epyc 9355 I get:&lt;/p&gt;\\n\\n&lt;table&gt;&lt;thead&gt;\\n&lt;tr&gt;\\n&lt;th&gt;model&lt;/th&gt;\\n&lt;th align=\\"right\\"&gt;size&lt;/th&gt;\\n&lt;th align=\\"right\\"&gt;params&lt;/th&gt;\\n&lt;th&gt;backend&lt;/th&gt;\\n&lt;th align=\\"right\\"&gt;ngl&lt;/th&gt;\\n&lt;th&gt;ot&lt;/th&gt;\\n&lt;th align=\\"right\\"&gt;test&lt;/th&gt;\\n&lt;th align=\\"right\\"&gt;t/s&lt;/th&gt;\\n&lt;/tr&gt;\\n&lt;/thead&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;td&gt;qwen3moe 235B.A22B Q8_0&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;232.77 GiB&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;235.09 B&lt;/td&gt;\\n&lt;td&gt;CUDA&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;99&lt;/td&gt;\\n&lt;td&gt;exps=CPU&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;pp512&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;45.91 ± 0.00&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td&gt;qwen3moe 235B.A22B Q8_0&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;232.77 GiB&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;235.09 B&lt;/td&gt;\\n&lt;td&gt;CUDA&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;99&lt;/td&gt;\\n&lt;td&gt;exps=CPU&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;tg128&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;12.64 ± 0.00&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td&gt;qwen3moe 235B.A22B Q8_0&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;232.77 GiB&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;235.09 B&lt;/td&gt;\\n&lt;td&gt;CUDA&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;99&lt;/td&gt;\\n&lt;td&gt;exps=CPU&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;pp512 @ d2000&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;45.28 ± 0.00&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td&gt;qwen3moe 235B.A22B Q8_0&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;232.77 GiB&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;235.09 B&lt;/td&gt;\\n&lt;td&gt;CUDA&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;99&lt;/td&gt;\\n&lt;td&gt;exps=CPU&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;tg128 @ d2000&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;11.42 ± 0.00&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;/tbody&gt;&lt;/table&gt;\\n\\n&lt;p&gt;EDIT: As a fun fact, Turin supports 16 max links to Genoa&amp;#39;s 12 so some (all?) of the Turin 8 CCD models will have dual-link architectures making them a better option than the 12 CCD, though you lose out a bit on L3.  I would be curious about a genuine 9015 benchmark because there&amp;#39;s one document that might imply that a CCD could have 4 links to the IO, but I suspect that&amp;#39;s not true and $600 is a little more than I want to spend to test it :D.&lt;/p&gt;\\n\\n&lt;p&gt;EDIT2: Just for completeness, here&amp;#39;s my normal execution parameters (48c with 4c x 12ccd) with a few different quants.  I do this to note that for whatever reason Qwen-235B is actually somewhat inefficient and not entirely memory bound at lower quants so you don&amp;#39;t lose as much performance as one might expect running Q8_0.  I noticed this because I was also testing ERNIE-4.5-300B-A47B yesterday and found that to run shockingly fast and double checked I wasn&amp;#39;t still running Qwen-235B-A22B since you&amp;#39;d expect that ERNIE having 2x the active parameters would mean running 1/2 the speed, but it&amp;#39;s only about 30% slower at Q4!?  So yeah, if you&amp;#39;re worried about quantization and have the RAM I guess just run the Q8.&lt;/p&gt;\\n\\n&lt;table&gt;&lt;thead&gt;\\n&lt;tr&gt;\\n&lt;th&gt;model&lt;/th&gt;\\n&lt;th align=\\"right\\"&gt;size&lt;/th&gt;\\n&lt;th align=\\"right\\"&gt;params&lt;/th&gt;\\n&lt;th&gt;backend&lt;/th&gt;\\n&lt;th align=\\"right\\"&gt;ngl&lt;/th&gt;\\n&lt;th&gt;ot&lt;/th&gt;\\n&lt;th align=\\"right\\"&gt;test&lt;/th&gt;\\n&lt;th align=\\"right\\"&gt;t/s&lt;/th&gt;\\n&lt;/tr&gt;\\n&lt;/thead&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;td&gt;qwen3moe 235B.A22B Q4_K_M&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;132.39 GiB&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;235.09 B&lt;/td&gt;\\n&lt;td&gt;CUDA&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;99&lt;/td&gt;\\n&lt;td&gt;exps=CPU&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;pp512&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;77.07 ± 0.02&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td&gt;qwen3moe 235B.A22B Q4_K_M&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;132.39 GiB&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;235.09 B&lt;/td&gt;\\n&lt;td&gt;CUDA&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;99&lt;/td&gt;\\n&lt;td&gt;exps=CPU&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;tg128&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;18.69 ± 0.11&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td&gt;qwen3moe 235B.A22B Q6_K&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;179.75 GiB&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;235.09 B&lt;/td&gt;\\n&lt;td&gt;CUDA&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;99&lt;/td&gt;\\n&lt;td&gt;exps=CPU&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;pp512&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;57.96 ± 0.02&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td&gt;qwen3moe 235B.A22B Q6_K&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;179.75 GiB&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;235.09 B&lt;/td&gt;\\n&lt;td&gt;CUDA&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;99&lt;/td&gt;\\n&lt;td&gt;exps=CPU&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;tg128&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;15.78 ± 0.01&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td&gt;qwen3moe 235B.A22B Q8_0&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;232.77 GiB&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;235.09 B&lt;/td&gt;\\n&lt;td&gt;CUDA&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;99&lt;/td&gt;\\n&lt;td&gt;exps=CPU&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;pp512&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;45.61 ± 0.02&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td&gt;qwen3moe 235B.A22B Q8_0&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;232.77 GiB&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;235.09 B&lt;/td&gt;\\n&lt;td&gt;CUDA&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;99&lt;/td&gt;\\n&lt;td&gt;exps=CPU&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;tg128&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;14.18 ± 0.09&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;/tbody&gt;&lt;/table&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6h67y/epyc_qwen3_235b_q8_speed/n4jsw73/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753201224,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6h67y","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4jngqo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"_xulion","can_mod_post":false,"created_utc":1753199690,"send_replies":true,"parent_id":"t3_1m6h67y","score":2,"author_fullname":"t2_a1dvxm4d","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"my dual 6140 can run it at about 3-4 t/s when fully loaded to ram using llama cpp. I don't have GPU.\\n\\nAccording to intel the 6140 has flops of 0.86T so dual 6140 may have around 1.7 Tflops of compute power (information from: [APP Metrics for Intel® Microprocessors - Intel® Xeon® Processor](https://www.intel.com/content/www/us/en/content-details/840270/app-metrics-for-intel-microprocessors-intel-xeon-processor.html)). But I do have loss due to the numa nodes problem.\\n\\n  \\naccording to this page ([AMD EPYC 9015 AI Performance and Hardware Specs | WareDB](https://www.waredb.com/processor/amd-epyc-9015)), your CPU is way faster than my setup. with enough ram you shall get better result than me.\\n\\n  \\nbtw, 256G is not enough to load the Q8 model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4jngqo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;my dual 6140 can run it at about 3-4 t/s when fully loaded to ram using llama cpp. I don&amp;#39;t have GPU.&lt;/p&gt;\\n\\n&lt;p&gt;According to intel the 6140 has flops of 0.86T so dual 6140 may have around 1.7 Tflops of compute power (information from: &lt;a href=\\"https://www.intel.com/content/www/us/en/content-details/840270/app-metrics-for-intel-microprocessors-intel-xeon-processor.html\\"&gt;APP Metrics for Intel® Microprocessors - Intel® Xeon® Processor&lt;/a&gt;). But I do have loss due to the numa nodes problem.&lt;/p&gt;\\n\\n&lt;p&gt;according to this page (&lt;a href=\\"https://www.waredb.com/processor/amd-epyc-9015\\"&gt;AMD EPYC 9015 AI Performance and Hardware Specs | WareDB&lt;/a&gt;), your CPU is way faster than my setup. with enough ram you shall get better result than me.&lt;/p&gt;\\n\\n&lt;p&gt;btw, 256G is not enough to load the Q8 model.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6h67y/epyc_qwen3_235b_q8_speed/n4jngqo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753199690,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6h67y","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4kfbeo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MidnightProgrammer","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4jwjx0","score":1,"author_fullname":"t2_ijzb7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You are not going to find the correct config or anything near it anywhere.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4kfbeo","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You are not going to find the correct config or anything near it anywhere.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6h67y","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6h67y/epyc_qwen3_235b_q8_speed/n4kfbeo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753207315,"author_flair_text":null,"treatment_tags":[],"created_utc":1753207315,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4jwjx0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"101m4n","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4jw3eg","score":1,"author_fullname":"t2_p7nc2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Honestly, if you've got the money to potentially drop on this, just rent something for a few afternoons and run some benchmarks yourself.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4jwjx0","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Honestly, if you&amp;#39;ve got the money to potentially drop on this, just rent something for a few afternoons and run some benchmarks yourself.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6h67y","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6h67y/epyc_qwen3_235b_q8_speed/n4jwjx0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753202241,"author_flair_text":null,"treatment_tags":[],"created_utc":1753202241,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4jw3eg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MidnightProgrammer","can_mod_post":false,"created_utc":1753202111,"send_replies":true,"parent_id":"t1_n4jvp6o","score":1,"author_fullname":"t2_ijzb7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah I wouldn’t get that chip but looking for anyone with that or better to benchmark.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4jw3eg","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah I wouldn’t get that chip but looking for anyone with that or better to benchmark.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6h67y","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6h67y/epyc_qwen3_235b_q8_speed/n4jw3eg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753202111,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4jvp6o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"101m4n","can_mod_post":false,"created_utc":1753201999,"send_replies":true,"parent_id":"t3_1m6h67y","score":1,"author_fullname":"t2_p7nc2","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"That's an 8 core chip with only 2 CCDs, you're going to need more cores than that. I investigated this recently and the best bet is probably an 8 CCD chip with 32 or more cores if you want to get full usage out of the memory bandwidth.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4jvp6o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s an 8 core chip with only 2 CCDs, you&amp;#39;re going to need more cores than that. I investigated this recently and the best bet is probably an 8 CCD chip with 32 or more cores if you want to get full usage out of the memory bandwidth.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6h67y/epyc_qwen3_235b_q8_speed/n4jvp6o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753201999,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6h67y","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4mqxzv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Informal-Spinach-345","can_mod_post":false,"created_utc":1753232547,"send_replies":true,"parent_id":"t3_1m6h67y","score":2,"author_fullname":"t2_e8woqvcq","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"EPYC 9355 will be the cheapest version with 16 CCDs","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4mqxzv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;EPYC 9355 will be the cheapest version with 16 CCDs&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6h67y/epyc_qwen3_235b_q8_speed/n4mqxzv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753232547,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6h67y","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),r=()=>t.jsx(e,{data:l});export{r as default};
