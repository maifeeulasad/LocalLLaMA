import{j as e}from"./index-MXAMZFAj.js";import{R as l}from"./RedditPostRenderer-AiHEQBDd.js";import"./index-B-YL2gc7.js";const t=JSON.parse('[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Quick question: which GPU should I buy to run local LLMs which wonâ€™t ruin my budget. ðŸ¥²\\n\\nCurrently running with an NVIDIA 1070 with 8GB VRAM. \\n\\nQwen3:8b runs fine. But these size of models seems a bit dump compared to everything above that. (But everything above wonâ€™t run on it (or slow as hell) ðŸ¤£\\n\\nId love to use it for:\\nRAG / CAG\\nTools (MCP)\\nResearch (deep research and e.g with searxng)\\nCoding \\n\\n\\nI know. Intense requests.. but, yeah. Wonâ€™t like to put my personal files for vectoring into the cloud ðŸ˜…)\\n\\nEven when youâ€™ve other recommendations, pls share. :)\\n\\nThanks in advance!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Which GPU to upgrade from 1070?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lnfdch","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.25,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_5lmlxzw0","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751205604,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Quick question: which GPU should I buy to run local LLMs which wonâ€™t ruin my budget. ðŸ¥²&lt;/p&gt;\\n\\n&lt;p&gt;Currently running with an NVIDIA 1070 with 8GB VRAM. &lt;/p&gt;\\n\\n&lt;p&gt;Qwen3:8b runs fine. But these size of models seems a bit dump compared to everything above that. (But everything above wonâ€™t run on it (or slow as hell) ðŸ¤£&lt;/p&gt;\\n\\n&lt;p&gt;Id love to use it for:\\nRAG / CAG\\nTools (MCP)\\nResearch (deep research and e.g with searxng)\\nCoding &lt;/p&gt;\\n\\n&lt;p&gt;I know. Intense requests.. but, yeah. Wonâ€™t like to put my personal files for vectoring into the cloud ðŸ˜…)&lt;/p&gt;\\n\\n&lt;p&gt;Even when youâ€™ve other recommendations, pls share. :)&lt;/p&gt;\\n\\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lnfdch","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"TjFr00","discussion_type":null,"num_comments":2,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lnfdch/which_gpu_to_upgrade_from_1070/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lnfdch/which_gpu_to_upgrade_from_1070/","subreddit_subscribers":492624,"created_utc":1751205604,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0etgnd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1751206810,"send_replies":true,"parent_id":"t3_1lnfdch","score":5,"author_fullname":"t2_uz37qfx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"3060 + your 1070 = 20GiB","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0etgnd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;3060 + your 1070 = 20GiB&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnfdch/which_gpu_to_upgrade_from_1070/n0etgnd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751206810,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnfdch","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0esbqo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"j0holo","can_mod_post":false,"created_utc":1751206421,"send_replies":true,"parent_id":"t3_1lnfdch","score":1,"author_fullname":"t2_hodrk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"8gb of vram is just not enough to run larger models. You are running in the issue that larger models spill over to system ram which is much slower in bandwidth compared to VRAM.\\n\\nDo you have a budget? You can look at second hand GPUs with 12 or 16gb of VRAM.  \\nPersonally I use an Intel Arc B580 but that does require some tinkering except when you run the Intel ML studio software on WIndows.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0esbqo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;8gb of vram is just not enough to run larger models. You are running in the issue that larger models spill over to system ram which is much slower in bandwidth compared to VRAM.&lt;/p&gt;\\n\\n&lt;p&gt;Do you have a budget? You can look at second hand GPUs with 12 or 16gb of VRAM.&lt;br/&gt;\\nPersonally I use an Intel Arc B580 but that does require some tinkering except when you run the Intel ML studio software on WIndows.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnfdch/which_gpu_to_upgrade_from_1070/n0esbqo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751206421,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnfdch","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]'),r=()=>e.jsx(l,{data:t});export{r as default};
