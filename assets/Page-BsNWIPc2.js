import{j as e}from"./index-BgwOAK4-.js";import{R as l}from"./RedditPostRenderer-BOBjDTFu.js";import"./index-BL22wVg5.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"As promised in the banana thread. OP delivers.\\n\\n**Benchmarks**\\n\\nThe following benchmarks were taken using official Qwen3 models from Huggingface's Qwen repo for consistency:\\n\\nMoE:\\n\\n* Qwen3 235B A22B GPTQ Int4 quant in Tensor Parallel\\n* Qwen3 30B A3B BF16 in Tensor Parallel\\n* Qwen3 30B A3B BF16 on a single GPU\\n* Qwen3 30B A3B GPTQ Int4 quant in Tensor Parallel\\n* Qwen3 30B A3B GPTQ Int4 quant on a single GPU\\n\\nDense:\\n\\n* Qwen3 32B BF16 on a single GPU\\n* Qwen3 32B BF16 in Tensor Parallel\\n* Qwen3 14B BF16 on a single GPU\\n* Qwen3 14B BF16 in Tensor Parallel\\n\\nAll benchmarking was done with \`vllm bench throughput ...\` using full context space of 32k and incrementing the number of input tokens through the tests. The 235B benchmarks were performed with input lengths of 1024, 4096, 8192, and 16384 tokens. In the name of expediency the remaining tests were performed with input lengths of 1024 and 4096 due to the scaling factors seeming to approximate well with the 235B model.\\n\\n**Hardware**\\n\\n2x Blackwell PRO 6000 Workstation GPUs, 1x EPYC 9745, ~~512GB~~ 768GB DDR5 5200 MT/s, PCIe 5.0 x16.\\n\\n**Software**\\n\\n* Ubuntu 24.04.2\\n* NVidia drivers 575.57.08\\n* CUDA 12.9\\n\\nThis was the magic Torch incantation that got everything working: \\n    \\n    pip install --pre torch==2.9.0.dev20250707+cu128 torchvision==0.24.0.dev20250707+cu128 torchaudio==2.8.0.dev20250707+cu128 --index-url https://download.pytorch.org/whl/nightly/cu128\\n\\nOtherwise these instructions worked well despite being for WSL: https://github.com/fuutott/how-to-run-vllm-on-rtx-pro-6000-under-wsl2-ubuntu-24.04-mistral-24b-qwen3\\n\\n\\n**MoE Results**\\n\\n**Qwen3 235B A22B GPTQ Int4 (Qwen official Int4) @ 1k input**\\n\\n    $ vllm bench throughput --model Qwen/Qwen3-235B-A22B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 1024\\n    Throughput: 5.03 requests/s, 5781.20 total tokens/s, 643.67 output tokens/s\\n    Total num prompt tokens:  1021646\\n    Total num output tokens:  128000\\n\\n**Qwen3 235B A22B GPTQ Int4 (Qwen official Int4) @ 4k input**\\n\\n    $ vllm bench throughput --model Qwen/Qwen3-235B-A22B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 4096\\n    Throughput: 1.34 requests/s, 5665.37 total tokens/s, 171.87 output tokens/s\\n    Total num prompt tokens:  4091212\\n    Total num output tokens:  128000\\n\\n**Qwen3 235B A22B GPTQ Int4 (Qwen official Int4) @ 8k input**\\n\\n    $ vllm bench throughput --model Qwen/Qwen3-235B-A22B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 8192\\n    Throughput: 0.65 requests/s, 5392.17 total tokens/s, 82.98 output tokens/s\\n    Total num prompt tokens:  8189599\\n    Total num output tokens:  128000\\n\\n**Qwen3 235B A22B GPTQ Int4 (Qwen official Int4) @ 16k input**\\n\\n    $ vllm bench throughput --model Qwen/Qwen3-235B-A22B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 16384\\n    Throughput: 0.30 requests/s, 4935.38 total tokens/s, 38.26 output tokens/s\\n    Total num prompt tokens:  16383966\\n    Total num output tokens:  128000\\n\\n\\n\\n\\n**Qwen3 30B A3B (Qwen official FP16) @ 1k input | tensor parallel**\\n\\n    $ vllm bench throughput --model Qwen/Qwen3-30B-A3B --max-model-len 32768 --tensor-parallel 2 --input-len 1024\\n    Throughput: 11.27 requests/s, 12953.87 total tokens/s, 1442.27 output tokens/s\\n    Total num prompt tokens:  1021646\\n    Total num output tokens:  128000\\n\\n**Qwen3 30B A3B (Qwen official FP16) @ 4k input | tensor parallel**\\n\\n    $ vllm bench throughput --model Qwen/Qwen3-30B-A3B --max-model-len 32768 --tensor-parallel 2 --input-len 4096\\n    Throughput: 5.13 requests/s, 21651.80 total tokens/s, 656.86 output tokens/s\\n    Total num prompt tokens:  4091212\\n    Total num output tokens:  128000\\n\\n**Qwen3 30B A3B (Qwen official FP16) @ 1k input | single GPU**\\n\\n    $ vllm bench throughput --model Qwen/Qwen3-30B-A3B --max-model-len 32768 --input-len 1024\\n    Throughput: 13.32 requests/s, 15317.81 total tokens/s, 1705.46 output tokens/s\\n    Total num prompt tokens:  1021646\\n    Total num output tokens:  128000\\n\\n**Qwen3 30B A3B (Qwen official FP16) @ 4k input | single GPU**\\n\\n    $ vllm bench throughput --model Qwen/Qwen3-30B-A3B --max-model-len 32768 --input-len 4096\\n    Throughput: 3.89 requests/s, 16402.36 total tokens/s, 497.61 output tokens/s\\n    Total num prompt tokens:  4091212\\n    Total num output tokens:  128000\\n\\n\\n\\n\\n**Qwen3 30B A3B (Qwen official GPTQ Int4) @ 1k input | tensor parallel**\\n\\n    $ vllm bench throughput --model Qwen/Qwen3-30B-A3B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 1024\\n    Throughput: 23.17 requests/s, 26643.04 total tokens/s, 2966.40 output tokens/s\\n    Total num prompt tokens:  1021646\\n    Total num output tokens:  128000\\n\\n**Qwen3 30B A3B FP16 (Qwen official GPTQ Int4) @ 4k input | tensor parallel**\\n\\n    $ vllm bench throughput --model Qwen/Qwen3-30B-A3B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 4096\\n    Throughput: 5.03 requests/s, 21229.35 total tokens/s, 644.04 output tokens/s\\n    Total num prompt tokens:  4091212\\n    Total num output tokens:  128000\\n\\n\\n\\n\\n**Qwen3 30B A3B (Qwen official GPTQ Int4) @ 1k input | single GPU**\\n\\n    $ vllm bench throughput --model Qwen/Qwen3-30B-A3B-GPTQ-Int4 --max-model-len 32768 --input-len 1024\\n    Throughput: 17.44 requests/s, 20046.60 total tokens/s, 2231.96 output tokens/s\\n    Total num prompt tokens:  1021646\\n    Total num output tokens:  128000\\n\\n**Qwen3 30B A3B (Qwen official GPTQ Int4) @ 4k input | single GPU**\\n\\n    $ vllm bench throughput --model Qwen/Qwen3-30B-A3B-GPTQ-Int4 --max-model-len 32768 --input-len 4096\\n    Throughput: 4.21 requests/s, 17770.35 total tokens/s, 539.11 output tokens/s\\n    Total num prompt tokens:  4091212\\n    Total num output tokens:  128000\\n\\n**Dense Model Results**\\n\\n**Qwen3 32B BF16 @ 1k input | single GPU**\\n\\n    $ vllm bench throughput --model Qwen/Qwen3-32B --max-model-len 32768 --input-len 1024\\n    Throughput: 2.87 requests/s, 3297.05 total tokens/s, 367.09 output tokens/s\\n    Total num prompt tokens:  1021646\\n    Total num output tokens:  128000\\n\\n**Qwen3 32B BF16 @ 4k input | single GPU**\\n\\n    $ vllm bench throughput --model Qwen/Qwen3-32B --max-model-len 32768 --input-len 4096\\n    Throughput: 0.77 requests/s, 3259.23 total tokens/s, 98.88 output tokens/s\\n    Total num prompt tokens:  4091212\\n    Total num output tokens:  128000\\n\\n**Qwen3 32B BF16 @ 8k input | single GPU**\\n\\n    $ vllm bench throughput --model Qwen/Qwen3-32B --max-model-len 32768 --input-len 8192\\n    Throughput: 0.37 requests/s, 3069.56 total tokens/s, 47.24 output tokens/s\\n    Total num prompt tokens:  8189599\\n    Total num output tokens:  128000\\n\\n\\n**Qwen3 32B BF16 @ 1k input | Tensor Parallel**\\n\\n    $ vllm bench throughput --model Qwen/Qwen3-32B --max-model-len 32768 --input-len 1024 --tensor-parallel 2\\n    Throughput: 5.18 requests/s, 5957.00 total tokens/s, 663.24 output tokens/s\\n    Total num prompt tokens:  1021646\\n    Total num output tokens:  128000\\n\\n**Qwen3 32B BF16 @ 4k input | Tensor Parallel**\\n\\n    $ vllm bench throughput --model Qwen/Qwen3-32B --max-model-len 32768 --input-len 4096 --tensor-parallel 2 \\n    Throughput: 1.44 requests/s, 6062.84 total tokens/s, 183.93 output tokens/s\\n    Total num prompt tokens:  4091212\\n    Total num output tokens:  128000\\n\\n**Qwen3 32B BF16 @ 8k input | Tensor Parallel**\\n\\n    $ vllm bench throughput --model Qwen/Qwen3-32B --max-model-len 32768 --input-len 8192 --tensor-parallel 2 \\n    Throughput: 0.70 requests/s, 5806.52 total tokens/s, 89.36 output tokens/s\\n    Total num prompt tokens:  8189599\\n    Total num output tokens:  128000\\n\\n**Qwen3 14B BF16 @ 1k input | single GPU**\\n\\n    $ vllm bench throughput --model Qwen/Qwen3-14B --max-model-len 32768 --input-len 1024\\n    Throughput: 7.26 requests/s, 8340.89 total tokens/s, 928.66 output tokens/s\\n    Total num prompt tokens:  1021646\\n    Total num output tokens:  128000\\n\\n**Qwen3 14B BF16 @ 4k input | single GPU**\\n\\n    $ vllm bench throughput --model Qwen/Qwen3-14B --max-model-len 32768 --input-len 4096\\n    Throughput: 2.00 requests/s, 8426.05 total tokens/s, 255.62 output tokens/s\\n    Total num prompt tokens:  4091212\\n    Total num output tokens:  128000\\n\\n**Qwen3 14B BF16 @ 8k input | single GPU**\\n\\n    $ vllm bench throughput --model Qwen/Qwen3-14B --max-model-len 32768 --input-len 8192\\n    Throughput: 0.97 requests/s, 8028.90 total tokens/s, 123.56 output tokens/s\\n    Total num prompt tokens:  8189599\\n    Total num output tokens:  128000\\n\\n**Qwen3 14B BF16 @ 1k input | Tensor Parallel**\\n\\n    $ vllm bench throughput --model Qwen/Qwen3-14B --max-model-len 32768 --input-len 1024 --tensor-parallel 2 \\n    Throughput: 10.68 requests/s, 12273.33 total tokens/s, 1366.50 output tokens/s\\n    Total num prompt tokens:  1021646\\n    Total num output tokens:  128000\\n\\n**Qwen3 14B BF16 @ 4k input | Tensor Parallel**\\n\\n    $ vllm bench throughput --model Qwen/Qwen3-14B --max-model-len 32768 --input-len 4096 --tensor-parallel 2 \\n    Throughput: 2.88 requests/s, 12140.81 total tokens/s, 368.32 output tokens/s\\n    Total num prompt tokens:  4091212\\n    Total num output tokens:  128000\\n\\n**Qwen3 14B BF16 @ 8k input | Tensor Parallel**\\n\\n    $ vllm bench throughput --model Qwen/Qwen3-14B --max-model-len 32768 --input-len 8192 --tensor-parallel 2 \\n    Throughput: 1.45 requests/s, 12057.89 total tokens/s, 185.56 output tokens/s\\n    Total num prompt tokens:  8189599\\n    Total num output tokens:  128000","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Benchmarking Qwen3 30B and 235B on dual RTX PRO 6000 Blackwell Workstation Edition","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lyxf1f","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.94,"author_flair_background_color":null,"subreddit_type":"public","ups":64,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1t7r9dkpud","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":64,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1752473261,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752425040,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;As promised in the banana thread. OP delivers.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Benchmarks&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;The following benchmarks were taken using official Qwen3 models from Huggingface&amp;#39;s Qwen repo for consistency:&lt;/p&gt;\\n\\n&lt;p&gt;MoE:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Qwen3 235B A22B GPTQ Int4 quant in Tensor Parallel&lt;/li&gt;\\n&lt;li&gt;Qwen3 30B A3B BF16 in Tensor Parallel&lt;/li&gt;\\n&lt;li&gt;Qwen3 30B A3B BF16 on a single GPU&lt;/li&gt;\\n&lt;li&gt;Qwen3 30B A3B GPTQ Int4 quant in Tensor Parallel&lt;/li&gt;\\n&lt;li&gt;Qwen3 30B A3B GPTQ Int4 quant on a single GPU&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Dense:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Qwen3 32B BF16 on a single GPU&lt;/li&gt;\\n&lt;li&gt;Qwen3 32B BF16 in Tensor Parallel&lt;/li&gt;\\n&lt;li&gt;Qwen3 14B BF16 on a single GPU&lt;/li&gt;\\n&lt;li&gt;Qwen3 14B BF16 in Tensor Parallel&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;All benchmarking was done with &lt;code&gt;vllm bench throughput ...&lt;/code&gt; using full context space of 32k and incrementing the number of input tokens through the tests. The 235B benchmarks were performed with input lengths of 1024, 4096, 8192, and 16384 tokens. In the name of expediency the remaining tests were performed with input lengths of 1024 and 4096 due to the scaling factors seeming to approximate well with the 235B model.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Hardware&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;2x Blackwell PRO 6000 Workstation GPUs, 1x EPYC 9745, &lt;del&gt;512GB&lt;/del&gt; 768GB DDR5 5200 MT/s, PCIe 5.0 x16.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Software&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Ubuntu 24.04.2&lt;/li&gt;\\n&lt;li&gt;NVidia drivers 575.57.08&lt;/li&gt;\\n&lt;li&gt;CUDA 12.9&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;This was the magic Torch incantation that got everything working: &lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;pip install --pre torch==2.9.0.dev20250707+cu128 torchvision==0.24.0.dev20250707+cu128 torchaudio==2.8.0.dev20250707+cu128 --index-url https://download.pytorch.org/whl/nightly/cu128\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;Otherwise these instructions worked well despite being for WSL: &lt;a href=\\"https://github.com/fuutott/how-to-run-vllm-on-rtx-pro-6000-under-wsl2-ubuntu-24.04-mistral-24b-qwen3\\"&gt;https://github.com/fuutott/how-to-run-vllm-on-rtx-pro-6000-under-wsl2-ubuntu-24.04-mistral-24b-qwen3&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;MoE Results&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Qwen3 235B A22B GPTQ Int4 (Qwen official Int4) @ 1k input&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-235B-A22B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 1024\\nThroughput: 5.03 requests/s, 5781.20 total tokens/s, 643.67 output tokens/s\\nTotal num prompt tokens:  1021646\\nTotal num output tokens:  128000\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Qwen3 235B A22B GPTQ Int4 (Qwen official Int4) @ 4k input&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-235B-A22B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 4096\\nThroughput: 1.34 requests/s, 5665.37 total tokens/s, 171.87 output tokens/s\\nTotal num prompt tokens:  4091212\\nTotal num output tokens:  128000\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Qwen3 235B A22B GPTQ Int4 (Qwen official Int4) @ 8k input&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-235B-A22B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 8192\\nThroughput: 0.65 requests/s, 5392.17 total tokens/s, 82.98 output tokens/s\\nTotal num prompt tokens:  8189599\\nTotal num output tokens:  128000\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Qwen3 235B A22B GPTQ Int4 (Qwen official Int4) @ 16k input&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-235B-A22B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 16384\\nThroughput: 0.30 requests/s, 4935.38 total tokens/s, 38.26 output tokens/s\\nTotal num prompt tokens:  16383966\\nTotal num output tokens:  128000\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Qwen3 30B A3B (Qwen official FP16) @ 1k input | tensor parallel&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-30B-A3B --max-model-len 32768 --tensor-parallel 2 --input-len 1024\\nThroughput: 11.27 requests/s, 12953.87 total tokens/s, 1442.27 output tokens/s\\nTotal num prompt tokens:  1021646\\nTotal num output tokens:  128000\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Qwen3 30B A3B (Qwen official FP16) @ 4k input | tensor parallel&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-30B-A3B --max-model-len 32768 --tensor-parallel 2 --input-len 4096\\nThroughput: 5.13 requests/s, 21651.80 total tokens/s, 656.86 output tokens/s\\nTotal num prompt tokens:  4091212\\nTotal num output tokens:  128000\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Qwen3 30B A3B (Qwen official FP16) @ 1k input | single GPU&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-30B-A3B --max-model-len 32768 --input-len 1024\\nThroughput: 13.32 requests/s, 15317.81 total tokens/s, 1705.46 output tokens/s\\nTotal num prompt tokens:  1021646\\nTotal num output tokens:  128000\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Qwen3 30B A3B (Qwen official FP16) @ 4k input | single GPU&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-30B-A3B --max-model-len 32768 --input-len 4096\\nThroughput: 3.89 requests/s, 16402.36 total tokens/s, 497.61 output tokens/s\\nTotal num prompt tokens:  4091212\\nTotal num output tokens:  128000\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Qwen3 30B A3B (Qwen official GPTQ Int4) @ 1k input | tensor parallel&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-30B-A3B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 1024\\nThroughput: 23.17 requests/s, 26643.04 total tokens/s, 2966.40 output tokens/s\\nTotal num prompt tokens:  1021646\\nTotal num output tokens:  128000\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Qwen3 30B A3B FP16 (Qwen official GPTQ Int4) @ 4k input | tensor parallel&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-30B-A3B-GPTQ-Int4 --max-model-len 32768 --tensor-parallel 2 --input-len 4096\\nThroughput: 5.03 requests/s, 21229.35 total tokens/s, 644.04 output tokens/s\\nTotal num prompt tokens:  4091212\\nTotal num output tokens:  128000\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Qwen3 30B A3B (Qwen official GPTQ Int4) @ 1k input | single GPU&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-30B-A3B-GPTQ-Int4 --max-model-len 32768 --input-len 1024\\nThroughput: 17.44 requests/s, 20046.60 total tokens/s, 2231.96 output tokens/s\\nTotal num prompt tokens:  1021646\\nTotal num output tokens:  128000\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Qwen3 30B A3B (Qwen official GPTQ Int4) @ 4k input | single GPU&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-30B-A3B-GPTQ-Int4 --max-model-len 32768 --input-len 4096\\nThroughput: 4.21 requests/s, 17770.35 total tokens/s, 539.11 output tokens/s\\nTotal num prompt tokens:  4091212\\nTotal num output tokens:  128000\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Dense Model Results&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Qwen3 32B BF16 @ 1k input | single GPU&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-32B --max-model-len 32768 --input-len 1024\\nThroughput: 2.87 requests/s, 3297.05 total tokens/s, 367.09 output tokens/s\\nTotal num prompt tokens:  1021646\\nTotal num output tokens:  128000\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Qwen3 32B BF16 @ 4k input | single GPU&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-32B --max-model-len 32768 --input-len 4096\\nThroughput: 0.77 requests/s, 3259.23 total tokens/s, 98.88 output tokens/s\\nTotal num prompt tokens:  4091212\\nTotal num output tokens:  128000\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Qwen3 32B BF16 @ 8k input | single GPU&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-32B --max-model-len 32768 --input-len 8192\\nThroughput: 0.37 requests/s, 3069.56 total tokens/s, 47.24 output tokens/s\\nTotal num prompt tokens:  8189599\\nTotal num output tokens:  128000\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Qwen3 32B BF16 @ 1k input | Tensor Parallel&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-32B --max-model-len 32768 --input-len 1024 --tensor-parallel 2\\nThroughput: 5.18 requests/s, 5957.00 total tokens/s, 663.24 output tokens/s\\nTotal num prompt tokens:  1021646\\nTotal num output tokens:  128000\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Qwen3 32B BF16 @ 4k input | Tensor Parallel&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-32B --max-model-len 32768 --input-len 4096 --tensor-parallel 2 \\nThroughput: 1.44 requests/s, 6062.84 total tokens/s, 183.93 output tokens/s\\nTotal num prompt tokens:  4091212\\nTotal num output tokens:  128000\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Qwen3 32B BF16 @ 8k input | Tensor Parallel&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-32B --max-model-len 32768 --input-len 8192 --tensor-parallel 2 \\nThroughput: 0.70 requests/s, 5806.52 total tokens/s, 89.36 output tokens/s\\nTotal num prompt tokens:  8189599\\nTotal num output tokens:  128000\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Qwen3 14B BF16 @ 1k input | single GPU&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-14B --max-model-len 32768 --input-len 1024\\nThroughput: 7.26 requests/s, 8340.89 total tokens/s, 928.66 output tokens/s\\nTotal num prompt tokens:  1021646\\nTotal num output tokens:  128000\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Qwen3 14B BF16 @ 4k input | single GPU&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-14B --max-model-len 32768 --input-len 4096\\nThroughput: 2.00 requests/s, 8426.05 total tokens/s, 255.62 output tokens/s\\nTotal num prompt tokens:  4091212\\nTotal num output tokens:  128000\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Qwen3 14B BF16 @ 8k input | single GPU&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-14B --max-model-len 32768 --input-len 8192\\nThroughput: 0.97 requests/s, 8028.90 total tokens/s, 123.56 output tokens/s\\nTotal num prompt tokens:  8189599\\nTotal num output tokens:  128000\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Qwen3 14B BF16 @ 1k input | Tensor Parallel&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-14B --max-model-len 32768 --input-len 1024 --tensor-parallel 2 \\nThroughput: 10.68 requests/s, 12273.33 total tokens/s, 1366.50 output tokens/s\\nTotal num prompt tokens:  1021646\\nTotal num output tokens:  128000\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Qwen3 14B BF16 @ 4k input | Tensor Parallel&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-14B --max-model-len 32768 --input-len 4096 --tensor-parallel 2 \\nThroughput: 2.88 requests/s, 12140.81 total tokens/s, 368.32 output tokens/s\\nTotal num prompt tokens:  4091212\\nTotal num output tokens:  128000\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Qwen3 14B BF16 @ 8k input | Tensor Parallel&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;$ vllm bench throughput --model Qwen/Qwen3-14B --max-model-len 32768 --input-len 8192 --tensor-parallel 2 \\nThroughput: 1.45 requests/s, 12057.89 total tokens/s, 185.56 output tokens/s\\nTotal num prompt tokens:  8189599\\nTotal num output tokens:  128000\\n&lt;/code&gt;&lt;/pre&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lyxf1f","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"blackwell_tart","discussion_type":null,"num_comments":48,"send_replies":false,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/","subreddit_subscribers":499297,"created_utc":1752425040,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n318hw0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"SandboChang","can_mod_post":false,"send_replies":true,"parent_id":"t1_n30f5fp","score":2,"author_fullname":"t2_10icmj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It does but I agree the fall off seems larger than expected.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n318hw0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It does but I agree the fall off seems larger than expected.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyxf1f","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/n318hw0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752475365,"author_flair_text":null,"treatment_tags":[],"created_utc":1752475365,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n30f5fp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"night0x63","can_mod_post":false,"send_replies":true,"parent_id":"t1_n304psk","score":2,"author_fullname":"t2_3h2irqtz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I thought input and output were independent? I guess not? \\n\\n\\nBig input... Means slower output token rate?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n30f5fp","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I thought input and output were independent? I guess not? &lt;/p&gt;\\n\\n&lt;p&gt;Big input... Means slower output token rate?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyxf1f","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/n30f5fp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752461612,"author_flair_text":null,"treatment_tags":[],"created_utc":1752461612,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n30h2bi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"blackwell_tart","can_mod_post":false,"send_replies":true,"parent_id":"t1_n304psk","score":3,"author_fullname":"t2_1t7r9dkpud","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; less\\n\\nFewer.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n30h2bi","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;less&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Fewer.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyxf1f","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/n30h2bi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752462365,"author_flair_text":null,"treatment_tags":[],"created_utc":1752462365,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n304psk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"GreenVirtual502","can_mod_post":false,"created_utc":1752457769,"send_replies":true,"parent_id":"t1_n2zofuo","score":3,"author_fullname":"t2_rt8ypnr0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"More input=more prefill time=less t/s","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n304psk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;More input=more prefill time=less t/s&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyxf1f","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/n304psk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752457769,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n38d03o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"101m4n","can_mod_post":false,"created_utc":1752571778,"send_replies":true,"parent_id":"t1_n2zofuo","score":1,"author_fullname":"t2_p7nc2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Attention scales with the square of the context length. So yeah, things slowing down as the context grows is expected behaviour.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n38d03o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Attention scales with the square of the context length. So yeah, things slowing down as the context grows is expected behaviour.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyxf1f","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/n38d03o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752571778,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2zofuo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Steuern_Runter","can_mod_post":false,"created_utc":1752451942,"send_replies":true,"parent_id":"t3_1lyxf1f","score":7,"author_fullname":"t2_w8pggsa4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The drop off that comes with more context length is huge. Is this the effect of parallelism becoming less efficient or something?\\n\\n1k input - 643.67 output tokens/s\\n\\n4k input - 171.87 output tokens/s\\n\\n8k input - 82.98 output tokens/s","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2zofuo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The drop off that comes with more context length is huge. Is this the effect of parallelism becoming less efficient or something?&lt;/p&gt;\\n\\n&lt;p&gt;1k input - 643.67 output tokens/s&lt;/p&gt;\\n\\n&lt;p&gt;4k input - 171.87 output tokens/s&lt;/p&gt;\\n\\n&lt;p&gt;8k input - 82.98 output tokens/s&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/n2zofuo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752451942,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyxf1f","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n34lyhw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"pointer_to_null","can_mod_post":false,"created_utc":1752520143,"send_replies":true,"parent_id":"t1_n2xd8oi","score":1,"author_fullname":"t2_9vmfr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; It shows that those models can be really fast on consumer hardware (Yes RTX 6000 is still semi consumer hardware).\\n\\n\\"consumer\\" or \\"semi-consumer\\" is an odd word choice. What hypothetical *consumer* hardware are you imagining?\\n\\nI'd imagine ~$20k just in GPU costs (2x RTX 6000 Blackwell cards) alone... on top of a server with 128 Zen5 cores and 768GB DDR5-5200 (12 channel) pushes hardware past your typical \\"prosumer\\" and well into \\"professional\\" category. Sure, each Blackwell card is running the same GB202 chip from the RTX 5090- albeit unlocked 10% more cores and with 3x the VRAM.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n34lyhw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;It shows that those models can be really fast on consumer hardware (Yes RTX 6000 is still semi consumer hardware).&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;&amp;quot;consumer&amp;quot; or &amp;quot;semi-consumer&amp;quot; is an odd word choice. What hypothetical &lt;em&gt;consumer&lt;/em&gt; hardware are you imagining?&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;d imagine ~$20k just in GPU costs (2x RTX 6000 Blackwell cards) alone... on top of a server with 128 Zen5 cores and 768GB DDR5-5200 (12 channel) pushes hardware past your typical &amp;quot;prosumer&amp;quot; and well into &amp;quot;professional&amp;quot; category. Sure, each Blackwell card is running the same GB202 chip from the RTX 5090- albeit unlocked 10% more cores and with 3x the VRAM.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyxf1f","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/n34lyhw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752520143,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ye2lw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No_Afternoon_4260","can_mod_post":false,"created_utc":1752437054,"send_replies":true,"parent_id":"t1_n2xd8oi","score":1,"author_fullname":"t2_cj9kap4bx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Well yeah, I mean any of those vscode extension can do that if you auto approve read,write, commands..  \\nI'm sure there's better way to do that but that's how I experiment, my main cobay is devstral these days.\\n\\nIn my experience they tend to achieve what you want as long as it's not too complicated and you don't try to be too explicit (constraints in another way than they'll do it by themself).\\n\\nAlso if it's too complicated they tend to diverge from you goal but that's because the agentic concept is still in its infancy imo\\n\\nBut still i prefer to keep the leash tight and go by small iterations (idk may be I just like yo use my brain and see what's happening)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ye2lw","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Well yeah, I mean any of those vscode extension can do that if you auto approve read,write, commands..&lt;br/&gt;\\nI&amp;#39;m sure there&amp;#39;s better way to do that but that&amp;#39;s how I experiment, my main cobay is devstral these days.&lt;/p&gt;\\n\\n&lt;p&gt;In my experience they tend to achieve what you want as long as it&amp;#39;s not too complicated and you don&amp;#39;t try to be too explicit (constraints in another way than they&amp;#39;ll do it by themself).&lt;/p&gt;\\n\\n&lt;p&gt;Also if it&amp;#39;s too complicated they tend to diverge from you goal but that&amp;#39;s because the agentic concept is still in its infancy imo&lt;/p&gt;\\n\\n&lt;p&gt;But still i prefer to keep the leash tight and go by small iterations (idk may be I just like yo use my brain and see what&amp;#39;s happening)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyxf1f","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/n2ye2lw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752437054,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2xd8oi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"polawiaczperel","can_mod_post":false,"created_utc":1752426165,"send_replies":true,"parent_id":"t3_1lyxf1f","score":13,"author_fullname":"t2_8n2w3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Great benchmarks, thanks for that. It shows that those models can be really fast on consumer hardware (Yes RTX 6000 is still semi consumer hardware). \\n\\nI am curious how good it can be for making a web applications using recursive agentic flow. So there is an error in logs, or design is not accurate to figma design and it is trying to fix it to the moment that everything is working fine. It is still not bruteforce, but kind of.\\n\\nHas anyone some experience with flow I described?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2xd8oi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Great benchmarks, thanks for that. It shows that those models can be really fast on consumer hardware (Yes RTX 6000 is still semi consumer hardware). &lt;/p&gt;\\n\\n&lt;p&gt;I am curious how good it can be for making a web applications using recursive agentic flow. So there is an error in logs, or design is not accurate to figma design and it is trying to fix it to the moment that everything is working fine. It is still not bruteforce, but kind of.&lt;/p&gt;\\n\\n&lt;p&gt;Has anyone some experience with flow I described?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/n2xd8oi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752426165,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyxf1f","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":13}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n315ca0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"blackwell_tart","can_mod_post":false,"send_replies":true,"parent_id":"t1_n314gbf","score":2,"author_fullname":"t2_1t7r9dkpud","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I have a soft spot for that 72B Instruct model. I'm downloading it now, both FP8 and Int4.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n315ca0","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have a soft spot for that 72B Instruct model. I&amp;#39;m downloading it now, both FP8 and Int4.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyxf1f","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/n315ca0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752473623,"author_flair_text":null,"treatment_tags":[],"created_utc":1752473623,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n314gbf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Substantial-Ebb-584","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2ycatu","score":1,"author_fullname":"t2_7dyqbygn","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"If you could add qwen 2.5 72b for reference, that would be very helpful. Good job anyway!","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n314gbf","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you could add qwen 2.5 72b for reference, that would be very helpful. Good job anyway!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyxf1f","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/n314gbf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752473150,"author_flair_text":null,"treatment_tags":[],"created_utc":1752473150,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ycatu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"blackwell_tart","can_mod_post":false,"created_utc":1752436528,"send_replies":true,"parent_id":"t1_n2xfial","score":3,"author_fullname":"t2_1t7r9dkpud","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Great observation. Benchmarks for the dense Qwen3 models would be illuminating and something to that effect will be added to the results very soon.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ycatu","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Great observation. Benchmarks for the dense Qwen3 models would be illuminating and something to that effect will be added to the results very soon.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyxf1f","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/n2ycatu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752436528,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n30c1ly","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"blackwell_tart","can_mod_post":false,"created_utc":1752460430,"send_replies":true,"parent_id":"t1_n2xfial","score":2,"author_fullname":"t2_1t7r9dkpud","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Dense model results have now been added to the MoE results.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n30c1ly","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Dense model results have now been added to the MoE results.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyxf1f","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/n30c1ly/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752460430,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2xfial","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Substantial-Ebb-584","can_mod_post":false,"created_utc":1752426827,"send_replies":true,"parent_id":"t3_1lyxf1f","score":4,"author_fullname":"t2_7dyqbygn","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thank you for the benchmarks! \\nThe one thing I noticed is how Moe models are bad at content size scaling, compared to dense models. I don't say it's bad per se. But like in model size equivalent. Those benchmarks just showed that in a very nice scale. \\nFrom one side they're faster, but the bigger the content the more it kills the purpose of moe as local LLM.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2xfial","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you for the benchmarks! \\nThe one thing I noticed is how Moe models are bad at content size scaling, compared to dense models. I don&amp;#39;t say it&amp;#39;s bad per se. But like in model size equivalent. Those benchmarks just showed that in a very nice scale. \\nFrom one side they&amp;#39;re faster, but the bigger the content the more it kills the purpose of moe as local LLM.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/n2xfial/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752426827,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyxf1f","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ymy44","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"blackwell_tart","can_mod_post":false,"created_utc":1752439668,"send_replies":true,"parent_id":"t1_n2y7rk1","score":2,"author_fullname":"t2_1t7r9dkpud","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Apologies, we have no plans to test GGUFs at present.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ymy44","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Apologies, we have no plans to test GGUFs at present.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyxf1f","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/n2ymy44/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752439668,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2y7rk1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Necessary_Bunch_4019","can_mod_post":false,"created_utc":1752435179,"send_replies":true,"parent_id":"t3_1lyxf1f","score":3,"author_fullname":"t2_1g8vkju3w3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Very good. But... Can you try DeepSeek R1 Unsloth?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2y7rk1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Very good. But... Can you try DeepSeek R1 Unsloth?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/n2y7rk1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752435179,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyxf1f","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n32vbmz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"blackwell_tart","can_mod_post":false,"send_replies":true,"parent_id":"t1_n32u1le","score":1,"author_fullname":"t2_1t7r9dkpud","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"To quote the young ‘uns cultural references: this is the way.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n32vbmz","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;To quote the young ‘uns cultural references: this is the way.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyxf1f","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/n32vbmz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752502471,"author_flair_text":null,"treatment_tags":[],"created_utc":1752502471,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n32u1le","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DAlmighty","can_mod_post":false,"send_replies":true,"parent_id":"t1_n30h93u","score":1,"author_fullname":"t2_a04uj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I’ve tried that method but it still isn’t working for me. I think a refreshed install of Ubuntu is in my future.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n32u1le","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I’ve tried that method but it still isn’t working for me. I think a refreshed install of Ubuntu is in my future.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyxf1f","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/n32u1le/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752502084,"author_flair_text":null,"treatment_tags":[],"created_utc":1752502084,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n30h93u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"blackwell_tart","can_mod_post":false,"created_utc":1752462439,"send_replies":true,"parent_id":"t1_n30ej5p","score":2,"author_fullname":"t2_1t7r9dkpud","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The instructions in the original post should work quite well, assuming you are running Ubuntu Linux.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n30h93u","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The instructions in the original post should work quite well, assuming you are running Ubuntu Linux.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyxf1f","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/n30h93u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752462439,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n30hex5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Traclo","can_mod_post":false,"created_utc":1752462503,"send_replies":true,"parent_id":"t1_n30ej5p","score":2,"author_fullname":"t2_5e3f8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"They added sm120 a couple of weeks ago, so the latest release should work with it out of the box.  Building from source definitely works without modification at least!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n30hex5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They added sm120 a couple of weeks ago, so the latest release should work with it out of the box.  Building from source definitely works without modification at least!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyxf1f","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/n30hex5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752462503,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n30ej5p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DAlmighty","can_mod_post":false,"created_utc":1752461375,"send_replies":true,"parent_id":"t3_1lyxf1f","score":3,"author_fullname":"t2_a04uj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Congrats on even getting vLLM to run on the pro 6000. That’s a feat I haven’t been about to accomplish yet.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n30ej5p","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Congrats on even getting vLLM to run on the pro 6000. That’s a feat I haven’t been about to accomplish yet.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/n30ej5p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752461375,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyxf1f","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ymigm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"blackwell_tart","can_mod_post":false,"created_utc":1752439541,"send_replies":true,"parent_id":"t1_n2xfoql","score":2,"author_fullname":"t2_1t7r9dkpud","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You are welcome.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ymigm","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You are welcome.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyxf1f","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/n2ymigm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752439541,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2xfoql","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Sea-Rope-31","can_mod_post":false,"created_utc":1752426880,"send_replies":true,"parent_id":"t3_1lyxf1f","score":2,"author_fullname":"t2_1sdssbj1gj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Cool! Thanks for sharing!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2xfoql","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Cool! Thanks for sharing!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/n2xfoql/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752426880,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyxf1f","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3275vq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DepthHour1669","can_mod_post":false,"created_utc":1752494041,"send_replies":true,"parent_id":"t1_n325m4z","score":2,"author_fullname":"t2_t6glzswk","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Correct. It’s a consequence of the hardware design of GPUs. Inference is much more efficient when batch processing. If you only have one small user query in the batch, then most compute cores are idle and can’t take advantage of the required memory bandwidth going through all the params.\\n\\nWhen you process one small query, you’re still moving a lot of data (all the parameters of the model). So the GPU becomes memory bandwidth-limited, not compute-limited.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n3275vq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Correct. It’s a consequence of the hardware design of GPUs. Inference is much more efficient when batch processing. If you only have one small user query in the batch, then most compute cores are idle and can’t take advantage of the required memory bandwidth going through all the params.&lt;/p&gt;\\n\\n&lt;p&gt;When you process one small query, you’re still moving a lot of data (all the parameters of the model). So the GPU becomes memory bandwidth-limited, not compute-limited.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lyxf1f","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/n3275vq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752494041,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n325m4z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Impossible_Art9151","can_mod_post":false,"send_replies":true,"parent_id":"t1_n322g2h","score":1,"author_fullname":"t2_1chawnfp64","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Okay - thanks again.   \\nThis means, the overall processing of one user keeps some hardwareressources unused. These ressources can be used otherwise?  \\nI am really deep into efficency, hardware usage, paralellism vs sequential workloads. But I am still learning - the whole GPU world has different aspects than its CPU counterpart.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n325m4z","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Okay - thanks again.&lt;br/&gt;\\nThis means, the overall processing of one user keeps some hardwareressources unused. These ressources can be used otherwise?&lt;br/&gt;\\nI am really deep into efficency, hardware usage, paralellism vs sequential workloads. But I am still learning - the whole GPU world has different aspects than its CPU counterpart.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lyxf1f","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/n325m4z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752493392,"author_flair_text":null,"treatment_tags":[],"created_utc":1752493392,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n322g2h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DepthHour1669","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3210g1","score":2,"author_fullname":"t2_t6glzswk","approved_by":null,"mod_note":null,"all_awardings":[],"body":"No, the idea is that below the token limit, it takes the same amount of time to process 1 user or 10 users. \\n\\nSequential would be slower.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n322g2h","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No, the idea is that below the token limit, it takes the same amount of time to process 1 user or 10 users. &lt;/p&gt;\\n\\n&lt;p&gt;Sequential would be slower.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lyxf1f","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/n322g2h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752492002,"author_flair_text":null,"treatment_tags":[],"created_utc":1752492002,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3210g1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Impossible_Art9151","can_mod_post":false,"send_replies":true,"parent_id":"t1_n30g97o","score":1,"author_fullname":"t2_1chawnfp64","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks for your input. Indeed the actual ollama is broken regarding multiuser usage. Two request are really killing the performance.  \\nNext ollama release will get a bug fix, parallel setting = 1 as standard.  \\nlink here: [https://github.com/ollama/ollama/releases/tag/v0.9.7-rc0](https://github.com/ollama/ollama/releases/tag/v0.9.7-rc0)\\n\\nIn my case a sequential processing is sufficient. Apart from heavy commercial systems I wonder about parallel processing anyway. I cannot see any big user advantage from this.\\n\\nOverall user experience will suffer because any multiuser processing will be slower than sequential proceesing (one GPU and Amdahls Law given).\\n\\nI will give ollama alternatives a try.","edited":false,"author_flair_css_class":null,"name":"t1_n3210g1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for your input. Indeed the actual ollama is broken regarding multiuser usage. Two request are really killing the performance.&lt;br/&gt;\\nNext ollama release will get a bug fix, parallel setting = 1 as standard.&lt;br/&gt;\\nlink here: &lt;a href=\\"https://github.com/ollama/ollama/releases/tag/v0.9.7-rc0\\"&gt;https://github.com/ollama/ollama/releases/tag/v0.9.7-rc0&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;In my case a sequential processing is sufficient. Apart from heavy commercial systems I wonder about parallel processing anyway. I cannot see any big user advantage from this.&lt;/p&gt;\\n\\n&lt;p&gt;Overall user experience will suffer because any multiuser processing will be slower than sequential proceesing (one GPU and Amdahls Law given).&lt;/p&gt;\\n\\n&lt;p&gt;I will give ollama alternatives a try.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lyxf1f","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/n3210g1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752491343,"author_flair_text":null,"collapsed":false,"created_utc":1752491343,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n30g97o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DepthHour1669","can_mod_post":false,"send_replies":true,"parent_id":"t1_n30fel1","score":2,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"VLLM batches parallel inference very well, plus features like chunked attention. So 1 user may get 60 token/sec, 2 users each get 55tok/sec each, and 3 users get 50tok/sec each, etc (up to a point). Whereas Ollama will serve 3 users at 20tok/sec.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n30g97o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;VLLM batches parallel inference very well, plus features like chunked attention. So 1 user may get 60 token/sec, 2 users each get 55tok/sec each, and 3 users get 50tok/sec each, etc (up to a point). Whereas Ollama will serve 3 users at 20tok/sec.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyxf1f","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/n30g97o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752462047,"author_flair_text":null,"treatment_tags":[],"created_utc":1752462047,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n30fel1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"night0x63","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2yhiho","score":1,"author_fullname":"t2_3h2irqtz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Why? I've heard this before... But always without reasons.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n30fel1","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why? I&amp;#39;ve heard this before... But always without reasons.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyxf1f","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/n30fel1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752461712,"author_flair_text":null,"treatment_tags":[],"created_utc":1752461712,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2yhiho","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DepthHour1669","can_mod_post":false,"created_utc":1752438067,"send_replies":true,"parent_id":"t1_n2y96zy","score":6,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"2. VLLM is a lot better than ollama for actual production workloads","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2yhiho","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;ol&gt;\\n&lt;li&gt;VLLM is a lot better than ollama for actual production workloads&lt;/li&gt;\\n&lt;/ol&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyxf1f","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/n2yhiho/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752438067,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2z1w2u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Lissanro","can_mod_post":false,"created_utc":1752444284,"send_replies":true,"parent_id":"t1_n2y96zy","score":2,"author_fullname":"t2_fpfao9g","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ollama is not efficient for GPU+CPU inference. For single user requests, ik\\\\_llama.cpp is the best, at least 2-3 times faster compared to llama.cpp (which Ollama is based on). For multiple users, vllm is probably better though.\\n\\nMultiple GPUs, depending on how they are used, either maintain about the same speed as a single GPU would, or even bring huge speed up if used with tensor parallelism enabled.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2z1w2u","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ollama is not efficient for GPU+CPU inference. For single user requests, ik_llama.cpp is the best, at least 2-3 times faster compared to llama.cpp (which Ollama is based on). For multiple users, vllm is probably better though.&lt;/p&gt;\\n\\n&lt;p&gt;Multiple GPUs, depending on how they are used, either maintain about the same speed as a single GPU would, or even bring huge speed up if used with tensor parallelism enabled.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyxf1f","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/n2z1w2u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752444284,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n30gdqf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"night0x63","can_mod_post":false,"created_utc":1752462097,"send_replies":true,"parent_id":"t1_n2y96zy","score":1,"author_fullname":"t2_3h2irqtz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"(Side north about lots of system memory:\\n  \\nAbout 3 months ago I specced out an AI server. And did Les CPU and memory. So like 400 GB memory. \\n\\n\\nNow with qwen 235b and and mixtral 176b and llama4 200b. And llama4 behemoth 2T... All are MOE... All split vram and CPU.\\n\\n\\nIf you want to run Kimi/moonshot or llama 4 behemoth... You need 1-2 TB memory.)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n30gdqf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;(Side north about lots of system memory:\\n  \\nAbout 3 months ago I specced out an AI server. And did Les CPU and memory. So like 400 GB memory. &lt;/p&gt;\\n\\n&lt;p&gt;Now with qwen 235b and and mixtral 176b and llama4 200b. And llama4 behemoth 2T... All are MOE... All split vram and CPU.&lt;/p&gt;\\n\\n&lt;p&gt;If you want to run Kimi/moonshot or llama 4 behemoth... You need 1-2 TB memory.)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyxf1f","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/n30gdqf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752462097,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2y96zy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Impossible_Art9151","can_mod_post":false,"created_utc":1752435606,"send_replies":true,"parent_id":"t3_1lyxf1f","score":2,"author_fullname":"t2_1chawnfp64","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"thanks for your reports!   \\nYou have built a setup that I am considering myself.  \\nCan I ask you a few details please.  \\n1) Your server just has 512GB RAM. Why didn't you go with 1TB or 2TB?  \\nCPU RAM isn't that expensive compared to VRAM.   \\nMy considerations go like: With 2TB I can load a deepseek, a qwen3:235 and a few more into memory preventing cold starts.  \\n2 x rtx6000 pro is high-end prosumer and I would aim for running the big models under it without heavy degradation from low quants.  \\nThis is no critic! I am just curious about your thoughts and your use case.\\n\\n2) You are using vllm. Does it have any advantage over ollama, that I am using right now? Especially anything regarding your specific setup?\\n\\n3) How do the MOE models scale with 2 x GPU? I would expect a qwen3:235b in q4  should run completely from GPU since 2 x 96GB = 192GB VRAM &lt;&lt; 144GB for qwen plus context. Does qwen run GPU only?   \\nSince I am using a nvida A6000 /48GB ollama shows me for the 235b 70% CPU/30%GPU. That means I am loosing &gt; 70% in speed due to VRAM limitations.  \\n  \\nCan you specify the loss from 2 x GPU versus a single GPU with 192GB? There must be some losses due to overhead and latency.\\n\\n4) How much did you pay for your 2 x rtx 6000 hardware overall?\\n\\n5) Last but not least: Who happend to the banana? is she save?\\n\\nthx in advance","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2y96zy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;thanks for your reports!&lt;br/&gt;\\nYou have built a setup that I am considering myself.&lt;br/&gt;\\nCan I ask you a few details please.&lt;br/&gt;\\n1) Your server just has 512GB RAM. Why didn&amp;#39;t you go with 1TB or 2TB?&lt;br/&gt;\\nCPU RAM isn&amp;#39;t that expensive compared to VRAM.&lt;br/&gt;\\nMy considerations go like: With 2TB I can load a deepseek, a qwen3:235 and a few more into memory preventing cold starts.&lt;br/&gt;\\n2 x rtx6000 pro is high-end prosumer and I would aim for running the big models under it without heavy degradation from low quants.&lt;br/&gt;\\nThis is no critic! I am just curious about your thoughts and your use case.&lt;/p&gt;\\n\\n&lt;p&gt;2) You are using vllm. Does it have any advantage over ollama, that I am using right now? Especially anything regarding your specific setup?&lt;/p&gt;\\n\\n&lt;p&gt;3) How do the MOE models scale with 2 x GPU? I would expect a qwen3:235b in q4  should run completely from GPU since 2 x 96GB = 192GB VRAM &amp;lt;&amp;lt; 144GB for qwen plus context. Does qwen run GPU only?&lt;br/&gt;\\nSince I am using a nvida A6000 /48GB ollama shows me for the 235b 70% CPU/30%GPU. That means I am loosing &amp;gt; 70% in speed due to VRAM limitations.  &lt;/p&gt;\\n\\n&lt;p&gt;Can you specify the loss from 2 x GPU versus a single GPU with 192GB? There must be some losses due to overhead and latency.&lt;/p&gt;\\n\\n&lt;p&gt;4) How much did you pay for your 2 x rtx 6000 hardware overall?&lt;/p&gt;\\n\\n&lt;p&gt;5) Last but not least: Who happend to the banana? is she save?&lt;/p&gt;\\n\\n&lt;p&gt;thx in advance&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/n2y96zy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752435606,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyxf1f","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n337u0a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Direct_Turn_1484","can_mod_post":false,"created_utc":1752506106,"send_replies":true,"parent_id":"t3_1lyxf1f","score":2,"author_fullname":"t2_6ywe9a9n5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thanks for this.\\n\\nI sure wish I could build a machine like this for somewhere closer to $5k. Happy for those that can build and play with such a nice rig.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n337u0a","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for this.&lt;/p&gt;\\n\\n&lt;p&gt;I sure wish I could build a machine like this for somewhere closer to $5k. Happy for those that can build and play with such a nice rig.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/n337u0a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752506106,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyxf1f","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n349slh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"blackwell_tart","can_mod_post":false,"created_utc":1752516653,"send_replies":true,"parent_id":"t3_1lyxf1f","score":2,"author_fullname":"t2_1t7r9dkpud","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"A small update: after following the instructions provided by Daniel (https://old.reddit.com/r/LocalLLaMA/comments/1lzps3b/kimi_k2_18bit_unsloth_dynamic_ggufs/) we were able to hit 20 tokens/sec chatting with the Kimi K2 UD_Q4_K_XL GGUF quite easily.","edited":1752516967,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n349slh","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;A small update: after following the instructions provided by Daniel (&lt;a href=\\"https://old.reddit.com/r/LocalLLaMA/comments/1lzps3b/kimi_k2_18bit_unsloth_dynamic_ggufs/\\"&gt;https://old.reddit.com/r/LocalLLaMA/comments/1lzps3b/kimi_k2_18bit_unsloth_dynamic_ggufs/&lt;/a&gt;) we were able to hit 20 tokens/sec chatting with the Kimi K2 UD_Q4_K_XL GGUF quite easily.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/n349slh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752516653,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyxf1f","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n31jxox","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"notwhobutwhat","can_mod_post":false,"send_replies":true,"parent_id":"t1_n313s5p","score":1,"author_fullname":"t2_hbflt","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This one is interesting, and I wonder if it's due to how the experts are distributed with -tp enabled by default in VLLM. From what I gather, if you're activating only experts on a single GPU (no idea how likely this might be), this might explain it.\\n\\nI'm running the 30B-A3B AWQ quant, and I did notice on boot that it disables MoE distribution across GPUs due to the quant I'm using, perhaps GPTQ might allow it?","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n31jxox","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This one is interesting, and I wonder if it&amp;#39;s due to how the experts are distributed with -tp enabled by default in VLLM. From what I gather, if you&amp;#39;re activating only experts on a single GPU (no idea how likely this might be), this might explain it.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m running the 30B-A3B AWQ quant, and I did notice on boot that it disables MoE distribution across GPUs due to the quant I&amp;#39;m using, perhaps GPTQ might allow it?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lyxf1f","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/n31jxox/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752481961,"author_flair_text":null,"treatment_tags":[],"created_utc":1752481961,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n313s5p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"blackwell_tart","can_mod_post":false,"send_replies":true,"parent_id":"t1_n30scdy","score":2,"author_fullname":"t2_1t7r9dkpud","approved_by":null,"mod_note":null,"all_awardings":[],"body":"It would seem that manners are indeed alive and well in the world, for which I am thankful. I retract my ire and apologize for being a curmudgeon.\\n\\nQwen3 235B A22B GPTQ Int4 runs at 75 tokens/second for the first thousand tokens or so, but starts to drop off quickly after that. \\n\\nQwen3 30B A3B GPTQ Int4 runs at 151 tokens/second in tensor parallel across both GPUs.\\n\\nInterestingly, 30B A3 Int4 also runs at 151 tokens/second on a single GPU.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n313s5p","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It would seem that manners are indeed alive and well in the world, for which I am thankful. I retract my ire and apologize for being a curmudgeon.&lt;/p&gt;\\n\\n&lt;p&gt;Qwen3 235B A22B GPTQ Int4 runs at 75 tokens/second for the first thousand tokens or so, but starts to drop off quickly after that. &lt;/p&gt;\\n\\n&lt;p&gt;Qwen3 30B A3B GPTQ Int4 runs at 151 tokens/second in tensor parallel across both GPUs.&lt;/p&gt;\\n\\n&lt;p&gt;Interestingly, 30B A3 Int4 also runs at 151 tokens/second on a single GPU.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lyxf1f","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/n313s5p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752472795,"author_flair_text":null,"treatment_tags":[],"created_utc":1752472795,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n30scdy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Expensive-Apricot-25","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2zbmq5","score":3,"author_fullname":"t2_idqkwio0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You're right, and I apologize.\\n\\nI didn't expect you to read/reply, and I was just frustrated because most real benchmarks posted here are throughput, and not single request speeds, which aren't as relevant to most people here. But thats no excuse for the poor manners, which I again apologize again for.\\n\\nI typed it very late at night on my phone, hence the poor grammar.\\n\\nI didn't mean to ask you to run more benchmarks, just if you had done it already.\\n\\nI don't mean to ask anything of you, I just don't want to leave things on a bad foot. Have fun with your new cards!","edited":false,"author_flair_css_class":null,"name":"t1_n30scdy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You&amp;#39;re right, and I apologize.&lt;/p&gt;\\n\\n&lt;p&gt;I didn&amp;#39;t expect you to read/reply, and I was just frustrated because most real benchmarks posted here are throughput, and not single request speeds, which aren&amp;#39;t as relevant to most people here. But thats no excuse for the poor manners, which I again apologize again for.&lt;/p&gt;\\n\\n&lt;p&gt;I typed it very late at night on my phone, hence the poor grammar.&lt;/p&gt;\\n\\n&lt;p&gt;I didn&amp;#39;t mean to ask you to run more benchmarks, just if you had done it already.&lt;/p&gt;\\n\\n&lt;p&gt;I don&amp;#39;t mean to ask anything of you, I just don&amp;#39;t want to leave things on a bad foot. Have fun with your new cards!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lyxf1f","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/n30scdy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752467182,"author_flair_text":null,"collapsed":false,"created_utc":1752467182,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2zbmq5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"blackwell_tart","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2zaa6l","score":2,"author_fullname":"t2_1t7r9dkpud","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Capitalization, punctuation and good manners cost nothing, unlike benchmarking models on your behalf, which costs time. Time is precious when one is old.\\n\\nNo, sir. You did not take the time to be polite and I have no mind to take the time to entertain your frippery. \\n\\nGood day.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2zbmq5","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Capitalization, punctuation and good manners cost nothing, unlike benchmarking models on your behalf, which costs time. Time is precious when one is old.&lt;/p&gt;\\n\\n&lt;p&gt;No, sir. You did not take the time to be polite and I have no mind to take the time to entertain your frippery. &lt;/p&gt;\\n\\n&lt;p&gt;Good day.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyxf1f","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/n2zbmq5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752447555,"author_flair_text":null,"treatment_tags":[],"created_utc":1752447555,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2zaa6l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Expensive-Apricot-25","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2ycnrj","score":3,"author_fullname":"t2_idqkwio0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"single request speeds.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2zaa6l","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;single request speeds.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyxf1f","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/n2zaa6l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752447106,"author_flair_text":null,"treatment_tags":[],"created_utc":1752447106,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2zapdx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Expensive-Apricot-25","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2z22m2","score":3,"author_fullname":"t2_idqkwio0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"no you are correct, generally speaking throughput is only useful if you are serving for a large group of people, like in the hundereds where concurrent requests are actually common.\\n\\nbut for the vast majority of people here its pretty irrelevant, so single request speeds are a more useful metric.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2zapdx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;no you are correct, generally speaking throughput is only useful if you are serving for a large group of people, like in the hundereds where concurrent requests are actually common.&lt;/p&gt;\\n\\n&lt;p&gt;but for the vast majority of people here its pretty irrelevant, so single request speeds are a more useful metric.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyxf1f","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/n2zapdx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752447246,"author_flair_text":null,"treatment_tags":[],"created_utc":1752447246,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n30hd4s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"blackwell_tart","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2z22m2","score":2,"author_fullname":"t2_1t7r9dkpud","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"75 tokens/second with Qwen3 235B A22B GPTQ Int4.\\n\\nhttps://i.imgur.com/5vGk4Qs.png","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n30hd4s","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;75 tokens/second with Qwen3 235B A22B GPTQ Int4.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://i.imgur.com/5vGk4Qs.png\\"&gt;https://i.imgur.com/5vGk4Qs.png&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyxf1f","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/n30hd4s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752462482,"author_flair_text":null,"treatment_tags":[],"created_utc":1752462482,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2z22m2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"sautdepage","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2ycnrj","score":2,"author_fullname":"t2_6qdom3n2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Throughput measures the maximum number of tok/sec achievable when processing parallel requests, is that right? I might be wrong - let me know.\\n\\nBut if so, that doesn't reveal the experience a single user gets, ie in agentic tasks. So sequential generation speed would be useful too.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2z22m2","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Throughput measures the maximum number of tok/sec achievable when processing parallel requests, is that right? I might be wrong - let me know.&lt;/p&gt;\\n\\n&lt;p&gt;But if so, that doesn&amp;#39;t reveal the experience a single user gets, ie in agentic tasks. So sequential generation speed would be useful too.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyxf1f","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/n2z22m2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752444344,"author_flair_text":null,"treatment_tags":[],"created_utc":1752444344,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2yv5fp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"jarec707","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2ycnrj","score":2,"author_fullname":"t2_mjsmz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ha ha good reply","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2yv5fp","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ha ha good reply&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyxf1f","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/n2yv5fp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752442122,"author_flair_text":null,"treatment_tags":[],"created_utc":1752442122,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ycnrj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"blackwell_tart","can_mod_post":false,"created_utc":1752436634,"send_replies":true,"parent_id":"t1_n2y8xfj","score":1,"author_fullname":"t2_1t7r9dkpud","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"No. Do you have any questions that explain what it is you wish to know?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ycnrj","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No. Do you have any questions that explain what it is you wish to know?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyxf1f","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/n2ycnrj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752436634,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2y8xfj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Expensive-Apricot-25","can_mod_post":false,"created_utc":1752435528,"send_replies":true,"parent_id":"t3_1lyxf1f","score":3,"author_fullname":"t2_idqkwio0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Man… do u have any Benchmarks that aren’t throughput?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2y8xfj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Man… do u have any Benchmarks that aren’t throughput?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyxf1f/benchmarking_qwen3_30b_and_235b_on_dual_rtx_pro/n2y8xfj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752435528,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyxf1f","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}}]`),r=()=>e.jsx(l,{data:t});export{r as default};
