import{j as e}from"./index-Cd3v0jxz.js";import{R as t}from"./RedditPostRenderer-cI96YLhy.js";import"./index-DGBoKyQm.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I am wondering if anyone did this before, at least I couldn't find information on it. I want to fine tune a coding model without changing the whole model (for hardware restriction reasons). Loras, in theory, would do that. But how? For image and video generation this is pretty much solved and common, but llms? ","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"How do you make Loras for Qwen coder /  devstral?","link_flair_richtext":[{"e":"text","t":"Other"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lyl697","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.84,"author_flair_background_color":null,"subreddit_type":"public","ups":8,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_iaby02kl","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Other","can_mod_post":false,"score":8,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752385140,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I am wondering if anyone did this before, at least I couldn&amp;#39;t find information on it. I want to fine tune a coding model without changing the whole model (for hardware restriction reasons). Loras, in theory, would do that. But how? For image and video generation this is pretty much solved and common, but llms? &lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"7a7848d2-bf8e-11ed-8c2f-765d15199f78","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#94e044","id":"1lyl697","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"ComprehensiveBird317","discussion_type":null,"num_comments":6,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lyl697/how_do_you_make_loras_for_qwen_coder_devstral/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lyl697/how_do_you_make_loras_for_qwen_coder_devstral/","subreddit_subscribers":498344,"created_utc":1752385140,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2wrxyg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Double_Cause4609","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2usm1t","score":1,"author_fullname":"t2_1kubzxt2ww","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"ChatML isn't generally a format that your data is stored in. Generally it's stored in jsonl in a multi-turn dialogue (unless it's been pretemplated which is weird). Typically you take the jsonl format, give it to the trainer, tell it what format you want, and it templates it for you. ChatML is nice because it's easy and well supported.\\n\\nI'm pretty sure all the training frameworks handle jsonl gracefully, and it's the standard, so that doesn't really narrow down your choices, and my notes about them stand.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2wrxyg","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;ChatML isn&amp;#39;t generally a format that your data is stored in. Generally it&amp;#39;s stored in jsonl in a multi-turn dialogue (unless it&amp;#39;s been pretemplated which is weird). Typically you take the jsonl format, give it to the trainer, tell it what format you want, and it templates it for you. ChatML is nice because it&amp;#39;s easy and well supported.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m pretty sure all the training frameworks handle jsonl gracefully, and it&amp;#39;s the standard, so that doesn&amp;#39;t really narrow down your choices, and my notes about them stand.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyl697","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyl697/how_do_you_make_loras_for_qwen_coder_devstral/n2wrxyg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752419742,"author_flair_text":null,"treatment_tags":[],"created_utc":1752419742,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2usm1t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ComprehensiveBird317","can_mod_post":false,"created_utc":1752386684,"send_replies":true,"parent_id":"t1_n2us3pq","score":1,"author_fullname":"t2_iaby02kl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thank you for this in depth answer! What would you recommend if I have the training data in chatml JsonL format? I am looking for adding a chat style to the model, not auto completion / FIM","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2usm1t","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you for this in depth answer! What would you recommend if I have the training data in chatml JsonL format? I am looking for adding a chat style to the model, not auto completion / FIM&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyl697","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyl697/how_do_you_make_loras_for_qwen_coder_devstral/n2usm1t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752386684,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2v7emt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"muxxington","can_mod_post":false,"created_utc":1752395068,"send_replies":true,"parent_id":"t1_n2us3pq","score":1,"author_fullname":"t2_1ktdmsvo","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thank you, perfect nutshell.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2v7emt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you, perfect nutshell.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyl697","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyl697/how_do_you_make_loras_for_qwen_coder_devstral/n2v7emt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752395068,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2us3pq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Double_Cause4609","can_mod_post":false,"created_utc":1752386414,"send_replies":true,"parent_id":"t3_1lyl697","score":10,"author_fullname":"t2_1kubzxt2ww","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Hundreds of thousands of people make LoRAs on a pretty regular basis in LLMs. This is a pretty common and \\"solved\\" problem, it's just we don't make a strong distinction between fine tunes and LoRAs as in the image generation space.\\n\\nIn fact, LoRAs in text to image work more like soft prompts in LLMs; it's more a way of invoking a specific concept than a general adaption.\\n\\nAnyway, you can pick your poison. Unsloth, Axolotl, Llama-Factory, Torchtune, or you can roll your own training code in Pytorch and Huggingface Transformers.\\n\\nMost of your time will be spent managing memory, because LLM fine tuning is more expensive than image generation (sort of) and there's a lot of steps like importing kernels, adjusting gradient checkpointing, setting custom settings to save memory, etc etc.\\n\\nFor an integrated solution: Unsloth's pretty nice if what you want to do matches up with what they have existing notebooks for.\\n\\nAxolotl is nice because they support a lot of popular features in the community.\\n\\nLlama-Factory is nice because they support a lot of cutting edge third party research and optimizers, and they have a really nice interface both in CLI and GUI.\\n\\nTorchtune is nice because they rolled their own model definitions and custom implementations of all features using only vanilla Pytorch (and special features from TorchAO), so they're in a weird spot of not having all of the gimicky unproven techniques cooked up in single author papers, while having the cutting edge for stable, well supported features. It also just works.\\n\\nUnsloth sucks because dependencies are a nightmare and they frame their project in a way as to be accessible, so it's only easy to do what they specifically set out on the table for you.\\n\\nAxolotl sucks because its dependencies are somehow even less stable and there can be asymmetric support for features.\\n\\nLlama-Factory sucks because there can be even more asymmetric support documented only in Chinese.\\n\\nTorchtune sucks because not all popular models are supported, and they have a strong focus on stable features, not necessarily all features common in the community. They do provide an easy interface to implement those yourself if you need them, but that is predicated on being able to code.\\n\\n\\n\\nPersonally, I'm a bit biased towards Torchtune or implementing a raw training loop myself, but everyone has their preferences.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2us3pq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hundreds of thousands of people make LoRAs on a pretty regular basis in LLMs. This is a pretty common and &amp;quot;solved&amp;quot; problem, it&amp;#39;s just we don&amp;#39;t make a strong distinction between fine tunes and LoRAs as in the image generation space.&lt;/p&gt;\\n\\n&lt;p&gt;In fact, LoRAs in text to image work more like soft prompts in LLMs; it&amp;#39;s more a way of invoking a specific concept than a general adaption.&lt;/p&gt;\\n\\n&lt;p&gt;Anyway, you can pick your poison. Unsloth, Axolotl, Llama-Factory, Torchtune, or you can roll your own training code in Pytorch and Huggingface Transformers.&lt;/p&gt;\\n\\n&lt;p&gt;Most of your time will be spent managing memory, because LLM fine tuning is more expensive than image generation (sort of) and there&amp;#39;s a lot of steps like importing kernels, adjusting gradient checkpointing, setting custom settings to save memory, etc etc.&lt;/p&gt;\\n\\n&lt;p&gt;For an integrated solution: Unsloth&amp;#39;s pretty nice if what you want to do matches up with what they have existing notebooks for.&lt;/p&gt;\\n\\n&lt;p&gt;Axolotl is nice because they support a lot of popular features in the community.&lt;/p&gt;\\n\\n&lt;p&gt;Llama-Factory is nice because they support a lot of cutting edge third party research and optimizers, and they have a really nice interface both in CLI and GUI.&lt;/p&gt;\\n\\n&lt;p&gt;Torchtune is nice because they rolled their own model definitions and custom implementations of all features using only vanilla Pytorch (and special features from TorchAO), so they&amp;#39;re in a weird spot of not having all of the gimicky unproven techniques cooked up in single author papers, while having the cutting edge for stable, well supported features. It also just works.&lt;/p&gt;\\n\\n&lt;p&gt;Unsloth sucks because dependencies are a nightmare and they frame their project in a way as to be accessible, so it&amp;#39;s only easy to do what they specifically set out on the table for you.&lt;/p&gt;\\n\\n&lt;p&gt;Axolotl sucks because its dependencies are somehow even less stable and there can be asymmetric support for features.&lt;/p&gt;\\n\\n&lt;p&gt;Llama-Factory sucks because there can be even more asymmetric support documented only in Chinese.&lt;/p&gt;\\n\\n&lt;p&gt;Torchtune sucks because not all popular models are supported, and they have a strong focus on stable features, not necessarily all features common in the community. They do provide an easy interface to implement those yourself if you need them, but that is predicated on being able to code.&lt;/p&gt;\\n\\n&lt;p&gt;Personally, I&amp;#39;m a bit biased towards Torchtune or implementing a raw training loop myself, but everyone has their preferences.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyl697/how_do_you_make_loras_for_qwen_coder_devstral/n2us3pq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752386414,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyl697","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2uq7em","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"neph1010","can_mod_post":false,"created_utc":1752385424,"send_replies":true,"parent_id":"t3_1lyl697","score":2,"author_fullname":"t2_v2z4wgzc","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You can use [https://github.com/unslothai/unsloth](https://github.com/unslothai/unsloth) . Example with qwen coder: [https://huggingface.co/neph1/Qwen2.5-Coder-7B-Instruct-Unity](https://huggingface.co/neph1/Qwen2.5-Coder-7B-Instruct-Unity)\\n\\nFinetuning is straight forward, it's the dataset creation that takes time.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2uq7em","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can use &lt;a href=\\"https://github.com/unslothai/unsloth\\"&gt;https://github.com/unslothai/unsloth&lt;/a&gt; . Example with qwen coder: &lt;a href=\\"https://huggingface.co/neph1/Qwen2.5-Coder-7B-Instruct-Unity\\"&gt;https://huggingface.co/neph1/Qwen2.5-Coder-7B-Instruct-Unity&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Finetuning is straight forward, it&amp;#39;s the dataset creation that takes time.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyl697/how_do_you_make_loras_for_qwen_coder_devstral/n2uq7em/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752385424,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyl697","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2upvbb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SlowFail2433","can_mod_post":false,"created_utc":1752385251,"send_replies":true,"parent_id":"t3_1lyl697","score":1,"author_fullname":"t2_131eezppgs","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Nvidia Nemo library","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2upvbb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nvidia Nemo library&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyl697/how_do_you_make_loras_for_qwen_coder_devstral/n2upvbb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752385251,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyl697","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(t,{data:a});export{r as default};
