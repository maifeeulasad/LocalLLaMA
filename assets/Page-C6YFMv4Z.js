import{j as e}from"./index-CNyNkRpk.js";import{R as l}from"./RedditPostRenderer-Dza0u9i2.js";import"./index-BUchu_-K.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hey all,\\n\\nI'm building a fully air-gapped deployment that integrates with Elastic Security and Observability, including Elastic AI Assistant via OpenInference API. My use case involves log summarisation, alert triage, threat intel enrichment (using MISP), and knowledge base retrieval. About 5000 users, about 2000 servers. All on-prem.\\n\\nI've shortlisted Meta's LLaMA 4 Maverick 17B 128E Instruct model as a candidate for this setup. Reason is it is instruction-tuned, long-context, and MoE-optimised. It fits Elastic's model requirements . I'm planning to run it at full precision (BF16 or FP16) using vLLM or Ollama, but happy to adapt if others have better suggestions.\\n\\nI did look at [https://www.elastic.co/docs/solutions/security/ai/large-language-model-performance-matrix](https://www.elastic.co/docs/solutions/security/ai/large-language-model-performance-matrix) but it is somewhat out of date now.\\n\\nI have a pretty solid budget (though 3 A100s is probably the limit once the rest of the hardware is taken into account)\\n\\nLooking for help with:\\n\\n* Model feedback: Anyone using LLaMA 4 Maverick or other Elastic-supported models (like Mistral Instruct or LLaMA 3.1 Instruct)?\\n* Hardware: What server setup did you use? Any success with Dell XE7745, HPE GPU nodes, or DIY rigs with A100s/H100s?\\n* Fine-tuning: Anyone LoRA-fine-tuned Maverick or similar for log alerting, ECS fields, or threat context?\\n\\nI have some constraints:\\n\\n* Must be air-gapped\\n* I can't use Chinese, Israeli or similar products. CISO doesn't allow it. I know some of the Chinese models would be a good fit, but its a no-go.\\n* Need to support long-context summarisation, RAG-style enrichment, and Elastic Assistant prompt structure\\n\\nWould love to hear from anyone who’s done this in production or lab.\\n\\nThanks in advance!\\n\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Local LLM to back Elastic AI","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lyq22j","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.89,"author_flair_background_color":null,"subreddit_type":"public","ups":7,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_s8xklsb6","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":7,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1752404403,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m building a fully air-gapped deployment that integrates with Elastic Security and Observability, including Elastic AI Assistant via OpenInference API. My use case involves log summarisation, alert triage, threat intel enrichment (using MISP), and knowledge base retrieval. About 5000 users, about 2000 servers. All on-prem.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ve shortlisted Meta&amp;#39;s LLaMA 4 Maverick 17B 128E Instruct model as a candidate for this setup. Reason is it is instruction-tuned, long-context, and MoE-optimised. It fits Elastic&amp;#39;s model requirements . I&amp;#39;m planning to run it at full precision (BF16 or FP16) using vLLM or Ollama, but happy to adapt if others have better suggestions.&lt;/p&gt;\\n\\n&lt;p&gt;I did look at &lt;a href=\\"https://www.elastic.co/docs/solutions/security/ai/large-language-model-performance-matrix\\"&gt;https://www.elastic.co/docs/solutions/security/ai/large-language-model-performance-matrix&lt;/a&gt; but it is somewhat out of date now.&lt;/p&gt;\\n\\n&lt;p&gt;I have a pretty solid budget (though 3 A100s is probably the limit once the rest of the hardware is taken into account)&lt;/p&gt;\\n\\n&lt;p&gt;Looking for help with:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Model feedback: Anyone using LLaMA 4 Maverick or other Elastic-supported models (like Mistral Instruct or LLaMA 3.1 Instruct)?&lt;/li&gt;\\n&lt;li&gt;Hardware: What server setup did you use? Any success with Dell XE7745, HPE GPU nodes, or DIY rigs with A100s/H100s?&lt;/li&gt;\\n&lt;li&gt;Fine-tuning: Anyone LoRA-fine-tuned Maverick or similar for log alerting, ECS fields, or threat context?&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;I have some constraints:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Must be air-gapped&lt;/li&gt;\\n&lt;li&gt;I can&amp;#39;t use Chinese, Israeli or similar products. CISO doesn&amp;#39;t allow it. I know some of the Chinese models would be a good fit, but its a no-go.&lt;/li&gt;\\n&lt;li&gt;Need to support long-context summarisation, RAG-style enrichment, and Elastic Assistant prompt structure&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Would love to hear from anyone who’s done this in production or lab.&lt;/p&gt;\\n\\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/G2yA00beNF7t7h7F-Vm0zYQ1_GPWt2mKaYLvl77xrgc.png?auto=webp&amp;s=9a1b4684102bb8c94296cbfa71ad3a31d0c0f257","width":1920,"height":1080},"resolutions":[{"url":"https://external-preview.redd.it/G2yA00beNF7t7h7F-Vm0zYQ1_GPWt2mKaYLvl77xrgc.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8e7dcc983e13bd8aed1654a54d718d49f54cdaae","width":108,"height":60},{"url":"https://external-preview.redd.it/G2yA00beNF7t7h7F-Vm0zYQ1_GPWt2mKaYLvl77xrgc.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6db7b95b72b6ba957c849b5433b50158cc281a2e","width":216,"height":121},{"url":"https://external-preview.redd.it/G2yA00beNF7t7h7F-Vm0zYQ1_GPWt2mKaYLvl77xrgc.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=54ac1a61a829459657d4fbc629848f4a7b86377b","width":320,"height":180},{"url":"https://external-preview.redd.it/G2yA00beNF7t7h7F-Vm0zYQ1_GPWt2mKaYLvl77xrgc.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=19ae5d46a63ca2b66411c4548a6cff1870e42360","width":640,"height":360},{"url":"https://external-preview.redd.it/G2yA00beNF7t7h7F-Vm0zYQ1_GPWt2mKaYLvl77xrgc.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b5dbba47c48a53ff43f7f01db90cc69cd595f0e8","width":960,"height":540},{"url":"https://external-preview.redd.it/G2yA00beNF7t7h7F-Vm0zYQ1_GPWt2mKaYLvl77xrgc.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=994bf868b62aa1188f41e6e7f154a8365d35c7fe","width":1080,"height":607}],"variants":{},"id":"G2yA00beNF7t7h7F-Vm0zYQ1_GPWt2mKaYLvl77xrgc"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lyq22j","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"OldManCyberNinja","discussion_type":null,"num_comments":20,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lyq22j/local_llm_to_back_elastic_ai/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lyq22j/local_llm_to_back_elastic_ai/","subreddit_subscribers":498850,"created_utc":1752404403,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2yp1ar","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Mediocre-Method782","can_mod_post":false,"created_utc":1752440287,"send_replies":true,"parent_id":"t1_n2ynudc","score":1,"author_fullname":"t2_13904b0dor","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Right? But, where there is money, especially theoretically easy money, agendas to capture some of it will emerge, and reddit also has a big astroturf problem. Ironically, AI makes it too cheap and easy. I'm not offended at all by your post and kudos to your CISO for healthy paranoia.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n2yp1ar","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Right? But, where there is money, especially theoretically easy money, agendas to capture some of it will emerge, and reddit also has a big astroturf problem. Ironically, AI makes it too cheap and easy. I&amp;#39;m not offended at all by your post and kudos to your CISO for healthy paranoia.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lyq22j","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyq22j/local_llm_to_back_elastic_ai/n2yp1ar/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752440287,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ynudc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"OldManCyberNinja","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2xdiwf","score":2,"author_fullname":"t2_s8xklsb6","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I wish I made 50% of that. Sorry if I offended anyone, thought I might be able to verify what I have designed, what the consultants say will work. Reddit tends to have people pushing the boundaries of what is known in interesting ways.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n2ynudc","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I wish I made 50% of that. Sorry if I offended anyone, thought I might be able to verify what I have designed, what the consultants say will work. Reddit tends to have people pushing the boundaries of what is known in interesting ways.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lyq22j","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyq22j/local_llm_to_back_elastic_ai/n2ynudc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752439933,"author_flair_text":null,"treatment_tags":[],"created_utc":1752439933,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2xdiwf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Mediocre-Method782","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2xai5f","score":1,"author_fullname":"t2_13904b0dor","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah, everyone's new in this space and everyone wants that sweet sweet $500k salary to themselves. But did you look at their comment history to infer their organizational affiliations and the constraints that probably accompany them?","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n2xdiwf","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, everyone&amp;#39;s new in this space and everyone wants that sweet sweet $500k salary to themselves. But did you look at their comment history to infer their organizational affiliations and the constraints that probably accompany them?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lyq22j","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyq22j/local_llm_to_back_elastic_ai/n2xdiwf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752426248,"author_flair_text":null,"treatment_tags":[],"created_utc":1752426248,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2xai5f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ekaj","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2x9p0y","score":1,"author_fullname":"t2_3cajs","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"lmao, and so they come to reddit for advice with their 'failure is not an option' project?  \\nOP is blatanlty fishing for free consulting advice despite clearly having a budget and financial need for solid advice. Instead of hiring a professional, they go to reddit, and make a vague post about their requirements, and hope that they(reddit) will solve their 'failure is not an option' project.\\n\\nThis the kind of thing companies get avoided for. Build a 'secure' project by people who don't know/understand the technology, and instead of hiring a professional, seek out amateurs on reddit.","edited":false,"author_flair_css_class":null,"name":"t1_n2xai5f","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;lmao, and so they come to reddit for advice with their &amp;#39;failure is not an option&amp;#39; project?&lt;br/&gt;\\nOP is blatanlty fishing for free consulting advice despite clearly having a budget and financial need for solid advice. Instead of hiring a professional, they go to reddit, and make a vague post about their requirements, and hope that they(reddit) will solve their &amp;#39;failure is not an option&amp;#39; project.&lt;/p&gt;\\n\\n&lt;p&gt;This the kind of thing companies get avoided for. Build a &amp;#39;secure&amp;#39; project by people who don&amp;#39;t know/understand the technology, and instead of hiring a professional, seek out amateurs on reddit.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lyq22j","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyq22j/local_llm_to_back_elastic_ai/n2xai5f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752425360,"author_flair_text":"llama.cpp","collapsed":false,"created_utc":1752425360,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2x9p0y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Mediocre-Method782","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2wp1m0","score":2,"author_fullname":"t2_13904b0dor","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Or OP's in a line of endeavor where failure is *not* an option...?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2x9p0y","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Or OP&amp;#39;s in a line of endeavor where failure is &lt;em&gt;not&lt;/em&gt; an option...?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyq22j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyq22j/local_llm_to_back_elastic_ai/n2x9p0y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752425118,"author_flair_text":null,"treatment_tags":[],"created_utc":1752425118,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2yn7cr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"OldManCyberNinja","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2wp1m0","score":2,"author_fullname":"t2_s8xklsb6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Australian Government has outright banned DeepSeek from their devices, and a lot of companies in Australia follow the Australian Cyber Security Centre's lead on tech bans. I have several consultants actually, and Elastic gives a lot of support, but its always worth \\\\*talking\\\\* to others. I asked a fairly straightforward series of questions without anyone having to do any heavy lifting.\\n\\nSorry if this upsets you, and I hope you have a better day going forward.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2yn7cr","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Australian Government has outright banned DeepSeek from their devices, and a lot of companies in Australia follow the Australian Cyber Security Centre&amp;#39;s lead on tech bans. I have several consultants actually, and Elastic gives a lot of support, but its always worth *talking* to others. I asked a fairly straightforward series of questions without anyone having to do any heavy lifting.&lt;/p&gt;\\n\\n&lt;p&gt;Sorry if this upsets you, and I hope you have a better day going forward.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyq22j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyq22j/local_llm_to_back_elastic_ai/n2yn7cr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752439744,"author_flair_text":null,"treatment_tags":[],"created_utc":1752439744,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2wp1m0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ekaj","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2wh066","score":1,"author_fullname":"t2_3cajs","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That’s just a survey paper. Where’s an example from non academia/an actual occurrence?\\n\\nThis isn’t meant to be antagonistic but rather point out theoretical risks are just that, theoretical, until they’ve actually occurred.\\n\\nI’m not aware of any public models by any major lab being backdoored as that would be a big news event, let alone if one of the big Chinese labs did it.\\n\\nIt just sounds like this person doesn’t want to hire a consultant and has a paranoid/out of their depth CISO.","edited":1752419159,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2wp1m0","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That’s just a survey paper. Where’s an example from non academia/an actual occurrence?&lt;/p&gt;\\n\\n&lt;p&gt;This isn’t meant to be antagonistic but rather point out theoretical risks are just that, theoretical, until they’ve actually occurred.&lt;/p&gt;\\n\\n&lt;p&gt;I’m not aware of any public models by any major lab being backdoored as that would be a big news event, let alone if one of the big Chinese labs did it.&lt;/p&gt;\\n\\n&lt;p&gt;It just sounds like this person doesn’t want to hire a consultant and has a paranoid/out of their depth CISO.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyq22j","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyq22j/local_llm_to_back_elastic_ai/n2wp1m0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752418870,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752418870,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2wh066","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cartogram","can_mod_post":false,"created_utc":1752416388,"send_replies":true,"parent_id":"t1_n2w4c9o","score":3,"author_fullname":"t2_3j5et","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"backdoors. see [A Survey on Backdoor Threats in LLMs](https://arxiv.org/abs/2502.05224)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2wh066","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;backdoors. see &lt;a href=\\"https://arxiv.org/abs/2502.05224\\"&gt;A Survey on Backdoor Threats in LLMs&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"link_id":"t3_1lyq22j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyq22j/local_llm_to_back_elastic_ai/n2wh066/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752416388,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ym1qn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jklre","can_mod_post":false,"created_utc":1752439402,"send_replies":true,"parent_id":"t1_n2w4c9o","score":1,"author_fullname":"t2_5ut58","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"DOD","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ym1qn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;DOD&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyq22j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyq22j/local_llm_to_back_elastic_ai/n2ym1qn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752439402,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2w4c9o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"indicava","can_mod_post":false,"created_utc":1752411915,"send_replies":true,"parent_id":"t3_1lyq22j","score":2,"author_fullname":"t2_4dvff","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If it’s air gapped, what’s the risk of using a “foreign” open weights model?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2w4c9o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If it’s air gapped, what’s the risk of using a “foreign” open weights model?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyq22j/local_llm_to_back_elastic_ai/n2w4c9o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752411915,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyq22j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2yn8w6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jklre","can_mod_post":false,"created_utc":1752439756,"send_replies":true,"parent_id":"t3_1lyq22j","score":1,"author_fullname":"t2_5ut58","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I was working on a similar project for fun. This is right up my alley moving from Monitoring / observibility into LLM's. You could take a long context model and use RAG as a buffer. Maybe consider making a LoRA or a QLoRA to get better perfomrance out of your chosen model.  Mistral released its small 3.2 24b but that is still 128k context. Maybe use a multi agent framework like crewai, hfagents and I was going to play with agno today.  When it comes to these tasks zero shot LLM infrence is really weak but using multi agents with memory works WAY better.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2yn8w6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I was working on a similar project for fun. This is right up my alley moving from Monitoring / observibility into LLM&amp;#39;s. You could take a long context model and use RAG as a buffer. Maybe consider making a LoRA or a QLoRA to get better perfomrance out of your chosen model.  Mistral released its small 3.2 24b but that is still 128k context. Maybe use a multi agent framework like crewai, hfagents and I was going to play with agno today.  When it comes to these tasks zero shot LLM infrence is really weak but using multi agents with memory works WAY better.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyq22j/local_llm_to_back_elastic_ai/n2yn8w6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752439756,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyq22j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ylpl2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"OldManCyberNinja","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2wffki","score":1,"author_fullname":"t2_s8xklsb6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Appreciate the reply, I hadn't noticed this, and appreciate the heads up.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ylpl2","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Appreciate the reply, I hadn&amp;#39;t noticed this, and appreciate the heads up.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyq22j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyq22j/local_llm_to_back_elastic_ai/n2ylpl2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752439302,"author_flair_text":null,"treatment_tags":[],"created_utc":1752439302,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2wffki","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"TheApadayo","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2vvijc","score":5,"author_fullname":"t2_8xrdj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"FYI a lot of newer model releases have dropped the “-instruct” part from the name and instead release the fine tuned variant as the main model and now have a “-base” model variant because 99% of people want the instruct model, not the base model.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2wffki","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;FYI a lot of newer model releases have dropped the “-instruct” part from the name and instead release the fine tuned variant as the main model and now have a “-base” model variant because 99% of people want the instruct model, not the base model.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyq22j","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyq22j/local_llm_to_back_elastic_ai/n2wffki/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752415875,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752415875,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ylrrx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"OldManCyberNinja","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2vvu1j","score":2,"author_fullname":"t2_s8xklsb6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thank you kindly.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ylrrx","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you kindly.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyq22j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyq22j/local_llm_to_back_elastic_ai/n2ylrrx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752439321,"author_flair_text":null,"treatment_tags":[],"created_utc":1752439321,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2vvu1j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ICanSeeYou7867","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2vvijc","score":2,"author_fullname":"t2_sqvpr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Nemotron 235B is based on Llama 405B Instruct.\\n\\n\`\`\`\\nLlama-3.1-Nemotron-Ultra-253B-v1 is a large language model (LLM) which is a derivative of Meta Llama-3.1-405B-Instruct\\n\`\`\`\\n\\nOr the smaller models\\nhttps://huggingface.co/nvidia/Llama-3_1-Nemotron-51B-Instruct\\n\\nMistral Large is also an instruct\\nhttps://huggingface.co/mistralai/Mistral-Large-Instruct-2411","edited":1752408792,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2vvu1j","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nemotron 235B is based on Llama 405B Instruct.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;\\nLlama-3.1-Nemotron-Ultra-253B-v1 is a large language model (LLM) which is a derivative of Meta Llama-3.1-405B-Instruct\\n&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Or the smaller models\\n&lt;a href=\\"https://huggingface.co/nvidia/Llama-3_1-Nemotron-51B-Instruct\\"&gt;https://huggingface.co/nvidia/Llama-3_1-Nemotron-51B-Instruct&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Mistral Large is also an instruct\\n&lt;a href=\\"https://huggingface.co/mistralai/Mistral-Large-Instruct-2411\\"&gt;https://huggingface.co/mistralai/Mistral-Large-Instruct-2411&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyq22j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyq22j/local_llm_to_back_elastic_ai/n2vvu1j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752408435,"author_flair_text":null,"treatment_tags":[],"created_utc":1752408435,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ynk6v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jklre","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2vvijc","score":1,"author_fullname":"t2_5ut58","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Nemo is really bad at longer context in my experiance.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2ynk6v","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nemo is really bad at longer context in my experiance.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyq22j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyq22j/local_llm_to_back_elastic_ai/n2ynk6v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752439849,"author_flair_text":null,"treatment_tags":[],"created_utc":1752439849,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2vvijc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"OldManCyberNinja","can_mod_post":false,"created_utc":1752408294,"send_replies":true,"parent_id":"t1_n2vs9te","score":3,"author_fullname":"t2_s8xklsb6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks for the reply. One constraint from Elastic is:\\n\\nSearch for an LLM (for example, \`Mistral-Nemo-Instruct-2407\`). Your chosen model must include \`instruct\` in its name in order to work with Elastic.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2vvijc","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for the reply. One constraint from Elastic is:&lt;/p&gt;\\n\\n&lt;p&gt;Search for an LLM (for example, &lt;code&gt;Mistral-Nemo-Instruct-2407&lt;/code&gt;). Your chosen model must include &lt;code&gt;instruct&lt;/code&gt; in its name in order to work with Elastic.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyq22j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyq22j/local_llm_to_back_elastic_ai/n2vvijc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752408294,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2vs9te","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ICanSeeYou7867","can_mod_post":false,"created_utc":1752406799,"send_replies":true,"parent_id":"t3_1lyq22j","score":1,"author_fullname":"t2_sqvpr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Im in a similar-ish scenario...\\n\\nI finally got my 4xH100 server setup as a gpu worker node in kubernetes... and im trying to find out which models to run.\\n\\nThe Qwen3 235B A22B would be a great fit,  but like you, im trying to (unfortunatrly) avoid Chinese models which is hard....\\n\\nThe Nvidia Nemotron Ultra 235B is probably the strongest, non-chinese model that I could fit on the 4 H100 cards using FP8.\\n\\nI have also considered using the smaller nemotron models (like the 70B or the 49B) and deploying 2-4 of those and loaded balancing them.\\n\\nLlama4's intelligence is pretty low compared to these other models, unfortunately.  But it would be consistent and fast.\\n\\nMistral/Pixtral large might be a good choice as well, but im not sure how well they perform compared to llama4.  Also sense they are dense models, they might be smarter but will definitely be slower.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2vs9te","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Im in a similar-ish scenario...&lt;/p&gt;\\n\\n&lt;p&gt;I finally got my 4xH100 server setup as a gpu worker node in kubernetes... and im trying to find out which models to run.&lt;/p&gt;\\n\\n&lt;p&gt;The Qwen3 235B A22B would be a great fit,  but like you, im trying to (unfortunatrly) avoid Chinese models which is hard....&lt;/p&gt;\\n\\n&lt;p&gt;The Nvidia Nemotron Ultra 235B is probably the strongest, non-chinese model that I could fit on the 4 H100 cards using FP8.&lt;/p&gt;\\n\\n&lt;p&gt;I have also considered using the smaller nemotron models (like the 70B or the 49B) and deploying 2-4 of those and loaded balancing them.&lt;/p&gt;\\n\\n&lt;p&gt;Llama4&amp;#39;s intelligence is pretty low compared to these other models, unfortunately.  But it would be consistent and fast.&lt;/p&gt;\\n\\n&lt;p&gt;Mistral/Pixtral large might be a good choice as well, but im not sure how well they perform compared to llama4.  Also sense they are dense models, they might be smarter but will definitely be slower.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyq22j/local_llm_to_back_elastic_ai/n2vs9te/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752406799,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyq22j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(l,{data:t});export{n as default};
