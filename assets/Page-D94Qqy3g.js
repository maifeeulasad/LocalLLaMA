import{j as e}from"./index-CjwP30j7.js";import{R as t}from"./RedditPostRenderer-BbYuEq_V.js";import"./index-C-yxLSPN.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Curious to hear from others building LLM-based chat apps: Do you implement **prompt caching** to store chat history or previous responses? Or do you send the chat history with each user's prompt?\\n\\nCaching is more expensive to write, but the costs are then net positive if the conversation becomes long, no?\\n\\nWould appreciate your insights — thanks!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Do you use prompt caching to save chat history in your LLM apps?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":true,"name":"t3_1ltze9d","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":1,"author_flair_background_color":null,"subreddit_type":"public","ups":5,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_8h2i7wiei","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":5,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751907224,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Curious to hear from others building LLM-based chat apps: Do you implement &lt;strong&gt;prompt caching&lt;/strong&gt; to store chat history or previous responses? Or do you send the chat history with each user&amp;#39;s prompt?&lt;/p&gt;\\n\\n&lt;p&gt;Caching is more expensive to write, but the costs are then net positive if the conversation becomes long, no?&lt;/p&gt;\\n\\n&lt;p&gt;Would appreciate your insights — thanks!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1ltze9d","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Physical_Ad9040","discussion_type":null,"num_comments":4,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1ltze9d/do_you_use_prompt_caching_to_save_chat_history_in/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1ltze9d/do_you_use_prompt_caching_to_save_chat_history_in/","subreddit_subscribers":495643,"created_utc":1751907224,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1u6r7r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Awwtifishal","can_mod_post":false,"created_utc":1751908026,"send_replies":true,"parent_id":"t3_1ltze9d","score":4,"author_fullname":"t2_1d96a8k10t","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"With typical LLM APIs (local or not) the whole conversation is sent on each request, regardless of \\"prompt caching\\" support. Usually \\"prompt caching\\" means storing the KV cache even if it's just temporarily, so you don't have to re-process the whole KV data on each request, and it's pretty common. For example llama.cpp and llama.cpp based projects all do it by default: they reuse the KV cache up to the point it changes or it ends. So for example editing the last message is much faster than editing the first message or the system message.\\n\\nNote that LLM inferencing doesn't distinguish between \\"messages\\", it's just one big string of text to autocomplete (usually just to let the \\"assistant\\" part of the conversation be generated and stop when this part ends). So it's not just the \\"prompt\\" what is cached, but everything.\\n\\nThen if you e.g. store the conversation server-side and just send each message individually, that's a different feature, meant for saving in bandwidth although it's usually not much so it may not be worth it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1u6r7r","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;With typical LLM APIs (local or not) the whole conversation is sent on each request, regardless of &amp;quot;prompt caching&amp;quot; support. Usually &amp;quot;prompt caching&amp;quot; means storing the KV cache even if it&amp;#39;s just temporarily, so you don&amp;#39;t have to re-process the whole KV data on each request, and it&amp;#39;s pretty common. For example llama.cpp and llama.cpp based projects all do it by default: they reuse the KV cache up to the point it changes or it ends. So for example editing the last message is much faster than editing the first message or the system message.&lt;/p&gt;\\n\\n&lt;p&gt;Note that LLM inferencing doesn&amp;#39;t distinguish between &amp;quot;messages&amp;quot;, it&amp;#39;s just one big string of text to autocomplete (usually just to let the &amp;quot;assistant&amp;quot; part of the conversation be generated and stop when this part ends). So it&amp;#39;s not just the &amp;quot;prompt&amp;quot; what is cached, but everything.&lt;/p&gt;\\n\\n&lt;p&gt;Then if you e.g. store the conversation server-side and just send each message individually, that&amp;#39;s a different feature, meant for saving in bandwidth although it&amp;#39;s usually not much so it may not be worth it.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltze9d/do_you_use_prompt_caching_to_save_chat_history_in/n1u6r7r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751908026,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltze9d","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1u5oev","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"picturpoet","can_mod_post":false,"created_utc":1751907715,"send_replies":true,"parent_id":"t3_1ltze9d","score":1,"author_fullname":"t2_naay5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"following","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1u5oev","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;following&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltze9d/do_you_use_prompt_caching_to_save_chat_history_in/n1u5oev/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751907715,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltze9d","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1udlcj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Flaky_Comedian2012","can_mod_post":false,"created_utc":1751910026,"send_replies":true,"parent_id":"t3_1ltze9d","score":1,"author_fullname":"t2_1d1v4h15w7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I just send it all as it is already cached by default in the backend. It only has to process whatever extra tokens that has been added to the context.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1udlcj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I just send it all as it is already cached by default in the backend. It only has to process whatever extra tokens that has been added to the context.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltze9d/do_you_use_prompt_caching_to_save_chat_history_in/n1udlcj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751910026,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltze9d","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1udyuw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Some-Cauliflower4902","can_mod_post":false,"created_utc":1751910140,"send_replies":true,"parent_id":"t3_1ltze9d","score":1,"author_fullname":"t2_1e2tnqudlj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"My hobby project sends the whole chat history. Would like to learn other ways.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1udyuw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;My hobby project sends the whole chat history. Would like to learn other ways.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltze9d/do_you_use_prompt_caching_to_save_chat_history_in/n1udyuw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751910140,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltze9d","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(t,{data:l});export{r as default};
