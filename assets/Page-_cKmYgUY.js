import{j as e}from"./index-BxgxThME.js";import{R as l}from"./RedditPostRenderer-BL_SOtuv.js";import"./index--Az3yIKM.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I am upgrading my system which will have a 5090.  Would adding my old 3090 be any benefit or would it slow down the 5090 too much?  Inference only.  I'd like to get large context window on high quant of 32B, potentially using 70B.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"5090 w/ 3090?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1ls5pbt","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.5,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_ijzb7","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751706639,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I am upgrading my system which will have a 5090.  Would adding my old 3090 be any benefit or would it slow down the 5090 too much?  Inference only.  I&amp;#39;d like to get large context window on high quant of 32B, potentially using 70B.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1ls5pbt","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"MidnightProgrammer","discussion_type":null,"num_comments":5,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1ls5pbt/5090_w_3090/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1ls5pbt/5090_w_3090/","subreddit_subscribers":494986,"created_utc":1751706639,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1g103k","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"stoppableDissolution","can_mod_post":false,"created_utc":1751707619,"send_replies":true,"parent_id":"t3_1ls5pbt","score":12,"author_fullname":"t2_1n0su21k4z","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If the model fits entirely in 32gb, just keep in the 5090, there will be no downsides.\\n\\nIf it doesnt, 5090+3090 will, of course, be slower than 2x5090, but lightyears ahead of 5090+cpu","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1g103k","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If the model fits entirely in 32gb, just keep in the 5090, there will be no downsides.&lt;/p&gt;\\n\\n&lt;p&gt;If it doesnt, 5090+3090 will, of course, be slower than 2x5090, but lightyears ahead of 5090+cpu&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ls5pbt/5090_w_3090/n1g103k/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751707619,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ls5pbt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1g0hcx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1751707282,"send_replies":true,"parent_id":"t3_1ls5pbt","score":5,"author_fullname":"t2_uz37qfx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You can select 5090 only for smaller or 3090+5090 for bigger ones. So the answer is no. It won't slow down.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1g0hcx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can select 5090 only for smaller or 3090+5090 for bigger ones. So the answer is no. It won&amp;#39;t slow down.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ls5pbt/5090_w_3090/n1g0hcx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751707282,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ls5pbt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1gq1wp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"davew111","can_mod_post":false,"created_utc":1751720646,"send_replies":true,"parent_id":"t1_n1g6za9","score":0,"author_fullname":"t2_t3m2b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Depending on his motherboard, adding a second GPU would likely cause his primary to run in 8 PCIe lanes rather than 16.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1gq1wp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Depending on his motherboard, adding a second GPU would likely cause his primary to run in 8 PCIe lanes rather than 16.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ls5pbt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ls5pbt/5090_w_3090/n1gq1wp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751720646,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n1g6za9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DepthHour1669","can_mod_post":false,"created_utc":1751711299,"send_replies":true,"parent_id":"t3_1ls5pbt","score":4,"author_fullname":"t2_t6glzswk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"There are 0 situations where it is slower than a 5090 only.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1g6za9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There are 0 situations where it is slower than a 5090 only.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ls5pbt/5090_w_3090/n1g6za9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751711299,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ls5pbt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1itsi1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ArsNeph","can_mod_post":false,"created_utc":1751745609,"send_replies":true,"parent_id":"t3_1ls5pbt","score":1,"author_fullname":"t2_vt0xkv60d","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"There is no reason not to add the 3090. If you need maximum speed, you can run the model on just the 5090, and if the model is too big to fit in 32 GB, then you can run it with the 3090. The 3090 has 700 GB/s less memory bandwidth, but that's still many times more memory bandwidth than RAM. Even if you aren't using the 3090 for an LLM, you can easily run a Diffusion model, STT, and TTS on it. 56 GB of vram is enough to run 70B at 5 bit or high context.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1itsi1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There is no reason not to add the 3090. If you need maximum speed, you can run the model on just the 5090, and if the model is too big to fit in 32 GB, then you can run it with the 3090. The 3090 has 700 GB/s less memory bandwidth, but that&amp;#39;s still many times more memory bandwidth than RAM. Even if you aren&amp;#39;t using the 3090 for an LLM, you can easily run a Diffusion model, STT, and TTS on it. 56 GB of vram is enough to run 70B at 5 bit or high context.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ls5pbt/5090_w_3090/n1itsi1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751745609,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ls5pbt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(l,{data:t});export{n as default};
