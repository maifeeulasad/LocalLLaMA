import{j as e}from"./index-BlGsFJYy.js";import{R as t}from"./RedditPostRenderer-B6uvq_Zl.js";import"./index-DDvVtNwD.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"\\"FlexOlmo: Open Language Models for Flexible Data Use\\" -- https://arxiv.org/abs/2507.07024\\n\\nAllenAI has published a mostly open source model (published weights, code, and theory, but not yet training data) called FlexOlmo which demonstrates how an MoE may be trained in a federated manner, without the incompatibility problems which normally plague experts which were trained independently.\\n\\nMainly they tout the flexibility of inference-time world knowledge selectivity, but the potential for federated training is *very* exciting for the open source world, because it demonstrates how we might piece together a large MoE from smaller dense models.\\n\\nIn a sense FlexOlmo is similar to [Goddard's clown-car MoE](https://goddard.blog/posts/clown-moe/) where each expert is a fine-tune of the same base model, but the clown-car MoE is limited in how much the experts can be fine-tuned without becoming mutually incompatible.  AllenAI's approach algorithmically keeps the models compatible, even after extensive continued pretraining, without training-time communication between trainers.\\n\\nTraining each expert also constructs the parts of a modular routing network which are merged together when the experts are combined into the MoE container model, so that post-merge training of the routing network (gates, in Goddard's parlance) is not necessary.\\n\\nWhat this means for the open source LLM community is that after preliminary co-ordination, different geographically dispersed participants can pour as much training and data into their local copies of the base expert as they can, and then merge the end results together at low resource cost, and produce an MoE with inference competence which reflects its aggregate training.  Unlike the clown-car MoE it is guaranteed to work correctly.\\n\\nThis approach gives us another option for becoming independent of GPU-rich companies, and advancing the progress of LLM technology ourselves.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"FlexOlmo: Open Language Models for Flexible Data Use | Implications for federated training in the open source community","link_flair_richtext":[{"e":"text","t":"New Model"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lxehv3","quarantine":false,"link_flair_text_color":"light","upvote_ratio":1,"author_flair_background_color":"#bbbdbf","subreddit_type":"public","ups":10,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","is_original_content":false,"author_fullname":"t2_cpegz","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"New Model","can_mod_post":false,"score":10,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752258544,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"richtext","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;&amp;quot;FlexOlmo: Open Language Models for Flexible Data Use&amp;quot; -- &lt;a href=\\"https://arxiv.org/abs/2507.07024\\"&gt;https://arxiv.org/abs/2507.07024&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;AllenAI has published a mostly open source model (published weights, code, and theory, but not yet training data) called FlexOlmo which demonstrates how an MoE may be trained in a federated manner, without the incompatibility problems which normally plague experts which were trained independently.&lt;/p&gt;\\n\\n&lt;p&gt;Mainly they tout the flexibility of inference-time world knowledge selectivity, but the potential for federated training is &lt;em&gt;very&lt;/em&gt; exciting for the open source world, because it demonstrates how we might piece together a large MoE from smaller dense models.&lt;/p&gt;\\n\\n&lt;p&gt;In a sense FlexOlmo is similar to &lt;a href=\\"https://goddard.blog/posts/clown-moe/\\"&gt;Goddard&amp;#39;s clown-car MoE&lt;/a&gt; where each expert is a fine-tune of the same base model, but the clown-car MoE is limited in how much the experts can be fine-tuned without becoming mutually incompatible.  AllenAI&amp;#39;s approach algorithmically keeps the models compatible, even after extensive continued pretraining, without training-time communication between trainers.&lt;/p&gt;\\n\\n&lt;p&gt;Training each expert also constructs the parts of a modular routing network which are merged together when the experts are combined into the MoE container model, so that post-merge training of the routing network (gates, in Goddard&amp;#39;s parlance) is not necessary.&lt;/p&gt;\\n\\n&lt;p&gt;What this means for the open source LLM community is that after preliminary co-ordination, different geographically dispersed participants can pour as much training and data into their local copies of the base expert as they can, and then merge the end results together at low resource cost, and produce an MoE with inference competence which reflects its aggregate training.  Unlike the clown-car MoE it is guaranteed to work correctly.&lt;/p&gt;\\n\\n&lt;p&gt;This approach gives us another option for becoming independent of GPU-rich companies, and advancing the progress of LLM technology ourselves.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ced98442-f5d3-11ed-b657-66d3b15490c6","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":"llama.cpp","treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#ffb000","id":"1lxehv3","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"ttkciar","discussion_type":null,"num_comments":1,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":"light","permalink":"/r/LocalLLaMA/comments/1lxehv3/flexolmo_open_language_models_for_flexible_data/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lxehv3/flexolmo_open_language_models_for_flexible_data/","subreddit_subscribers":497825,"created_utc":1752258544,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2loht3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ttkciar","can_mod_post":false,"created_utc":1752261133,"send_replies":true,"parent_id":"t3_1lxehv3","score":1,"author_fullname":"t2_cpegz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It occurs to me, belatedly, that this technique might lend itself to more reliable passthrough-merges of dense models, as well.\\n\\nThat's totally something that needs investigation.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2loht3","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It occurs to me, belatedly, that this technique might lend itself to more reliable passthrough-merges of dense models, as well.&lt;/p&gt;\\n\\n&lt;p&gt;That&amp;#39;s totally something that needs investigation.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxehv3/flexolmo_open_language_models_for_flexible_data/n2loht3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752261133,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lxehv3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),i=()=>e.jsx(t,{data:a});export{i as default};
