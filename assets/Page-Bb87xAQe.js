import{j as e}from"./index-CeRg6Q3f.js";import{R as t}from"./RedditPostRenderer-D7n1g-D8.js";import"./index-DPToWe3n.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I have an MacBook M4 Max with 128 GB and LM Studio.  Playing around with Gemma 3 models and Llama 4 Scout.  What is the best reasoning model that will fit into my RAM?  \\n\\nAlso, running HF Diffusers app.  Running SD3 Medium for txt2img, anything else I should be looking at?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Best reasoning model for Apple silicon with 128GB","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1ltbr1t","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.83,"author_flair_background_color":null,"subreddit_type":"public","ups":38,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_r3xju","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":38,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751835143,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I have an MacBook M4 Max with 128 GB and LM Studio.  Playing around with Gemma 3 models and Llama 4 Scout.  What is the best reasoning model that will fit into my RAM?  &lt;/p&gt;\\n\\n&lt;p&gt;Also, running HF Diffusers app.  Running SD3 Medium for txt2img, anything else I should be looking at?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1ltbr1t","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"FuguSandwich","discussion_type":null,"num_comments":15,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1ltbr1t/best_reasoning_model_for_apple_silicon_with_128gb/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1ltbr1t/best_reasoning_model_for_apple_silicon_with_128gb/","subreddit_subscribers":496034,"created_utc":1751835143,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1paku8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Baldur-Norddahl","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1p99ut","score":14,"author_fullname":"t2_bvqb8ng0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Forgot to add that you need to disable LM Studio checks for what is too big :-)\\n\\nGo to Mission Control, App Settings, General, Model loading guardrails and set to OFF. It will say not recommended but without this it wont load the model.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1paku8","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Forgot to add that you need to disable LM Studio checks for what is too big :-)&lt;/p&gt;\\n\\n&lt;p&gt;Go to Mission Control, App Settings, General, Model loading guardrails and set to OFF. It will say not recommended but without this it wont load the model.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltbr1t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltbr1t/best_reasoning_model_for_apple_silicon_with_128gb/n1paku8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751837077,"author_flair_text":null,"treatment_tags":[],"created_utc":1751837077,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":14}}],"before":null}},"user_reports":[],"saved":false,"id":"n1p99ut","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FuguSandwich","can_mod_post":false,"created_utc":1751836661,"send_replies":true,"parent_id":"t1_n1p8f77","score":5,"author_fullname":"t2_r3xju","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thank you!  I was just looking at this model earlier today but LM Studio was saying it was probably too big so I didn't download it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1p99ut","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you!  I was just looking at this model earlier today but LM Studio was saying it was probably too big so I didn&amp;#39;t download it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltbr1t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltbr1t/best_reasoning_model_for_apple_silicon_with_128gb/n1p99ut/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751836661,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1rwqqf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Baldur-Norddahl","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1quw5s","score":2,"author_fullname":"t2_bvqb8ng0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I need to try the MLX DWQ tonight when I am home. Can't download those huge beasts from here :-)\\n\\nThe settings are from here:\\n\\n[https://docs.unsloth.ai/basics/qwen3-how-to-run-and-fine-tune](https://docs.unsloth.ai/basics/qwen3-how-to-run-and-fine-tune)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1rwqqf","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I need to try the MLX DWQ tonight when I am home. Can&amp;#39;t download those huge beasts from here :-)&lt;/p&gt;\\n\\n&lt;p&gt;The settings are from here:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://docs.unsloth.ai/basics/qwen3-how-to-run-and-fine-tune\\"&gt;https://docs.unsloth.ai/basics/qwen3-how-to-run-and-fine-tune&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltbr1t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltbr1t/best_reasoning_model_for_apple_silicon_with_128gb/n1rwqqf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751876764,"author_flair_text":null,"treatment_tags":[],"created_utc":1751876764,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1quw5s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"mxforest","can_mod_post":false,"created_utc":1751857358,"send_replies":true,"parent_id":"t1_n1p8f77","score":5,"author_fullname":"t2_kenmq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Just yesterday I ran the MLX DWQ quant (3-6 bit mixed) on full ~40k context. Get 28-29 tps initially which drops to around 15-18 as context fills up. I also used 128000 allocation (125GB) but i think that was overkill. The whole system was still running under 120GB usage. Which was strange because the download itself is 117 GB. Also my settings were identical to yours except i didn't quantize the kv cache.\\n\\nI have a puzzle which i use to test thinking models and surprisingly the default settings came close to the right answer but the custom one you mentioned gave wrong answer outright. So I am questioning whether they are right in the first place.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1quw5s","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just yesterday I ran the MLX DWQ quant (3-6 bit mixed) on full ~40k context. Get 28-29 tps initially which drops to around 15-18 as context fills up. I also used 128000 allocation (125GB) but i think that was overkill. The whole system was still running under 120GB usage. Which was strange because the download itself is 117 GB. Also my settings were identical to yours except i didn&amp;#39;t quantize the kv cache.&lt;/p&gt;\\n\\n&lt;p&gt;I have a puzzle which i use to test thinking models and surprisingly the default settings came close to the right answer but the custom one you mentioned gave wrong answer outright. So I am questioning whether they are right in the first place.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltbr1t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltbr1t/best_reasoning_model_for_apple_silicon_with_128gb/n1quw5s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751857358,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1q2gxa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"__JockY__","can_mod_post":false,"created_utc":1751846704,"send_replies":true,"parent_id":"t1_n1p8f77","score":7,"author_fullname":"t2_qf8h7ka8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Insane that we can run this on a _laptop_.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1q2gxa","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Insane that we can run this on a &lt;em&gt;laptop&lt;/em&gt;.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltbr1t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltbr1t/best_reasoning_model_for_apple_silicon_with_128gb/n1q2gxa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751846704,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1svhk3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Baldur-Norddahl","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1skjkm","score":2,"author_fullname":"t2_bvqb8ng0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I don't think you are supposed to change the number of experts from the default. 20 tps sounds familiar but that is with empty context. It slows somewhat when you have a lot in there. Which is typically the case when doing agentic coding.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1svhk3","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t think you are supposed to change the number of experts from the default. 20 tps sounds familiar but that is with empty context. It slows somewhat when you have a lot in there. Which is typically the case when doing agentic coding.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltbr1t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltbr1t/best_reasoning_model_for_apple_silicon_with_128gb/n1svhk3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751893760,"author_flair_text":null,"treatment_tags":[],"created_utc":1751893760,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1skjkm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AlwaysInconsistant","can_mod_post":false,"created_utc":1751889490,"send_replies":true,"parent_id":"t1_n1p8f77","score":1,"author_fullname":"t2_65e1spw7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thank you for sharing - working for me now with these recommendations. How many experts are you enabling while getting 10-15tps? Default of 8 had me at around 20 tps, but maxing it out to 128 has me sitting around 2 tps with my M3 Max.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1skjkm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you for sharing - working for me now with these recommendations. How many experts are you enabling while getting 10-15tps? Default of 8 had me at around 20 tps, but maxing it out to 128 has me sitting around 2 tps with my M3 Max.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltbr1t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltbr1t/best_reasoning_model_for_apple_silicon_with_128gb/n1skjkm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751889490,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1p8f77","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Baldur-Norddahl","can_mod_post":false,"created_utc":1751836393,"send_replies":true,"parent_id":"t3_1ltbr1t","score":62,"author_fullname":"t2_bvqb8ng0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Run the command to expand the max VRAM allocation (needs to be done after each reboot):\\n\\nsudo sysctl iogpu.wired\\\\_limit\\\\_mb=120000\\n\\nThen using LM Studio search for Qwen3-235B-A22B-128K-GGUF by Unsloth and select the Q3\\\\_K\\\\_XL 103.72 GB file and download.\\n\\nWhen loading the model be sure to uncheck \\"Keep model in memory\\" and \\"Try mmap\\". Enable flash attention and set both K and V cache quantization type to Q8\\\\_0. Also set context length to max (131072).\\n\\nFor best results set temperature to 0.6, Top K sampling to 20, repeat penalty to 1.1, Min P sampling to 0, Top P sampling to 0.95.\\n\\nThis is the absolute max quality reasoning model it is possible to run on M4 Max 128 GB at the moment. You may find it is a bit taxing on the machine as it uses most of your RAM, but it is actually not that slow. Expect about 10-15 tps.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1p8f77","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Run the command to expand the max VRAM allocation (needs to be done after each reboot):&lt;/p&gt;\\n\\n&lt;p&gt;sudo sysctl iogpu.wired_limit_mb=120000&lt;/p&gt;\\n\\n&lt;p&gt;Then using LM Studio search for Qwen3-235B-A22B-128K-GGUF by Unsloth and select the Q3_K_XL 103.72 GB file and download.&lt;/p&gt;\\n\\n&lt;p&gt;When loading the model be sure to uncheck &amp;quot;Keep model in memory&amp;quot; and &amp;quot;Try mmap&amp;quot;. Enable flash attention and set both K and V cache quantization type to Q8_0. Also set context length to max (131072).&lt;/p&gt;\\n\\n&lt;p&gt;For best results set temperature to 0.6, Top K sampling to 20, repeat penalty to 1.1, Min P sampling to 0, Top P sampling to 0.95.&lt;/p&gt;\\n\\n&lt;p&gt;This is the absolute max quality reasoning model it is possible to run on M4 Max 128 GB at the moment. You may find it is a bit taxing on the machine as it uses most of your RAM, but it is actually not that slow. Expect about 10-15 tps.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltbr1t/best_reasoning_model_for_apple_silicon_with_128gb/n1p8f77/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751836393,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltbr1t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":62}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1p5yw2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"gnad","can_mod_post":false,"created_utc":1751835635,"send_replies":true,"parent_id":"t3_1ltbr1t","score":5,"author_fullname":"t2_giuiz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think Qwen 235B is the best option","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1p5yw2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think Qwen 235B is the best option&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltbr1t/best_reasoning_model_for_apple_silicon_with_128gb/n1p5yw2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751835635,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltbr1t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1pjpg4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Longjumpingfish0403","can_mod_post":false,"created_utc":1751840102,"send_replies":true,"parent_id":"t3_1ltbr1t","score":4,"author_fullname":"t2_jarttha4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you're exploring reasoning models on Apple Silicon, check out [Google's \\"Data Gemma\\"](https://pub.towardsai.net/demystifying-googles-data-gemma-f07a470c2a39). It stabilizes retrieval without hallucinations by integrating with a structured knowledge graph via natural-language queries. This could optimize your workflow, especially when combined with your existing setup. Plus, it's designed to be efficient on consumer hardware, which might be ideal given your MacBook specs.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1pjpg4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you&amp;#39;re exploring reasoning models on Apple Silicon, check out &lt;a href=\\"https://pub.towardsai.net/demystifying-googles-data-gemma-f07a470c2a39\\"&gt;Google&amp;#39;s &amp;quot;Data Gemma&amp;quot;&lt;/a&gt;. It stabilizes retrieval without hallucinations by integrating with a structured knowledge graph via natural-language queries. This could optimize your workflow, especially when combined with your existing setup. Plus, it&amp;#39;s designed to be efficient on consumer hardware, which might be ideal given your MacBook specs.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltbr1t/best_reasoning_model_for_apple_silicon_with_128gb/n1pjpg4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751840102,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltbr1t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1rat0p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No_Conversation9561","can_mod_post":false,"created_utc":1751864461,"send_replies":true,"parent_id":"t3_1ltbr1t","score":4,"author_fullname":"t2_jqxb4pte","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"dots.llm1 and qwen3 235B","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1rat0p","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;dots.llm1 and qwen3 235B&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltbr1t/best_reasoning_model_for_apple_silicon_with_128gb/n1rat0p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751864461,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltbr1t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1re0nv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RemindMeBot","can_mod_post":false,"created_utc":1751866096,"send_replies":true,"parent_id":"t1_n1rdxsd","score":1,"author_fullname":"t2_gbm4p","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I will be messaging you in 12 hours on [**2025-07-07 17:27:34 UTC**](http://www.wolframalpha.com/input/?i=2025-07-07%2017:27:34%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/LocalLLaMA/comments/1ltbr1t/best_reasoning_model_for_apple_silicon_with_128gb/n1rdxsd/?context=3)\\n\\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FLocalLLaMA%2Fcomments%2F1ltbr1t%2Fbest_reasoning_model_for_apple_silicon_with_128gb%2Fn1rdxsd%2F%5D%0A%0ARemindMe%21%202025-07-07%2017%3A27%3A34%20UTC) to send a PM to also be reminded and to reduce spam.\\n\\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete%20Comment&amp;message=Delete%21%201ltbr1t)\\n\\n*****\\n\\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List%20Of%20Reminders&amp;message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&amp;subject=RemindMeBot%20Feedback)|\\n|-|-|-|-|","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1re0nv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I will be messaging you in 12 hours on &lt;a href=\\"http://www.wolframalpha.com/input/?i=2025-07-07%2017:27:34%20UTC%20To%20Local%20Time\\"&gt;&lt;strong&gt;2025-07-07 17:27:34 UTC&lt;/strong&gt;&lt;/a&gt; to remind you of &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1ltbr1t/best_reasoning_model_for_apple_silicon_with_128gb/n1rdxsd/?context=3\\"&gt;&lt;strong&gt;this link&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.reddit.com/message/compose/?to=RemindMeBot&amp;amp;subject=Reminder&amp;amp;message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FLocalLLaMA%2Fcomments%2F1ltbr1t%2Fbest_reasoning_model_for_apple_silicon_with_128gb%2Fn1rdxsd%2F%5D%0A%0ARemindMe%21%202025-07-07%2017%3A27%3A34%20UTC\\"&gt;&lt;strong&gt;CLICK THIS LINK&lt;/strong&gt;&lt;/a&gt; to send a PM to also be reminded and to reduce spam.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;sup&gt;Parent commenter can &lt;/sup&gt; &lt;a href=\\"https://www.reddit.com/message/compose/?to=RemindMeBot&amp;amp;subject=Delete%20Comment&amp;amp;message=Delete%21%201ltbr1t\\"&gt;&lt;sup&gt;delete this message to hide from others.&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;table&gt;&lt;thead&gt;\\n&lt;tr&gt;\\n&lt;th&gt;&lt;a href=\\"https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/\\"&gt;&lt;sup&gt;Info&lt;/sup&gt;&lt;/a&gt;&lt;/th&gt;\\n&lt;th&gt;&lt;a href=\\"https://www.reddit.com/message/compose/?to=RemindMeBot&amp;amp;subject=Reminder&amp;amp;message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here\\"&gt;&lt;sup&gt;Custom&lt;/sup&gt;&lt;/a&gt;&lt;/th&gt;\\n&lt;th&gt;&lt;a href=\\"https://www.reddit.com/message/compose/?to=RemindMeBot&amp;amp;subject=List%20Of%20Reminders&amp;amp;message=MyReminders%21\\"&gt;&lt;sup&gt;Your Reminders&lt;/sup&gt;&lt;/a&gt;&lt;/th&gt;\\n&lt;th&gt;&lt;a href=\\"https://www.reddit.com/message/compose/?to=Watchful1&amp;amp;subject=RemindMeBot%20Feedback\\"&gt;&lt;sup&gt;Feedback&lt;/sup&gt;&lt;/a&gt;&lt;/th&gt;\\n&lt;/tr&gt;\\n&lt;/thead&gt;&lt;tbody&gt;\\n&lt;/tbody&gt;&lt;/table&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltbr1t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltbr1t/best_reasoning_model_for_apple_silicon_with_128gb/n1re0nv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751866096,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1rdxsd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"--Tintin","can_mod_post":false,"created_utc":1751866054,"send_replies":true,"parent_id":"t3_1ltbr1t","score":1,"author_fullname":"t2_3rsks8xy","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Remindme! 12h","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1rdxsd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Remindme! 12h&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltbr1t/best_reasoning_model_for_apple_silicon_with_128gb/n1rdxsd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751866054,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltbr1t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1t5x0o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"norcom","can_mod_post":false,"created_utc":1751897282,"send_replies":true,"parent_id":"t3_1ltbr1t","score":2,"author_fullname":"t2_8u0qp","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I have an M4 128GB and I agree with others, Qwen3 235B Q3\\\\_K\\\\_XL is the best, largest, model you can fit. But unless you dedicated your laptop to only run inference, it's not very practical to use. I've locked up my system more than a few times running out of memory while running other heavy apps. \\n\\n  \\nAll I can suggest is you try all the models you can, find what you like and add it to your quiver. There's really no single \\"best\\", do it all, model in this size. They all respond slightly differently. To me, these models have their uses when I want to process something private or just want an idea from a less intelligent model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1t5x0o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have an M4 128GB and I agree with others, Qwen3 235B Q3_K_XL is the best, largest, model you can fit. But unless you dedicated your laptop to only run inference, it&amp;#39;s not very practical to use. I&amp;#39;ve locked up my system more than a few times running out of memory while running other heavy apps. &lt;/p&gt;\\n\\n&lt;p&gt;All I can suggest is you try all the models you can, find what you like and add it to your quiver. There&amp;#39;s really no single &amp;quot;best&amp;quot;, do it all, model in this size. They all respond slightly differently. To me, these models have their uses when I want to process something private or just want an idea from a less intelligent model.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltbr1t/best_reasoning_model_for_apple_silicon_with_128gb/n1t5x0o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751897282,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltbr1t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),s=()=>e.jsx(t,{data:l});export{s as default};
