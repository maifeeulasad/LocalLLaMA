import{j as e}from"./index-Cd3v0jxz.js";import{R as t}from"./RedditPostRenderer-cI96YLhy.js";import"./index-DGBoKyQm.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"TLDR; local models like Gemma 27b, Qwen 3 32b can't use the file edit tool in void code\\n\\nI'm trying to create a simple snake game to test. So far, I've been failing with almost all of the Gemma 4/12/27 models; Qwen 32b seems to do a bit better, but still breaks with editing files.\\n\\nAnyone has had any luck with Void Code or something similar where these model can use tools correctly? Specifically, I notice that every tool breaks when trying to update the file with 'edit\\\\_file' tool.\\n\\nLLMs via APIs work perfectly -- which is now starting to give me a feeling that a local setup might not work for even simpler use case\\n\\nPrompt:  \\nCreate a snake game using html and javascript\\n\\nIf you've had better luck, please help\\n\\nEdit1: I understand that it could just be an editor issue. My previous experience with continue dev in VsCode was quite good with Gemma models. ","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Using local models with Void","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lml6eo","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.79,"author_flair_background_color":null,"subreddit_type":"public","ups":8,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1gdl5ph","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":8,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1751112446,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751112178,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;TLDR; local models like Gemma 27b, Qwen 3 32b can&amp;#39;t use the file edit tool in void code&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m trying to create a simple snake game to test. So far, I&amp;#39;ve been failing with almost all of the Gemma 4/12/27 models; Qwen 32b seems to do a bit better, but still breaks with editing files.&lt;/p&gt;\\n\\n&lt;p&gt;Anyone has had any luck with Void Code or something similar where these model can use tools correctly? Specifically, I notice that every tool breaks when trying to update the file with &amp;#39;edit_file&amp;#39; tool.&lt;/p&gt;\\n\\n&lt;p&gt;LLMs via APIs work perfectly -- which is now starting to give me a feeling that a local setup might not work for even simpler use case&lt;/p&gt;\\n\\n&lt;p&gt;Prompt:&lt;br/&gt;\\nCreate a snake game using html and javascript&lt;/p&gt;\\n\\n&lt;p&gt;If you&amp;#39;ve had better luck, please help&lt;/p&gt;\\n\\n&lt;p&gt;Edit1: I understand that it could just be an editor issue. My previous experience with continue dev in VsCode was quite good with Gemma models. &lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lml6eo","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"nuketro0p3r","discussion_type":null,"num_comments":3,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lml6eo/using_local_models_with_void/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lml6eo/using_local_models_with_void/","subreddit_subscribers":492840,"created_utc":1751112178,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n08b7w3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"nuketro0p3r","can_mod_post":false,"created_utc":1751113404,"send_replies":true,"parent_id":"t1_n08a4t1","score":1,"author_fullname":"t2_1gdl5ph","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"thanks a lot for the suggestion. will try it right now\\n\\nhave you had luck with other models? or we just wait it out for the next release where they fix it","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n08b7w3","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;thanks a lot for the suggestion. will try it right now&lt;/p&gt;\\n\\n&lt;p&gt;have you had luck with other models? or we just wait it out for the next release where they fix it&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lml6eo","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lml6eo/using_local_models_with_void/n08b7w3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751113404,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n08a4t1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AXYZE8","can_mod_post":false,"created_utc":1751112932,"send_replies":true,"parent_id":"t3_1lml6eo","score":3,"author_fullname":"t2_10dacl","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Use devstral model instead, its trained for this kind of tasks\\nhttps://www.reddit.com/r/void_editor_user/comments/1l459mq/useful_links_to_setup_void_with_devstral/","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n08a4t1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Use devstral model instead, its trained for this kind of tasks\\n&lt;a href=\\"https://www.reddit.com/r/void_editor_user/comments/1l459mq/useful_links_to_setup_void_with_devstral/\\"&gt;https://www.reddit.com/r/void_editor_user/comments/1l459mq/useful_links_to_setup_void_with_devstral/&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lml6eo/using_local_models_with_void/n08a4t1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751112932,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lml6eo","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ambx3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"nuketro0p3r","can_mod_post":false,"created_utc":1751140815,"send_replies":true,"parent_id":"t3_1lml6eo","score":1,"author_fullname":"t2_1gdl5ph","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Upon further testing, I can see the following free models perform really well.\\n\\nGroq: Llama 70b (both available versions with good usage limit)  \\nGemini: gemini 2.0 flash\\n\\nIt could be that the models I'm using are impaired because of Q4 / Q8 I'm using.\\n\\nMy setup is LMStudio (for server) and Void Editor for agentic coding. Hardware 32g ram / 16gb 4060 Ti. Almost all models I try are Q4; some 8B models are Q8.  \\n  \\nSo far I've tried and wasted a lot of time on the following test projects:\\n\\n\\\\- Mortgage application with complex calculation  \\n\\\\- Snake game in html and js  \\n\\\\- Snake game in Python\\n\\nIn all these, Qwen 32b Q4 seems to be the most capable at tool usage. Although almost all model seem to suck and using the edit\\\\_file commend (used to updated parts of the code). I'm not sure if it's something that can be fixed by System Prompts -- based on the error that LLM gets, I think it's a formatting issue.\\n\\nRegardless, using [continue.dev](http://continue.dev) and void with manual updates, the 12B+ models seem to do really well. DeepSeek R1 is the only usable exception that can produce somewhat complex code. Particularly, Gemma models are my favorite so far as they run really fast, so the debug cycle is quite efficient.\\n\\nIn these cases, I'm trying to get a feel for the model capacity for complexity -- so it's not really an academic study. I hope it helps someone","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ambx3","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Upon further testing, I can see the following free models perform really well.&lt;/p&gt;\\n\\n&lt;p&gt;Groq: Llama 70b (both available versions with good usage limit)&lt;br/&gt;\\nGemini: gemini 2.0 flash&lt;/p&gt;\\n\\n&lt;p&gt;It could be that the models I&amp;#39;m using are impaired because of Q4 / Q8 I&amp;#39;m using.&lt;/p&gt;\\n\\n&lt;p&gt;My setup is LMStudio (for server) and Void Editor for agentic coding. Hardware 32g ram / 16gb 4060 Ti. Almost all models I try are Q4; some 8B models are Q8.  &lt;/p&gt;\\n\\n&lt;p&gt;So far I&amp;#39;ve tried and wasted a lot of time on the following test projects:&lt;/p&gt;\\n\\n&lt;p&gt;- Mortgage application with complex calculation&lt;br/&gt;\\n- Snake game in html and js&lt;br/&gt;\\n- Snake game in Python&lt;/p&gt;\\n\\n&lt;p&gt;In all these, Qwen 32b Q4 seems to be the most capable at tool usage. Although almost all model seem to suck and using the edit_file commend (used to updated parts of the code). I&amp;#39;m not sure if it&amp;#39;s something that can be fixed by System Prompts -- based on the error that LLM gets, I think it&amp;#39;s a formatting issue.&lt;/p&gt;\\n\\n&lt;p&gt;Regardless, using &lt;a href=\\"http://continue.dev\\"&gt;continue.dev&lt;/a&gt; and void with manual updates, the 12B+ models seem to do really well. DeepSeek R1 is the only usable exception that can produce somewhat complex code. Particularly, Gemma models are my favorite so far as they run really fast, so the debug cycle is quite efficient.&lt;/p&gt;\\n\\n&lt;p&gt;In these cases, I&amp;#39;m trying to get a feel for the model capacity for complexity -- so it&amp;#39;s not really an academic study. I hope it helps someone&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lml6eo/using_local_models_with_void/n0ambx3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751140815,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lml6eo","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(t,{data:l});export{n as default};
