import{j as t}from"./index-DACS7Nh6.js";import{R as l}from"./RedditPostRenderer-Dqa1NZuX.js";import"./index-DiMIVQx4.js";const e=JSON.parse('[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Update to my [previous post](https://www.reddit.com/r/LocalLLaMA/comments/1luiigi/tool_release_finetune_quantize_13b_llms_on_8gb/) â€” the repo is **finally public**!\\n\\n# ðŸ”¥ TL;DR\\n\\n* **GitHub**: [diptanshu1991/LoFT](https://github.com/diptanshu1991/LoFT)\\n* **What you get**: 5 CLI commands: `loft finetune`, `merge`, `export`, `quantize`, `chat`\\n* **Hardware**: Tested on 8GB MacBook Air  â€” peak RAM **330MB**\\n* **Performance**: 300 Dolly samples, 2 epochs â†’ 1.5 hrs total wall-time\\n* **Inference speed**: 6.9 tok/sec (Q4\\\\_0) on CPU\\n* **License**: MIT â€“ 100% open-source\\n\\n# ðŸ§  What is LoFT?\\n\\n**LoFT CLI** is a lightweight, CPU-friendly toolkit that lets you:\\n\\n* âœ… Finetune 1â€“3B LLMs like TinyLlama using **QLoRA**\\n* ðŸ”„ Merge and export models to **GGUF**\\n* ðŸ§± Quantize models (Q4\\\\_0, Q5\\\\_1, etc.)\\n* ðŸ’¬ Run **offline inference** using `llama.cpp`\\n\\nAll from a **command-line interface** on your **local laptop**. No Colab. No GPUs. No cloud.\\n\\n# ðŸ“Š Benchmarks (8GB MacBook Air)\\n\\n|Step|Output|Size|Peak RAM|Time|\\n|:-|:-|:-|:-|:-|\\n|Finetune|LoRA Adapter|4.3 MB|308 MB|23 min|\\n|Merge|HF Model|4.2 GB|322 MB|4.7 min|\\n|Export|GGUF (FP16)|2.1 GB|322 MB|83 sec|\\n|Quantize|GGUF (Q4\\\\_0)|607 MB|322 MB|21 sec|\\n|Chat|6.9 tok/sec|â€“|322 MB|79 sec|\\n\\nðŸ§ª Trained on: 300 Dolly samples, 2 epochs â†’ loss &lt; 1.0\\n\\n# ðŸ§ª 5-Command Lifecycle\\n\\nLoFT runs the complete LLM workflow â€” from training to chat â€” in just 5 commands:\\n\\n    loft finetune  \\n    loft merge  \\n    loft export  \\n    loft quantize  \\n    loft chat\\n\\n&gt;\\n\\n# ðŸ§ª Coming Soon in LoFT\\n\\n**ðŸ“¦ Plug-and-Play Recipes**\\n\\n* Legal Q&amp;A bots (air-gapped, offline)\\n* Customer support assistants\\n* Contract summarizers\\n\\n**ðŸŒ± Early Experiments**\\n\\n* Multi-turn finetuning\\n* Adapter-sharing for niche domains\\n* Dataset templating tools\\n\\n&gt;\\n\\nLoFT is built for indie builders, researchers, and OSS devs who want **local GenAI without GPU constraints**. Would love your feedback on:\\n\\n* What models/datasets you would like to see supported next\\n* Edge cases or bugs during install/training\\n* Use cases where this unlocks new workflows\\n\\nðŸ”— GitHub: [https://github.com/diptanshu1991/LoFT](https://github.com/diptanshu1991/LoFT)  \\nðŸªª MIT licensed â€” feel free to fork, contribute, and ship your own CLI tools on top","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"ðŸ“¢ [RELEASE] LoFT CLI: Fine-tune &amp; Deploy LLMs on CPU (8GB RAM, No GPU, No Cloud)","link_flair_richtext":[{"e":"text","t":"New Model"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":true,"name":"t3_1m1aj8n","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.93,"author_flair_background_color":null,"subreddit_type":"public","ups":12,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_46jj4viw","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"New Model","can_mod_post":false,"score":12,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752666706,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Update to my &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1luiigi/tool_release_finetune_quantize_13b_llms_on_8gb/\\"&gt;previous post&lt;/a&gt; â€” the repo is &lt;strong&gt;finally public&lt;/strong&gt;!&lt;/p&gt;\\n\\n&lt;h1&gt;ðŸ”¥ TL;DR&lt;/h1&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;strong&gt;GitHub&lt;/strong&gt;: &lt;a href=\\"https://github.com/diptanshu1991/LoFT\\"&gt;diptanshu1991/LoFT&lt;/a&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;What you get&lt;/strong&gt;: 5 CLI commands: &lt;code&gt;loft finetune&lt;/code&gt;, &lt;code&gt;merge&lt;/code&gt;, &lt;code&gt;export&lt;/code&gt;, &lt;code&gt;quantize&lt;/code&gt;, &lt;code&gt;chat&lt;/code&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Hardware&lt;/strong&gt;: Tested on 8GB MacBook Air  â€” peak RAM &lt;strong&gt;330MB&lt;/strong&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt;: 300 Dolly samples, 2 epochs â†’ 1.5 hrs total wall-time&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Inference speed&lt;/strong&gt;: 6.9 tok/sec (Q4_0) on CPU&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;License&lt;/strong&gt;: MIT â€“ 100% open-source&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;h1&gt;ðŸ§  What is LoFT?&lt;/h1&gt;\\n\\n&lt;p&gt;&lt;strong&gt;LoFT CLI&lt;/strong&gt; is a lightweight, CPU-friendly toolkit that lets you:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;âœ… Finetune 1â€“3B LLMs like TinyLlama using &lt;strong&gt;QLoRA&lt;/strong&gt;&lt;/li&gt;\\n&lt;li&gt;ðŸ”„ Merge and export models to &lt;strong&gt;GGUF&lt;/strong&gt;&lt;/li&gt;\\n&lt;li&gt;ðŸ§± Quantize models (Q4_0, Q5_1, etc.)&lt;/li&gt;\\n&lt;li&gt;ðŸ’¬ Run &lt;strong&gt;offline inference&lt;/strong&gt; using &lt;code&gt;llama.cpp&lt;/code&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;All from a &lt;strong&gt;command-line interface&lt;/strong&gt; on your &lt;strong&gt;local laptop&lt;/strong&gt;. No Colab. No GPUs. No cloud.&lt;/p&gt;\\n\\n&lt;h1&gt;ðŸ“Š Benchmarks (8GB MacBook Air)&lt;/h1&gt;\\n\\n&lt;table&gt;&lt;thead&gt;\\n&lt;tr&gt;\\n&lt;th align=\\"left\\"&gt;Step&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;Output&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;Size&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;Peak RAM&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;Time&lt;/th&gt;\\n&lt;/tr&gt;\\n&lt;/thead&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;Finetune&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;LoRA Adapter&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;4.3 MB&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;308 MB&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;23 min&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;Merge&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;HF Model&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;4.2 GB&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;322 MB&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;4.7 min&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;Export&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;GGUF (FP16)&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;2.1 GB&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;322 MB&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;83 sec&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;Quantize&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;GGUF (Q4_0)&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;607 MB&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;322 MB&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;21 sec&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;Chat&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;6.9 tok/sec&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;â€“&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;322 MB&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;79 sec&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;/tbody&gt;&lt;/table&gt;\\n\\n&lt;p&gt;ðŸ§ª Trained on: 300 Dolly samples, 2 epochs â†’ loss &amp;lt; 1.0&lt;/p&gt;\\n\\n&lt;h1&gt;ðŸ§ª 5-Command Lifecycle&lt;/h1&gt;\\n\\n&lt;p&gt;LoFT runs the complete LLM workflow â€” from training to chat â€” in just 5 commands:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;loft finetune  \\nloft merge  \\nloft export  \\nloft quantize  \\nloft chat\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;blockquote&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;h1&gt;ðŸ§ª Coming Soon in LoFT&lt;/h1&gt;\\n\\n&lt;p&gt;&lt;strong&gt;ðŸ“¦ Plug-and-Play Recipes&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Legal Q&amp;amp;A bots (air-gapped, offline)&lt;/li&gt;\\n&lt;li&gt;Customer support assistants&lt;/li&gt;\\n&lt;li&gt;Contract summarizers&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;&lt;strong&gt;ðŸŒ± Early Experiments&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Multi-turn finetuning&lt;/li&gt;\\n&lt;li&gt;Adapter-sharing for niche domains&lt;/li&gt;\\n&lt;li&gt;Dataset templating tools&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;blockquote&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;LoFT is built for indie builders, researchers, and OSS devs who want &lt;strong&gt;local GenAI without GPU constraints&lt;/strong&gt;. Would love your feedback on:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;What models/datasets you would like to see supported next&lt;/li&gt;\\n&lt;li&gt;Edge cases or bugs during install/training&lt;/li&gt;\\n&lt;li&gt;Use cases where this unlocks new workflows&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;ðŸ”— GitHub: &lt;a href=\\"https://github.com/diptanshu1991/LoFT\\"&gt;https://github.com/diptanshu1991/LoFT&lt;/a&gt;&lt;br/&gt;\\nðŸªª MIT licensed â€” feel free to fork, contribute, and ship your own CLI tools on top&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ced98442-f5d3-11ed-b657-66d3b15490c6","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#ffb000","id":"1m1aj8n","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"diptanshu1991","discussion_type":null,"num_comments":4,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m1aj8n/release_loft_cli_finetune_deploy_llms_on_cpu_8gb/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m1aj8n/release_loft_cli_finetune_deploy_llms_on_cpu_8gb/","subreddit_subscribers":499773,"created_utc":1752666706,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3fpnyb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"diptanshu1991","can_mod_post":false,"created_utc":1752669375,"send_replies":true,"parent_id":"t1_n3fkz66","score":1,"author_fullname":"t2_46jj4viw","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks! Appreciate you bookmarking it ðŸ™Œ  \\nWould love to hear how it performs on your setup when you get a chance, especially if you try a different model or dataset.\\n\\nIâ€™ll be shipping the first LoFT Recipe soon, so feel free to follow the repo or drop a use case you\'d like to see!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3fpnyb","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks! Appreciate you bookmarking it ðŸ™Œ&lt;br/&gt;\\nWould love to hear how it performs on your setup when you get a chance, especially if you try a different model or dataset.&lt;/p&gt;\\n\\n&lt;p&gt;Iâ€™ll be shipping the first LoFT Recipe soon, so feel free to follow the repo or drop a use case you&amp;#39;d like to see!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m1aj8n","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1aj8n/release_loft_cli_finetune_deploy_llms_on_cpu_8gb/n3fpnyb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752669375,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3fkz66","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"dugavo","can_mod_post":false,"created_utc":1752667620,"send_replies":true,"parent_id":"t3_1m1aj8n","score":2,"author_fullname":"t2_1nge67um4h","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Looks cool! I bookmarked this, will try it later.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3fkz66","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Looks cool! I bookmarked this, will try it later.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1aj8n/release_loft_cli_finetune_deploy_llms_on_cpu_8gb/n3fkz66/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752667620,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m1aj8n","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3fmuyl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"bhupesh-g","can_mod_post":false,"created_utc":1752668336,"send_replies":true,"parent_id":"t3_1m1aj8n","score":2,"author_fullname":"t2_182usrqcei","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"loved it, thanks for it. Will wait for plug-and-play recipes, they looks promising","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3fmuyl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;loved it, thanks for it. Will wait for plug-and-play recipes, they looks promising&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1aj8n/release_loft_cli_finetune_deploy_llms_on_cpu_8gb/n3fmuyl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752668336,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m1aj8n","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]'),r=()=>t.jsx(l,{data:e});export{r as default};
