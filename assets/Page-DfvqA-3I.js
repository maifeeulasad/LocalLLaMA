import{j as e}from"./index-DQXiEb7D.js";import{R as l}from"./RedditPostRenderer-BjndLgq8.js";import"./index-B-ILyjT1.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"is tensor parallel the problem? im not sure what i do wrong, here are server logs for when i run 50k token prompt\\n\\n    2025-07-09 21:17:10.781 | [GIN] 2025/07/09 - 19:17:10 | 200 | 27.813µs |      172.18.0.1 | GET \\"/api/version\\"\\n    2025-07-09 21:17:22.229 | time=2025-07-09T19:17:22.229Z level=WARN source=sched.go:687 msg=\\"gpu VRAM usage didn't recover within timeout\\" seconds=5.000059067 runner.size=\\"25.2 GiB\\" runner.vram=\\"25.2 GiB\\" runner.parallel=1 runner.pid=122 runner.model=/root/.ollama/models/blobs/sha256-ccc0cddac56136ef0969cf2e3e9ac051124c937be42503b47ec570dead85ff87\\n    2025-07-09 21:17:22.480 | time=2025-07-09T19:17:22.480Z level=WARN source=sched.go:687 msg=\\"gpu VRAM usage didn't recover within timeout\\" seconds=5.250843748 runner.size=\\"25.2 GiB\\" runner.vram=\\"25.2 GiB\\" runner.parallel=1 runner.pid=122 runner.model=/root/.ollama/models/blobs/sha256-ccc0cddac56136ef0969cf2e3e9ac051124c937be42503b47ec570dead85ff87\\n    2025-07-09 21:17:22.896 | time=2025-07-09T19:17:22.896Z level=WARN source=sched.go:687 msg=\\"gpu VRAM usage didn't recover within timeout\\" seconds=5.667126536 runner.size=\\"25.2 GiB\\" runner.vram=\\"25.2 GiB\\" runner.parallel=1 runner.pid=122 runner.model=/root/.ollama/models/blobs/sha256-ccc0cddac56136ef0969cf2e3e9ac051124c937be42503b47ec570dead85ff87\\n    2025-07-09 21:17:24.522 | time=2025-07-09T19:17:24.521Z level=INFO source=server.go:135 msg=\\"system memory\\" total=\\"86.3 GiB\\" free=\\"77.9 GiB\\" free_swap=\\"0 B\\"\\n    2025-07-09 21:17:24.778 | time=2025-07-09T19:17:24.778Z level=INFO source=server.go:175 msg=offload library=cuda layers.requested=256 layers.model=63 layers.offload=63 layers.split=32,31 memory.available=\\"[22.8 GiB 22.8 GiB]\\" memory.gpu_overhead=\\"0 B\\" memory.required.full=\\"30.8 GiB\\" memory.required.partial=\\"30.8 GiB\\" memory.required.kv=\\"2.8 GiB\\" memory.required.allocations=\\"[16.4 GiB 14.4 GiB]\\" memory.weights.total=\\"16.0 GiB\\" memory.weights.repeating=\\"13.4 GiB\\" memory.weights.nonrepeating=\\"2.6 GiB\\" memory.graph.full=\\"4.4 GiB\\" memory.graph.partial=\\"4.4 GiB\\" projector.weights=\\"806.2 MiB\\" projector.graph=\\"1.0 GiB\\"\\n    2025-07-09 21:17:24.778 | time=2025-07-09T19:17:24.778Z level=INFO source=server.go:218 msg=\\"enabling flash attention\\"\\n    2025-07-09 21:17:24.815 | time=2025-07-09T19:17:24.815Z level=INFO source=server.go:438 msg=\\"starting llama server\\" cmd=\\"/usr/bin/ollama runner --ollama-engine --model /root/.ollama/models/blobs/sha256-ccc0cddac56136ef0969cf2e3e9ac051124c937be42503b47ec570dead85ff87 --ctx-size 65536 --batch-size 512 --n-gpu-layers 256 --threads 3 --flash-attn --kv-cache-type q8_0 --parallel 1 --tensor-split 32,31 --port 35413\\"\\n    2025-07-09 21:17:24.815 | time=2025-07-09T19:17:24.815Z level=INFO source=sched.go:483 msg=\\"loaded runners\\" count=1\\n    2025-07-09 21:17:24.815 | time=2025-07-09T19:17:24.815Z level=INFO source=server.go:598 msg=\\"waiting for llama runner to start responding\\"\\n    2025-07-09 21:17:24.815 | time=2025-07-09T19:17:24.815Z level=INFO source=server.go:632 msg=\\"waiting for server to become available\\" status=\\"llm server not responding\\"\\n    2025-07-09 21:17:24.825 | time=2025-07-09T19:17:24.825Z level=INFO source=runner.go:925 msg=\\"starting ollama engine\\"\\n    2025-07-09 21:17:24.833 | time=2025-07-09T19:17:24.833Z level=INFO source=runner.go:983 msg=\\"Server listening on 127.0.0.1:35413\\"\\n    2025-07-09 21:17:24.866 | time=2025-07-09T19:17:24.866Z level=INFO source=ggml.go:92 msg=\\"\\" architecture=gemma3 file_type=Q4_0 name=\\"\\" description=\\"\\" num_tensors=1247 num_key_values=40\\n    2025-07-09 21:17:24.914 | ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no\\n    2025-07-09 21:17:24.914 | ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\\n    2025-07-09 21:17:24.914 | ggml_cuda_init: found 2 CUDA devices:\\n    2025-07-09 21:17:24.914 | Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\\n    2025-07-09 21:17:24.914 | Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\\n    2025-07-09 21:17:25.013 | load_backend: loaded CUDA backend from /usr/lib/ollama/libggml-cuda.so\\n    2025-07-09 21:17:25.016 | load_backend: loaded CPU backend from /usr/lib/ollama/libggml-cpu-haswell.so\\n    2025-07-09 21:17:25.016 | time=2025-07-09T19:17:25.016Z level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 CUDA.1.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.1.USE_GRAPHS=1 CUDA.1.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)\\n    2025-07-09 21:17:25.067 | time=2025-07-09T19:17:25.066Z level=INFO source=server.go:632 msg=\\"waiting for server to become available\\" status=\\"llm server loading model\\"\\n    2025-07-09 21:17:25.187 | time=2025-07-09T19:17:25.187Z level=INFO source=ggml.go:362 msg=\\"model weights\\" buffer=CPU size=\\"2.6 GiB\\"\\n    2025-07-09 21:17:25.187 | time=2025-07-09T19:17:25.187Z level=INFO source=ggml.go:362 msg=\\"model weights\\" buffer=CUDA0 size=\\"6.9 GiB\\"\\n    2025-07-09 21:17:25.187 | time=2025-07-09T19:17:25.187Z level=INFO source=ggml.go:362 msg=\\"model weights\\" buffer=CUDA1 size=\\"9.9 GiB\\"\\n    2025-07-09 21:17:25.335 | time=2025-07-09T19:17:25.335Z level=INFO source=ggml.go:651 msg=\\"compute graph\\" backend=CUDA0 buffer_type=CUDA0 size=\\"0 B\\"\\n    2025-07-09 21:17:25.335 | time=2025-07-09T19:17:25.335Z level=INFO source=ggml.go:651 msg=\\"compute graph\\" backend=CUDA1 buffer_type=CUDA1 size=\\"1.1 GiB\\"\\n    2025-07-09 21:17:25.335 | time=2025-07-09T19:17:25.335Z level=INFO source=ggml.go:651 msg=\\"compute graph\\" backend=CPU buffer_type=CPU size=\\"0 B\\"\\n    2025-07-09 21:17:25.520 | time=2025-07-09T19:17:25.520Z level=INFO source=ggml.go:651 msg=\\"compute graph\\" backend=CUDA0 buffer_type=CUDA0 size=\\"456.5 MiB\\"\\n    2025-07-09 21:17:25.520 | time=2025-07-09T19:17:25.520Z level=INFO source=ggml.go:651 msg=\\"compute graph\\" backend=CUDA1 buffer_type=CUDA1 size=\\"1.1 GiB\\"\\n    2025-07-09 21:17:25.520 | time=2025-07-09T19:17:25.520Z level=INFO source=ggml.go:651 msg=\\"compute graph\\" backend=CPU buffer_type=CPU size=\\"10.5 MiB\\"\\n    2025-07-09 21:17:29.329 | time=2025-07-09T19:17:29.329Z level=INFO source=server.go:637 msg=\\"llama runner started in 4.51 seconds\\"\\n\\nthank you very much for your attention","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"2x3090, Ollama: gemma3:27b-it-qat keeps partial offloading to cpu","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lvs37w","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.55,"author_flair_background_color":null,"subreddit_type":"public","ups":1,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_7eqdugsy","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":1,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752089802,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;is tensor parallel the problem? im not sure what i do wrong, here are server logs for when i run 50k token prompt&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;2025-07-09 21:17:10.781 | [GIN] 2025/07/09 - 19:17:10 | 200 | 27.813µs |      172.18.0.1 | GET &amp;quot;/api/version&amp;quot;\\n2025-07-09 21:17:22.229 | time=2025-07-09T19:17:22.229Z level=WARN source=sched.go:687 msg=&amp;quot;gpu VRAM usage didn&amp;#39;t recover within timeout&amp;quot; seconds=5.000059067 runner.size=&amp;quot;25.2 GiB&amp;quot; runner.vram=&amp;quot;25.2 GiB&amp;quot; runner.parallel=1 runner.pid=122 runner.model=/root/.ollama/models/blobs/sha256-ccc0cddac56136ef0969cf2e3e9ac051124c937be42503b47ec570dead85ff87\\n2025-07-09 21:17:22.480 | time=2025-07-09T19:17:22.480Z level=WARN source=sched.go:687 msg=&amp;quot;gpu VRAM usage didn&amp;#39;t recover within timeout&amp;quot; seconds=5.250843748 runner.size=&amp;quot;25.2 GiB&amp;quot; runner.vram=&amp;quot;25.2 GiB&amp;quot; runner.parallel=1 runner.pid=122 runner.model=/root/.ollama/models/blobs/sha256-ccc0cddac56136ef0969cf2e3e9ac051124c937be42503b47ec570dead85ff87\\n2025-07-09 21:17:22.896 | time=2025-07-09T19:17:22.896Z level=WARN source=sched.go:687 msg=&amp;quot;gpu VRAM usage didn&amp;#39;t recover within timeout&amp;quot; seconds=5.667126536 runner.size=&amp;quot;25.2 GiB&amp;quot; runner.vram=&amp;quot;25.2 GiB&amp;quot; runner.parallel=1 runner.pid=122 runner.model=/root/.ollama/models/blobs/sha256-ccc0cddac56136ef0969cf2e3e9ac051124c937be42503b47ec570dead85ff87\\n2025-07-09 21:17:24.522 | time=2025-07-09T19:17:24.521Z level=INFO source=server.go:135 msg=&amp;quot;system memory&amp;quot; total=&amp;quot;86.3 GiB&amp;quot; free=&amp;quot;77.9 GiB&amp;quot; free_swap=&amp;quot;0 B&amp;quot;\\n2025-07-09 21:17:24.778 | time=2025-07-09T19:17:24.778Z level=INFO source=server.go:175 msg=offload library=cuda layers.requested=256 layers.model=63 layers.offload=63 layers.split=32,31 memory.available=&amp;quot;[22.8 GiB 22.8 GiB]&amp;quot; memory.gpu_overhead=&amp;quot;0 B&amp;quot; memory.required.full=&amp;quot;30.8 GiB&amp;quot; memory.required.partial=&amp;quot;30.8 GiB&amp;quot; memory.required.kv=&amp;quot;2.8 GiB&amp;quot; memory.required.allocations=&amp;quot;[16.4 GiB 14.4 GiB]&amp;quot; memory.weights.total=&amp;quot;16.0 GiB&amp;quot; memory.weights.repeating=&amp;quot;13.4 GiB&amp;quot; memory.weights.nonrepeating=&amp;quot;2.6 GiB&amp;quot; memory.graph.full=&amp;quot;4.4 GiB&amp;quot; memory.graph.partial=&amp;quot;4.4 GiB&amp;quot; projector.weights=&amp;quot;806.2 MiB&amp;quot; projector.graph=&amp;quot;1.0 GiB&amp;quot;\\n2025-07-09 21:17:24.778 | time=2025-07-09T19:17:24.778Z level=INFO source=server.go:218 msg=&amp;quot;enabling flash attention&amp;quot;\\n2025-07-09 21:17:24.815 | time=2025-07-09T19:17:24.815Z level=INFO source=server.go:438 msg=&amp;quot;starting llama server&amp;quot; cmd=&amp;quot;/usr/bin/ollama runner --ollama-engine --model /root/.ollama/models/blobs/sha256-ccc0cddac56136ef0969cf2e3e9ac051124c937be42503b47ec570dead85ff87 --ctx-size 65536 --batch-size 512 --n-gpu-layers 256 --threads 3 --flash-attn --kv-cache-type q8_0 --parallel 1 --tensor-split 32,31 --port 35413&amp;quot;\\n2025-07-09 21:17:24.815 | time=2025-07-09T19:17:24.815Z level=INFO source=sched.go:483 msg=&amp;quot;loaded runners&amp;quot; count=1\\n2025-07-09 21:17:24.815 | time=2025-07-09T19:17:24.815Z level=INFO source=server.go:598 msg=&amp;quot;waiting for llama runner to start responding&amp;quot;\\n2025-07-09 21:17:24.815 | time=2025-07-09T19:17:24.815Z level=INFO source=server.go:632 msg=&amp;quot;waiting for server to become available&amp;quot; status=&amp;quot;llm server not responding&amp;quot;\\n2025-07-09 21:17:24.825 | time=2025-07-09T19:17:24.825Z level=INFO source=runner.go:925 msg=&amp;quot;starting ollama engine&amp;quot;\\n2025-07-09 21:17:24.833 | time=2025-07-09T19:17:24.833Z level=INFO source=runner.go:983 msg=&amp;quot;Server listening on 127.0.0.1:35413&amp;quot;\\n2025-07-09 21:17:24.866 | time=2025-07-09T19:17:24.866Z level=INFO source=ggml.go:92 msg=&amp;quot;&amp;quot; architecture=gemma3 file_type=Q4_0 name=&amp;quot;&amp;quot; description=&amp;quot;&amp;quot; num_tensors=1247 num_key_values=40\\n2025-07-09 21:17:24.914 | ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no\\n2025-07-09 21:17:24.914 | ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\\n2025-07-09 21:17:24.914 | ggml_cuda_init: found 2 CUDA devices:\\n2025-07-09 21:17:24.914 | Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\\n2025-07-09 21:17:24.914 | Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\\n2025-07-09 21:17:25.013 | load_backend: loaded CUDA backend from /usr/lib/ollama/libggml-cuda.so\\n2025-07-09 21:17:25.016 | load_backend: loaded CPU backend from /usr/lib/ollama/libggml-cpu-haswell.so\\n2025-07-09 21:17:25.016 | time=2025-07-09T19:17:25.016Z level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 CUDA.1.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.1.USE_GRAPHS=1 CUDA.1.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)\\n2025-07-09 21:17:25.067 | time=2025-07-09T19:17:25.066Z level=INFO source=server.go:632 msg=&amp;quot;waiting for server to become available&amp;quot; status=&amp;quot;llm server loading model&amp;quot;\\n2025-07-09 21:17:25.187 | time=2025-07-09T19:17:25.187Z level=INFO source=ggml.go:362 msg=&amp;quot;model weights&amp;quot; buffer=CPU size=&amp;quot;2.6 GiB&amp;quot;\\n2025-07-09 21:17:25.187 | time=2025-07-09T19:17:25.187Z level=INFO source=ggml.go:362 msg=&amp;quot;model weights&amp;quot; buffer=CUDA0 size=&amp;quot;6.9 GiB&amp;quot;\\n2025-07-09 21:17:25.187 | time=2025-07-09T19:17:25.187Z level=INFO source=ggml.go:362 msg=&amp;quot;model weights&amp;quot; buffer=CUDA1 size=&amp;quot;9.9 GiB&amp;quot;\\n2025-07-09 21:17:25.335 | time=2025-07-09T19:17:25.335Z level=INFO source=ggml.go:651 msg=&amp;quot;compute graph&amp;quot; backend=CUDA0 buffer_type=CUDA0 size=&amp;quot;0 B&amp;quot;\\n2025-07-09 21:17:25.335 | time=2025-07-09T19:17:25.335Z level=INFO source=ggml.go:651 msg=&amp;quot;compute graph&amp;quot; backend=CUDA1 buffer_type=CUDA1 size=&amp;quot;1.1 GiB&amp;quot;\\n2025-07-09 21:17:25.335 | time=2025-07-09T19:17:25.335Z level=INFO source=ggml.go:651 msg=&amp;quot;compute graph&amp;quot; backend=CPU buffer_type=CPU size=&amp;quot;0 B&amp;quot;\\n2025-07-09 21:17:25.520 | time=2025-07-09T19:17:25.520Z level=INFO source=ggml.go:651 msg=&amp;quot;compute graph&amp;quot; backend=CUDA0 buffer_type=CUDA0 size=&amp;quot;456.5 MiB&amp;quot;\\n2025-07-09 21:17:25.520 | time=2025-07-09T19:17:25.520Z level=INFO source=ggml.go:651 msg=&amp;quot;compute graph&amp;quot; backend=CUDA1 buffer_type=CUDA1 size=&amp;quot;1.1 GiB&amp;quot;\\n2025-07-09 21:17:25.520 | time=2025-07-09T19:17:25.520Z level=INFO source=ggml.go:651 msg=&amp;quot;compute graph&amp;quot; backend=CPU buffer_type=CPU size=&amp;quot;10.5 MiB&amp;quot;\\n2025-07-09 21:17:29.329 | time=2025-07-09T19:17:29.329Z level=INFO source=server.go:637 msg=&amp;quot;llama runner started in 4.51 seconds&amp;quot;\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;thank you very much for your attention&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lvs37w","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Sea_Calendar_3912","discussion_type":null,"num_comments":7,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lvs37w/2x3090_ollama_gemma327bitqat_keeps_partial/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lvs37w/2x3090_ollama_gemma327bitqat_keeps_partial/","subreddit_subscribers":497023,"created_utc":1752089802,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n28xfge","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"coolestmage","can_mod_post":false,"created_utc":1752094872,"send_replies":true,"parent_id":"t1_n28jin9","score":1,"author_fullname":"t2_6dtdz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"https://github.com/ollama/ollama/pull/11090\\nI've tested this and it is working much better.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n28xfge","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://github.com/ollama/ollama/pull/11090\\"&gt;https://github.com/ollama/ollama/pull/11090&lt;/a&gt;\\nI&amp;#39;ve tested this and it is working much better.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvs37w","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvs37w/2x3090_ollama_gemma327bitqat_keeps_partial/n28xfge/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752094872,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n28jin9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"maglat","can_mod_post":false,"created_utc":1752091060,"send_replies":true,"parent_id":"t3_1lvs37w","score":6,"author_fullname":"t2_clzcglj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"the ollama memory management is super bugged. the ollama team call it conservative. it estimates way to much memory usage as required. same story with mistral-small-3.2 for example.\\n\\nMistral 3.2 with a context of 20k require 31GB with ollama. Using the same model and quant with a 45k context in llama cpp require 29GB of vram.\\n\\nwhen you look into the issue tab of the ollama git, there are alot of post regarding this memory allocation issue. \\n\\nlower your context is a option or move to llama cpp.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n28jin9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;the ollama memory management is super bugged. the ollama team call it conservative. it estimates way to much memory usage as required. same story with mistral-small-3.2 for example.&lt;/p&gt;\\n\\n&lt;p&gt;Mistral 3.2 with a context of 20k require 31GB with ollama. Using the same model and quant with a 45k context in llama cpp require 29GB of vram.&lt;/p&gt;\\n\\n&lt;p&gt;when you look into the issue tab of the ollama git, there are alot of post regarding this memory allocation issue. &lt;/p&gt;\\n\\n&lt;p&gt;lower your context is a option or move to llama cpp.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvs37w/2x3090_ollama_gemma327bitqat_keeps_partial/n28jin9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752091060,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvs37w","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n28o4a3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Lissanro","can_mod_post":false,"created_utc":1752092334,"send_replies":true,"parent_id":"t3_1lvs37w","score":1,"author_fullname":"t2_fpfao9g","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It is good idea to use TabbyAPI and EXL2 quants when you can fully fit the model in VRAM if you care about performance. It also supports Q6 context cache which has quality about as good as Q8 but takes less VRAM, and have well implemented auto-split for multi-GPU rigs.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n28o4a3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It is good idea to use TabbyAPI and EXL2 quants when you can fully fit the model in VRAM if you care about performance. It also supports Q6 context cache which has quality about as good as Q8 but takes less VRAM, and have well implemented auto-split for multi-GPU rigs.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvs37w/2x3090_ollama_gemma327bitqat_keeps_partial/n28o4a3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752092334,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvs37w","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n29ygv2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Secure_Reflection409","can_mod_post":false,"created_utc":1752106482,"send_replies":true,"parent_id":"t1_n28pezh","score":1,"author_fullname":"t2_by77ogdhr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Just download Lmstudio, lol.\\n\\n\\nAll these ollama black box headaches disappear and suddenly you know exactly what you can and can't do.\\n\\n\\nEasy.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n29ygv2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just download Lmstudio, lol.&lt;/p&gt;\\n\\n&lt;p&gt;All these ollama black box headaches disappear and suddenly you know exactly what you can and can&amp;#39;t do.&lt;/p&gt;\\n\\n&lt;p&gt;Easy.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvs37w","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvs37w/2x3090_ollama_gemma327bitqat_keeps_partial/n29ygv2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752106482,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n28pezh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Threatening-Silence-","can_mod_post":false,"created_utc":1752092689,"send_replies":true,"parent_id":"t3_1lvs37w","score":2,"author_fullname":"t2_15wqsifdjf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You've hit the limits of the ollama wrapper and you'll now need to learn to use llama-cpp.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n28pezh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You&amp;#39;ve hit the limits of the ollama wrapper and you&amp;#39;ll now need to learn to use llama-cpp.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvs37w/2x3090_ollama_gemma327bitqat_keeps_partial/n28pezh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752092689,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvs37w","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n28zw4w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"EmilPi","can_mod_post":false,"created_utc":1752095555,"send_replies":true,"parent_id":"t3_1lvs37w","score":2,"author_fullname":"t2_jti45lwl","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"1. Have CUDA and git installed.\\n2. Go to [https://github.com/ggml-org/llama.cpp/releases](https://github.com/ggml-org/llama.cpp/releases)\\n3. Find you OS .zip file, download it\\n4. Unpack it to the folder of your choice\\n5. At the same folder level, download Gemma 3 27B QAT Q4\\\\_0: \`git clone\` [\`https://huggingface.co/google/gemma-3-27b-it-qat-q4_0-gguf\`](https://huggingface.co/google/gemma-3-27b-it-qat-q4_0-gguf)\\n6. Run command (for Linux, your slashes/extension may vary for Windows) and enjoy 128k context window for 3 parallel requests at once:\\n\\n&amp;#8203;\\n\\n    ./build/bin/llama-server --host localhost --port 1234  --model ./gemma-3-27b-it-qat-q4_0-gguf/gemma-3-27b-it-q4_0.gguf  --mmproj ./gemma-3-27b-it-qat-q4_0-gguf/mmproj-model-f16-27B.gguf  --alias Gemma3-27B-VISION-128k --parallel 3 -c 393216 -fa -ctv q8_0 -ctk q8_0 --ngl 999 -ts 30,31","edited":1752095987,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n28zw4w","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;ol&gt;\\n&lt;li&gt;Have CUDA and git installed.&lt;/li&gt;\\n&lt;li&gt;Go to &lt;a href=\\"https://github.com/ggml-org/llama.cpp/releases\\"&gt;https://github.com/ggml-org/llama.cpp/releases&lt;/a&gt;&lt;/li&gt;\\n&lt;li&gt;Find you OS .zip file, download it&lt;/li&gt;\\n&lt;li&gt;Unpack it to the folder of your choice&lt;/li&gt;\\n&lt;li&gt;At the same folder level, download Gemma 3 27B QAT Q4_0: &lt;code&gt;git clone&lt;/code&gt; &lt;a href=\\"https://huggingface.co/google/gemma-3-27b-it-qat-q4_0-gguf\\"&gt;&lt;code&gt;https://huggingface.co/google/gemma-3-27b-it-qat-q4_0-gguf&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;\\n&lt;li&gt;Run command (for Linux, your slashes/extension may vary for Windows) and enjoy 128k context window for 3 parallel requests at once:&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;&amp;#8203;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;./build/bin/llama-server --host localhost --port 1234  --model ./gemma-3-27b-it-qat-q4_0-gguf/gemma-3-27b-it-q4_0.gguf  --mmproj ./gemma-3-27b-it-qat-q4_0-gguf/mmproj-model-f16-27B.gguf  --alias Gemma3-27B-VISION-128k --parallel 3 -c 393216 -fa -ctv q8_0 -ctk q8_0 --ngl 999 -ts 30,31\\n&lt;/code&gt;&lt;/pre&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvs37w/2x3090_ollama_gemma327bitqat_keeps_partial/n28zw4w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752095555,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvs37w","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2bd3uh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"chibop1","can_mod_post":false,"created_utc":1752126247,"send_replies":true,"parent_id":"t3_1lvs37w","score":1,"author_fullname":"t2_e9jh97s","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Ollama often overestimates the memory requirement, but you can force offload 9999 layers by sending num_gpu parameter via API or creating a modelfile.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2bd3uh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ollama often overestimates the memory requirement, but you can force offload 9999 layers by sending num_gpu parameter via API or creating a modelfile.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvs37w/2x3090_ollama_gemma327bitqat_keeps_partial/n2bd3uh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752126247,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvs37w","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),s=()=>e.jsx(l,{data:a});export{s as default};
