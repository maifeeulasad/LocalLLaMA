import{j as e}from"./index-cvG704yx.js";import{R as l}from"./RedditPostRenderer-CBthLTAH.js";import"./index-D-GavSZU.js";const t=[{kind:"Listing",data:{after:null,dist:1,modhash:"",geo_filter:"",children:[{kind:"t3",data:{approved_at_utc:null,subreddit:"LocalLLaMA",selftext:"Just trying to summon new models by asking the question. Seeing all these new Nemo models coming out makes me wonder if we'll see a pared-down Llama 4 Maverick that's been given the Nemotron treatment. I feel like that may be much harder with MoE architecture, but maybe not.",user_reports:[],saved:!1,mod_reason_title:null,gilded:0,clicked:!1,title:"When Llama4 Nemotron 250B MoE?",link_flair_richtext:[{e:"text",t:"Other"}],subreddit_name_prefixed:"r/LocalLLaMA",hidden:!1,pwls:6,link_flair_css_class:"",downs:0,thumbnail_height:null,top_awarded_type:null,hide_score:!0,name:"t3_1m3nc51",quarantine:!1,link_flair_text_color:"light",upvote_ratio:1,author_flair_background_color:null,subreddit_type:"public",ups:1,total_awards_received:0,media_embed:{},thumbnail_width:null,author_flair_template_id:null,is_original_content:!1,author_fullname:"t2_m78cdz1nv",secure_media:null,is_reddit_media_domain:!1,is_meta:!1,category:null,secure_media_embed:{},link_flair_text:"Other",can_mod_post:!1,score:1,approved_by:null,is_created_from_ads_ui:!1,author_premium:!1,thumbnail:"self",edited:!1,author_flair_css_class:null,author_flair_richtext:[],gildings:{},content_categories:null,is_self:!0,mod_note:null,created:1752899656,link_flair_type:"richtext",wls:6,removed_by_category:null,banned_by:null,author_flair_type:"text",domain:"self.LocalLLaMA",allow_live_comments:!1,selftext_html:`&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just trying to summon new models by asking the question. Seeing all these new Nemo models coming out makes me wonder if we&amp;#39;ll see a pared-down Llama 4 Maverick that&amp;#39;s been given the Nemotron treatment. I feel like that may be much harder with MoE architecture, but maybe not.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;`,likes:null,suggested_sort:null,banned_at_utc:null,view_count:null,archived:!1,no_follow:!0,is_crosspostable:!1,pinned:!1,over_18:!1,all_awardings:[],awarders:[],media_only:!1,link_flair_template_id:"7a7848d2-bf8e-11ed-8c2f-765d15199f78",can_gild:!1,spoiler:!1,locked:!1,author_flair_text:null,treatment_tags:[],visited:!1,removed_by:null,num_reports:null,distinguished:null,subreddit_id:"t5_81eyvm",author_is_blocked:!1,mod_reason_by:null,removal_reason:null,link_flair_background_color:"#94e044",id:"1m3nc51",is_robot_indexable:!0,num_duplicates:0,report_reasons:null,author:"RobotRobotWhatDoUSee",discussion_type:null,num_comments:1,send_replies:!0,media:null,contest_mode:!1,author_patreon_flair:!1,author_flair_text_color:null,permalink:"/r/LocalLLaMA/comments/1m3nc51/when_llama4_nemotron_250b_moe/",stickied:!1,url:"https://www.reddit.com/r/LocalLLaMA/comments/1m3nc51/when_llama4_nemotron_250b_moe/",subreddit_subscribers:501074,created_utc:1752899656,num_crossposts:0,mod_reports:[],is_video:!1}}],before:null}},{kind:"Listing",data:{after:null,dist:null,modhash:"",geo_filter:"",children:[{kind:"t1",data:{subreddit_id:"t5_81eyvm",approved_at_utc:null,author_is_blocked:!1,comment_type:null,awarders:[],mod_reason_by:null,banned_by:null,author_flair_type:"text",total_awards_received:0,subreddit:"LocalLLaMA",author_flair_template_id:null,likes:null,replies:"",user_reports:[],saved:!1,id:"n3y1lhd",banned_at_utc:null,mod_reason_title:null,gilded:0,archived:!1,collapsed_reason_code:null,no_follow:!0,author:"eloquentemu",can_mod_post:!1,created_utc:1752901219,send_replies:!0,parent_id:"t3_1m3nc51",score:2,author_fullname:"t2_lpdsy",approved_by:null,mod_note:null,all_awardings:[],collapsed:!1,body:`I wouldn't hold my breath...  The rumors / reports I've seen blame the mediocre performance of Llama 4 on the core design and its use of chunked attention.  (I feel like I even practically noticed it before I read up on that.)  This would also explain why Behemoth was cancelled.  As a result, I don't know if Nvidia would want to work on refining it knowing there are (probably) insurmountable design issues at the base.

That said, it does sort of seem like they're avoiding MoE, but OTOH it's only gotten popular quite recently, so maybe it's in the pipeline.  I think the more obvious choice would be R1 or V3 but now we also have Kimi and dots.llm1 which both have base models available and good licenses (though the Nemos are mostly off Instruct anyways).`,edited:!1,top_awarded_type:null,author_flair_css_class:null,name:"t1_n3y1lhd",is_submitter:!1,downs:0,author_flair_richtext:[],author_patreon_flair:!1,body_html:`&lt;div class="md"&gt;&lt;p&gt;I wouldn&amp;#39;t hold my breath...  The rumors / reports I&amp;#39;ve seen blame the mediocre performance of Llama 4 on the core design and its use of chunked attention.  (I feel like I even practically noticed it before I read up on that.)  This would also explain why Behemoth was cancelled.  As a result, I don&amp;#39;t know if Nvidia would want to work on refining it knowing there are (probably) insurmountable design issues at the base.&lt;/p&gt;

&lt;p&gt;That said, it does sort of seem like they&amp;#39;re avoiding MoE, but OTOH it&amp;#39;s only gotten popular quite recently, so maybe it&amp;#39;s in the pipeline.  I think the more obvious choice would be R1 or V3 but now we also have Kimi and dots.llm1 which both have base models available and good licenses (though the Nemos are mostly off Instruct anyways).&lt;/p&gt;
&lt;/div&gt;`,removal_reason:null,collapsed_reason:null,distinguished:null,associated_award:null,stickied:!1,author_premium:!1,can_gild:!1,gildings:{},unrepliable_reason:null,author_flair_text_color:null,score_hidden:!1,permalink:"/r/LocalLLaMA/comments/1m3nc51/when_llama4_nemotron_250b_moe/n3y1lhd/",subreddit_type:"public",locked:!1,report_reasons:null,created:1752901219,author_flair_text:null,treatment_tags:[],link_id:"t3_1m3nc51",subreddit_name_prefixed:"r/LocalLLaMA",controversiality:0,depth:0,author_flair_background_color:null,collapsed_because_crowd_control:null,mod_reports:[],num_reports:null,ups:2}}],before:null}}],s=()=>e.jsx(l,{data:t});export{s as default};
