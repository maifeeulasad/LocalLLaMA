import{j as e}from"./index-DLSqWzaI.js";import{R as l}from"./RedditPostRenderer-CysRo2D_.js";import"./index-COXiL3Lo.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Don't get me wrong I am very thankfull for both, but I feel that there would be much to be gained if the projects re-merged. There are very usefull things in both, but the user has to choose: \\"Do I want the better quants or do I want the better infrastructure?\\" I really do think that the mutually missing parts are becoming more and more evident with each passing day. The work on the quants in ik is great, but with all the work which has gone into cpp in all other directions, cpp is really the better product. E.g. take gemma3 vision, that is currently non-functioning in ik, or even if it was functioning, the flag \\"--no-mmproj-offload\\" would still be missing.\\n\\nI don't know what the history of the split was, but really I don't care. I need to assume we're all grown ups here, and looking from outside the two projects fit together perfectly with ik taking care of the technicalities and cpp of the infrastructure.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"I feel that the duality of llama.cpp and ik-llama is worrysome","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m0wji2","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.67,"author_flair_background_color":null,"subreddit_type":"public","ups":16,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_6z7m9i7r","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":16,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752620356,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Don&amp;#39;t get me wrong I am very thankfull for both, but I feel that there would be much to be gained if the projects re-merged. There are very usefull things in both, but the user has to choose: &amp;quot;Do I want the better quants or do I want the better infrastructure?&amp;quot; I really do think that the mutually missing parts are becoming more and more evident with each passing day. The work on the quants in ik is great, but with all the work which has gone into cpp in all other directions, cpp is really the better product. E.g. take gemma3 vision, that is currently non-functioning in ik, or even if it was functioning, the flag &amp;quot;--no-mmproj-offload&amp;quot; would still be missing.&lt;/p&gt;\\n\\n&lt;p&gt;I don&amp;#39;t know what the history of the split was, but really I don&amp;#39;t care. I need to assume we&amp;#39;re all grown ups here, and looking from outside the two projects fit together perfectly with ik taking care of the technicalities and cpp of the infrastructure.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1m0wji2","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"erazortt","discussion_type":null,"num_comments":29,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m0wji2/i_feel_that_the_duality_of_llamacpp_and_ikllama/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m0wji2/i_feel_that_the_duality_of_llamacpp_and_ikllama/","subreddit_subscribers":499774,"created_utc":1752620356,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3dspn6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Marksta","can_mod_post":false,"created_utc":1752635386,"send_replies":true,"parent_id":"t1_n3cq8vv","score":6,"author_fullname":"t2_559a1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt;you can port ik-lcpp's advancements to upstream.\\n\\nThat's really the thing, don't think you can. Re-implement is the better word since yeah, all the underlying APIs keep changing over and over again. I'm not close enough to the project to really judge, but I don't exactly know what's going on architecture wise. llama.cpp mainline has a massive split in just what is available between llama-cli, llama-server, and llama-bench. I don't know why or how all three of these even became different things. Run the inference engine directly in the shell interactively, run it as a server with a port open, run it with a benchmark mode. It sounds like all the same exact thing to me with different flags passed.\\n\\nIt's all really just a sign of building the aircraft while flying the aircraft at the same time. And then I guess now merging these entry points would be even more API churn, so makes sense where that complaint comes from.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dspn6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;you can port ik-lcpp&amp;#39;s advancements to upstream.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;That&amp;#39;s really the thing, don&amp;#39;t think you can. Re-implement is the better word since yeah, all the underlying APIs keep changing over and over again. I&amp;#39;m not close enough to the project to really judge, but I don&amp;#39;t exactly know what&amp;#39;s going on architecture wise. llama.cpp mainline has a massive split in just what is available between llama-cli, llama-server, and llama-bench. I don&amp;#39;t know why or how all three of these even became different things. Run the inference engine directly in the shell interactively, run it as a server with a port open, run it with a benchmark mode. It sounds like all the same exact thing to me with different flags passed.&lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s all really just a sign of building the aircraft while flying the aircraft at the same time. And then I guess now merging these entry points would be even more API churn, so makes sense where that complaint comes from.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0wji2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0wji2/i_feel_that_the_duality_of_llamacpp_and_ikllama/n3dspn6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752635386,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n3cq8vv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Double_Cause4609","can_mod_post":false,"created_utc":1752621841,"send_replies":true,"parent_id":"t3_1m0wji2","score":32,"author_fullname":"t2_1kubzxt2ww","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What it comes down to is that ikrakow was sort of the lead quant engineer of LlamaCPP and disliked a lot of the API churn the project was seeing, so they just didn't want to deal with it.\\n\\nObviously I'd like everything to work together seamlessly, too, but I also think it's not fair to expect someone contributing to open source for free with their own time to deal with the constantly changing API in a big project like LlamaCPP if they don't want to deal with it.\\n\\nIn truth, the problem with a general purpose project is that it has to satisfy a lot of different needs, so a lot of dev time is going to things like vision and speech which aren't necessarily core targets that somebody just interested in high performance LLMs would want to deal with, and even just within LLMs, usually people are interested in a specific type, like small modes, or large dense models, or MoEs, etc.\\n\\nAgain, it's really not fair to expect somebody to contribute to the parts that they don't want to work on.\\n\\nIf you truly think it's that worrysome both projects are open source and you can port ik-lcpp's advancements to upstream.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3cq8vv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What it comes down to is that ikrakow was sort of the lead quant engineer of LlamaCPP and disliked a lot of the API churn the project was seeing, so they just didn&amp;#39;t want to deal with it.&lt;/p&gt;\\n\\n&lt;p&gt;Obviously I&amp;#39;d like everything to work together seamlessly, too, but I also think it&amp;#39;s not fair to expect someone contributing to open source for free with their own time to deal with the constantly changing API in a big project like LlamaCPP if they don&amp;#39;t want to deal with it.&lt;/p&gt;\\n\\n&lt;p&gt;In truth, the problem with a general purpose project is that it has to satisfy a lot of different needs, so a lot of dev time is going to things like vision and speech which aren&amp;#39;t necessarily core targets that somebody just interested in high performance LLMs would want to deal with, and even just within LLMs, usually people are interested in a specific type, like small modes, or large dense models, or MoEs, etc.&lt;/p&gt;\\n\\n&lt;p&gt;Again, it&amp;#39;s really not fair to expect somebody to contribute to the parts that they don&amp;#39;t want to work on.&lt;/p&gt;\\n\\n&lt;p&gt;If you truly think it&amp;#39;s that worrysome both projects are open source and you can port ik-lcpp&amp;#39;s advancements to upstream.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0wji2/i_feel_that_the_duality_of_llamacpp_and_ikllama/n3cq8vv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752621841,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0wji2","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":32}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3fc47v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"createthiscom","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3elq8x","score":2,"author_fullname":"t2_ozxxf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"ik_llama will run the existing quants btw. I didn’t know that until a few days ago.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3fc47v","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;ik_llama will run the existing quants btw. I didn’t know that until a few days ago.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0wji2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0wji2/i_feel_that_the_duality_of_llamacpp_and_ikllama/n3fc47v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752663888,"author_flair_text":null,"treatment_tags":[],"created_utc":1752663888,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3elq8x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"OutrageousMinimum191","can_mod_post":false,"created_utc":1752649259,"send_replies":true,"parent_id":"t1_n3duhus","score":3,"author_fullname":"t2_13ejj8rbqm","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The same. Deepseek Q4 with ik\\\\_llama.cpp runs on my sever with max 11-12 t/s, with llama.cpp with max 10-11 t/s. It's not worth switching to ik\\\\_llama and downloading/making new quants.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3elq8x","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The same. Deepseek Q4 with ik_llama.cpp runs on my sever with max 11-12 t/s, with llama.cpp with max 10-11 t/s. It&amp;#39;s not worth switching to ik_llama and downloading/making new quants.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0wji2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0wji2/i_feel_that_the_duality_of_llamacpp_and_ikllama/n3elq8x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752649259,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n3duhus","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"createthiscom","can_mod_post":false,"created_utc":1752636082,"send_replies":true,"parent_id":"t3_1m0wji2","score":4,"author_fullname":"t2_ozxxf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I haven’t seen anything huge in ik_llama, performance wise, vs llama.cpp. ik_llama seems to have ever so slightly better PP performance and a better benchmark tool. Llama.cpp has better numa performance and better generation performance.\\n\\nI feel like llama.cpp has already gobbled up most of the big performance tweaks.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3duhus","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I haven’t seen anything huge in ik_llama, performance wise, vs llama.cpp. ik_llama seems to have ever so slightly better PP performance and a better benchmark tool. Llama.cpp has better numa performance and better generation performance.&lt;/p&gt;\\n\\n&lt;p&gt;I feel like llama.cpp has already gobbled up most of the big performance tweaks.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0wji2/i_feel_that_the_duality_of_llamacpp_and_ikllama/n3duhus/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752636082,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0wji2","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3crwwu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FullstackSensei","can_mod_post":false,"created_utc":1752622389,"send_replies":true,"parent_id":"t3_1m0wji2","score":19,"author_fullname":"t2_17n3nqtj56","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"There seem to be a lot of assumptions in this post, without much reading to understand what is what.\\n\\nFirst, where did this \\"better quants\\" notion come from? Is there any empirical evidence to back it up? Just because one project supports a quant the other doesn't, doesn't automatically imply the former has better quants than the latter.\\n\\nSecond, I'm not sure where this \\"split\\" notion comes from. Looking at older versions of the ik\\\\_llama.cpp readme, there used to be asection titled \\"Why?\\" and the answer begins with \\"Mostly out of curiosity... Note that I have published some, but not all, of the code in this repository in a series of llamafile PRs.\\" This section has been removed recently, but you don't have to go that far in the project's readme to find it. Some of said llamafile PRs were later contributed by Tunney back into llama.cpp.\\n\\nThird, being a project that started out of personal curiosity, ik\\\\_llama.cpp has a different focus than llama.cpp. Asking the ik\\\\_llama to merge with llama.cpp is asking the dev(s) to put their personal interests aside for your own benefit.\\n\\nFourth, you ***should*** know the history before assuming there was \\"a split.\\" That would have been \\"grown up\\" thing to do. Your suggestion is perfectly valid, but you should do your homework before stating such a thing as \\"the history of the split.\\" It gives the wrong impression to anyone reading this. Worse, in 3-4 months, all LLM's might perpetuate this idea creating a new divide where none existed.\\n\\nFinally, open source developers don't owe anyone anything. Kawrakow is free to do whatever they want with their time and energy.  How would you feel if some random stranger told you how you should use your own free time and energy to benefit them?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3crwwu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There seem to be a lot of assumptions in this post, without much reading to understand what is what.&lt;/p&gt;\\n\\n&lt;p&gt;First, where did this &amp;quot;better quants&amp;quot; notion come from? Is there any empirical evidence to back it up? Just because one project supports a quant the other doesn&amp;#39;t, doesn&amp;#39;t automatically imply the former has better quants than the latter.&lt;/p&gt;\\n\\n&lt;p&gt;Second, I&amp;#39;m not sure where this &amp;quot;split&amp;quot; notion comes from. Looking at older versions of the ik_llama.cpp readme, there used to be asection titled &amp;quot;Why?&amp;quot; and the answer begins with &amp;quot;Mostly out of curiosity... Note that I have published some, but not all, of the code in this repository in a series of llamafile PRs.&amp;quot; This section has been removed recently, but you don&amp;#39;t have to go that far in the project&amp;#39;s readme to find it. Some of said llamafile PRs were later contributed by Tunney back into llama.cpp.&lt;/p&gt;\\n\\n&lt;p&gt;Third, being a project that started out of personal curiosity, ik_llama.cpp has a different focus than llama.cpp. Asking the ik_llama to merge with llama.cpp is asking the dev(s) to put their personal interests aside for your own benefit.&lt;/p&gt;\\n\\n&lt;p&gt;Fourth, you &lt;strong&gt;&lt;em&gt;should&lt;/em&gt;&lt;/strong&gt; know the history before assuming there was &amp;quot;a split.&amp;quot; That would have been &amp;quot;grown up&amp;quot; thing to do. Your suggestion is perfectly valid, but you should do your homework before stating such a thing as &amp;quot;the history of the split.&amp;quot; It gives the wrong impression to anyone reading this. Worse, in 3-4 months, all LLM&amp;#39;s might perpetuate this idea creating a new divide where none existed.&lt;/p&gt;\\n\\n&lt;p&gt;Finally, open source developers don&amp;#39;t owe anyone anything. Kawrakow is free to do whatever they want with their time and energy.  How would you feel if some random stranger told you how you should use your own free time and energy to benefit them?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0wji2/i_feel_that_the_duality_of_llamacpp_and_ikllama/n3crwwu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752622389,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0wji2","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":19}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3cqnyx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Willing_Landscape_61","can_mod_post":false,"created_utc":1752621979,"send_replies":true,"parent_id":"t3_1m0wji2","score":7,"author_fullname":"t2_8lvrytgw","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think it would probably be easier for llama.cpp to copy the parts form ik_llama.cpp than the other way around.\\nBut it would require some pride to be swallowed.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3cqnyx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think it would probably be easier for llama.cpp to copy the parts form ik_llama.cpp than the other way around.\\nBut it would require some pride to be swallowed.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0wji2/i_feel_that_the_duality_of_llamacpp_and_ikllama/n3cqnyx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752621979,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0wji2","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3coc36","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"erazortt","can_mod_post":false,"created_utc":1752621213,"send_replies":true,"parent_id":"t1_n3cnwxs","score":6,"author_fullname":"t2_6z7m9i7r","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Releases are here: [https://github.com/Thireus/ik\\\\_llama.cpp/releases](https://github.com/Thireus/ik_llama.cpp/releases)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3coc36","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Releases are here: &lt;a href=\\"https://github.com/Thireus/ik_llama.cpp/releases\\"&gt;https://github.com/Thireus/ik_llama.cpp/releases&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0wji2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0wji2/i_feel_that_the_duality_of_llamacpp_and_ikllama/n3coc36/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752621213,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ej54k","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Mkengine","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3cthz5","score":1,"author_fullname":"t2_9p2xe","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What I am really missing in ik_llama is speculative decoding. It may be faster when used with a single model in GPU+CPU, but I had 10 token/s with llama.cpp with Qwen3-30B-A3B + Qwen3-0.6B as draft model in comparison to 9 token/s with only Qwen3-30B-A3B only on ik_llama, so I would not rate it universally better for hybrid setups.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3ej54k","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What I am really missing in ik_llama is speculative decoding. It may be faster when used with a single model in GPU+CPU, but I had 10 token/s with llama.cpp with Qwen3-30B-A3B + Qwen3-0.6B as draft model in comparison to 9 token/s with only Qwen3-30B-A3B only on ik_llama, so I would not rate it universally better for hybrid setups.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0wji2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0wji2/i_feel_that_the_duality_of_llamacpp_and_ikllama/n3ej54k/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752647874,"author_flair_text":null,"treatment_tags":[],"created_utc":1752647874,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3cthz5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"panchovix","can_mod_post":false,"created_utc":1752622913,"send_replies":true,"parent_id":"t1_n3cnwxs","score":5,"author_fullname":"t2_j1kqr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I think for pure GPU, iklcpp may be a bit slower than lcpp, at least it was some months ago testing on my multiGPU system, so can't confirm how it is nowadays.\\n\\nBut for sure if you offload to CPU or use CPU, ikllamacpp takes the lead.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3cthz5","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think for pure GPU, iklcpp may be a bit slower than lcpp, at least it was some months ago testing on my multiGPU system, so can&amp;#39;t confirm how it is nowadays.&lt;/p&gt;\\n\\n&lt;p&gt;But for sure if you offload to CPU or use CPU, ikllamacpp takes the lead.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0wji2","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0wji2/i_feel_that_the_duality_of_llamacpp_and_ikllama/n3cthz5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752622913,"author_flair_text":"Llama 405B","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3dqbsr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Marksta","can_mod_post":false,"created_utc":1752634473,"send_replies":true,"parent_id":"t1_n3cnwxs","score":2,"author_fullname":"t2_559a1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The commands are more or less the same for both projects. Check out ubergarm's huggingface, he has commands on his model pages and they're fancy new quants too. [Qwen3-235B one here.](https://huggingface.co/ubergarm/Qwen3-235B-A22B-GGUF)","edited":1752634690,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dqbsr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The commands are more or less the same for both projects. Check out ubergarm&amp;#39;s huggingface, he has commands on his model pages and they&amp;#39;re fancy new quants too. &lt;a href=\\"https://huggingface.co/ubergarm/Qwen3-235B-A22B-GGUF\\"&gt;Qwen3-235B one here.&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0wji2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0wji2/i_feel_that_the_duality_of_llamacpp_and_ikllama/n3dqbsr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752634473,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3cnwxs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AdamDhahabi","can_mod_post":false,"created_utc":1752621076,"send_replies":true,"parent_id":"t3_1m0wji2","score":2,"author_fullname":"t2_x5lnbc2","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I would like to try ik-llama with dots.llm1 or Qwen 235b on my dual GPU setup (2x 16 GB) + 64 GB DDR5 but I can't find example commands. I mostly see single GPU + CPU examples.  \\nAlso, is there a release published somewhere so that I don't have to build myself?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3cnwxs","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I would like to try ik-llama with dots.llm1 or Qwen 235b on my dual GPU setup (2x 16 GB) + 64 GB DDR5 but I can&amp;#39;t find example commands. I mostly see single GPU + CPU examples.&lt;br/&gt;\\nAlso, is there a release published somewhere so that I don&amp;#39;t have to build myself?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0wji2/i_feel_that_the_duality_of_llamacpp_and_ikllama/n3cnwxs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752621076,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0wji2","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3d5jh5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LA_rent_Aficionado","can_mod_post":false,"created_utc":1752627069,"send_replies":true,"parent_id":"t3_1m0wji2","score":2,"author_fullname":"t2_t8zbiflk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"At one point I would have agreed with you but looking at a number of PRs in llama.cpp you will see a trend of some PRs being rejected because it is an edge case or will be difficult to maintain moving forward.  llama.cpp is fantastic in terms of its ease of use and efficienies but the more you keep adding some of that you wash away and then you also compound how hard it is to implement future changes.  Conversely, with ik\\\\_llama you get some of the more bleeding edge where they can focus their efforts while keeping what its intended user sees as bloat down in the other areas - same deal with slightly different focuses. \\n\\nI will say though, I wish I could see less focus on TTS and some other less performance maximizing edge features on mainline llama and instead focus on bleeding edge and performance enhancements.  Full blackwell implementation is still missing on llama.cpp and it has been months since release.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3d5jh5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;At one point I would have agreed with you but looking at a number of PRs in llama.cpp you will see a trend of some PRs being rejected because it is an edge case or will be difficult to maintain moving forward.  llama.cpp is fantastic in terms of its ease of use and efficienies but the more you keep adding some of that you wash away and then you also compound how hard it is to implement future changes.  Conversely, with ik_llama you get some of the more bleeding edge where they can focus their efforts while keeping what its intended user sees as bloat down in the other areas - same deal with slightly different focuses. &lt;/p&gt;\\n\\n&lt;p&gt;I will say though, I wish I could see less focus on TTS and some other less performance maximizing edge features on mainline llama and instead focus on bleeding edge and performance enhancements.  Full blackwell implementation is still missing on llama.cpp and it has been months since release.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0wji2/i_feel_that_the_duality_of_llamacpp_and_ikllama/n3d5jh5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752627069,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0wji2","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3d4aum","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"plankalkul-z1","can_mod_post":false,"created_utc":1752626641,"send_replies":true,"parent_id":"t3_1m0wji2","score":3,"author_fullname":"t2_w73n3yrsx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I kind of agree with the premise of your post in that it would have been nice if the projects didn't split.\\n\\n\\nI also disagree with many of the replies criticizing you. That is, they sound very, very reasonable... until they tell you to go merge them yourself. Yeah, right.\\n\\n\\nBut...\\n\\n\\nI can tell you that for me, personally, such splits are not a problem. Or, if more exactly, not a problem I can't live with. I've created a script for launching LLMs with best inference engines for them, and adding an engine (that can then be used with any LLM in a suitable format) is a matter of adding few records to the YAML config.\\n\\n\\nThe list of engines changes all the time... I've started with Aphrodite engine, which is arguably an enhanced clone of vLLM, even before I added vLLM itself. But guess what, Aphrodite is currently all but dead, with more than 3 months of no updates. When I figured where it was going, I just removed it. SGLang took its place.\\n\\n\\nI also use llama.cpp and Ollama. ik_llama? I might as well add it if I stumble upon a model that would make that little extra effort worthwhile. Chances are though that I will add MAX earlier.\\n\\n\\nMy point is, don't get married to a particular engine; they come and go (I fully expect llama.cpp to outlast most if not all the others though...) Just use them all. And don't expect two or thee of your favorite ones to [re-]merge into one \\"ideal\\" -- not gonna happen. Somebody has already mentioned XKCD 927 in this thread... And I quite agree with what it implies.","edited":1752629909,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3d4aum","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I kind of agree with the premise of your post in that it would have been nice if the projects didn&amp;#39;t split.&lt;/p&gt;\\n\\n&lt;p&gt;I also disagree with many of the replies criticizing you. That is, they sound very, very reasonable... until they tell you to go merge them yourself. Yeah, right.&lt;/p&gt;\\n\\n&lt;p&gt;But...&lt;/p&gt;\\n\\n&lt;p&gt;I can tell you that for me, personally, such splits are not a problem. Or, if more exactly, not a problem I can&amp;#39;t live with. I&amp;#39;ve created a script for launching LLMs with best inference engines for them, and adding an engine (that can then be used with any LLM in a suitable format) is a matter of adding few records to the YAML config.&lt;/p&gt;\\n\\n&lt;p&gt;The list of engines changes all the time... I&amp;#39;ve started with Aphrodite engine, which is arguably an enhanced clone of vLLM, even before I added vLLM itself. But guess what, Aphrodite is currently all but dead, with more than 3 months of no updates. When I figured where it was going, I just removed it. SGLang took its place.&lt;/p&gt;\\n\\n&lt;p&gt;I also use llama.cpp and Ollama. ik_llama? I might as well add it if I stumble upon a model that would make that little extra effort worthwhile. Chances are though that I will add MAX earlier.&lt;/p&gt;\\n\\n&lt;p&gt;My point is, don&amp;#39;t get married to a particular engine; they come and go (I fully expect llama.cpp to outlast most if not all the others though...) Just use them all. And don&amp;#39;t expect two or thee of your favorite ones to [re-]merge into one &amp;quot;ideal&amp;quot; -- not gonna happen. Somebody has already mentioned XKCD 927 in this thread... And I quite agree with what it implies.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0wji2/i_feel_that_the_duality_of_llamacpp_and_ikllama/n3d4aum/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752626641,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0wji2","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3dqa7n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Plastic-Letterhead44","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3csgvw","score":1,"author_fullname":"t2_rzop9e71d","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"That's fair, ty","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dqa7n","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s fair, ty&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0wji2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0wji2/i_feel_that_the_duality_of_llamacpp_and_ikllama/n3dqa7n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752634456,"author_flair_text":null,"treatment_tags":[],"created_utc":1752634456,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3csgvw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"panchovix","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3cqwvg","score":2,"author_fullname":"t2_j1kqr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Sadly I don't know, I use both lcpp and iklcpp directly.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3csgvw","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sadly I don&amp;#39;t know, I use both lcpp and iklcpp directly.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0wji2","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0wji2/i_feel_that_the_duality_of_llamacpp_and_ikllama/n3csgvw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752622572,"author_flair_text":"Llama 405B","treatment_tags":[],"created_utc":1752622572,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3dq8un","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Plastic-Letterhead44","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3cyate","score":2,"author_fullname":"t2_rzop9e71d","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Okay cool, I'll try to set that up. Ty","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dq8un","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Okay cool, I&amp;#39;ll try to set that up. Ty&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0wji2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0wji2/i_feel_that_the_duality_of_llamacpp_and_ikllama/n3dq8un/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752634442,"author_flair_text":null,"treatment_tags":[],"created_utc":1752634442,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3cyate","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LA_rent_Aficionado","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3cqwvg","score":1,"author_fullname":"t2_t8zbiflk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You could just edit the ooba config to point to a prebuilt ik-llama, it’ll take you like 5 minutes tops with Claude code","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3cyate","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You could just edit the ooba config to point to a prebuilt ik-llama, it’ll take you like 5 minutes tops with Claude code&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0wji2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0wji2/i_feel_that_the_duality_of_llamacpp_and_ikllama/n3cyate/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752624549,"author_flair_text":null,"treatment_tags":[],"created_utc":1752624549,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3cqwvg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Plastic-Letterhead44","can_mod_post":false,"created_utc":1752622061,"send_replies":true,"parent_id":"t1_n3cn15e","score":1,"author_fullname":"t2_rzop9e71d","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Do you know if there is a backend like an Ooba fork or something that uses ikllama?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3cqwvg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Do you know if there is a backend like an Ooba fork or something that uses ikllama?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0wji2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0wji2/i_feel_that_the_duality_of_llamacpp_and_ikllama/n3cqwvg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752622061,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3cn15e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"panchovix","can_mod_post":false,"created_utc":1752620788,"send_replies":true,"parent_id":"t3_1m0wji2","score":3,"author_fullname":"t2_j1kqr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I don't think it's just taking one and merging it into the another, they're pretty different if you check the backend code in some parts (where some others are pretty similar). And that's also beside the point of the own interest of each authors on each project, that they are giving us for free.","edited":1752622792,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3cn15e","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t think it&amp;#39;s just taking one and merging it into the another, they&amp;#39;re pretty different if you check the backend code in some parts (where some others are pretty similar). And that&amp;#39;s also beside the point of the own interest of each authors on each project, that they are giving us for free.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0wji2/i_feel_that_the_duality_of_llamacpp_and_ikllama/n3cn15e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752620788,"author_flair_text":"Llama 405B","treatment_tags":[],"link_id":"t3_1m0wji2","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3cpjai","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"tat_tvam_asshole","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3cp9yz","score":3,"author_fullname":"t2_jxuvgdyj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"\\"why won't other people build the things I want!? 😤\\"","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3cpjai","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&amp;quot;why won&amp;#39;t other people build the things I want!? 😤&amp;quot;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0wji2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0wji2/i_feel_that_the_duality_of_llamacpp_and_ikllama/n3cpjai/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752621605,"author_flair_text":null,"treatment_tags":[],"created_utc":1752621605,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n3cp9yz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"erazortt","can_mod_post":false,"created_utc":1752621520,"send_replies":true,"parent_id":"t1_n3cn5w4","score":10,"author_fullname":"t2_6z7m9i7r","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Orchestrating a merge between diverging projects is not going to be a successfull strategy, when its done by someoue else than the projects involved.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3cp9yz","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Orchestrating a merge between diverging projects is not going to be a successfull strategy, when its done by someoue else than the projects involved.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0wji2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0wji2/i_feel_that_the_duality_of_llamacpp_and_ikllama/n3cp9yz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752621520,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3cpa2r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BurritoOverflow","can_mod_post":false,"created_utc":1752621521,"send_replies":true,"parent_id":"t1_n3cn5w4","score":7,"author_fullname":"t2_1i1de8akil","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"https://imgs.xkcd.com/comics/standards.png","edited":1752624269,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3cpa2r","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://imgs.xkcd.com/comics/standards.png\\"&gt;https://imgs.xkcd.com/comics/standards.png&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0wji2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0wji2/i_feel_that_the_duality_of_llamacpp_and_ikllama/n3cpa2r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752621521,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}}],"before":null}},"user_reports":[],"saved":false,"id":"n3cn5w4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"tat_tvam_asshole","can_mod_post":false,"created_utc":1752620832,"send_replies":true,"parent_id":"t3_1m0wji2","score":0,"author_fullname":"t2_jxuvgdyj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"then fork both and merge them","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3cn5w4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;then fork both and merge them&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0wji2/i_feel_that_the_duality_of_llamacpp_and_ikllama/n3cn5w4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752620832,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0wji2","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3coksl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Efficiency_1144","can_mod_post":false,"created_utc":1752621293,"send_replies":true,"parent_id":"t3_1m0wji2","score":1,"author_fullname":"t2_1nkj9l14b0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This is just a software thing.\\n\\n\\nImportant libraries can end up having dozens of meaningful forks.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3coksl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is just a software thing.&lt;/p&gt;\\n\\n&lt;p&gt;Important libraries can end up having dozens of meaningful forks.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0wji2/i_feel_that_the_duality_of_llamacpp_and_ikllama/n3coksl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752621293,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0wji2","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3dyemi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"JerryWong048","can_mod_post":false,"created_utc":1752637635,"send_replies":true,"parent_id":"t3_1m0wji2","score":1,"author_fullname":"t2_23tc1x7z","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Why don't these volunteers work exactly like I want. They are grown ups right?\\n\\nMake a pull request if you want anything done. If you can't, just appreciate the thing as it is","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dyemi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why don&amp;#39;t these volunteers work exactly like I want. They are grown ups right?&lt;/p&gt;\\n\\n&lt;p&gt;Make a pull request if you want anything done. If you can&amp;#39;t, just appreciate the thing as it is&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0wji2/i_feel_that_the_duality_of_llamacpp_and_ikllama/n3dyemi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752637635,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0wji2","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3d8z65","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ArchdukeofHyperbole","can_mod_post":false,"created_utc":1752628268,"send_replies":true,"parent_id":"t3_1m0wji2","score":0,"author_fullname":"t2_1p41v97q5d","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I've never heard of ik-llama. They support ggufs? The GitHub page mentions legacy hf to gguf, so seems they do, just not sure if that's the primary file type to be used with ik_llama.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3d8z65","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve never heard of ik-llama. They support ggufs? The GitHub page mentions legacy hf to gguf, so seems they do, just not sure if that&amp;#39;s the primary file type to be used with ik_llama.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0wji2/i_feel_that_the_duality_of_llamacpp_and_ikllama/n3d8z65/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752628268,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0wji2","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}}]`),r=()=>e.jsx(l,{data:t});export{r as default};
