import{j as e}from"./index-DOAmItP2.js";import{R as l}from"./RedditPostRenderer-KKgzpPpv.js";import"./index-YSfz60vQ.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hi everyone,\\n\\nSome models require more VRAM to run. I was thinking of getting 2 AMD Ryzen™ AI Max+ 395 and trying to run them in parallel. I wonder if anyone has tried this? Does anyone have any information?\\n\\nHave a nice one:)","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Has anyone tried running 2 AMD Ryzen™ AI Max+ 395 in parallel?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lo5uz6","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.79,"author_flair_background_color":null,"subreddit_type":"public","ups":14,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_hspv3","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":14,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751285371,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\\n\\n&lt;p&gt;Some models require more VRAM to run. I was thinking of getting 2 AMD Ryzen™ AI Max+ 395 and trying to run them in parallel. I wonder if anyone has tried this? Does anyone have any information?&lt;/p&gt;\\n\\n&lt;p&gt;Have a nice one:)&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lo5uz6","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"orkutmuratyilmaz","discussion_type":null,"num_comments":15,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lo5uz6/has_anyone_tried_running_2_amd_ryzen_ai_max_395/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lo5uz6/has_anyone_tried_running_2_amd_ryzen_ai_max_395/","subreddit_subscribers":493242,"created_utc":1751285371,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0kji5r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Zyguard7777777","can_mod_post":false,"created_utc":1751288426,"send_replies":true,"parent_id":"t3_1lo5uz6","score":7,"author_fullname":"t2_zo1h5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"One option is [https://github.com/ggml-org/llama.cpp/tree/master/tools/rpc](https://github.com/ggml-org/llama.cpp/tree/master/tools/rpc), this won't be fast inference though","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0kji5r","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;One option is &lt;a href=\\"https://github.com/ggml-org/llama.cpp/tree/master/tools/rpc\\"&gt;https://github.com/ggml-org/llama.cpp/tree/master/tools/rpc&lt;/a&gt;, this won&amp;#39;t be fast inference though&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo5uz6/has_anyone_tried_running_2_amd_ryzen_ai_max_395/n0kji5r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751288426,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lo5uz6","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0lm2ps","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"orkutmuratyilmaz","can_mod_post":false,"created_utc":1751300264,"send_replies":true,"parent_id":"t1_n0kmxzk","score":1,"author_fullname":"t2_hspv3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I was thinking about vllm and [ray.io](http://ray.io), thanks for recommending sglang.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0lm2ps","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I was thinking about vllm and &lt;a href=\\"http://ray.io\\"&gt;ray.io&lt;/a&gt;, thanks for recommending sglang.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lo5uz6","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo5uz6/has_anyone_tried_running_2_amd_ryzen_ai_max_395/n0lm2ps/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751300264,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0kmxzk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Phaelon74","can_mod_post":false,"created_utc":1751289636,"send_replies":true,"parent_id":"t3_1lo5uz6","score":5,"author_fullname":"t2_4w2rf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Both vllm and sglang support multiple nodes, but when you add multiple gpus, things can get hairy, and when. You add multiple nodes, it can get REALLY hair pulling frustrating.  I would not do this unless you are committed to dozens of hours of elbow grease to get a single model working.  Then when you want to use different models, like Command A or MoE, it will be all over again.\\n\\nLocalLLMs on consumer and prosumer hardware is not worth, take it from a dude with multiple rigs/nodes loaded with 3090s.  It is incredibly frustrating and honestly, not worth your time.  Find a smaller model, and let rip or quantity a bigger model, and enjoy.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0kmxzk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Both vllm and sglang support multiple nodes, but when you add multiple gpus, things can get hairy, and when. You add multiple nodes, it can get REALLY hair pulling frustrating.  I would not do this unless you are committed to dozens of hours of elbow grease to get a single model working.  Then when you want to use different models, like Command A or MoE, it will be all over again.&lt;/p&gt;\\n\\n&lt;p&gt;LocalLLMs on consumer and prosumer hardware is not worth, take it from a dude with multiple rigs/nodes loaded with 3090s.  It is incredibly frustrating and honestly, not worth your time.  Find a smaller model, and let rip or quantity a bigger model, and enjoy.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo5uz6/has_anyone_tried_running_2_amd_ryzen_ai_max_395/n0kmxzk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751289636,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lo5uz6","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0kg9je","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"acelia200","can_mod_post":false,"created_utc":1751287223,"send_replies":true,"parent_id":"t3_1lo5uz6","score":3,"author_fullname":"t2_9a40upyw","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"also was wondering the same question about and amd ryzen has quite positive reviews but will it run as smooth as i wanted it to be? up","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0kg9je","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;also was wondering the same question about and amd ryzen has quite positive reviews but will it run as smooth as i wanted it to be? up&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo5uz6/has_anyone_tried_running_2_amd_ryzen_ai_max_395/n0kg9je/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751287223,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lo5uz6","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0n2183","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0n15bs","score":1,"author_fullname":"t2_o65i6kx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt; OP was specifically asking about using two AI Max+ SOC based systems to run them in parallel.\\n\\nAnd that's even more troubled. Since ROCm 6.4.1 only kind of works with the Max+ as it stands. I have to use a bootleg 6.5 to get it working as well as it does. And that's not particularly well. I can't get triton working for example.\\n\\n&gt; I am not aware of any other technique than tp or pp, hence my previous comment.\\n\\nUsing two systems together doesn't have to involve tensor parallel. Which is what you asked about in your previous comment. It's much easier to do it by breaking up the model and have each GPU run a piece. That works on anything. You don't need to have identical GPUs. Which is what OP really wants. Yes, I know OP said parallel, but what they really want is \\"Some models require more VRAM to run.\\" They don't need TP for that. Thus Vulkan works just fine. And what I said about network bandwidth is apropos.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n0n2183","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;OP was specifically asking about using two AI Max+ SOC based systems to run them in parallel.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;And that&amp;#39;s even more troubled. Since ROCm 6.4.1 only kind of works with the Max+ as it stands. I have to use a bootleg 6.5 to get it working as well as it does. And that&amp;#39;s not particularly well. I can&amp;#39;t get triton working for example.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;I am not aware of any other technique than tp or pp, hence my previous comment.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Using two systems together doesn&amp;#39;t have to involve tensor parallel. Which is what you asked about in your previous comment. It&amp;#39;s much easier to do it by breaking up the model and have each GPU run a piece. That works on anything. You don&amp;#39;t need to have identical GPUs. Which is what OP really wants. Yes, I know OP said parallel, but what they really want is &amp;quot;Some models require more VRAM to run.&amp;quot; They don&amp;#39;t need TP for that. Thus Vulkan works just fine. And what I said about network bandwidth is apropos.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lo5uz6","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo5uz6/has_anyone_tried_running_2_amd_ryzen_ai_max_395/n0n2183/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751315116,"author_flair_text":null,"treatment_tags":[],"created_utc":1751315116,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0n15bs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"twack3r","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0mfaod","score":2,"author_fullname":"t2_ts7cw","approved_by":null,"mod_note":null,"all_awardings":[],"body":"OP was specifically asking about using two AI Max+ SOC based systems to run them in parallel. I am not aware of any other technique than tp or pp, hence my previous comment.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0n15bs","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;OP was specifically asking about using two AI Max+ SOC based systems to run them in parallel. I am not aware of any other technique than tp or pp, hence my previous comment.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lo5uz6","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo5uz6/has_anyone_tried_running_2_amd_ryzen_ai_max_395/n0n15bs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751314859,"author_flair_text":null,"treatment_tags":[],"created_utc":1751314859,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0mfaod","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0m73ck","score":2,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; That is counterintuitive, how does it work?\\n\\nHow much data do you think is sent between nodes? Many people think it's the entire layer. But according to the devs, it's only activation data. Which is only KBs.\\n\\n&gt; Do you do tensor parallelism on both machines? What kind of setup is it?\\n\\nHow can I do that? You need to have clones to do tensor parallelism. Say 2x3090s. If you have say a 7900xtx and a 3090, you can't do tensor parallelism. If you know of a way to do TP with amongst others, a 7900xtx, an A770 and a M1 Max, please let me know.","edited":false,"author_flair_css_class":null,"name":"t1_n0mfaod","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;That is counterintuitive, how does it work?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;How much data do you think is sent between nodes? Many people think it&amp;#39;s the entire layer. But according to the devs, it&amp;#39;s only activation data. Which is only KBs.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Do you do tensor parallelism on both machines? What kind of setup is it?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;How can I do that? You need to have clones to do tensor parallelism. Say 2x3090s. If you have say a 7900xtx and a 3090, you can&amp;#39;t do tensor parallelism. If you know of a way to do TP with amongst others, a 7900xtx, an A770 and a M1 Max, please let me know.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lo5uz6","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo5uz6/has_anyone_tried_running_2_amd_ryzen_ai_max_395/n0mfaod/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751308456,"author_flair_text":null,"collapsed":false,"created_utc":1751308456,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0m73ck","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"twack3r","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0lv4ng","score":1,"author_fullname":"t2_ts7cw","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"That is counterintuitive, how does it work?\\n\\nDo you do tensor parallelism on both machines? What kind of setup is it?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0m73ck","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That is counterintuitive, how does it work?&lt;/p&gt;\\n\\n&lt;p&gt;Do you do tensor parallelism on both machines? What kind of setup is it?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lo5uz6","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo5uz6/has_anyone_tried_running_2_amd_ryzen_ai_max_395/n0m73ck/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751306120,"author_flair_text":null,"treatment_tags":[],"created_utc":1751306120,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0lv4ng","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0kpgjw","score":2,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"From someone that does it, it's not. The software is the biggest issue in performance. Running over gigabit internet or all internal networking, thus taking bandwidth out of the equation, isn't as different as you think.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0lv4ng","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;From someone that does it, it&amp;#39;s not. The software is the biggest issue in performance. Running over gigabit internet or all internal networking, thus taking bandwidth out of the equation, isn&amp;#39;t as different as you think.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lo5uz6","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo5uz6/has_anyone_tried_running_2_amd_ryzen_ai_max_395/n0lv4ng/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751302856,"author_flair_text":null,"treatment_tags":[],"created_utc":1751302856,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0kpgjw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"twack3r","can_mod_post":false,"created_utc":1751290499,"send_replies":true,"parent_id":"t1_n0ki1ob","score":2,"author_fullname":"t2_ts7cw","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"By measuring the bandwidth between the two devices. That will be the biggest issue with a setup like this.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0kpgjw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;By measuring the bandwidth between the two devices. That will be the biggest issue with a setup like this.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lo5uz6","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo5uz6/has_anyone_tried_running_2_amd_ryzen_ai_max_395/n0kpgjw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751290499,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ki1ob","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"uti24","can_mod_post":false,"created_utc":1751287893,"send_replies":true,"parent_id":"t3_1lo5uz6","score":2,"author_fullname":"t2_13hbro","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"How do we calculate even potential speed of using two different machines for inference?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ki1ob","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How do we calculate even potential speed of using two different machines for inference?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo5uz6/has_anyone_tried_running_2_amd_ryzen_ai_max_395/n0ki1ob/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751287893,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lo5uz6","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ptbnh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"b3081a","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0psrjv","score":1,"author_fullname":"t2_17n5yh7l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yeah their TCP-based RPC implementation is far from ideal. A shared memory ring buffer + busy polling would work much better than going through a socket, regardless of where the peers are located.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ptbnh","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah their TCP-based RPC implementation is far from ideal. A shared memory ring buffer + busy polling would work much better than going through a socket, regardless of where the peers are located.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lo5uz6","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo5uz6/has_anyone_tried_running_2_amd_ryzen_ai_max_395/n0ptbnh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751351861,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1751351861,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0psrjv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0psk9l","score":1,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; Latency is the key bottleneck.\\n\\nWouldn't running all internally in the same box have the lowest latency? Even then, the speed penalty is there. If it has that problem with internal networking, nothing thunderbolt is going to fix it.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0psrjv","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Latency is the key bottleneck.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Wouldn&amp;#39;t running all internally in the same box have the lowest latency? Even then, the speed penalty is there. If it has that problem with internal networking, nothing thunderbolt is going to fix it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lo5uz6","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo5uz6/has_anyone_tried_running_2_amd_ryzen_ai_max_395/n0psrjv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751351545,"author_flair_text":null,"treatment_tags":[],"created_utc":1751351545,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0psk9l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"b3081a","can_mod_post":false,"created_utc":1751351431,"send_replies":true,"parent_id":"t1_n0lvr1u","score":1,"author_fullname":"t2_17n5yh7l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Latency is the key bottleneck. Technically doing thunderbolt P2P DMA (rather than thunderbolt-net) can fix it, but there's no software solution leveraging that. The kernel mode thunderbolt/usb4 DMA API isn't even exposed to user space applications. Thunderbolt-net is based in P2P DMA but tcp/ip stack has too much overhead.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0psk9l","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Latency is the key bottleneck. Technically doing thunderbolt P2P DMA (rather than thunderbolt-net) can fix it, but there&amp;#39;s no software solution leveraging that. The kernel mode thunderbolt/usb4 DMA API isn&amp;#39;t even exposed to user space applications. Thunderbolt-net is based in P2P DMA but tcp/ip stack has too much overhead.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lo5uz6","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo5uz6/has_anyone_tried_running_2_amd_ryzen_ai_max_395/n0psk9l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751351431,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0lvr1u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"created_utc":1751303029,"send_replies":true,"parent_id":"t3_1lo5uz6","score":2,"author_fullname":"t2_o65i6kx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I only have one Max+, so I haven't tried two. But I've been running multiple boxes for a while now. A Max+ shouldn't be any different.\\n\\nIt's easy to do. Just run RPC using llama.cpp. There is a speed penalty. A pretty significant penalty. It's because the communication isn't async. It has nothing to do with network speed. Since you see the same speed penalty when running all internally on the same box. It's a software problem. Hopefully it gets fixed someday. But considering it's been brought up as a problem for months now and is still a problem, I wouldn't hold your breath.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0lvr1u","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I only have one Max+, so I haven&amp;#39;t tried two. But I&amp;#39;ve been running multiple boxes for a while now. A Max+ shouldn&amp;#39;t be any different.&lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s easy to do. Just run RPC using llama.cpp. There is a speed penalty. A pretty significant penalty. It&amp;#39;s because the communication isn&amp;#39;t async. It has nothing to do with network speed. Since you see the same speed penalty when running all internally on the same box. It&amp;#39;s a software problem. Hopefully it gets fixed someday. But considering it&amp;#39;s been brought up as a problem for months now and is still a problem, I wouldn&amp;#39;t hold your breath.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo5uz6/has_anyone_tried_running_2_amd_ryzen_ai_max_395/n0lvr1u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751303029,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lo5uz6","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),r=()=>e.jsx(l,{data:t});export{r as default};
