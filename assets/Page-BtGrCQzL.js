import{j as e}from"./index-BOnf-UhU.js";import{R as l}from"./RedditPostRenderer-Ce39qICS.js";import"./index-CDase6VD.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"As everyone knows, google search has been getting worse the past few years. ChatGPT with web search enabled has become a big tool that is replacing Google for me. \\n\\nHere are some example queries: \\n\\n[\\"List the median, 25th/75th percentile MCAT scores for medical schools in California in a table. Sort by rank.\\"](https://chatgpt.com/share/686c6b7d-099c-8012-915b-71e2e2e67a06)\\n\\n[\\"What has happened in the war between Israel and Iran in the past week?\\"](https://chatgpt.com/share/686c6bff-4594-8012-a0ea-ac5091cc621d).\\n\\nChatGPT's responses are pretty good. It's a lot easier than googling and compiling the information yourself. The responses are **even better- basically perfect- if you use o3 or o4-mini**, but I don't have a Plus account and prefer to use the API. Using o4-mini with my brother's account literally saves me so much time google searching already.\\n\\n-------\\n\\nSo... can we replicate this locally? Maybe use Qwen 32b with a good system prompt, and have Serper to do google search API, and then some way of loading the pages in the results into context? Has anyone tried to build such a system that works similarly smoothly as how ChatGPT the product works?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"So, does anyone have a good workflow to replace google search yet?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lubdcg","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.86,"author_flair_background_color":null,"subreddit_type":"public","ups":15,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_t6glzswk","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":15,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751936288,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;As everyone knows, google search has been getting worse the past few years. ChatGPT with web search enabled has become a big tool that is replacing Google for me. &lt;/p&gt;\\n\\n&lt;p&gt;Here are some example queries: &lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://chatgpt.com/share/686c6b7d-099c-8012-915b-71e2e2e67a06\\"&gt;&amp;quot;List the median, 25th/75th percentile MCAT scores for medical schools in California in a table. Sort by rank.&amp;quot;&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://chatgpt.com/share/686c6bff-4594-8012-a0ea-ac5091cc621d\\"&gt;&amp;quot;What has happened in the war between Israel and Iran in the past week?&amp;quot;&lt;/a&gt;.&lt;/p&gt;\\n\\n&lt;p&gt;ChatGPT&amp;#39;s responses are pretty good. It&amp;#39;s a lot easier than googling and compiling the information yourself. The responses are &lt;strong&gt;even better- basically perfect- if you use o3 or o4-mini&lt;/strong&gt;, but I don&amp;#39;t have a Plus account and prefer to use the API. Using o4-mini with my brother&amp;#39;s account literally saves me so much time google searching already.&lt;/p&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;p&gt;So... can we replicate this locally? Maybe use Qwen 32b with a good system prompt, and have Serper to do google search API, and then some way of loading the pages in the results into context? Has anyone tried to build such a system that works similarly smoothly as how ChatGPT the product works?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lubdcg","is_robot_indexable":true,"num_duplicates":1,"report_reasons":null,"author":"DepthHour1669","discussion_type":null,"num_comments":29,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lubdcg/so_does_anyone_have_a_good_workflow_to_replace/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lubdcg/so_does_anyone_have_a_good_workflow_to_replace/","subreddit_subscribers":496034,"created_utc":1751936288,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1y8zca","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Trotskyist","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1y84gn","score":1,"author_fullname":"t2_350qe","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Also, how close will this get to openAIs Deep Research?\\n\\nNot very. The reality is that deep research (i.e. o3) is a considerably more capable model than anything you're going to run at home, assuming you dont have  $100-200K worth of hardware.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1y8zca","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Also, how close will this get to openAIs Deep Research?&lt;/p&gt;\\n\\n&lt;p&gt;Not very. The reality is that deep research (i.e. o3) is a considerably more capable model than anything you&amp;#39;re going to run at home, assuming you dont have  $100-200K worth of hardware.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lubdcg","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lubdcg/so_does_anyone_have_a_good_workflow_to_replace/n1y8zca/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751962291,"author_flair_text":null,"treatment_tags":[],"created_utc":1751962291,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1y84gn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"twack3r","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1wt65f","score":1,"author_fullname":"t2_ts7cw","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Any recommendations on the models? Also, how close will this get to openAIs Deep Research? That has pretty much become the gold standard for me personally and I'm still chasing a locally run equivalent solution.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1y84gn","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Any recommendations on the models? Also, how close will this get to openAIs Deep Research? That has pretty much become the gold standard for me personally and I&amp;#39;m still chasing a locally run equivalent solution.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lubdcg","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lubdcg/so_does_anyone_have_a_good_workflow_to_replace/n1y84gn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751961783,"author_flair_text":null,"treatment_tags":[],"created_utc":1751961783,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1wt65f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MattOnePointO","can_mod_post":false,"created_utc":1751938818,"send_replies":true,"parent_id":"t1_n1woulo","score":3,"author_fullname":"t2_2gwqixdb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Second this as a great local resource especially paired with the right model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1wt65f","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Second this as a great local resource especially paired with the right model.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lubdcg","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lubdcg/so_does_anyone_have_a_good_workflow_to_replace/n1wt65f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751938818,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1xd8i1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ArsNeph","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1xbxnt","score":2,"author_fullname":"t2_vt0xkv60d","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"No I know, it's just the difference of whether to use an embedding model as an extra step, or skip it at the risk of ingesting irrelevant information. \\n\\nModern context length is not actually as robust as you might think, take a look at various long context benchmarks like RULER, fiction.livebench, and Nolima, it's mostly only frontier models that do really well with long context, and extracting anything but the crucial parts of a web page can really take up a ton of context.","edited":false,"author_flair_css_class":null,"name":"t1_n1xd8i1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No I know, it&amp;#39;s just the difference of whether to use an embedding model as an extra step, or skip it at the risk of ingesting irrelevant information. &lt;/p&gt;\\n\\n&lt;p&gt;Modern context length is not actually as robust as you might think, take a look at various long context benchmarks like RULER, fiction.livebench, and Nolima, it&amp;#39;s mostly only frontier models that do really well with long context, and extracting anything but the crucial parts of a web page can really take up a ton of context.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lubdcg","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lubdcg/so_does_anyone_have_a_good_workflow_to_replace/n1xd8i1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751946179,"author_flair_text":null,"collapsed":false,"created_utc":1751946179,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1xbxnt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1x6dyh","score":3,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The last step of RAG is still loading the raw text of that vector into context, though. That’s unavoidable. \\n\\nAnd nowadays, with modern context lengths, it’s probably a lot easier just to dump the entire webpage in context.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1xbxnt","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The last step of RAG is still loading the raw text of that vector into context, though. That’s unavoidable. &lt;/p&gt;\\n\\n&lt;p&gt;And nowadays, with modern context lengths, it’s probably a lot easier just to dump the entire webpage in context.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lubdcg","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lubdcg/so_does_anyone_have_a_good_workflow_to_replace/n1xbxnt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751945650,"author_flair_text":null,"treatment_tags":[],"created_utc":1751945650,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n1x6dyh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ArsNeph","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1x5zf1","score":1,"author_fullname":"t2_vt0xkv60d","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Web search is technically a RAG task, so generally it uses search, then uses an embedding model to vectorize the text, but it's possible to also just inject it as part of the prompt.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1x6dyh","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Web search is technically a RAG task, so generally it uses search, then uses an embedding model to vectorize the text, but it&amp;#39;s possible to also just inject it as part of the prompt.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lubdcg","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lubdcg/so_does_anyone_have_a_good_workflow_to_replace/n1x6dyh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751943485,"author_flair_text":null,"treatment_tags":[],"created_utc":1751943485,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1x5zf1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"created_utc":1751943334,"send_replies":true,"parent_id":"t1_n1woulo","score":1,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This seems like the best option so far. I’m not seeing how it loads webpages into context like ChatGPT though, but I’ll give it a whirl and see if it works.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1x5zf1","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This seems like the best option so far. I’m not seeing how it loads webpages into context like ChatGPT though, but I’ll give it a whirl and see if it works.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lubdcg","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lubdcg/so_does_anyone_have_a_good_workflow_to_replace/n1x5zf1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751943334,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1xfvjf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ArsNeph","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1xeokd","score":1,"author_fullname":"t2_vt0xkv60d","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I haven't run it myself, but assuming a relatively fast internet connection, a fast instance of SearXNG, and parallel API requests, it would take 1-3 seconds to grab the webpages, then maybe 3-5 seconds to vectorize them with an embedding model, though it depends on the size of the embedding model and how many web pages you are using, then a few seconds of prompt processing depending on which model you are using. End to end, assuming best case circumstances on a reasonably fast consumer GPU, around 10 seconds. Worst case, excluding slow prompt processing speeds as a bottleneck, probably around 30 seconds.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1xfvjf","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I haven&amp;#39;t run it myself, but assuming a relatively fast internet connection, a fast instance of SearXNG, and parallel API requests, it would take 1-3 seconds to grab the webpages, then maybe 3-5 seconds to vectorize them with an embedding model, though it depends on the size of the embedding model and how many web pages you are using, then a few seconds of prompt processing depending on which model you are using. End to end, assuming best case circumstances on a reasonably fast consumer GPU, around 10 seconds. Worst case, excluding slow prompt processing speeds as a bottleneck, probably around 30 seconds.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lubdcg","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lubdcg/so_does_anyone_have_a_good_workflow_to_replace/n1xfvjf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751947261,"author_flair_text":null,"treatment_tags":[],"created_utc":1751947261,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1xeokd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"superfluid","can_mod_post":false,"created_utc":1751946768,"send_replies":true,"parent_id":"t1_n1woulo","score":1,"author_fullname":"t2_3hjbp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What sort of end-to-end run time do you see for any search given what tokens/s? It seems like a very compelling solution, I just question how long it takes.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1xeokd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What sort of end-to-end run time do you see for any search given what tokens/s? It seems like a very compelling solution, I just question how long it takes.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lubdcg","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lubdcg/so_does_anyone_have_a_good_workflow_to_replace/n1xeokd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751946768,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1woulo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ArsNeph","can_mod_post":false,"created_utc":1751937330,"send_replies":true,"parent_id":"t3_1lubdcg","score":12,"author_fullname":"t2_vt0xkv60d","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Perplexica is a locally hosted AI search engine that uses SearXNG as a search API. It's reasonably good for what it is","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1woulo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Perplexica is a locally hosted AI search engine that uses SearXNG as a search API. It&amp;#39;s reasonably good for what it is&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lubdcg/so_does_anyone_have_a_good_workflow_to_replace/n1woulo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751937330,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lubdcg","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1wo7ei","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"jl23423f23r323223r3","can_mod_post":false,"created_utc":1751937109,"send_replies":true,"parent_id":"t3_1lubdcg","score":6,"author_fullname":"t2_zrj48","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Perplexity!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1wo7ei","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Perplexity!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lubdcg/so_does_anyone_have_a_good_workflow_to_replace/n1wo7ei/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751937109,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lubdcg","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1x4qhr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"created_utc":1751942875,"send_replies":true,"parent_id":"t1_n1wqdmg","score":2,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The problem is even with a search MCP, results are poor. Try the 2 examples above with any local 32b tier model and a search api and you’ll see. \\n\\nYou need a lot more scaffolding than just “throw a search API at a local model”. Yes, I suspect that to get ChatGPT quality results you essentially have to do RAG on the webpages. The question is if someone has already done that.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1x4qhr","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The problem is even with a search MCP, results are poor. Try the 2 examples above with any local 32b tier model and a search api and you’ll see. &lt;/p&gt;\\n\\n&lt;p&gt;You need a lot more scaffolding than just “throw a search API at a local model”. Yes, I suspect that to get ChatGPT quality results you essentially have to do RAG on the webpages. The question is if someone has already done that.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lubdcg","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lubdcg/so_does_anyone_have_a_good_workflow_to_replace/n1x4qhr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751942875,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1wqdmg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"themaxx2","can_mod_post":false,"created_utc":1751937858,"send_replies":true,"parent_id":"t3_1lubdcg","score":2,"author_fullname":"t2_dkjf0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What exactly are you wanting to reproduce locally? If it's just the LLM, like someone said before, any reasoning model should suffice. If you want search and scrape api you can use firecrawl search/scrape, or Tavily with an API key. You can also use Duckduckgo with no API key for searching, and also Bing and Google's search API's. If you're wanting to replicate the whole search engine thing, it really comes down to \\"Which websites do I go to to download information to this question\\", at runtime, or automating the crawler for your own index and basically doing the vector store for RAG on your own copied webpages.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1wqdmg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What exactly are you wanting to reproduce locally? If it&amp;#39;s just the LLM, like someone said before, any reasoning model should suffice. If you want search and scrape api you can use firecrawl search/scrape, or Tavily with an API key. You can also use Duckduckgo with no API key for searching, and also Bing and Google&amp;#39;s search API&amp;#39;s. If you&amp;#39;re wanting to replicate the whole search engine thing, it really comes down to &amp;quot;Which websites do I go to to download information to this question&amp;quot;, at runtime, or automating the crawler for your own index and basically doing the vector store for RAG on your own copied webpages.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lubdcg/so_does_anyone_have_a_good_workflow_to_replace/n1wqdmg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751937858,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lubdcg","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1x3p71","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"TheRealMasonMac","can_mod_post":false,"created_utc":1751942498,"send_replies":true,"parent_id":"t3_1lubdcg","score":2,"author_fullname":"t2_101haj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Non-local options if you're just looking for any alternative: You can use [https://aistudio.google.com/](https://aistudio.google.com/) for free if you are okay with zero privacy. Gemini Pro is also free for students if you have a relative who can hook you up.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1x3p71","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Non-local options if you&amp;#39;re just looking for any alternative: You can use &lt;a href=\\"https://aistudio.google.com/\\"&gt;https://aistudio.google.com/&lt;/a&gt; for free if you are okay with zero privacy. Gemini Pro is also free for students if you have a relative who can hook you up.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lubdcg/so_does_anyone_have_a_good_workflow_to_replace/n1x3p71/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751942498,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lubdcg","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1xo9jh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kor34l","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1xnedq","score":1,"author_fullname":"t2_5r1lfqng","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Personal preference.  I like Hermes a lot.  It's 10.7B parameters so it's fairly smart and blazing fast without eating all my VRAM, it's an Instruct model so it listens well and doesn't get distracted constantly (looking at you, QWQ), and it's probably the quickest and lowest memory usage model that still does a good job understanding and summarizing search results, most of the time.\\n\\nI have a TON of models though, as I spent pretty much all my free time for the last couple years doing absolutely everything AI can do (I'm obsessed, especially programming with it).  So for more complex searches I sometimes invoke a smarter model.  Mixtral 8x22B is very good at this too (and also an Instruct model).  QWQ-32B is good at pretty much everything, and can handle tool calling and pure json output and reasoning/chain-of-thought awesomely, but takes every bit of my 24GB of VRAM (RTX3090) to run at decent speed at Q5_UD_XL (thanks, Unsloth!) and can occasionally distract itself and go off on a tangent, especially if you don't use the recommended prompt formatting it was trained on.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1xo9jh","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Personal preference.  I like Hermes a lot.  It&amp;#39;s 10.7B parameters so it&amp;#39;s fairly smart and blazing fast without eating all my VRAM, it&amp;#39;s an Instruct model so it listens well and doesn&amp;#39;t get distracted constantly (looking at you, QWQ), and it&amp;#39;s probably the quickest and lowest memory usage model that still does a good job understanding and summarizing search results, most of the time.&lt;/p&gt;\\n\\n&lt;p&gt;I have a TON of models though, as I spent pretty much all my free time for the last couple years doing absolutely everything AI can do (I&amp;#39;m obsessed, especially programming with it).  So for more complex searches I sometimes invoke a smarter model.  Mixtral 8x22B is very good at this too (and also an Instruct model).  QWQ-32B is good at pretty much everything, and can handle tool calling and pure json output and reasoning/chain-of-thought awesomely, but takes every bit of my 24GB of VRAM (RTX3090) to run at decent speed at Q5_UD_XL (thanks, Unsloth!) and can occasionally distract itself and go off on a tangent, especially if you don&amp;#39;t use the recommended prompt formatting it was trained on.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lubdcg","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lubdcg/so_does_anyone_have_a_good_workflow_to_replace/n1xo9jh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751951038,"author_flair_text":null,"treatment_tags":[],"created_utc":1751951038,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1xnedq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"created_utc":1751950627,"send_replies":true,"parent_id":"t1_n1xf0mk","score":1,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Why Hermes?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1xnedq","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why Hermes?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lubdcg","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lubdcg/so_does_anyone_have_a_good_workflow_to_replace/n1xnedq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751950627,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1xf0mk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kor34l","can_mod_post":false,"created_utc":1751946904,"send_replies":true,"parent_id":"t3_1lubdcg","score":1,"author_fullname":"t2_5r1lfqng","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I use a project called Perplexica (NOT perplexia) with SearXNG as a backend, implemented via vibe coding with qwen-2.5-coder (using python).\\n\\nWhen I want to search, I type \\"python -m src research &lt;query&gt;\\" and it searches the top 10 results of each search engine, for like a dozen search engines (including bing and google and github and wikipedia etc), then it uses a local LLM (Hermes-2-Pro in my case) to read the top 10 results from every engine and give me a detailed summary, both of each result and of the entire query.\\n\\nAnd it takes around 25 seconds total.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1xf0mk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I use a project called Perplexica (NOT perplexia) with SearXNG as a backend, implemented via vibe coding with qwen-2.5-coder (using python).&lt;/p&gt;\\n\\n&lt;p&gt;When I want to search, I type &amp;quot;python -m src research &amp;lt;query&amp;gt;&amp;quot; and it searches the top 10 results of each search engine, for like a dozen search engines (including bing and google and github and wikipedia etc), then it uses a local LLM (Hermes-2-Pro in my case) to read the top 10 results from every engine and give me a detailed summary, both of each result and of the entire query.&lt;/p&gt;\\n\\n&lt;p&gt;And it takes around 25 seconds total.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lubdcg/so_does_anyone_have_a_good_workflow_to_replace/n1xf0mk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751946904,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lubdcg","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1y7scm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"oxygen_addiction","can_mod_post":false,"created_utc":1751961584,"send_replies":true,"parent_id":"t1_n1xfbhn","score":1,"author_fullname":"t2_66k6x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You can pay someone else to scrape it for you: [https://serper.dev/](https://serper.dev/)  \\n[https://github.com/menloresearch/jan](https://github.com/menloresearch/jan) has a built-in MCP server that can call Serper for websearches.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1y7scm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can pay someone else to scrape it for you: &lt;a href=\\"https://serper.dev/\\"&gt;https://serper.dev/&lt;/a&gt;&lt;br/&gt;\\n&lt;a href=\\"https://github.com/menloresearch/jan\\"&gt;https://github.com/menloresearch/jan&lt;/a&gt; has a built-in MCP server that can call Serper for websearches.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lubdcg","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lubdcg/so_does_anyone_have_a_good_workflow_to_replace/n1y7scm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751961584,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1xfbhn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ttkciar","can_mod_post":false,"created_utc":1751947030,"send_replies":true,"parent_id":"t3_1lubdcg","score":1,"author_fullname":"t2_cpegz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"In 2003 I wrote a script called \\"research\\" which wrapped Google web search, scraping the first 100 pages of hits for a search term and forking off subprocesses to retrieve hit pages.  It then parsed those pages' contents for sentences which took the syntactic form of statements of fact, and made a list of them.\\n\\nGoogle noticed I was scraping their search's web interface and blocked my home IP for a while.  It became unblocked eventually but I've been more careful since then.\\n\\nThere's a lot we could do to implement a better web search, with or without LLM inference, if we had a search service that didn't mind being abused like that.\\n\\nWe could pay $$$ to use the Google Search API, but fuck that noise.\\n\\nI keep contemplating hooking into the [YaCy peer-to-peer web search network](https://wikipedia.org/wiki/YaCy) but really, really detest Java.\\n\\nLooking around, though, I see https://github.com/yacy/yacy_expert is a thing, written in Python.  That seems to be mostly like what OP is wishing for, already (YaCy + LLM).  Maybe build on that?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1xfbhn","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;In 2003 I wrote a script called &amp;quot;research&amp;quot; which wrapped Google web search, scraping the first 100 pages of hits for a search term and forking off subprocesses to retrieve hit pages.  It then parsed those pages&amp;#39; contents for sentences which took the syntactic form of statements of fact, and made a list of them.&lt;/p&gt;\\n\\n&lt;p&gt;Google noticed I was scraping their search&amp;#39;s web interface and blocked my home IP for a while.  It became unblocked eventually but I&amp;#39;ve been more careful since then.&lt;/p&gt;\\n\\n&lt;p&gt;There&amp;#39;s a lot we could do to implement a better web search, with or without LLM inference, if we had a search service that didn&amp;#39;t mind being abused like that.&lt;/p&gt;\\n\\n&lt;p&gt;We could pay $$$ to use the Google Search API, but fuck that noise.&lt;/p&gt;\\n\\n&lt;p&gt;I keep contemplating hooking into the &lt;a href=\\"https://wikipedia.org/wiki/YaCy\\"&gt;YaCy peer-to-peer web search network&lt;/a&gt; but really, really detest Java.&lt;/p&gt;\\n\\n&lt;p&gt;Looking around, though, I see &lt;a href=\\"https://github.com/yacy/yacy_expert\\"&gt;https://github.com/yacy/yacy_expert&lt;/a&gt; is a thing, written in Python.  That seems to be mostly like what OP is wishing for, already (YaCy + LLM).  Maybe build on that?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lubdcg/so_does_anyone_have_a_good_workflow_to_replace/n1xfbhn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751947030,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lubdcg","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1xksdr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"TotesMessenger","can_mod_post":false,"created_utc":1751949419,"send_replies":true,"parent_id":"t3_1lubdcg","score":1,"author_fullname":"t2_kq14w","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:\\n\\n- [/r/radllama] [So, does anyone have a good workflow to replace google search yet?](https://www.reddit.com/r/RadLLaMA/comments/1lufne1/so_does_anyone_have_a_good_workflow_to_replace/)\\n\\n&amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\\\\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1xksdr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m a bot, &lt;em&gt;bleep&lt;/em&gt;, &lt;em&gt;bloop&lt;/em&gt;. Someone has linked to this thread from another place on reddit:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;[&lt;a href=\\"/r/radllama\\"&gt;/r/radllama&lt;/a&gt;] &lt;a href=\\"https://www.reddit.com/r/RadLLaMA/comments/1lufne1/so_does_anyone_have_a_good_workflow_to_replace/\\"&gt;So, does anyone have a good workflow to replace google search yet?&lt;/a&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;&amp;nbsp;&lt;em&gt;&lt;sup&gt;If you follow any of the above links, please respect the rules of reddit and don&amp;#39;t vote in the other threads.&lt;/sup&gt; &lt;sup&gt;(&lt;a href=\\"/r/TotesMessenger\\"&gt;Info&lt;/a&gt;&lt;/sup&gt; &lt;sup&gt;/&lt;/sup&gt; &lt;sup&gt;&lt;a href=\\"/message/compose?to=/r/TotesMessenger\\"&gt;Contact&lt;/a&gt;)&lt;/sup&gt;&lt;/em&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lubdcg/so_does_anyone_have_a_good_workflow_to_replace/n1xksdr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751949419,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lubdcg","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1xx853","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"binheap","can_mod_post":false,"created_utc":1751955590,"send_replies":true,"parent_id":"t3_1lubdcg","score":1,"author_fullname":"t2_gaorh8k","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"So like everyone else here is saying, Perplexica probably is what you are looking for.\\n\\nHowever, as a word of caution, even in your examples, it looks like ChatGPT is hallucinating a bit. For example, the cite it lists for the 25th and 75th percentile MCATs for UCSF don't actually appear in the site. It looks like it's copying over from the Stanford result. There's are also multiple citations to [dev.time.com](http://dev.time.com) for your news query which seems suspicious as that looks like a dev site not meant to be seen by others. It also says that \\"Disruptions followed Khamenei’s return, with mass evacuations from Tehran and an exodus of around 300,000 people\\" but the problem is that didn't occur in the last week. The actual wikipedia link it cites says that occurred in June 13th though the hoverbox says it the article is from July 7th. I'm guessing that's some kind of crawl date and not an actual article date. Similarly, \\"U.S. military strikes on Iran’s nuclear sites\\" did not occur in the last week as far as I know.\\n\\nThat's just what I could catch on first glance and I wanted to say that the open source variants have similar issues if not more severe.","edited":1751955945,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1xx853","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So like everyone else here is saying, Perplexica probably is what you are looking for.&lt;/p&gt;\\n\\n&lt;p&gt;However, as a word of caution, even in your examples, it looks like ChatGPT is hallucinating a bit. For example, the cite it lists for the 25th and 75th percentile MCATs for UCSF don&amp;#39;t actually appear in the site. It looks like it&amp;#39;s copying over from the Stanford result. There&amp;#39;s are also multiple citations to &lt;a href=\\"http://dev.time.com\\"&gt;dev.time.com&lt;/a&gt; for your news query which seems suspicious as that looks like a dev site not meant to be seen by others. It also says that &amp;quot;Disruptions followed Khamenei’s return, with mass evacuations from Tehran and an exodus of around 300,000 people&amp;quot; but the problem is that didn&amp;#39;t occur in the last week. The actual wikipedia link it cites says that occurred in June 13th though the hoverbox says it the article is from July 7th. I&amp;#39;m guessing that&amp;#39;s some kind of crawl date and not an actual article date. Similarly, &amp;quot;U.S. military strikes on Iran’s nuclear sites&amp;quot; did not occur in the last week as far as I know.&lt;/p&gt;\\n\\n&lt;p&gt;That&amp;#39;s just what I could catch on first glance and I wanted to say that the open source variants have similar issues if not more severe.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lubdcg/so_does_anyone_have_a_good_workflow_to_replace/n1xx853/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751955590,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lubdcg","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1xxq95","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ArtfulGenie69","can_mod_post":false,"created_utc":1751955861,"send_replies":true,"parent_id":"t3_1lubdcg","score":1,"author_fullname":"t2_1d6ghm3sq0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Searxng on docker + openwebui hooked in. Searxng runs in json mode. ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1xxq95","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Searxng on docker + openwebui hooked in. Searxng runs in json mode. &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lubdcg/so_does_anyone_have_a_good_workflow_to_replace/n1xxq95/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751955861,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lubdcg","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1xysq8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok-Application-2261","can_mod_post":false,"created_utc":1751956449,"send_replies":true,"parent_id":"t3_1lubdcg","score":1,"author_fullname":"t2_3qmqtbmo","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Search youtube for \\"Someordinarygamers\\" and watch his video titled \\"Pwediepie hates google\\". He has a segment on there that shows you how to download Docker Desktop and run something called SearXNG along side a local LLM to search the web for you. His video has bookmarks so you can easily find it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1xysq8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Search youtube for &amp;quot;Someordinarygamers&amp;quot; and watch his video titled &amp;quot;Pwediepie hates google&amp;quot;. He has a segment on there that shows you how to download Docker Desktop and run something called SearXNG along side a local LLM to search the web for you. His video has bookmarks so you can easily find it.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lubdcg/so_does_anyone_have_a_good_workflow_to_replace/n1xysq8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751956449,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lubdcg","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1xjax6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Bitter-Ad640","can_mod_post":false,"created_utc":1751948754,"send_replies":true,"parent_id":"t3_1lubdcg","score":1,"author_fullname":"t2_djjpv9or","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I don't know how you're all using google, but ChatGPT gives me false information with very high confidence many times a day.\\n\\nMy Google searches are a little bit of a wade through slop now (really, more ads and SEO than slop) but its still pretty easy to find what im looking for with just a few quotation marks.\\n\\nChatGPT's deep research option on the other hand is NUTS. Slow, but wow is that powerful. Deep dives into accurate information with sources provided even on some very obscure questions.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1xjax6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t know how you&amp;#39;re all using google, but ChatGPT gives me false information with very high confidence many times a day.&lt;/p&gt;\\n\\n&lt;p&gt;My Google searches are a little bit of a wade through slop now (really, more ads and SEO than slop) but its still pretty easy to find what im looking for with just a few quotation marks.&lt;/p&gt;\\n\\n&lt;p&gt;ChatGPT&amp;#39;s deep research option on the other hand is NUTS. Slow, but wow is that powerful. Deep dives into accurate information with sources provided even on some very obscure questions.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lubdcg/so_does_anyone_have_a_good_workflow_to_replace/n1xjax6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751948754,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lubdcg","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1x68so","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"created_utc":1751943432,"send_replies":true,"parent_id":"t1_n1wo1fs","score":1,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Well, the question is what search api and page loading and what prompts work the best.\\n\\nNotice the 2 questions that I used as benchmark examples- they would fail if you just used a reasoning model on top of a search result page. It requires that you open the web pages and run the AI on the contents.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1x68so","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Well, the question is what search api and page loading and what prompts work the best.&lt;/p&gt;\\n\\n&lt;p&gt;Notice the 2 questions that I used as benchmark examples- they would fail if you just used a reasoning model on top of a search result page. It requires that you open the web pages and run the AI on the contents.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lubdcg","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lubdcg/so_does_anyone_have_a_good_workflow_to_replace/n1x68so/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751943432,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1wo1fs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ii_social","can_mod_post":false,"created_utc":1751937052,"send_replies":true,"parent_id":"t3_1lubdcg","score":0,"author_fullname":"t2_tohvxz80x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"ChatGPT is best for search yes. \\n\\nIf you are a tinkerer and want a local option GitHub copilot with ollama plus a search API as an MCP could allow you to autonomously research and write content or reports based on your findings.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1wo1fs","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;ChatGPT is best for search yes. &lt;/p&gt;\\n\\n&lt;p&gt;If you are a tinkerer and want a local option GitHub copilot with ollama plus a search API as an MCP could allow you to autonomously research and write content or reports based on your findings.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lubdcg/so_does_anyone_have_a_good_workflow_to_replace/n1wo1fs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751937052,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lubdcg","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1xbhtn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ArchdukeofHyperbole","can_mod_post":false,"created_utc":1751945471,"send_replies":true,"parent_id":"t3_1lubdcg","score":0,"author_fullname":"t2_1p41v97q5d","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"That reminds me, when is chatgpt gonna release a local model?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1xbhtn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That reminds me, when is chatgpt gonna release a local model?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lubdcg/so_does_anyone_have_a_good_workflow_to_replace/n1xbhtn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751945471,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lubdcg","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1x4a1j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"created_utc":1751942709,"send_replies":true,"parent_id":"t1_n1wmzj1","score":1,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This is clearly untrue- try asking Qwen3 32b the Israel question even with Serper mcp.\\n\\nThe results will be pretty trash.","edited":1751943030,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1x4a1j","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is clearly untrue- try asking Qwen3 32b the Israel question even with Serper mcp.&lt;/p&gt;\\n\\n&lt;p&gt;The results will be pretty trash.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lubdcg","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lubdcg/so_does_anyone_have_a_good_workflow_to_replace/n1x4a1j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751942709,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1wmzj1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BusRevolutionary9893","can_mod_post":false,"created_utc":1751936693,"send_replies":true,"parent_id":"t3_1lubdcg","score":-3,"author_fullname":"t2_1by73qs5e5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Pretty much any reasoning model with search will give you better results than a Google search. ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1wmzj1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Pretty much any reasoning model with search will give you better results than a Google search. &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lubdcg/so_does_anyone_have_a_good_workflow_to_replace/n1wmzj1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751936693,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lubdcg","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-3}}],"before":null}}]`),s=()=>e.jsx(l,{data:a});export{s as default};
