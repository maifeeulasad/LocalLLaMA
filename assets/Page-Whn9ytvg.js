import{j as e}from"./index-BgwOAK4-.js";import{R as t}from"./RedditPostRenderer-BOBjDTFu.js";import"./index-BL22wVg5.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Been using mostly Claude Code, works great. Yet feels like Im starting to hit the limits of what it can do. Im wondering what others are using for coding? Last time I checked Gemini 2.5 Pro and o3 and o4, they did not felt on par with Claude, maybe things changed recently?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"What is the top model for coding?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m5x04m","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.4,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_m8971","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753136675,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Been using mostly Claude Code, works great. Yet feels like Im starting to hit the limits of what it can do. Im wondering what others are using for coding? Last time I checked Gemini 2.5 Pro and o3 and o4, they did not felt on par with Claude, maybe things changed recently?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1m5x04m","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"estebansaa","discussion_type":null,"num_comments":13,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m5x04m/what_is_the_top_model_for_coding/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m5x04m/what_is_the_top_model_for_coding/","subreddit_subscribers":502515,"created_utc":1753136675,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4fs487","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Efficiency_1144","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4fqr28","score":1,"author_fullname":"t2_1nkj9l14b0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It’s difficult because strong local coding model doesn’t currently exist yet.\\n\\n\\nKimi K2 comes closest in terms of actual syntax abilities but being a non-reasoning model puts a cap on the complexity it can handle.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4fs487","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It’s difficult because strong local coding model doesn’t currently exist yet.&lt;/p&gt;\\n\\n&lt;p&gt;Kimi K2 comes closest in terms of actual syntax abilities but being a non-reasoning model puts a cap on the complexity it can handle.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5x04m","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5x04m/what_is_the_top_model_for_coding/n4fs487/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753142991,"author_flair_text":null,"treatment_tags":[],"created_utc":1753142991,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4fqr28","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"quuuub","can_mod_post":false,"created_utc":1753142514,"send_replies":true,"parent_id":"t1_n4ffy36","score":1,"author_fullname":"t2_cal7sdfa","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"i agree, this is off topic","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4fqr28","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;i agree, this is off topic&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5x04m","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5x04m/what_is_the_top_model_for_coding/n4fqr28/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753142514,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4g083f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"estebansaa","can_mod_post":false,"created_utc":1753145807,"send_replies":true,"parent_id":"t1_n4ffy36","score":1,"author_fullname":"t2_m8971","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"really wish there was something I could run locally that does better than CLaude.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4g083f","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;really wish there was something I could run locally that does better than CLaude.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5x04m","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5x04m/what_is_the_top_model_for_coding/n4g083f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753145807,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4ffy36","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Voxandr","can_mod_post":false,"created_utc":1753138946,"send_replies":true,"parent_id":"t3_1m5x04m","score":6,"author_fullname":"t2_86dk0gye","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Can you keep things to locallms? Looks like new flood of vibe coders don't even know what local LLMs means","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ffy36","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Can you keep things to locallms? Looks like new flood of vibe coders don&amp;#39;t even know what local LLMs means&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5x04m/what_is_the_top_model_for_coding/n4ffy36/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753138946,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5x04m","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4fsf8b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Efficiency_1144","can_mod_post":false,"created_utc":1753143098,"send_replies":true,"parent_id":"t1_n4f9wa4","score":1,"author_fullname":"t2_1nkj9l14b0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"2.5 Flash handles agentic tasks well so even though it makes mistakes in its code if it is being walked through an agentic loop step by step it can fix them well.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4fsf8b","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;2.5 Flash handles agentic tasks well so even though it makes mistakes in its code if it is being walked through an agentic loop step by step it can fix them well.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5x04m","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5x04m/what_is_the_top_model_for_coding/n4fsf8b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753143098,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4f9wa4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jkh911208","can_mod_post":false,"created_utc":1753136972,"send_replies":true,"parent_id":"t3_1m5x04m","score":2,"author_fullname":"t2_eq49q5k","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Claude is top performer right now, 2.5 pro and 2.5 flash works great.\\n\\nsince I have to pay for Claude, I use Gemini CLI exclusively it use 2.5pro and move on to 2.5 flash after certain quota\\n\\nI am absolutely happy with its performance.\\n\\nI mainly write Python, other programming languages might experience something different","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4f9wa4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Claude is top performer right now, 2.5 pro and 2.5 flash works great.&lt;/p&gt;\\n\\n&lt;p&gt;since I have to pay for Claude, I use Gemini CLI exclusively it use 2.5pro and move on to 2.5 flash after certain quota&lt;/p&gt;\\n\\n&lt;p&gt;I am absolutely happy with its performance.&lt;/p&gt;\\n\\n&lt;p&gt;I mainly write Python, other programming languages might experience something different&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5x04m/what_is_the_top_model_for_coding/n4f9wa4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753136972,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5x04m","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ftnys","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Environmental-Metal9","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4fr1f0","score":1,"author_fullname":"t2_6x9o42az","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You know, for certain tasks yes, hands down not even a comparison. Opus4 was swiftly able to debug and improve a custom ONNX pipeline I was working on in two prompts, which no other model even got close to, not even sonnet3.7. But the reality is that I still reach for it quite often for pretty mundane tasks like “ok, here are the models, these are the service objects, write me the fastapi endpoint for foo” because that’s pretty rote and id rather use a cheaper model. But here DeepSeek (and I’m hoping to test with kimi k2 and qwen max too) does just as good of a job.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ftnys","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You know, for certain tasks yes, hands down not even a comparison. Opus4 was swiftly able to debug and improve a custom ONNX pipeline I was working on in two prompts, which no other model even got close to, not even sonnet3.7. But the reality is that I still reach for it quite often for pretty mundane tasks like “ok, here are the models, these are the service objects, write me the fastapi endpoint for foo” because that’s pretty rote and id rather use a cheaper model. But here DeepSeek (and I’m hoping to test with kimi k2 and qwen max too) does just as good of a job.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5x04m","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5x04m/what_is_the_top_model_for_coding/n4ftnys/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753143530,"author_flair_text":null,"treatment_tags":[],"created_utc":1753143530,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4fr1f0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Efficiency_1144","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4fbqz8","score":1,"author_fullname":"t2_1nkj9l14b0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Is it a big step up from Claude 3.7 Sonnet?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4fr1f0","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Is it a big step up from Claude 3.7 Sonnet?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5x04m","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5x04m/what_is_the_top_model_for_coding/n4fr1f0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753142616,"author_flair_text":null,"treatment_tags":[],"created_utc":1753142616,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4fbqz8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Environmental-Metal9","can_mod_post":false,"created_utc":1753137580,"send_replies":true,"parent_id":"t1_n4farm6","score":1,"author_fullname":"t2_6x9o42az","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Opus is the first model in a couple of years that feels like a real leap in ability to understand code, not just spit out code. But it is so damn expensive that I can’t help but feel like people like you and I aren’t the target consumers of it, but rather people hoping to augment their dev teams while shrinking their workforce. With Opus I mostly shift into product manager with a high degree of technical skills for 80% of the time, and senior dev the rest of the time where the app needs deep business logic. But the cost of using it ends up being a sizeable fraction of a software dev salary. I would say it is definitely about as good as a competent junior that you can delegate tasks to and just come back and check in from time to time. Sonnet 4 in comparison feels like the well intentioned little brother who definitely picked up a few tricks here and there but is largely cosplaying as the bigger brother","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4fbqz8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Opus is the first model in a couple of years that feels like a real leap in ability to understand code, not just spit out code. But it is so damn expensive that I can’t help but feel like people like you and I aren’t the target consumers of it, but rather people hoping to augment their dev teams while shrinking their workforce. With Opus I mostly shift into product manager with a high degree of technical skills for 80% of the time, and senior dev the rest of the time where the app needs deep business logic. But the cost of using it ends up being a sizeable fraction of a software dev salary. I would say it is definitely about as good as a competent junior that you can delegate tasks to and just come back and check in from time to time. Sonnet 4 in comparison feels like the well intentioned little brother who definitely picked up a few tricks here and there but is largely cosplaying as the bigger brother&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5x04m","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5x04m/what_is_the_top_model_for_coding/n4fbqz8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753137580,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4farm6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ForsookComparison","can_mod_post":false,"created_utc":1753137256,"send_replies":true,"parent_id":"t3_1m5x04m","score":2,"author_fullname":"t2_on5es7pe3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Claude 4.0 Sonnet is the best at implementing what you know you want to implement.\\n\\nDeepseek-R1-0528 beats Sonnet in problem solving and debugging, but isn't quite as strong a coder. When Sonnet fails to fix something and I can't guide it to exactly where the fault in logic exists, Deepseek-r1-0528 tends to be my savior.\\n\\nDeepseek-V3-0324 is the best open-weight straight-shot model. It is an order of magnitude cheaper than Sonnet and Opus and generally gets the job done.\\n\\nQwen3-235-a22b (the \\"old\\" one as of a few hours ago) is the best for quick edits where you know what you want changed. Llama4-Maverick isn't terrible for this, but I've phased it out since.\\n\\nOpus is ridiculously good but I can't afford to use it long enough to tell you more than that.\\n\\no3 pro is probably best but my wallet cannot survive the cost of Opus the *REASONING* tokens of o3.","edited":1753137845,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4farm6","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Claude 4.0 Sonnet is the best at implementing what you know you want to implement.&lt;/p&gt;\\n\\n&lt;p&gt;Deepseek-R1-0528 beats Sonnet in problem solving and debugging, but isn&amp;#39;t quite as strong a coder. When Sonnet fails to fix something and I can&amp;#39;t guide it to exactly where the fault in logic exists, Deepseek-r1-0528 tends to be my savior.&lt;/p&gt;\\n\\n&lt;p&gt;Deepseek-V3-0324 is the best open-weight straight-shot model. It is an order of magnitude cheaper than Sonnet and Opus and generally gets the job done.&lt;/p&gt;\\n\\n&lt;p&gt;Qwen3-235-a22b (the &amp;quot;old&amp;quot; one as of a few hours ago) is the best for quick edits where you know what you want changed. Llama4-Maverick isn&amp;#39;t terrible for this, but I&amp;#39;ve phased it out since.&lt;/p&gt;\\n\\n&lt;p&gt;Opus is ridiculously good but I can&amp;#39;t afford to use it long enough to tell you more than that.&lt;/p&gt;\\n\\n&lt;p&gt;o3 pro is probably best but my wallet cannot survive the cost of Opus the &lt;em&gt;REASONING&lt;/em&gt; tokens of o3.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5x04m/what_is_the_top_model_for_coding/n4farm6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753137256,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m5x04m","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ffeu8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Creepy-Potential3408","can_mod_post":false,"created_utc":1753138770,"send_replies":true,"parent_id":"t3_1m5x04m","score":1,"author_fullname":"t2_1tz5xea0uy","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Gemini 2.5 Pro now has a giant 2M token context, great code quality, and fewer “hallucinations,” while GPT-4o is close to Claude with 1M tokens and strong integration. Both now rival or surpass Claude in many tasks. definitely worth revisiting. I use each AI for it's strengths per the task.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ffeu8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Gemini 2.5 Pro now has a giant 2M token context, great code quality, and fewer “hallucinations,” while GPT-4o is close to Claude with 1M tokens and strong integration. Both now rival or surpass Claude in many tasks. definitely worth revisiting. I use each AI for it&amp;#39;s strengths per the task.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5x04m/what_is_the_top_model_for_coding/n4ffeu8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753138770,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5x04m","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(t,{data:l});export{r as default};
