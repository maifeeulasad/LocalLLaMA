import{j as e}from"./index-DACS7Nh6.js";import{R as l}from"./RedditPostRenderer-Dqa1NZuX.js";import"./index-DiMIVQx4.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I just checked the monthly LLM API costs at my firm, and it's insanely high. I don’t see this being sustainable for much longer. Eventually, senior management will realize it and start cutting down on these expenses. Companies will likely shift towards hosting smaller LLMs internally for agentic use cases instead of relying on external APIs.\\n\\nAnd honestly, who better to understand the nitty-gritty details of an ML model than data scientists? For the past two years, it felt like ML engineers were contributing more than data scientists, but I think that trend is going to slowly reverse.\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Traditional Data Science work is going to be back","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lxvrjm","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.81,"author_flair_background_color":null,"subreddit_type":"public","ups":43,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_h1zso7cq","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":43,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752310104,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I just checked the monthly LLM API costs at my firm, and it&amp;#39;s insanely high. I don’t see this being sustainable for much longer. Eventually, senior management will realize it and start cutting down on these expenses. Companies will likely shift towards hosting smaller LLMs internally for agentic use cases instead of relying on external APIs.&lt;/p&gt;\\n\\n&lt;p&gt;And honestly, who better to understand the nitty-gritty details of an ML model than data scientists? For the past two years, it felt like ML engineers were contributing more than data scientists, but I think that trend is going to slowly reverse.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lxvrjm","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Competitive_Push5407","discussion_type":null,"num_comments":42,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/","subreddit_subscribers":498346,"created_utc":1752310104,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2snbw1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mr_birkenblatt","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2s1gcv","score":3,"author_fullname":"t2_d7qfg","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Reminds me of the person who asked their boss why they lease the office building, surely buying it would be cheaper and the boss said \\"we're not in the building management business\\"\\n\\n\\nunless you have expertise already in house, just hiring people to manage the model api infra would probably cost as much as they're paying currently. and it's not even clear whether companies offering llm apis (eg anthropic/openai) actually have positive cash flow (they definitely do not because of r&amp;d to stay relevant; but it's unclear if even just the inference endpoint management is profitable in itself)","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n2snbw1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Reminds me of the person who asked their boss why they lease the office building, surely buying it would be cheaper and the boss said &amp;quot;we&amp;#39;re not in the building management business&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;unless you have expertise already in house, just hiring people to manage the model api infra would probably cost as much as they&amp;#39;re paying currently. and it&amp;#39;s not even clear whether companies offering llm apis (eg anthropic/openai) actually have positive cash flow (they definitely do not because of r&amp;amp;d to stay relevant; but it&amp;#39;s unclear if even just the inference endpoint management is profitable in itself)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lxvrjm","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2snbw1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752356160,"author_flair_text":null,"treatment_tags":[],"created_utc":1752356160,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2s1gcv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"PhilosophyforOne","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2rzzq5","score":7,"author_fullname":"t2_cyla8","approved_by":null,"mod_note":null,"all_awardings":[],"body":"uhmm, not really. 1) You’d still need to fork over like half a mil, and that’s assuming concurrent traffic is low enough that you could handle it on a single dgx. \\n\\nIf your api costs are in the 80-100k range, you’d likely need multiple dgx’s. Plus you’d need people to set up, manage and run those. Suddenly you’re adding a total of a few fte’s atleast for load balancing, management, networks etc. Shit adds up fast. \\n\\nI doubt that unless you’re talking 7-8 digits annually it’s really going to make sense to byo from financial perspective, and even then you’re tying up a lot of capex and resources. + Depreciation, which neither your CFO and CTO will be a fan of.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n2s1gcv","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;uhmm, not really. 1) You’d still need to fork over like half a mil, and that’s assuming concurrent traffic is low enough that you could handle it on a single dgx. &lt;/p&gt;\\n\\n&lt;p&gt;If your api costs are in the 80-100k range, you’d likely need multiple dgx’s. Plus you’d need people to set up, manage and run those. Suddenly you’re adding a total of a few fte’s atleast for load balancing, management, networks etc. Shit adds up fast. &lt;/p&gt;\\n\\n&lt;p&gt;I doubt that unless you’re talking 7-8 digits annually it’s really going to make sense to byo from financial perspective, and even then you’re tying up a lot of capex and resources. + Depreciation, which neither your CFO and CTO will be a fan of.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lxvrjm","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2s1gcv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752349072,"author_flair_text":null,"treatment_tags":[],"created_utc":1752349072,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2scgto","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullOf_Bad_Ideas","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2rzzq5","score":3,"author_fullname":"t2_9s7pmakgx","approved_by":null,"mod_note":null,"all_awardings":[],"body":"You can't run Claude 4 Sonnet/Opus or OpenAI 4o/o3/o4-mini on 8xB200 DGX.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n2scgto","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can&amp;#39;t run Claude 4 Sonnet/Opus or OpenAI 4o/o3/o4-mini on 8xB200 DGX.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lxvrjm","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2scgto/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752352609,"author_flair_text":null,"treatment_tags":[],"created_utc":1752352609,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2rzzq5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MelodicRecognition7","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2rir5v","score":2,"author_fullname":"t2_1eex9ug5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"and absolutely insane compared to hardware costs, a DGX with 8xB200 would pay for itself in like 2 months.","edited":false,"author_flair_css_class":null,"name":"t1_n2rzzq5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;and absolutely insane compared to hardware costs, a DGX with 8xB200 would pay for itself in like 2 months.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lxvrjm","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2rzzq5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752348592,"author_flair_text":null,"collapsed":false,"created_utc":1752348592,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2rir5v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"mr_birkenblatt","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2qke2a","score":8,"author_fullname":"t2_d7qfg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Compared to payroll that is actually a tiny amount","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2rir5v","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Compared to payroll that is actually a tiny amount&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxvrjm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2rir5v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752343104,"author_flair_text":null,"treatment_tags":[],"created_utc":1752343104,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}}],"before":null}},"user_reports":[],"saved":false,"id":"n2qke2a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MelodicRecognition7","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2qgvox","score":8,"author_fullname":"t2_1eex9ug5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"omfg, that's 80k+/mo","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2qke2a","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;omfg, that&amp;#39;s 80k+/mo&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxvrjm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2qke2a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752332519,"author_flair_text":null,"treatment_tags":[],"created_utc":1752332519,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2scsnh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullOf_Bad_Ideas","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2qvmo1","score":2,"author_fullname":"t2_9s7pmakgx","approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; And also we aren't really using the best model out there. We are using Claude sonnet 4 for most cases\\n\\nClaude 4 Sonnet is one of the best models out there. You can't run a comparable model locally right now. DeepSeek R1/R1-0528/V3-0324 is ok for some things but it's worse at agentic coding or coding in general.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n2scsnh","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;And also we aren&amp;#39;t really using the best model out there. We are using Claude sonnet 4 for most cases&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Claude 4 Sonnet is one of the best models out there. You can&amp;#39;t run a comparable model locally right now. DeepSeek R1/R1-0528/V3-0324 is ok for some things but it&amp;#39;s worse at agentic coding or coding in general.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lxvrjm","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2scsnh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752352715,"author_flair_text":null,"treatment_tags":[],"created_utc":1752352715,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2qvmo1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Competitive_Push5407","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2qtgv1","score":3,"author_fullname":"t2_h1zso7cq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That's true. We have access to only a set of llms. Currently, constrained on time and resources to optimise the workflow for costs.\\n\\nAnd also we aren't really using the best model out there. We are using Claude sonnet 4 for most cases. It's around 3 dollars for a million input tokens.","edited":1752336268,"author_flair_css_class":null,"name":"t1_n2qvmo1","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s true. We have access to only a set of llms. Currently, constrained on time and resources to optimise the workflow for costs.&lt;/p&gt;\\n\\n&lt;p&gt;And also we aren&amp;#39;t really using the best model out there. We are using Claude sonnet 4 for most cases. It&amp;#39;s around 3 dollars for a million input tokens.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lxvrjm","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2qvmo1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752336027,"author_flair_text":null,"collapsed":false,"created_utc":1752336027,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2qtgv1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SryUsrNameIsTaken","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2qqetw","score":1,"author_fullname":"t2_8w2fagrd","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"OP might have restrictions on approved models, which could drive up price. Or a particular model for a particular set of use cases. I don’t think low seven figure spend on third party LLM’s is actually that much for a medium to large org, especially when you consider that’s 5-10 total cost FTEs.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2qtgv1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;OP might have restrictions on approved models, which could drive up price. Or a particular model for a particular set of use cases. I don’t think low seven figure spend on third party LLM’s is actually that much for a medium to large org, especially when you consider that’s 5-10 total cost FTEs.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxvrjm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2qtgv1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752335356,"author_flair_text":null,"treatment_tags":[],"created_utc":1752335356,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2qqetw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"IcyUse33","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2qgvox","score":3,"author_fullname":"t2_ua8a2ccha","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"How?\\n\\nCache common inputs. Switch to \\"mini\\" or \\"flash\\" models. \\n\\nGemini 2.5-flash is a fraction of the Pro price per API call but nearly as good. If it's just summarizing documents and contracts you don't need Pro or Opus4 for that.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2qqetw","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How?&lt;/p&gt;\\n\\n&lt;p&gt;Cache common inputs. Switch to &amp;quot;mini&amp;quot; or &amp;quot;flash&amp;quot; models. &lt;/p&gt;\\n\\n&lt;p&gt;Gemini 2.5-flash is a fraction of the Pro price per API call but nearly as good. If it&amp;#39;s just summarizing documents and contracts you don&amp;#39;t need Pro or Opus4 for that.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxvrjm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2qqetw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752334413,"author_flair_text":null,"treatment_tags":[],"created_utc":1752334413,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2qgvox","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Competitive_Push5407","can_mod_post":false,"created_utc":1752331401,"send_replies":true,"parent_id":"t1_n2p6ne4","score":13,"author_fullname":"t2_h1zso7cq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This year, it would cross a million USD","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2qgvox","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This year, it would cross a million USD&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxvrjm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2qgvox/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752331401,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":13}}],"before":null}},"user_reports":[],"saved":false,"id":"n2p6ne4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MelodicRecognition7","can_mod_post":false,"created_utc":1752310774,"send_replies":true,"parent_id":"t3_1lxvrjm","score":14,"author_fullname":"t2_1eex9ug5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"could you share the costs please, is it 3, 4, 5 digits in USD?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2p6ne4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;could you share the costs please, is it 3, 4, 5 digits in USD?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2p6ne4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752310774,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxvrjm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":14}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2q9zrq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"a_slay_nub","can_mod_post":false,"created_utc":1752329113,"send_replies":true,"parent_id":"t3_1lxvrjm","score":5,"author_fullname":"t2_u8o4d","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you think API costs are expensive, wait until you see my hourly rate plus GPU costs","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2q9zrq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you think API costs are expensive, wait until you see my hourly rate plus GPU costs&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2q9zrq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752329113,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxvrjm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"d2642412-d9ce-11ed-ae30-32b11309f5bd","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2pw6k4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"annakhouri2150","can_mod_post":false,"created_utc":1752324003,"send_replies":true,"parent_id":"t1_n2p9es2","score":16,"author_fullname":"t2_1jmvw9zdnb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"There's actually no evidence that LLM Cloud providers are under pricing their product that much:\\n\\n\\n\\"\\n\\nThe LLM API prices must be subsidized to grab market share -- i.e. the prices might be low, but the costs are high - I don't think they are, for a few reasons. I'd instead assume APIs are typically profitable on a unit basis. I have not found any credible analysis suggesting otherwise.\\n\\nFirst, there's not that much motive to gain API market share with unsustainably cheap prices. Any gains would be temporary, since there's no long-term lock-in, and better models are released weekly. Data from paid API queries will also typically not be used for training or tuning the models, so getting access to more data wouldn't explain it. Note that it's not just that you'd be losing money on each of these queries for no benefit, you're losing the compute that could be spent on training, research, or more useful types of inference.\\n\\nSecond, some of those models have been released with open weights and API access is also available from third-party providers who would have no motive to subsidize inference. (Or the number in the table isn't even first party hosting -- I sure can't figure out what the Vertex AI pricing for Gemma 3 is). The pricing of those third-party hosted APIs appears competitive with first-party hosted APIs. For example, the Artificial Analysis summary on Deepseek R1 hosting.\\n\\nThird, Deepseek released actual numbers on their inference efficiency in February. Those numbers suggest that their normal R1 API pricing has about 80% margins when considering the GPU costs, though not any other serving costs.\\n\\nFourth, there are a bunch of first-principles analyses on the cost structure of models with various architectures should be. Those are of course mathematical models, but those costs line up pretty well with the observed end-user pricing of models whose architecture is known. See the references section for links.\\"\\n\\nhttps://www.snellman.net/blog/archive/2025-06-02-llms-are-cheap/","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2pw6k4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There&amp;#39;s actually no evidence that LLM Cloud providers are under pricing their product that much:&lt;/p&gt;\\n\\n&lt;p&gt;&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;The LLM API prices must be subsidized to grab market share -- i.e. the prices might be low, but the costs are high - I don&amp;#39;t think they are, for a few reasons. I&amp;#39;d instead assume APIs are typically profitable on a unit basis. I have not found any credible analysis suggesting otherwise.&lt;/p&gt;\\n\\n&lt;p&gt;First, there&amp;#39;s not that much motive to gain API market share with unsustainably cheap prices. Any gains would be temporary, since there&amp;#39;s no long-term lock-in, and better models are released weekly. Data from paid API queries will also typically not be used for training or tuning the models, so getting access to more data wouldn&amp;#39;t explain it. Note that it&amp;#39;s not just that you&amp;#39;d be losing money on each of these queries for no benefit, you&amp;#39;re losing the compute that could be spent on training, research, or more useful types of inference.&lt;/p&gt;\\n\\n&lt;p&gt;Second, some of those models have been released with open weights and API access is also available from third-party providers who would have no motive to subsidize inference. (Or the number in the table isn&amp;#39;t even first party hosting -- I sure can&amp;#39;t figure out what the Vertex AI pricing for Gemma 3 is). The pricing of those third-party hosted APIs appears competitive with first-party hosted APIs. For example, the Artificial Analysis summary on Deepseek R1 hosting.&lt;/p&gt;\\n\\n&lt;p&gt;Third, Deepseek released actual numbers on their inference efficiency in February. Those numbers suggest that their normal R1 API pricing has about 80% margins when considering the GPU costs, though not any other serving costs.&lt;/p&gt;\\n\\n&lt;p&gt;Fourth, there are a bunch of first-principles analyses on the cost structure of models with various architectures should be. Those are of course mathematical models, but those costs line up pretty well with the observed end-user pricing of models whose architecture is known. See the references section for links.&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.snellman.net/blog/archive/2025-06-02-llms-are-cheap/\\"&gt;https://www.snellman.net/blog/archive/2025-06-02-llms-are-cheap/&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxvrjm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2pw6k4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752324003,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":16}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ph67d","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"meta_voyager7","can_mod_post":false,"created_utc":1752317036,"send_replies":true,"parent_id":"t1_n2p9es2","score":2,"author_fullname":"t2_1fywh12qqy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Also small open source llms in 1 to 2 years would be as good as gpt 4.1 or O3. So it wont need as much as energy, also harder and software for llm inference is getting optimized like what happened with cpus ","edited":1752322387,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ph67d","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Also small open source llms in 1 to 2 years would be as good as gpt 4.1 or O3. So it wont need as much as energy, also harder and software for llm inference is getting optimized like what happened with cpus &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxvrjm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2ph67d/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752317036,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2q18rn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"moofunk","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2pgwr5","score":2,"author_fullname":"t2_6gpjg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Solar is the easiest to install in sizes small enough for your needs.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2q18rn","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Solar is the easiest to install in sizes small enough for your needs.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxvrjm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2q18rn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752325955,"author_flair_text":null,"treatment_tags":[],"created_utc":1752325955,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2pgwr5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"meta_voyager7","can_mod_post":false,"created_utc":1752316892,"send_replies":true,"parent_id":"t1_n2p9es2","score":1,"author_fullname":"t2_1fywh12qqy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"why solar panel related companies would be winners? there are other green energy companies like wind turbine, hydro, hydrogen etc","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2pgwr5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;why solar panel related companies would be winners? there are other green energy companies like wind turbine, hydro, hydrogen etc&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxvrjm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2pgwr5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752316892,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2szl5p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullOf_Bad_Ideas","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2sn6gl","score":1,"author_fullname":"t2_9s7pmakgx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah, I agree.","edited":false,"author_flair_css_class":null,"name":"t1_n2szl5p","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, I agree.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lxvrjm","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2szl5p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752360380,"author_flair_text":null,"collapsed":false,"created_utc":1752360380,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2sn6gl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LazloStPierre","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2sdcov","score":3,"author_fullname":"t2_f5s12","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yeah I'm not arguing the companies themselves are profitable, because they invest a ridiculous amount in infrastructure and R&amp;D, but I do not believe their APIs are being sold below cost (as in, it costs them less to serve me a token than I pay them for it, not as in it's profitable post all development). They're almost certainly being sold at quite a profit\\n\\nSo future SOTA models may be more expensive than current ones for all those reasons, but there's no reason to believe costs for current models would do anything but come down as hardware gets more powerful and efficient. And I'd imagine if VC money suddenly stopped coming into OpenAI, or Google just stopped putting their own money into their prorgram, you'd b more likely to see the R&amp;D slow down and the SOTA model page move much slower than the costs skyrocket to being unaffordable I'd assume","edited":1752356372,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2sn6gl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah I&amp;#39;m not arguing the companies themselves are profitable, because they invest a ridiculous amount in infrastructure and R&amp;amp;D, but I do not believe their APIs are being sold below cost (as in, it costs them less to serve me a token than I pay them for it, not as in it&amp;#39;s profitable post all development). They&amp;#39;re almost certainly being sold at quite a profit&lt;/p&gt;\\n\\n&lt;p&gt;So future SOTA models may be more expensive than current ones for all those reasons, but there&amp;#39;s no reason to believe costs for current models would do anything but come down as hardware gets more powerful and efficient. And I&amp;#39;d imagine if VC money suddenly stopped coming into OpenAI, or Google just stopped putting their own money into their prorgram, you&amp;#39;d b more likely to see the R&amp;amp;D slow down and the SOTA model page move much slower than the costs skyrocket to being unaffordable I&amp;#39;d assume&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxvrjm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2sn6gl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752356111,"author_flair_text":null,"treatment_tags":[],"created_utc":1752356111,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2sdcov","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullOf_Bad_Ideas","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2qynoq","score":1,"author_fullname":"t2_9s7pmakgx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This argument hinges on the development cost.\\n\\nInference of those models is more expensive than hardware renting used to host them. But it's not expensive enough to recoup the development cost of those models - engineer hours and compute for all of the runs needed to create a model.\\n\\nROI would look better if improvements would slow down, but as models get better and better, and require more and more compute, old models lose use and are probably negative in terms of absolute revenue. If models will be leading the market for longer and improvement will slow down, there's a higher chance of turning a profit.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2sdcov","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This argument hinges on the development cost.&lt;/p&gt;\\n\\n&lt;p&gt;Inference of those models is more expensive than hardware renting used to host them. But it&amp;#39;s not expensive enough to recoup the development cost of those models - engineer hours and compute for all of the runs needed to create a model.&lt;/p&gt;\\n\\n&lt;p&gt;ROI would look better if improvements would slow down, but as models get better and better, and require more and more compute, old models lose use and are probably negative in terms of absolute revenue. If models will be leading the market for longer and improvement will slow down, there&amp;#39;s a higher chance of turning a profit.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxvrjm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2sdcov/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752352894,"author_flair_text":null,"treatment_tags":[],"created_utc":1752352894,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2qynoq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LazloStPierre","can_mod_post":false,"created_utc":1752336960,"send_replies":true,"parent_id":"t1_n2p9es2","score":1,"author_fullname":"t2_f5s12","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"\\"Remember cloud LLMs are heavily subsidized by VCs\\"\\n\\n\\nWhy do you assume this? Looking at equivalent capability open source model pricing would imply they're actually making a pretty tidy profit, since they're usually cheaper and we know those are sold at a profit and likely on less efficient hardware and built by lower funded research teams ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2qynoq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&amp;quot;Remember cloud LLMs are heavily subsidized by VCs&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;Why do you assume this? Looking at equivalent capability open source model pricing would imply they&amp;#39;re actually making a pretty tidy profit, since they&amp;#39;re usually cheaper and we know those are sold at a profit and likely on less efficient hardware and built by lower funded research teams &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxvrjm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2qynoq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752336960,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2p9es2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ortegaalfredo","can_mod_post":false,"created_utc":1752312481,"send_replies":true,"parent_id":"t3_1lxvrjm","score":27,"author_fullname":"t2_g177e","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Remember cloud LLMs are heavily subsidized by VCs, when that stop prices will get higher.\\n\\nThen business will start deploying local agents just for their power bill to skyrocket.\\n\\nThe ultimate winner of AI business will be the solar panel vendors.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2p9es2","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Alpaca"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Remember cloud LLMs are heavily subsidized by VCs, when that stop prices will get higher.&lt;/p&gt;\\n\\n&lt;p&gt;Then business will start deploying local agents just for their power bill to skyrocket.&lt;/p&gt;\\n\\n&lt;p&gt;The ultimate winner of AI business will be the solar panel vendors.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2p9es2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752312481,"author_flair_text":"Alpaca","treatment_tags":[],"link_id":"t3_1lxvrjm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bd9e9e","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":27}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2qujgn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FuguSandwich","can_mod_post":false,"created_utc":1752335688,"send_replies":true,"parent_id":"t3_1lxvrjm","score":4,"author_fullname":"t2_r3xju","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I keep saying the same thing about AI Agents.  Running an LLM in a loop to execute a deterministic set of tasks that could have just been done in under 100 lines of code burning a crazy amount of tokens can't possibly be the future of application development.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2qujgn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I keep saying the same thing about AI Agents.  Running an LLM in a loop to execute a deterministic set of tasks that could have just been done in under 100 lines of code burning a crazy amount of tokens can&amp;#39;t possibly be the future of application development.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2qujgn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752335688,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxvrjm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"d2642412-d9ce-11ed-ae30-32b11309f5bd","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2r2pn9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Competitive_Push5407","can_mod_post":false,"created_utc":1752338238,"send_replies":true,"parent_id":"t1_n2r1y8h","score":3,"author_fullname":"t2_h1zso7cq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Almost the same scenario in my company too. Just a few months behind than your company I guess","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2r2pn9","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Almost the same scenario in my company too. Just a few months behind than your company I guess&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxvrjm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2r2pn9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752338238,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2r1y8h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"harrro","can_mod_post":false,"created_utc":1752337998,"send_replies":true,"parent_id":"t3_1lxvrjm","score":5,"author_fullname":"t2_4axt7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"My company also went all in on AI mandating every dev to use it with at least monthly reminders from managers to use it all the time.\\n\\nIn the last month, they suddenly announced they want us to use cheap models mostly and that there is now a limit on how much we can use it.\\n\\nLooks like they finally took a look at the bill.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2r1y8h","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Alpaca"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;My company also went all in on AI mandating every dev to use it with at least monthly reminders from managers to use it all the time.&lt;/p&gt;\\n\\n&lt;p&gt;In the last month, they suddenly announced they want us to use cheap models mostly and that there is now a limit on how much we can use it.&lt;/p&gt;\\n\\n&lt;p&gt;Looks like they finally took a look at the bill.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2r1y8h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752337998,"author_flair_text":"Alpaca","treatment_tags":[],"link_id":"t3_1lxvrjm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bd9e9e","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2r8fbm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RhubarbSimilar1683","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2qvy8h","score":1,"author_fullname":"t2_1k4sjdwzk2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I don't think hiring back employees will actually be back like what you said in your title. ","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2r8fbm","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t think hiring back employees will actually be back like what you said in your title. &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxvrjm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2r8fbm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752339995,"author_flair_text":null,"treatment_tags":[],"created_utc":1752339995,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2qvy8h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Competitive_Push5407","can_mod_post":false,"created_utc":1752336127,"send_replies":true,"parent_id":"t1_n2qgpgj","score":1,"author_fullname":"t2_h1zso7cq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Not debating on the effectiveness of LLM systems but when there is scope to reduce costs, they will definitely do it. It's just a matter of time.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2qvy8h","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not debating on the effectiveness of LLM systems but when there is scope to reduce costs, they will definitely do it. It&amp;#39;s just a matter of time.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxvrjm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2qvy8h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752336127,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2qgpgj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Faintly_glowing_fish","can_mod_post":false,"created_utc":1752331345,"send_replies":true,"parent_id":"t3_1lxvrjm","score":1,"author_fullname":"t2_97avhniv","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"They cost a lot yes, but they do a lot more than one ds though.   \\n\\nIf Claude is running full time it actually is about the same hourly wage as an average dev.  Their quality is lower than an average dev but the throughput is an order of magnitude higher.\\n\\nYou will need to compare their cost to 20 entry level employees","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2qgpgj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They cost a lot yes, but they do a lot more than one ds though.   &lt;/p&gt;\\n\\n&lt;p&gt;If Claude is running full time it actually is about the same hourly wage as an average dev.  Their quality is lower than an average dev but the throughput is an order of magnitude higher.&lt;/p&gt;\\n\\n&lt;p&gt;You will need to compare their cost to 20 entry level employees&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2qgpgj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752331345,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxvrjm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2rletu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Competitive_Push5407","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2rex7y","score":1,"author_fullname":"t2_h1zso7cq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"True. R&amp;D is very neglected here.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2rletu","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;True. R&amp;amp;D is very neglected here.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxvrjm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2rletu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752343929,"author_flair_text":null,"treatment_tags":[],"created_utc":1752343929,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2rex7y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RhubarbSimilar1683","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2rb5bb","score":1,"author_fullname":"t2_1k4sjdwzk2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":" \\n\\n\\n&gt;research that doesn't generate revenue is a big no\\n\\n\\nThis is incredibly short sighted. This explains why India is struggling in AI. no one thought ChatGPT would generate revenue. maybe you can change that. Every research project is a potential money maker just like Deepseek R1.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2rex7y","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt; &lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;research that doesn&amp;#39;t generate revenue is a big no&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;This is incredibly short sighted. This explains why India is struggling in AI. no one thought ChatGPT would generate revenue. maybe you can change that. Every research project is a potential money maker just like Deepseek R1.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxvrjm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2rex7y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752341947,"author_flair_text":null,"treatment_tags":[],"created_utc":1752341947,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2rb5bb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Competitive_Push5407","can_mod_post":false,"created_utc":1752340815,"send_replies":true,"parent_id":"t1_n2r7xpj","score":0,"author_fullname":"t2_h1zso7cq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"True but in the Indian IT industry, research that doesn't generate revenue is a big no. So, the role expectations are quite different.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2rb5bb","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;True but in the Indian IT industry, research that doesn&amp;#39;t generate revenue is a big no. So, the role expectations are quite different.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxvrjm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2rb5bb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752340815,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n2r7xpj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RhubarbSimilar1683","can_mod_post":false,"created_utc":1752339846,"send_replies":true,"parent_id":"t3_1lxvrjm","score":2,"author_fullname":"t2_1k4sjdwzk2","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt;nitty-gritty details of an ML model than data scientists\\n\\n\\nIsn't this a misconception? There are specialized masters and PHDs for AI and machine learning. Don't you need one of those? ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2r7xpj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;nitty-gritty details of an ML model than data scientists&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Isn&amp;#39;t this a misconception? There are specialized masters and PHDs for AI and machine learning. Don&amp;#39;t you need one of those? &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2r7xpj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752339846,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxvrjm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2rbg0b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"960be6dde311","can_mod_post":false,"created_utc":1752340904,"send_replies":true,"parent_id":"t3_1lxvrjm","score":2,"author_fullname":"t2_1mf6icgwm4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Use Claude 3.5 Haiku or Google Gemini 2.5 Flash. They're both inexpensive.\\n\\nIf you're constantly generating massive responses from the latest Claude 3.5 or 4 Sonnet, then yeah, it'll get expensive. Do you have an infinite loop that's sending prompts to these models or something?\\n\\nI self-host Ollama on multiple systems, and it's definitely nice to have the option of using it for privacy.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2rbg0b","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Use Claude 3.5 Haiku or Google Gemini 2.5 Flash. They&amp;#39;re both inexpensive.&lt;/p&gt;\\n\\n&lt;p&gt;If you&amp;#39;re constantly generating massive responses from the latest Claude 3.5 or 4 Sonnet, then yeah, it&amp;#39;ll get expensive. Do you have an infinite loop that&amp;#39;s sending prompts to these models or something?&lt;/p&gt;\\n\\n&lt;p&gt;I self-host Ollama on multiple systems, and it&amp;#39;s definitely nice to have the option of using it for privacy.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2rbg0b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752340904,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxvrjm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2rgo63","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DoomsdayMcDoom","can_mod_post":false,"created_utc":1752342471,"send_replies":true,"parent_id":"t3_1lxvrjm","score":2,"author_fullname":"t2_a5oh21ly","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Cost of inference will come down quick.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2rgo63","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Cost of inference will come down quick.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2rgo63/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752342471,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxvrjm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2pxhhp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BidWestern1056","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2ps1id","score":1,"author_fullname":"t2_uzxql7po","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"the core npcpy library took me about 6-7 months to get to more or less stable point. past couple months been simplifying and trying to build on top of it. for a while i tried to have this kind of interactivity be a \\"data\\" mode within npcsh but just couldnt figure out a way that made sense that wasnt just like implicitly assuming pandas or st or other. in the end i realized i could just do the thing i had initially intended which was to apply the npcsh flow (assume bash, otherwise natural language) to python and then the first version of this took me a day or two. been using it for some research consulting ive been doing over the past month or so so trying to ensure as few bugs as possible.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2pxhhp","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;the core npcpy library took me about 6-7 months to get to more or less stable point. past couple months been simplifying and trying to build on top of it. for a while i tried to have this kind of interactivity be a &amp;quot;data&amp;quot; mode within npcsh but just couldnt figure out a way that made sense that wasnt just like implicitly assuming pandas or st or other. in the end i realized i could just do the thing i had initially intended which was to apply the npcsh flow (assume bash, otherwise natural language) to python and then the first version of this took me a day or two. been using it for some research consulting ive been doing over the past month or so so trying to ensure as few bugs as possible.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxvrjm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2pxhhp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752324521,"author_flair_text":null,"treatment_tags":[],"created_utc":1752324521,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ps1id","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"__JockY__","can_mod_post":false,"created_utc":1752322282,"send_replies":true,"parent_id":"t1_n2pfkr3","score":2,"author_fullname":"t2_qf8h7ka8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That must have taken a while. 😎","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ps1id","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That must have taken a while. 😎&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxvrjm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2ps1id/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752322282,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2pfkr3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BidWestern1056","can_mod_post":false,"created_utc":1752316141,"send_replies":true,"parent_id":"t3_1lxvrjm","score":1,"author_fullname":"t2_uzxql7po","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"one of the key pain points for me as a data scientist was not having AI integrated well into my iterative python lifestyle, so i built a modified python repl which lets you have AI execute code directly and build with you as you go, and the variables and functions it produces you can then inspect directly to build on yourself.\\nworks w local or api models, and it is pomodoro inspired to encourage you to occasionally take your experimentation and turn it into automations so you dont get lost in a sea of tinkering.\\ncheck it out you may like it as a DS urself\\nhttps://github.com/NPC-Worldwide/npcsh?tab=readme-ov-file#guac","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2pfkr3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;one of the key pain points for me as a data scientist was not having AI integrated well into my iterative python lifestyle, so i built a modified python repl which lets you have AI execute code directly and build with you as you go, and the variables and functions it produces you can then inspect directly to build on yourself.\\nworks w local or api models, and it is pomodoro inspired to encourage you to occasionally take your experimentation and turn it into automations so you dont get lost in a sea of tinkering.\\ncheck it out you may like it as a DS urself\\n&lt;a href=\\"https://github.com/NPC-Worldwide/npcsh?tab=readme-ov-file#guac\\"&gt;https://github.com/NPC-Worldwide/npcsh?tab=readme-ov-file#guac&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2pfkr3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752316141,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxvrjm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2p6wbh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Fit-Produce420","can_mod_post":false,"created_utc":1752310928,"send_replies":true,"parent_id":"t3_1lxvrjm","score":1,"author_fullname":"t2_tewf9bdwg","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Bro, this new automation tech works. It's not just some Dragon Naturally Speaking on steroids this time. We're close, we had 10e24 units of compute, it browns out some grids and warms up the salmon, but bro we only need to scale to 10e48 units of compute and the benches will be very close to wiping out jobs across the board!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2p6wbh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Bro, this new automation tech works. It&amp;#39;s not just some Dragon Naturally Speaking on steroids this time. We&amp;#39;re close, we had 10e24 units of compute, it browns out some grids and warms up the salmon, but bro we only need to scale to 10e48 units of compute and the benches will be very close to wiping out jobs across the board!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2p6wbh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752310928,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxvrjm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2rajy9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Competitive_Push5407","can_mod_post":false,"created_utc":1752340637,"send_replies":true,"parent_id":"t1_n2r3v5k","score":1,"author_fullname":"t2_h1zso7cq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I come from India. Here, those roles are quite rare. Either you are a data scientist/ ML engineer. Recently, AI engineer roles are picking up but not research engineer roles.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2rajy9","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I come from India. Here, those roles are quite rare. Either you are a data scientist/ ML engineer. Recently, AI engineer roles are picking up but not research engineer roles.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxvrjm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2rajy9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752340637,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2r3v5k","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"enoonoone","can_mod_post":false,"created_utc":1752338603,"send_replies":true,"parent_id":"t3_1lxvrjm","score":1,"author_fullname":"t2_19mks4ip","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"There’s probably a reason that a Research Scientist/ Engineer is not called a data scientist.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2r3v5k","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There’s probably a reason that a Research Scientist/ Engineer is not called a data scientist.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2r3v5k/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752338603,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxvrjm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2sbn19","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullOf_Bad_Ideas","can_mod_post":false,"created_utc":1752352345,"send_replies":true,"parent_id":"t3_1lxvrjm","score":1,"author_fullname":"t2_9s7pmakgx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt;Companies will likely shift towards hosting smaller LLMs internally for agentic use cases instead of relying on external APIs.\\n\\nSmaller LLMs are also cheaper on API. That's still cheaper than hosting them yourself. Hosting on your own barely makes sense when you can host even your finetuned models for cheap with autoscaling to 0 on many platforms, and off the shelf models are even cheaper.\\n\\n&gt;And honestly, who better to understand the nitty-gritty details of an ML model than data scientists? For the past two years, it felt like ML engineers were contributing more than data scientists, but I think that trend is going to slowly reverse.\\n\\nI never thought they would be going anywhere. Did data science die recently or something?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2sbn19","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Companies will likely shift towards hosting smaller LLMs internally for agentic use cases instead of relying on external APIs.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Smaller LLMs are also cheaper on API. That&amp;#39;s still cheaper than hosting them yourself. Hosting on your own barely makes sense when you can host even your finetuned models for cheap with autoscaling to 0 on many platforms, and off the shelf models are even cheaper.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;And honestly, who better to understand the nitty-gritty details of an ML model than data scientists? For the past two years, it felt like ML engineers were contributing more than data scientists, but I think that trend is going to slowly reverse.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I never thought they would be going anywhere. Did data science die recently or something?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2sbn19/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752352345,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxvrjm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2tb9wo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"pedrosorio","can_mod_post":false,"created_utc":1752364481,"send_replies":true,"parent_id":"t3_1lxvrjm","score":1,"author_fullname":"t2_5t6gb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt;Companies will likely shift towards hosting smaller LLMs internally \\n\\n\\\\+\\n\\n&gt;For the past two years, it felt like ML engineers were contributing more than data scientists, but I think that trend is going to slowly reverse.\\n\\nWe must have really different definitions of what an ML engineer is if you think they are not the people working on hosting LLMs internally.\\n\\nWhen you say ML engineers, are you thinking of \\"AI engineers\\" (aka people who learned to build applications that call LLM provider APIs in the last couple of years and think the concept of an \\"eval\\" is revolutionary)?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2tb9wo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Companies will likely shift towards hosting smaller LLMs internally &lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;+&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;For the past two years, it felt like ML engineers were contributing more than data scientists, but I think that trend is going to slowly reverse.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;We must have really different definitions of what an ML engineer is if you think they are not the people working on hosting LLMs internally.&lt;/p&gt;\\n\\n&lt;p&gt;When you say ML engineers, are you thinking of &amp;quot;AI engineers&amp;quot; (aka people who learned to build applications that call LLM provider APIs in the last couple of years and think the concept of an &amp;quot;eval&amp;quot; is revolutionary)?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/n2tb9wo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752364481,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxvrjm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(l,{data:t});export{n as default};
