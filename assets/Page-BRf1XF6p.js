import{j as e}from"./index-DACS7Nh6.js";import{R as l}from"./RedditPostRenderer-Dqa1NZuX.js";import"./index-DiMIVQx4.js";const t=[{kind:"Listing",data:{after:null,dist:1,modhash:"",geo_filter:"",children:[{kind:"t3",data:{approved_at_utc:null,subreddit:"LocalLLaMA",selftext:`Hey all, running into an interesting quirk....

I'm running this setup on my small local box with a 4090, but I'd like to OCR ~4e6 images. On my small scale tests, it performs really well, but it takes ~1s per image on average. I've looked into batched passes and that seems to unroll internally into sequential passes. I've yet to have any look to try to stack and pass big volumes of data in parallel through the encoding blocks. Ideally I'd process 10-20 images at a time (applying the same tokenized prompt to each). Wasn't sure of the best way to do this currently... 

I've poked around with using the generate calls from the model (from HF), but haven't had much luck in getting this work. I can keep barking up this tree, but was wondering other options/ideas on how to scale running this more quickly.`,user_reports:[],saved:!1,mod_reason_title:null,gilded:0,clicked:!1,title:"Batch processing for MiniCPM",link_flair_richtext:[{e:"text",t:"Question | Help"}],subreddit_name_prefixed:"r/LocalLLaMA",hidden:!1,pwls:6,link_flair_css_class:"",downs:0,thumbnail_height:null,top_awarded_type:null,hide_score:!1,name:"t3_1m2fkw6",quarantine:!1,link_flair_text_color:"dark",upvote_ratio:1,author_flair_background_color:null,subreddit_type:"public",ups:2,total_awards_received:0,media_embed:{},thumbnail_width:null,author_flair_template_id:null,is_original_content:!1,author_fullname:"t2_9mj3r",secure_media:null,is_reddit_media_domain:!1,is_meta:!1,category:null,secure_media_embed:{},link_flair_text:"Question | Help",can_mod_post:!1,score:2,approved_by:null,is_created_from_ads_ui:!1,author_premium:!1,thumbnail:"self",edited:!1,author_flair_css_class:null,author_flair_richtext:[],gildings:{},content_categories:null,is_self:!0,mod_note:null,created:1752777542,link_flair_type:"richtext",wls:6,removed_by_category:null,banned_by:null,author_flair_type:"text",domain:"self.LocalLLaMA",allow_live_comments:!1,selftext_html:`&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, running into an interesting quirk....&lt;/p&gt;

&lt;p&gt;I&amp;#39;m running this setup on my small local box with a 4090, but I&amp;#39;d like to OCR ~4e6 images. On my small scale tests, it performs really well, but it takes ~1s per image on average. I&amp;#39;ve looked into batched passes and that seems to unroll internally into sequential passes. I&amp;#39;ve yet to have any look to try to stack and pass big volumes of data in parallel through the encoding blocks. Ideally I&amp;#39;d process 10-20 images at a time (applying the same tokenized prompt to each). Wasn&amp;#39;t sure of the best way to do this currently... &lt;/p&gt;

&lt;p&gt;I&amp;#39;ve poked around with using the generate calls from the model (from HF), but haven&amp;#39;t had much luck in getting this work. I can keep barking up this tree, but was wondering other options/ideas on how to scale running this more quickly.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;`,likes:null,suggested_sort:null,banned_at_utc:null,view_count:null,archived:!1,no_follow:!1,is_crosspostable:!1,pinned:!1,over_18:!1,all_awardings:[],awarders:[],media_only:!1,link_flair_template_id:"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",can_gild:!1,spoiler:!1,locked:!1,author_flair_text:null,treatment_tags:[],visited:!1,removed_by:null,num_reports:null,distinguished:null,subreddit_id:"t5_81eyvm",author_is_blocked:!1,mod_reason_by:null,removal_reason:null,link_flair_background_color:"#5a74cc",id:"1m2fkw6",is_robot_indexable:!0,num_duplicates:0,report_reasons:null,author:"R2FuckYou",discussion_type:null,num_comments:1,send_replies:!0,media:null,contest_mode:!1,author_patreon_flair:!1,author_flair_text_color:null,permalink:"/r/LocalLLaMA/comments/1m2fkw6/batch_processing_for_minicpm/",stickied:!1,url:"https://www.reddit.com/r/LocalLLaMA/comments/1m2fkw6/batch_processing_for_minicpm/",subreddit_subscribers:500897,created_utc:1752777542,num_crossposts:0,mod_reports:[],is_video:!1}}],before:null}},{kind:"Listing",data:{after:null,dist:null,modhash:"",geo_filter:"",children:[{kind:"t1",data:{subreddit_id:"t5_81eyvm",approved_at_utc:null,author_is_blocked:!1,comment_type:null,awarders:[],mod_reason_by:null,banned_by:null,author_flair_type:"text",total_awards_received:0,subreddit:"LocalLLaMA",author_flair_template_id:null,likes:null,replies:"",user_reports:[],saved:!1,id:"n3sss8x",banned_at_utc:null,mod_reason_title:null,gilded:0,archived:!1,collapsed_reason_code:null,no_follow:!0,author:"Few-Welcome3297",can_mod_post:!1,created_utc:1752839008,send_replies:!0,parent_id:"t3_1m2fkw6",score:2,author_fullname:"t2_9f8ab953",approved_by:null,mod_note:null,all_awardings:[],collapsed:!1,body:"[vLLM Batch inference](https://docs.vllm.ai/en/v0.9.0/examples/offline_inference/batch_llm_inference.html)",edited:!1,top_awarded_type:null,author_flair_css_class:null,name:"t1_n3sss8x",is_submitter:!1,downs:0,author_flair_richtext:[],author_patreon_flair:!1,body_html:`&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://docs.vllm.ai/en/v0.9.0/examples/offline_inference/batch_llm_inference.html"&gt;vLLM Batch inference&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;`,removal_reason:null,collapsed_reason:null,distinguished:null,associated_award:null,stickied:!1,author_premium:!1,can_gild:!1,gildings:{},unrepliable_reason:null,author_flair_text_color:null,score_hidden:!1,permalink:"/r/LocalLLaMA/comments/1m2fkw6/batch_processing_for_minicpm/n3sss8x/",subreddit_type:"public",locked:!1,report_reasons:null,created:1752839008,author_flair_text:null,treatment_tags:[],link_id:"t3_1m2fkw6",subreddit_name_prefixed:"r/LocalLLaMA",controversiality:0,depth:0,author_flair_background_color:null,collapsed_because_crowd_control:null,mod_reports:[],num_reports:null,ups:2}}],before:null}}],o=()=>e.jsx(l,{data:t});export{o as default};
