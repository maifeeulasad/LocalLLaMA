import{j as e}from"./index-BgwOAK4-.js";import{R as t}from"./RedditPostRenderer-BOBjDTFu.js";import"./index-BL22wVg5.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"https://preview.redd.it/dbs9gal713af1.png?width=1024&amp;format=png&amp;auto=webp&amp;s=cea1a073a4106381b16f3f732c8c137a894c4dc7\\n\\n  \\n  \\nIf you‚Äôve ever peeked inside models like GPT or BERT and wondered *how* they understand the *order* of words, the secret sauce is something called positional embedding.\\n\\nWithout it, a language model can‚Äôt tell the difference between:\\n\\n* ‚ÄúThe cat sat on the mat‚Äù\\n* ‚ÄúThe mat sat on the cat‚Äù\\n\\n# The Problem: Transformers Don‚Äôt Understand Word Order\\n\\nTransformers process all tokens at once, which is great for speed, but unlike RNNs, they don‚Äôt read text sequentially. That means they don‚Äôt naturally know the order of words.\\n\\nTo a plain Transformer, ‚ÄúI love AI‚Äù could mean the same as ‚ÄúAI love I.‚Äù\\n\\n# The Solution: Positional Embeddings\\n\\nTo fix this, we add a second layer of information: positional embeddings. These vectors tell the model *where* each word appears in the input sequence.\\n\\nSo instead of just using word embeddings, we do:\\n\\n    Final Input = Word Embedding + Positional Embedding\\n    \\n\\nNow the model knows both the meaning of each word and its position in the sentence.\\n\\n# Why Not Let the Model Learn Position on Its Own?\\n\\nIn theory, a large model *could* infer word order from patterns. But in practice, that‚Äôs inefficient and unreliable. Positional embeddings provide the model with a strong starting point, akin to adding page numbers to a shuffled book.\\n\\n# Two Common Types of Positional Embeddings\\n\\n1. **Sinusoidal Positional Embeddings**\\n   * Used in the original Transformer paper\\n   * Not learned, uses sine and cosine functions\\n   * Good for generalizing to longer sequences\\n2. **Learned Positional Embeddings**\\n   * Used in models like BERT\\n   * Learned during training, like word embeddings\\n   * Flexible, but may not generalize well to unseen sequence lengths\\n\\n# Real Example: Why It Matters\\n\\nCompare:\\n\\n* ‚ÄúThe dog chased the cat.‚Äù\\n* ‚ÄúThe cat chased the dog‚Äù\\n\\nSame words, totally different meaning. Without positional embeddings, the model can‚Äôt tell which animal is doing the chasing.\\n\\n# What‚Äôs New: Rotary Positional Embeddings (RoPE)\\n\\nModern models, such as DeepSeek and LLaMA, utilize RoPE to integrate position into the attention mechanism itself. It‚Äôs more efficient for long sequences and performs better in certain settings.\\n\\n# TL;DR\\n\\nPositional embeddings help Transformers make sense of word order. Without them, a model is just guessing how words relate to each other, like trying to read a book with the pages shuffled.\\n\\nüëâ Tomorrow, we‚Äôre going to code positional embeddings from scratch‚Äîso stay tuned!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"[Day 6/50] Building a Small Language Model from Scratch - What Is Positional Embedding and Why Does It¬†Matter?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":140,"top_awarded_type":null,"hide_score":false,"media_metadata":{"dbs9gal713af1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":162,"x":108,"u":"https://preview.redd.it/dbs9gal713af1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=90eb63a0dec42efe4920987f7d66408269b02cea"},{"y":324,"x":216,"u":"https://preview.redd.it/dbs9gal713af1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=487d2371e0ce8ad3fd845e78e494e9c6121df41a"},{"y":480,"x":320,"u":"https://preview.redd.it/dbs9gal713af1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=da8a344bf0843c05a25d47f1242fbb9a80814143"},{"y":960,"x":640,"u":"https://preview.redd.it/dbs9gal713af1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5e454f9a9b9c42bb3665194830a779e920c52d28"},{"y":1440,"x":960,"u":"https://preview.redd.it/dbs9gal713af1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=19913f659c802b0e4775c5298dbfe07f13c17b9f"}],"s":{"y":1536,"x":1024,"u":"https://preview.redd.it/dbs9gal713af1.png?width=1024&amp;format=png&amp;auto=webp&amp;s=cea1a073a4106381b16f3f732c8c137a894c4dc7"},"id":"dbs9gal713af1"}},"name":"t3_1load8a","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.86,"author_flair_background_color":null,"subreddit_type":"public","ups":43,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_8ht7a116","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":43,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://b.thumbs.redditmedia.com/QrXwS0MMtdu4-LCvZnP-VTv25rOcYvXpPucHJrkYiSQ.jpg","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751297032,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://preview.redd.it/dbs9gal713af1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cea1a073a4106381b16f3f732c8c137a894c4dc7\\"&gt;https://preview.redd.it/dbs9gal713af1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cea1a073a4106381b16f3f732c8c137a894c4dc7&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;If you‚Äôve ever peeked inside models like GPT or BERT and wondered &lt;em&gt;how&lt;/em&gt; they understand the &lt;em&gt;order&lt;/em&gt; of words, the secret sauce is something called positional embedding.&lt;/p&gt;\\n\\n&lt;p&gt;Without it, a language model can‚Äôt tell the difference between:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;‚ÄúThe cat sat on the mat‚Äù&lt;/li&gt;\\n&lt;li&gt;‚ÄúThe mat sat on the cat‚Äù&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;h1&gt;The Problem: Transformers Don‚Äôt Understand Word Order&lt;/h1&gt;\\n\\n&lt;p&gt;Transformers process all tokens at once, which is great for speed, but unlike RNNs, they don‚Äôt read text sequentially. That means they don‚Äôt naturally know the order of words.&lt;/p&gt;\\n\\n&lt;p&gt;To a plain Transformer, ‚ÄúI love AI‚Äù could mean the same as ‚ÄúAI love I.‚Äù&lt;/p&gt;\\n\\n&lt;h1&gt;The Solution: Positional Embeddings&lt;/h1&gt;\\n\\n&lt;p&gt;To fix this, we add a second layer of information: positional embeddings. These vectors tell the model &lt;em&gt;where&lt;/em&gt; each word appears in the input sequence.&lt;/p&gt;\\n\\n&lt;p&gt;So instead of just using word embeddings, we do:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;Final Input = Word Embedding + Positional Embedding\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;Now the model knows both the meaning of each word and its position in the sentence.&lt;/p&gt;\\n\\n&lt;h1&gt;Why Not Let the Model Learn Position on Its Own?&lt;/h1&gt;\\n\\n&lt;p&gt;In theory, a large model &lt;em&gt;could&lt;/em&gt; infer word order from patterns. But in practice, that‚Äôs inefficient and unreliable. Positional embeddings provide the model with a strong starting point, akin to adding page numbers to a shuffled book.&lt;/p&gt;\\n\\n&lt;h1&gt;Two Common Types of Positional Embeddings&lt;/h1&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;&lt;strong&gt;Sinusoidal Positional Embeddings&lt;/strong&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Used in the original Transformer paper&lt;/li&gt;\\n&lt;li&gt;Not learned, uses sine and cosine functions&lt;/li&gt;\\n&lt;li&gt;Good for generalizing to longer sequences&lt;/li&gt;\\n&lt;/ul&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Learned Positional Embeddings&lt;/strong&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Used in models like BERT&lt;/li&gt;\\n&lt;li&gt;Learned during training, like word embeddings&lt;/li&gt;\\n&lt;li&gt;Flexible, but may not generalize well to unseen sequence lengths&lt;/li&gt;\\n&lt;/ul&gt;&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;h1&gt;Real Example: Why It Matters&lt;/h1&gt;\\n\\n&lt;p&gt;Compare:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;‚ÄúThe dog chased the cat.‚Äù&lt;/li&gt;\\n&lt;li&gt;‚ÄúThe cat chased the dog‚Äù&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Same words, totally different meaning. Without positional embeddings, the model can‚Äôt tell which animal is doing the chasing.&lt;/p&gt;\\n\\n&lt;h1&gt;What‚Äôs New: Rotary Positional Embeddings (RoPE)&lt;/h1&gt;\\n\\n&lt;p&gt;Modern models, such as DeepSeek and LLaMA, utilize RoPE to integrate position into the attention mechanism itself. It‚Äôs more efficient for long sequences and performs better in certain settings.&lt;/p&gt;\\n\\n&lt;h1&gt;TL;DR&lt;/h1&gt;\\n\\n&lt;p&gt;Positional embeddings help Transformers make sense of word order. Without them, a model is just guessing how words relate to each other, like trying to read a book with the pages shuffled.&lt;/p&gt;\\n\\n&lt;p&gt;üëâ Tomorrow, we‚Äôre going to code positional embeddings from scratch‚Äîso stay tuned!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1load8a","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Prashant-Lakhera","discussion_type":null,"num_comments":7,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1load8a/day_650_building_a_small_language_model_from/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1load8a/day_650_building_a_small_language_model_from/","subreddit_subscribers":493242,"created_utc":1751297032,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0lhors","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Prashant-Lakhera","can_mod_post":false,"created_utc":1751298997,"send_replies":true,"parent_id":"t3_1load8a","score":2,"author_fullname":"t2_8ht7a116","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"üîó Complete blog: [https://www.ideaweaver.ai/blog/day6.html](https://www.ideaweaver.ai/blog/day6.html)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0lhors","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;üîó Complete blog: &lt;a href=\\"https://www.ideaweaver.ai/blog/day6.html\\"&gt;https://www.ideaweaver.ai/blog/day6.html&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1load8a/day_650_building_a_small_language_model_from/n0lhors/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751298997,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1load8a","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0nh0qa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Prashant-Lakhera","can_mod_post":false,"created_utc":1751319508,"send_replies":true,"parent_id":"t1_n0mtp86","score":1,"author_fullname":"t2_8ht7a116","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The best way to reach out and subscribe is through LinkedIn, where I post updates daily. Unfortunately, Reddit sometimes takes a while to approve posts.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0nh0qa","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The best way to reach out and subscribe is through LinkedIn, where I post updates daily. Unfortunately, Reddit sometimes takes a while to approve posts.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1load8a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1load8a/day_650_building_a_small_language_model_from/n0nh0qa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751319508,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0mtp86","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"hi87","can_mod_post":false,"created_utc":1751312667,"send_replies":true,"parent_id":"t3_1load8a","score":1,"author_fullname":"t2_etrdb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Is there any way to subscribe to these? Ive been building the gpt2 model from scratch and these would be helpful.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0mtp86","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Is there any way to subscribe to these? Ive been building the gpt2 model from scratch and these would be helpful.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1load8a/day_650_building_a_small_language_model_from/n0mtp86/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751312667,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1load8a","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0nis3a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MoffKalast","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ngh7v","score":3,"author_fullname":"t2_d2nyh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I think half of the layers or something like that use NoPE which doesn't use positional encodings at all, but yes the rest still use it. I wouldn't be surprised if this contributed to the relatively low performance of L4 in the end. Iirc there were a few other architectures that also attempted to avoid it and were all extremely bad.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0nis3a","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think half of the layers or something like that use NoPE which doesn&amp;#39;t use positional encodings at all, but yes the rest still use it. I wouldn&amp;#39;t be surprised if this contributed to the relatively low performance of L4 in the end. Iirc there were a few other architectures that also attempted to avoid it and were all extremely bad.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1load8a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1load8a/day_650_building_a_small_language_model_from/n0nis3a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751320054,"author_flair_text":null,"treatment_tags":[],"created_utc":1751320054,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ngh7v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Prashant-Lakhera","can_mod_post":false,"created_utc":1751319344,"send_replies":true,"parent_id":"t1_n0n3kyw","score":2,"author_fullname":"t2_8ht7a116","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"If you check the blog I shared in this post, you'll see that I mentioned DeepSeek and LLaMA both use RoPE :-)  [https://www.ideaweaver.ai/blog/day6.html](https://www.ideaweaver.ai/blog/day6.html)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ngh7v","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you check the blog I shared in this post, you&amp;#39;ll see that I mentioned DeepSeek and LLaMA both use RoPE :-)  &lt;a href=\\"https://www.ideaweaver.ai/blog/day6.html\\"&gt;https://www.ideaweaver.ai/blog/day6.html&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1load8a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1load8a/day_650_building_a_small_language_model_from/n0ngh7v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751319344,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0n3kyw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MoffKalast","can_mod_post":false,"created_utc":1751315564,"send_replies":true,"parent_id":"t3_1load8a","score":4,"author_fullname":"t2_d2nyh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"[Meta when they trained Llama 4 without positional embeddings](https://media.tenor.com/NidJyk78iAkAAAAe/rick-and-morty-crying.png)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0n3kyw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://media.tenor.com/NidJyk78iAkAAAAe/rick-and-morty-crying.png\\"&gt;Meta when they trained Llama 4 without positional embeddings&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1load8a/day_650_building_a_small_language_model_from/n0n3kyw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751315564,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1load8a","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}}]`),r=()=>e.jsx(t,{data:l});export{r as default};
