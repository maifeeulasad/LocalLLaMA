import{j as e}from"./index-Bu7qcPAU.js";import{R as l}from"./RedditPostRenderer-CbHA7O5q.js";import"./index-BKgbfxhf.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I've been enjoying Ollama for the ability to have an easy web interface to download models with and that I can make API calls to a single endpoint and Port while specifying different models that I want used. As far as I understand it, llama.cpp requires one running instance per model, and obviously different ports. I'm enjoying being able to be lazy without needing to SSH to my server and manually manage model download or server instances, but most importantly to query multiple models on a single endpoint and port. Am I giving all that up by moving directly to llama.cpp?\\n\\nThanks! Just want to make sure before I decide to stick with Ollama.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Am I correct that to run multiple models with Llama.cpp I need multiple instances on multiple ports?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lrqj68","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.88,"author_flair_background_color":null,"subreddit_type":"public","ups":6,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_fvs8r","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":6,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751655462,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve been enjoying Ollama for the ability to have an easy web interface to download models with and that I can make API calls to a single endpoint and Port while specifying different models that I want used. As far as I understand it, llama.cpp requires one running instance per model, and obviously different ports. I&amp;#39;m enjoying being able to be lazy without needing to SSH to my server and manually manage model download or server instances, but most importantly to query multiple models on a single endpoint and port. Am I giving all that up by moving directly to llama.cpp?&lt;/p&gt;\\n\\n&lt;p&gt;Thanks! Just want to make sure before I decide to stick with Ollama.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lrqj68","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"CharlesStross","discussion_type":null,"num_comments":9,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lrqj68/am_i_correct_that_to_run_multiple_models_with/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lrqj68/am_i_correct_that_to_run_multiple_models_with/","subreddit_subscribers":494898,"created_utc":1751655462,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1e88c4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullstackSensei","can_mod_post":false,"created_utc":1751674704,"send_replies":true,"parent_id":"t1_n1d5zxd","score":2,"author_fullname":"t2_17n3nqtj56","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This is the only proper answer to OP.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1e88c4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is the only proper answer to OP.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrqj68","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrqj68/am_i_correct_that_to_run_multiple_models_with/n1e88c4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751674704,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1d5zxd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ezirel","can_mod_post":false,"created_utc":1751660854,"send_replies":true,"parent_id":"t3_1lrqj68","score":7,"author_fullname":"t2_cp3v7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You can use [llama-swap](https://github.com/mostlygeek/llama-swap)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1d5zxd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can use &lt;a href=\\"https://github.com/mostlygeek/llama-swap\\"&gt;llama-swap&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrqj68/am_i_correct_that_to_run_multiple_models_with/n1d5zxd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751660854,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrqj68","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1edb71","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"SkyFeistyLlama8","can_mod_post":false,"created_utc":1751676830,"send_replies":true,"parent_id":"t3_1lrqj68","score":4,"author_fullname":"t2_1hgbaqgbnq","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The proper answer is llama-swap.\\n\\nThe hacker answer is yes, you can load multiple models using llama-server and specify a --port parameter. That's what I do. You'll need lots of unified RAM or VRAM to do this because all models have to be kept in RAM.\\n\\nI keep a Gemma 4B or Qwen 4B permanently loaded on port 8090 for quick code fixes in Continue.dev and for fast summaries. I also have Mistral 24B, Gemma 27B or GLM 32B running on the default port (8080), the model depending on what task I need it for. I find it's much faster to keep larger models always in RAM to avoid loading delays and to keep contexts cached, instead of having to recompute the entire prompt on each new request.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1edb71","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The proper answer is llama-swap.&lt;/p&gt;\\n\\n&lt;p&gt;The hacker answer is yes, you can load multiple models using llama-server and specify a --port parameter. That&amp;#39;s what I do. You&amp;#39;ll need lots of unified RAM or VRAM to do this because all models have to be kept in RAM.&lt;/p&gt;\\n\\n&lt;p&gt;I keep a Gemma 4B or Qwen 4B permanently loaded on port 8090 for quick code fixes in Continue.dev and for fast summaries. I also have Mistral 24B, Gemma 27B or GLM 32B running on the default port (8080), the model depending on what task I need it for. I find it&amp;#39;s much faster to keep larger models always in RAM to avoid loading delays and to keep contexts cached, instead of having to recompute the entire prompt on each new request.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrqj68/am_i_correct_that_to_run_multiple_models_with/n1edb71/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751676830,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrqj68","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1cwfpn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Visible-Employee-403","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1cvu9h","score":1,"author_fullname":"t2_1ho2jsstl8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Well, yes, that's the trade-off then. But you are free to collect the models of your choice in a directory and then run a bash one-liner to create a skeleton config as the author also suggested in the same PR a bit later:\\n\\n[https://github.com/abetlen/llama-cpp-python/pull/931#issuecomment-1867538992](https://github.com/abetlen/llama-cpp-python/pull/931#issuecomment-1867538992)","edited":false,"author_flair_css_class":null,"name":"t1_n1cwfpn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Well, yes, that&amp;#39;s the trade-off then. But you are free to collect the models of your choice in a directory and then run a bash one-liner to create a skeleton config as the author also suggested in the same PR a bit later:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/abetlen/llama-cpp-python/pull/931#issuecomment-1867538992\\"&gt;https://github.com/abetlen/llama-cpp-python/pull/931#issuecomment-1867538992&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lrqj68","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrqj68/am_i_correct_that_to_run_multiple_models_with/n1cwfpn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751657745,"author_flair_text":null,"collapsed":false,"created_utc":1751657745,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1cvu9h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CharlesStross","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1cv66e","score":2,"author_fullname":"t2_fvs8r","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Ah, perhaps poorly phrased haha -- needing to make a manual config file entry for each model rather than Ollama's download-&gt;run loop is more labor than I want while I'm actively swapping out and testing models. Maybe getting off Ollama makes sense once I'm more settled in model choice.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1cvu9h","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ah, perhaps poorly phrased haha -- needing to make a manual config file entry for each model rather than Ollama&amp;#39;s download-&amp;gt;run loop is more labor than I want while I&amp;#39;m actively swapping out and testing models. Maybe getting off Ollama makes sense once I&amp;#39;m more settled in model choice.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrqj68","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrqj68/am_i_correct_that_to_run_multiple_models_with/n1cvu9h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751657545,"author_flair_text":null,"treatment_tags":[],"created_utc":1751657545,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1cv66e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Visible-Employee-403","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1ctxnw","score":1,"author_fullname":"t2_1ho2jsstl8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Excatly. As former SWE I agree with the author on it's decision to stick with one config file.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1cv66e","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Excatly. As former SWE I agree with the author on it&amp;#39;s decision to stick with one config file.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrqj68","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrqj68/am_i_correct_that_to_run_multiple_models_with/n1cv66e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751657320,"author_flair_text":null,"treatment_tags":[],"created_utc":1751657320,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1ctxnw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CharlesStross","can_mod_post":false,"created_utc":1751656910,"send_replies":true,"parent_id":"t1_n1cto39","score":2,"author_fullname":"t2_fvs8r","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ah interesting, thanks! Manually managing a file for each model sounds like a pain though ugh","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ctxnw","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ah interesting, thanks! Manually managing a file for each model sounds like a pain though ugh&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrqj68","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrqj68/am_i_correct_that_to_run_multiple_models_with/n1ctxnw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751656910,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1cto39","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Visible-Employee-403","can_mod_post":false,"created_utc":1751656822,"send_replies":true,"parent_id":"t3_1lrqj68","score":1,"author_fullname":"t2_1ho2jsstl8","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"No. According to [https://github.com/abetlen/llama-cpp-python/pull/931#issuecomment-1867529039](https://github.com/abetlen/llama-cpp-python/pull/931#issuecomment-1867529039) (*I read the history of the PR*), it is possible to have a config file with multiple models running on the same port.\\n\\n&gt;The server selects the appropriate model based on the \`model\` key passed to it and the \`model_alias\` defined in the config fiile.\\n\\nAnd if you're lucky, someone will some day create a PR for [https://github.com/abetlen/llama-cpp-python/issues/736](https://github.com/abetlen/llama-cpp-python/issues/736) (model auto-load/unload).\\n\\nedit: this is especially for the python bindings of llama.cpp. idk if this also applies to the main project but maybe this is what you are looking for.","edited":1751657256,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1cto39","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No. According to &lt;a href=\\"https://github.com/abetlen/llama-cpp-python/pull/931#issuecomment-1867529039\\"&gt;https://github.com/abetlen/llama-cpp-python/pull/931#issuecomment-1867529039&lt;/a&gt; (&lt;em&gt;I read the history of the PR&lt;/em&gt;), it is possible to have a config file with multiple models running on the same port.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;The server selects the appropriate model based on the &lt;code&gt;model&lt;/code&gt; key passed to it and the &lt;code&gt;model_alias&lt;/code&gt; defined in the config fiile.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;And if you&amp;#39;re lucky, someone will some day create a PR for &lt;a href=\\"https://github.com/abetlen/llama-cpp-python/issues/736\\"&gt;https://github.com/abetlen/llama-cpp-python/issues/736&lt;/a&gt; (model auto-load/unload).&lt;/p&gt;\\n\\n&lt;p&gt;edit: this is especially for the python bindings of llama.cpp. idk if this also applies to the main project but maybe this is what you are looking for.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrqj68/am_i_correct_that_to_run_multiple_models_with/n1cto39/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751656822,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrqj68","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(l,{data:t});export{r as default};
