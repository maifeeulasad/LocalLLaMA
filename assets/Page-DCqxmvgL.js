import{j as e}from"./index-DLSqWzaI.js";import{R as t}from"./RedditPostRenderer-CysRo2D_.js";import"./index-COXiL3Lo.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"This is a quick story of how a focus on usability turned into 2000 LLM tests cases (well 2631 to be exact), and why the results might be helpful to you.\\n\\n# The problem: too many options\\n\\nI've been building [Kiln AI](https://github.com/kiln-ai/kiln): an open tool to help you find the best way to run your AI workload. Part of Kilnâ€™s goal is testing various different models on your AI task to see which ones work best. We hit a usability problem on day one: too many options. We supported hundreds of models, each with their own parameters, capabilities, and formats. Trying a new model wasn't easy. If evaluating an additional model is painful, you're less likely to do it, which makes you less likely to find the best way to run your AI workload.\\n\\nHere's a sampling of the many different options you need to choose: structured data mode (JSON schema, JSON mode, instruction, tool calls), reasoning support, reasoning format (\`&lt;think&gt;...&lt;/think&gt;\`), censorship/limits, use case support (generating synthetic data, evals), runtime parameters (logprobs, temperature, top\\\\_p, etc), and much more.\\n\\n# How a focus on usability turned into over 2000 test cases\\n\\nI wanted things to \\"just work\\" as much as possible in Kiln. You should be able to run a new model without writing a new API integration, writing a parser, or experimenting with API parameters.\\n\\nTo make it easy to use, we needed reasonable defaults for every major model. That's no small feat when new models pop up every week, and there are dozens of AI providers competing on inference.\\n\\nThe solution: a whole bunch of test cases! 2631 to be exact, with more added every week. We test every model on every provider across a range of functionality: structured data (JSON/tool calls), plaintext, reasoning, chain of thought, logprobs/G-eval, evals, synthetic data generation, and more. The result of all these tests is a detailed configuration file with up-to-date details on which models and providers support which features.\\n\\n# Wait, doesn't that cost a lot of money and take forever?\\n\\n**Yes it does!** Each time we run these tests, we're making thousands of LLM calls against a wide variety of providers. There's no getting around it: we want to know these features work well on every provider and model. The only way to be sure is to test, test, test. We regularly see providers regress or decommission models, so testing once isn't an option.\\n\\nOur blog has some details on the [Python pytest setup we used to make this manageable](https://getkiln.ai/blog/i_wrote_2000_llm_test_cases_so_you_dont_have_to#cost-and-time).\\n\\n# The Result\\n\\nThe end result is that it's much easier to rapidly evaluate AI models and methods. It includes\\n\\n* The model selection dropdown is aware of your current task needs, and will only show models known to work. The filters include things like structured data support (JSON/tools), needing an uncensored model for eval data generation, needing a model which supports logprobs for G-eval, and many more use cases.\\n* Automatic defaults for complex parameters. For example, automatically selecting the best JSON generation method from the many options (JSON schema, JSON mode, instructions, tools, etc).\\n\\nHowever, you're in control. You can always override any suggestion.\\n\\n# Next Step: A Giant Ollama Server\\n\\nI can run a decent sampling of our Ollama tests locally, but I lack the \\\\~1TB of VRAM needed to run things like Deepseek R1 or Kimi K2 locally. I'd love an easy-to-use test environment for these without breaking the bank. Suggestions welcome!\\n\\n# How to Find the Best Model for Your Task with Kiln\\n\\nAll of this testing infrastructure exists to serve one goal: making it easier for you to find the best way to run your specific use case. The 2000+ test cases ensure that when you use Kiln, you get reliable recommendations and easy model switching without the trial-and-error process.\\n\\nKiln is a free open tool for finding the best way to build your AI system. You can rapidly compare models, providers, prompts, parameters and even fine-tunes to get the optimal system for your use case â€” all backed by the extensive testing described above.\\n\\nTo get started, check out the tool or our guides:\\n\\n* [Kiln AI on Github - over 3900 stars](https://getkiln.ai/)\\n* [Quickstart Guide](https://docs.getkiln.ai/docs/quickstart)\\n* [Kiln Discord](https://getkiln.ai/discord)\\n* [Blog post with more details on our LLM testing (more detailed version of above)](https://getkiln.ai/blog/i_wrote_2000_llm_test_cases_so_you_dont_have_to#cost-and-time)\\n\\nI'm happy to answer questions if anyone wants to dive deeper on specific aspects!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"I wrote 2000 LLM test cases so you don't have to","link_flair_richtext":[{"e":"text","t":"Resources"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m6gq8e","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.91,"author_flair_background_color":null,"subreddit_type":"public","ups":45,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_slbscky","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Resources","can_mod_post":false,"score":45,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1753199300,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1753197164,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;This is a quick story of how a focus on usability turned into 2000 LLM tests cases (well 2631 to be exact), and why the results might be helpful to you.&lt;/p&gt;\\n\\n&lt;h1&gt;The problem: too many options&lt;/h1&gt;\\n\\n&lt;p&gt;I&amp;#39;ve been building &lt;a href=\\"https://github.com/kiln-ai/kiln\\"&gt;Kiln AI&lt;/a&gt;: an open tool to help you find the best way to run your AI workload. Part of Kilnâ€™s goal is testing various different models on your AI task to see which ones work best. We hit a usability problem on day one: too many options. We supported hundreds of models, each with their own parameters, capabilities, and formats. Trying a new model wasn&amp;#39;t easy. If evaluating an additional model is painful, you&amp;#39;re less likely to do it, which makes you less likely to find the best way to run your AI workload.&lt;/p&gt;\\n\\n&lt;p&gt;Here&amp;#39;s a sampling of the many different options you need to choose: structured data mode (JSON schema, JSON mode, instruction, tool calls), reasoning support, reasoning format (&lt;code&gt;&amp;lt;think&amp;gt;...&amp;lt;/think&amp;gt;&lt;/code&gt;), censorship/limits, use case support (generating synthetic data, evals), runtime parameters (logprobs, temperature, top_p, etc), and much more.&lt;/p&gt;\\n\\n&lt;h1&gt;How a focus on usability turned into over 2000 test cases&lt;/h1&gt;\\n\\n&lt;p&gt;I wanted things to &amp;quot;just work&amp;quot; as much as possible in Kiln. You should be able to run a new model without writing a new API integration, writing a parser, or experimenting with API parameters.&lt;/p&gt;\\n\\n&lt;p&gt;To make it easy to use, we needed reasonable defaults for every major model. That&amp;#39;s no small feat when new models pop up every week, and there are dozens of AI providers competing on inference.&lt;/p&gt;\\n\\n&lt;p&gt;The solution: a whole bunch of test cases! 2631 to be exact, with more added every week. We test every model on every provider across a range of functionality: structured data (JSON/tool calls), plaintext, reasoning, chain of thought, logprobs/G-eval, evals, synthetic data generation, and more. The result of all these tests is a detailed configuration file with up-to-date details on which models and providers support which features.&lt;/p&gt;\\n\\n&lt;h1&gt;Wait, doesn&amp;#39;t that cost a lot of money and take forever?&lt;/h1&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Yes it does!&lt;/strong&gt; Each time we run these tests, we&amp;#39;re making thousands of LLM calls against a wide variety of providers. There&amp;#39;s no getting around it: we want to know these features work well on every provider and model. The only way to be sure is to test, test, test. We regularly see providers regress or decommission models, so testing once isn&amp;#39;t an option.&lt;/p&gt;\\n\\n&lt;p&gt;Our blog has some details on the &lt;a href=\\"https://getkiln.ai/blog/i_wrote_2000_llm_test_cases_so_you_dont_have_to#cost-and-time\\"&gt;Python pytest setup we used to make this manageable&lt;/a&gt;.&lt;/p&gt;\\n\\n&lt;h1&gt;The Result&lt;/h1&gt;\\n\\n&lt;p&gt;The end result is that it&amp;#39;s much easier to rapidly evaluate AI models and methods. It includes&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;The model selection dropdown is aware of your current task needs, and will only show models known to work. The filters include things like structured data support (JSON/tools), needing an uncensored model for eval data generation, needing a model which supports logprobs for G-eval, and many more use cases.&lt;/li&gt;\\n&lt;li&gt;Automatic defaults for complex parameters. For example, automatically selecting the best JSON generation method from the many options (JSON schema, JSON mode, instructions, tools, etc).&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;However, you&amp;#39;re in control. You can always override any suggestion.&lt;/p&gt;\\n\\n&lt;h1&gt;Next Step: A Giant Ollama Server&lt;/h1&gt;\\n\\n&lt;p&gt;I can run a decent sampling of our Ollama tests locally, but I lack the ~1TB of VRAM needed to run things like Deepseek R1 or Kimi K2 locally. I&amp;#39;d love an easy-to-use test environment for these without breaking the bank. Suggestions welcome!&lt;/p&gt;\\n\\n&lt;h1&gt;How to Find the Best Model for Your Task with Kiln&lt;/h1&gt;\\n\\n&lt;p&gt;All of this testing infrastructure exists to serve one goal: making it easier for you to find the best way to run your specific use case. The 2000+ test cases ensure that when you use Kiln, you get reliable recommendations and easy model switching without the trial-and-error process.&lt;/p&gt;\\n\\n&lt;p&gt;Kiln is a free open tool for finding the best way to build your AI system. You can rapidly compare models, providers, prompts, parameters and even fine-tunes to get the optimal system for your use case â€” all backed by the extensive testing described above.&lt;/p&gt;\\n\\n&lt;p&gt;To get started, check out the tool or our guides:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;a href=\\"https://getkiln.ai/\\"&gt;Kiln AI on Github - over 3900 stars&lt;/a&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;a href=\\"https://docs.getkiln.ai/docs/quickstart\\"&gt;Quickstart Guide&lt;/a&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;a href=\\"https://getkiln.ai/discord\\"&gt;Kiln Discord&lt;/a&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;a href=\\"https://getkiln.ai/blog/i_wrote_2000_llm_test_cases_so_you_dont_have_to#cost-and-time\\"&gt;Blog post with more details on our LLM testing (more detailed version of above)&lt;/a&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;I&amp;#39;m happy to answer questions if anyone wants to dive deeper on specific aspects!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/YF2mZrP2LZphKjmsRiHyL6Oic0sw2vC0c9Q1XWpEOGA.png?auto=webp&amp;s=23e4ff0dbe2d03ff352aea774053e4e9cdb80d20","width":1280,"height":640},"resolutions":[{"url":"https://external-preview.redd.it/YF2mZrP2LZphKjmsRiHyL6Oic0sw2vC0c9Q1XWpEOGA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fd9815f077288b33817e75895d23e661f1193778","width":108,"height":54},{"url":"https://external-preview.redd.it/YF2mZrP2LZphKjmsRiHyL6Oic0sw2vC0c9Q1XWpEOGA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7df51b519d6d99631039f2563f587d4f7fb7f337","width":216,"height":108},{"url":"https://external-preview.redd.it/YF2mZrP2LZphKjmsRiHyL6Oic0sw2vC0c9Q1XWpEOGA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=584735f7b916c00d422195a7ea012563d4e134db","width":320,"height":160},{"url":"https://external-preview.redd.it/YF2mZrP2LZphKjmsRiHyL6Oic0sw2vC0c9Q1XWpEOGA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7ceb01849b330103f92aaf6b1331cd97e415c722","width":640,"height":320},{"url":"https://external-preview.redd.it/YF2mZrP2LZphKjmsRiHyL6Oic0sw2vC0c9Q1XWpEOGA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f0594f7e041119a136f22914764b2a128e73d5ff","width":960,"height":480},{"url":"https://external-preview.redd.it/YF2mZrP2LZphKjmsRiHyL6Oic0sw2vC0c9Q1XWpEOGA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=415b728bd16022b553cb45cb75a1a8fee65a2e5b","width":1080,"height":540}],"variants":{},"id":"YF2mZrP2LZphKjmsRiHyL6Oic0sw2vC0c9Q1XWpEOGA"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#ccac2b","id":"1m6gq8e","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"davernow","discussion_type":null,"num_comments":10,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m6gq8e/i_wrote_2000_llm_test_cases_so_you_dont_have_to/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m6gq8e/i_wrote_2000_llm_test_cases_so_you_dont_have_to/","subreddit_subscribers":502981,"created_utc":1753197164,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4k6z3v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"davernow","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4k2j2a","score":2,"author_fullname":"t2_slbscky","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Results are stored here: [https://github.com/Kiln-AI/Kiln/blob/main/libs/core/kiln\\\\_ai/adapters/ml\\\\_model\\\\_list.py](https://github.com/Kiln-AI/Kiln/blob/main/libs/core/kiln_ai/adapters/ml_model_list.py)\\n\\nA pretty UI for it is in the works, but not done yet: [https://github.com/Kiln-AI/Kiln/pull/439](https://github.com/Kiln-AI/Kiln/pull/439)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4k6z3v","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Results are stored here: &lt;a href=\\"https://github.com/Kiln-AI/Kiln/blob/main/libs/core/kiln_ai/adapters/ml_model_list.py\\"&gt;https://github.com/Kiln-AI/Kiln/blob/main/libs/core/kiln_ai/adapters/ml_model_list.py&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;A pretty UI for it is in the works, but not done yet: &lt;a href=\\"https://github.com/Kiln-AI/Kiln/pull/439\\"&gt;https://github.com/Kiln-AI/Kiln/pull/439&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6gq8e","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6gq8e/i_wrote_2000_llm_test_cases_so_you_dont_have_to/n4k6z3v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753205059,"author_flair_text":null,"treatment_tags":[],"created_utc":1753205059,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4k2j2a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"T2WIN","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4k0m8v","score":1,"author_fullname":"t2_38ilynpn","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ok thanks. I saw that qwen3 models are in your supported models but i didn't see any recommendations for them. Is there a place i can find the results of your tests ?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4k2j2a","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ok thanks. I saw that qwen3 models are in your supported models but i didn&amp;#39;t see any recommendations for them. Is there a place i can find the results of your tests ?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6gq8e","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6gq8e/i_wrote_2000_llm_test_cases_so_you_dont_have_to/n4k2j2a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753203872,"author_flair_text":null,"treatment_tags":[],"created_utc":1753203872,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4m2ds9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"plztNeo","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4k0m8v","score":1,"author_fullname":"t2_cl1vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"9 believe abacus.ai ChatLLM has Kimi and Deepseek for cheap if not free once signed up","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4m2ds9","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;9 believe abacus.ai ChatLLM has Kimi and Deepseek for cheap if not free once signed up&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6gq8e","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6gq8e/i_wrote_2000_llm_test_cases_so_you_dont_have_to/n4m2ds9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753224413,"author_flair_text":null,"treatment_tags":[],"created_utc":1753224413,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4k0m8v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"davernow","can_mod_post":false,"created_utc":1753203357,"send_replies":true,"parent_id":"t1_n4jypds","score":6,"author_fullname":"t2_slbscky","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"A full test pass is about $6. Since I run it once a week tops it's not breaking the bank ðŸ˜€.","edited":1753205261,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4k0m8v","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;A full test pass is about $6. Since I run it once a week tops it&amp;#39;s not breaking the bank ðŸ˜€.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6gq8e","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6gq8e/i_wrote_2000_llm_test_cases_so_you_dont_have_to/n4k0m8v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753203357,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n4jypds","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"T2WIN","can_mod_post":false,"created_utc":1753202842,"send_replies":true,"parent_id":"t3_1m6gq8e","score":5,"author_fullname":"t2_38ilynpn","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Cool project, how do you fund yourself though ?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4jypds","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Cool project, how do you fund yourself though ?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6gq8e/i_wrote_2000_llm_test_cases_so_you_dont_have_to/n4jypds/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753202842,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6gq8e","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ky8ui","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kholejones8888","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4kxzdw","score":2,"author_fullname":"t2_1jp9h6pxqa","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Start an LLC and get cloud credit lmao\\n\\nI am very interested in the deviations detectable in different inference provider, cool project","edited":false,"author_flair_css_class":null,"name":"t1_n4ky8ui","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Start an LLC and get cloud credit lmao&lt;/p&gt;\\n\\n&lt;p&gt;I am very interested in the deviations detectable in different inference provider, cool project&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m6gq8e","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6gq8e/i_wrote_2000_llm_test_cases_so_you_dont_have_to/n4ky8ui/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753212672,"author_flair_text":null,"collapsed":false,"created_utc":1753212672,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4kxzdw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"davernow","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4kcpq8","score":2,"author_fullname":"t2_slbscky","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"oh, all the inference clouds are already tested independently as part of this. This is to find all the nitty gritty difference between them. I'd need to run on actual Ollama, which tends to do things its own way (great constrained decode for example). But a hosted Ollama box is an option.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4kxzdw","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;oh, all the inference clouds are already tested independently as part of this. This is to find all the nitty gritty difference between them. I&amp;#39;d need to run on actual Ollama, which tends to do things its own way (great constrained decode for example). But a hosted Ollama box is an option.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6gq8e","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6gq8e/i_wrote_2000_llm_test_cases_so_you_dont_have_to/n4kxzdw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753212596,"author_flair_text":null,"treatment_tags":[],"created_utc":1753212596,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4kcpq8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kholejones8888","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4k7mff","score":1,"author_fullname":"t2_1jp9h6pxqa","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"When youâ€™re billed per hour tokens/s matters, itâ€™s a cost optimization problem. Hyperbolic also has inference as a service but new R1 is one of their more expensive options. I know openrouter has slow and presumably shrunk Kimi K2 for free and good deals on inference for other stuff.\\n\\nYou can use the cloud APIs and stuff for automation and make it pretty efficient to spin up your own stuff. Im experimenting with Linode.","edited":1753206788,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4kcpq8","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;When youâ€™re billed per hour tokens/s matters, itâ€™s a cost optimization problem. Hyperbolic also has inference as a service but new R1 is one of their more expensive options. I know openrouter has slow and presumably shrunk Kimi K2 for free and good deals on inference for other stuff.&lt;/p&gt;\\n\\n&lt;p&gt;You can use the cloud APIs and stuff for automation and make it pretty efficient to spin up your own stuff. Im experimenting with Linode.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6gq8e","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6gq8e/i_wrote_2000_llm_test_cases_so_you_dont_have_to/n4kcpq8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753206607,"author_flair_text":null,"treatment_tags":[],"created_utc":1753206607,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4k7mff","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"davernow","can_mod_post":false,"created_utc":1753205233,"send_replies":true,"parent_id":"t1_n4k23ca","score":2,"author_fullname":"t2_slbscky","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"For the Ollama option? Yeah, I was thinking something like that. But R1 is still needs a 8xH100 setup (or something wild like that). I might do a huge RAM+CPU server as I don't care much about tokens/s.\\n\\nStill both require a bunch of baby sitting (spinning up servers, containers, etc). A nice \\"automatically scale up/down\\" option would be ideal.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4k7mff","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For the Ollama option? Yeah, I was thinking something like that. But R1 is still needs a 8xH100 setup (or something wild like that). I might do a huge RAM+CPU server as I don&amp;#39;t care much about tokens/s.&lt;/p&gt;\\n\\n&lt;p&gt;Still both require a bunch of baby sitting (spinning up servers, containers, etc). A nice &amp;quot;automatically scale up/down&amp;quot; option would be ideal.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6gq8e","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6gq8e/i_wrote_2000_llm_test_cases_so_you_dont_have_to/n4k7mff/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753205233,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4k23ca","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kholejones8888","can_mod_post":false,"created_utc":1753203752,"send_replies":true,"parent_id":"t3_1m6gq8e","score":1,"author_fullname":"t2_1jp9h6pxqa","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For something like a test run hyperbolic is $1.50 per H100 per hour which might make it feasible for you. They also have spot instances which are even cheaper.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4k23ca","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For something like a test run hyperbolic is $1.50 per H100 per hour which might make it feasible for you. They also have spot instances which are even cheaper.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6gq8e/i_wrote_2000_llm_test_cases_so_you_dont_have_to/n4k23ca/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753203752,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6gq8e","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(t,{data:l});export{n as default};
