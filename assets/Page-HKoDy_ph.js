import{j as e}from"./index-DFOnUtq9.js";import{R as l}from"./RedditPostRenderer-B-dx19nm.js";import"./index-CUOQn61u.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I have an archive of several thousand maintenance documents.   They are all very structured and similar but not identical.   They cover 5 major classes of big industrial equipment.    For a single class there may be 20 or more specific builds but not every build in a class is identical.    Sometimes we want information about a whole class, and sometimes we want information about a specific build.\\n\\nI've had very good luck using an LLM with a well engineered prompt and defined JSON schema.   And basically I'm getting the answers I want, but not fast enough.   These may take 20 seconds each.    \\n\\nRight now I just do all these in a loop, one at a time and I'm wondering if there is a way to configure the server for better performance.    I have *plenty* of both CPU and GPU resources.   I want to better understand things like continuous batching, kv cache optimizing, threads and anything else that can improve performance when the prompts are nearly the same thing over and over.\\n\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Llama.cpp and continuous batching for performance","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lroopr","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.78,"author_flair_background_color":null,"subreddit_type":"public","ups":5,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_7osuk","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":5,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751650803,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I have an archive of several thousand maintenance documents.   They are all very structured and similar but not identical.   They cover 5 major classes of big industrial equipment.    For a single class there may be 20 or more specific builds but not every build in a class is identical.    Sometimes we want information about a whole class, and sometimes we want information about a specific build.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ve had very good luck using an LLM with a well engineered prompt and defined JSON schema.   And basically I&amp;#39;m getting the answers I want, but not fast enough.   These may take 20 seconds each.    &lt;/p&gt;\\n\\n&lt;p&gt;Right now I just do all these in a loop, one at a time and I&amp;#39;m wondering if there is a way to configure the server for better performance.    I have &lt;em&gt;plenty&lt;/em&gt; of both CPU and GPU resources.   I want to better understand things like continuous batching, kv cache optimizing, threads and anything else that can improve performance when the prompts are nearly the same thing over and over.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lroopr","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Simusid","discussion_type":null,"num_comments":5,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lroopr/llamacpp_and_continuous_batching_for_performance/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lroopr/llamacpp_and_continuous_batching_for_performance/","subreddit_subscribers":494898,"created_utc":1751650803,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1fw7t4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Chromix_","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1eeoq2","score":2,"author_fullname":"t2_k7w2h","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes, OP would need to provide more information about the usage scenario. To me the issue sounded like prompt processing time - something that could be precomputed. RAG chunks might miss relevant information, if the whole document fits the context and answers are always for one document only.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1fw7t4","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, OP would need to provide more information about the usage scenario. To me the issue sounded like prompt processing time - something that could be precomputed. RAG chunks might miss relevant information, if the whole document fits the context and answers are always for one document only.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lroopr","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lroopr/llamacpp_and_continuous_batching_for_performance/n1fw7t4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751704608,"author_flair_text":null,"treatment_tags":[],"created_utc":1751704608,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1eeoq2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SkyFeistyLlama8","can_mod_post":false,"created_utc":1751677421,"send_replies":true,"parent_id":"t1_n1cck5j","score":2,"author_fullname":"t2_1hgbaqgbnq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Wouldn't RAG work better? You chunk those documents, compute an embedding vector for each chunk and store the vectors and chunk text in a vector DB. During query time, you do a vector similarity search between the query vector and all the chunk vectors. Get the highest scoring chunks and include those as part of your LLM prompt.\\n\\nSkip the JSON output, go straight to a vector similarity search.\\n\\nThen again, the OP could be constrained by slow prompt processing for all those RAG chunks.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1eeoq2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Wouldn&amp;#39;t RAG work better? You chunk those documents, compute an embedding vector for each chunk and store the vectors and chunk text in a vector DB. During query time, you do a vector similarity search between the query vector and all the chunk vectors. Get the highest scoring chunks and include those as part of your LLM prompt.&lt;/p&gt;\\n\\n&lt;p&gt;Skip the JSON output, go straight to a vector similarity search.&lt;/p&gt;\\n\\n&lt;p&gt;Then again, the OP could be constrained by slow prompt processing for all those RAG chunks.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lroopr","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lroopr/llamacpp_and_continuous_batching_for_performance/n1eeoq2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751677421,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1cck5j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Chromix_","can_mod_post":false,"created_utc":1751651361,"send_replies":true,"parent_id":"t3_1lroopr","score":4,"author_fullname":"t2_k7w2h","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you want to do preprocessing you could [trade disk space](https://www.reddit.com/r/LocalLLaMA/comments/1lewhla/comment/myl30uy/) for (almost) instant time to first token. Arrange your data so that the variable part is at the end, if possible.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1cck5j","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you want to do preprocessing you could &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1lewhla/comment/myl30uy/\\"&gt;trade disk space&lt;/a&gt; for (almost) instant time to first token. Arrange your data so that the variable part is at the end, if possible.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lroopr/llamacpp_and_continuous_batching_for_performance/n1cck5j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751651361,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lroopr","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1chtpp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DeProgrammer99","can_mod_post":false,"created_utc":1751652996,"send_replies":true,"parent_id":"t1_n1ceacr","score":2,"author_fullname":"t2_w4j8t","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Specifically, see \`--parallel\` [here](https://github.com/osllmai/llama.cpp/blob/main/examples/server/README.md).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1chtpp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Specifically, see &lt;code&gt;--parallel&lt;/code&gt; &lt;a href=\\"https://github.com/osllmai/llama.cpp/blob/main/examples/server/README.md\\"&gt;here&lt;/a&gt;.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lroopr","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lroopr/llamacpp_and_continuous_batching_for_performance/n1chtpp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751652996,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1ceacr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Informal_Librarian","can_mod_post":false,"created_utc":1751651886,"send_replies":true,"parent_id":"t3_1lroopr","score":2,"author_fullname":"t2_obq9bdp","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Are you setting a number of slots in Llama.cpp? For example, you could set four or eight slots, and then it will simultaneously process all of them at once in parallel.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ceacr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Are you setting a number of slots in Llama.cpp? For example, you could set four or eight slots, and then it will simultaneously process all of them at once in parallel.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lroopr/llamacpp_and_continuous_batching_for_performance/n1ceacr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751651886,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lroopr","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),n=()=>e.jsx(l,{data:a});export{n as default};
