import{j as e}from"./index-DLSqWzaI.js";import{R as t}from"./RedditPostRenderer-CysRo2D_.js";import"./index-COXiL3Lo.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"https://preview.redd.it/ypc8zweungdf1.jpg?width=750&amp;format=pjpg&amp;auto=webp&amp;s=56d1c0515e0a511341268c19f0f578b9bf06baf0\\n\\nSelf-play has long been a key topic in artificial intelligence research. By allowing AI to compete against itself, researchers have been able to observe the emergence of intelligence. Numerous algorithms have already demonstrated that agents trained through self-play can surpass human experts.\\n\\nSo, what happens if we apply self-play to large language models (LLMs)? Can LLMs become even more intelligent with self-play training?\\n\\nA recent study conducted by researchers from institutions including the National University of Singapore, Centre for Frontier AI Research (CFAR), Northeastern University, Sea AI Lab, Plastic Labs, and the University of Washington confirms this: LLM agents trained through self-play can significantly enhance their reasoning capabilities!\\n\\nRead our interpretation of this groundbreaking paper here:  \\n[https://blog.netmind.ai/article/LLMs\\\\_Playing\\\\_Competitive\\\\_Games\\\\_Emerge\\\\_Critical\\\\_Reasoning%3A\\\\_A\\\\_Latest\\\\_Study\\\\_Showing\\\\_Surprising\\\\_Results](https://blog.netmind.ai/article/LLMs_Playing_Competitive_Games_Emerge_Critical_Reasoning%3A_A_Latest_Study_Showing_Surprising_Results)","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"LLMs Playing Competitive Games Emerge Critical Reasoning: A Latest Study Showing Surprising Results","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":72,"top_awarded_type":null,"hide_score":false,"media_metadata":{"ypc8zweungdf1":{"status":"valid","e":"Image","m":"image/jpg","p":[{"y":55,"x":108,"u":"https://preview.redd.it/ypc8zweungdf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7e3c75a41f3e407c4558db213bda8c51e093b51d"},{"y":111,"x":216,"u":"https://preview.redd.it/ypc8zweungdf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=38e5ca29442710df67ca42986b4a3e2ba592dcd8"},{"y":165,"x":320,"u":"https://preview.redd.it/ypc8zweungdf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c2ea292e9ff5e36f5c122462fd42d65c1d26d66c"},{"y":330,"x":640,"u":"https://preview.redd.it/ypc8zweungdf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b18b42a6b8a30a8326e6fba1a22fe2079d81871d"}],"s":{"y":387,"x":750,"u":"https://preview.redd.it/ypc8zweungdf1.jpg?width=750&amp;format=pjpg&amp;auto=webp&amp;s=56d1c0515e0a511341268c19f0f578b9bf06baf0"},"id":"ypc8zweungdf1"}},"name":"t3_1m2c9w6","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.77,"author_flair_background_color":null,"ups":16,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1mz24a41z0","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":16,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://external-preview.redd.it/GMmAQl8cXhjszVZRasZjEE7PH09yiLGlFTDIar7oBtk.jpeg?width=140&amp;height=72&amp;crop=140:72,smart&amp;auto=webp&amp;s=a96184dab9f05bdbe26b3d1cad800b667f2a8647","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"subreddit_type":"public","created":1752770033,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://preview.redd.it/ypc8zweungdf1.jpg?width=750&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=56d1c0515e0a511341268c19f0f578b9bf06baf0\\"&gt;https://preview.redd.it/ypc8zweungdf1.jpg?width=750&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=56d1c0515e0a511341268c19f0f578b9bf06baf0&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Self-play has long been a key topic in artificial intelligence research. By allowing AI to compete against itself, researchers have been able to observe the emergence of intelligence. Numerous algorithms have already demonstrated that agents trained through self-play can surpass human experts.&lt;/p&gt;\\n\\n&lt;p&gt;So, what happens if we apply self-play to large language models (LLMs)? Can LLMs become even more intelligent with self-play training?&lt;/p&gt;\\n\\n&lt;p&gt;A recent study conducted by researchers from institutions including the National University of Singapore, Centre for Frontier AI Research (CFAR), Northeastern University, Sea AI Lab, Plastic Labs, and the University of Washington confirms this: LLM agents trained through self-play can significantly enhance their reasoning capabilities!&lt;/p&gt;\\n\\n&lt;p&gt;Read our interpretation of this groundbreaking paper here:&lt;br/&gt;\\n&lt;a href=\\"https://blog.netmind.ai/article/LLMs_Playing_Competitive_Games_Emerge_Critical_Reasoning%3A_A_Latest_Study_Showing_Surprising_Results\\"&gt;https://blog.netmind.ai/article/LLMs_Playing_Competitive_Games_Emerge_Critical_Reasoning%3A_A_Latest_Study_Showing_Surprising_Results&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/GMmAQl8cXhjszVZRasZjEE7PH09yiLGlFTDIar7oBtk.jpeg?auto=webp&amp;s=38a1ea012d9835596bac766bd4664980e1abcf0b","width":750,"height":387},"resolutions":[{"url":"https://external-preview.redd.it/GMmAQl8cXhjszVZRasZjEE7PH09yiLGlFTDIar7oBtk.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fab4ba96990b849665bcef4e9f39f1550ffd59f3","width":108,"height":55},{"url":"https://external-preview.redd.it/GMmAQl8cXhjszVZRasZjEE7PH09yiLGlFTDIar7oBtk.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b7487dbcd9db271b0b5ea0591151ecbec843bf25","width":216,"height":111},{"url":"https://external-preview.redd.it/GMmAQl8cXhjszVZRasZjEE7PH09yiLGlFTDIar7oBtk.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=aef3995b31290a43b0e9765d1ea3e4d8dd66921e","width":320,"height":165},{"url":"https://external-preview.redd.it/GMmAQl8cXhjszVZRasZjEE7PH09yiLGlFTDIar7oBtk.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4fe7497b5d0c0e775a986c8fd1afb9bbc017574a","width":640,"height":330}],"variants":{},"id":"GMmAQl8cXhjszVZRasZjEE7PH09yiLGlFTDIar7oBtk"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"mod_note":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"num_reports":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1m2c9w6","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"MarketingNetMind","discussion_type":null,"num_comments":10,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m2c9w6/llms_playing_competitive_games_emerge_critical/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m2c9w6/llms_playing_competitive_games_emerge_critical/","subreddit_subscribers":501104,"created_utc":1752770033,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3wonzq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"TheRealGentlefox","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3s5s6b","score":1,"author_fullname":"t2_f471r","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Definitely possible, but your point is still covered by my linguistic fart theory. No reason \\"Finetuned on\\" and \\"Trained on\\" wouldn't be missed by a Chinese -&gt; English translation.\\n\\nThey should have mentioned Qwen though. Also looks like they updated the language for your critique now haha.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3wonzq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Definitely possible, but your point is still covered by my linguistic fart theory. No reason &amp;quot;Finetuned on&amp;quot; and &amp;quot;Trained on&amp;quot; wouldn&amp;#39;t be missed by a Chinese -&amp;gt; English translation.&lt;/p&gt;\\n\\n&lt;p&gt;They should have mentioned Qwen though. Also looks like they updated the language for your critique now haha.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2c9w6","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2c9w6/llms_playing_competitive_games_emerge_critical/n3wonzq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752881762,"author_flair_text":null,"treatment_tags":[],"created_utc":1752881762,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3s5s6b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Chromix_","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3pmd23","score":2,"author_fullname":"t2_k7w2h","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes, that might be if it was just that, but there is more. The authors took Qwen3 base, which was of course trained on math, equations and tons of other things. Then they did the fine-tuning with just Kuhn-Poker data on top, which improved math performance, despite not containing explicit training on math.\\n\\nAs a human I thus wouldn't write \\"model trained exclusively on Poker\\" and \\"never saw math\\". That'd be highly misleading, especially as the article never mentions anything about a Qwen3 fine-tune - that info was just in a screenshot of a graph.\\n\\nIt's however something that LLMs are prone to do. The article sounded \\"write a hyped summary of the paper\\"-generated to me. In fact, when giving the full paper to Qwen3 32B (thinking) and asking \\"Is it correct that they trained a model exclusively on Kuhn Poker, the model never saw a single math equation during training, and it did better on math?\\" it'll respond with \\"Yes, the paper confirms that the model was trained exclusively on Kuhn Poker (a zero-sum game) without exposure to any mathematical equations or domain-specific training data during its training process.\\" - because it took some quotes in the article out of context and disregarded the implications of the existing base model.","edited":1752831134,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3s5s6b","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, that might be if it was just that, but there is more. The authors took Qwen3 base, which was of course trained on math, equations and tons of other things. Then they did the fine-tuning with just Kuhn-Poker data on top, which improved math performance, despite not containing explicit training on math.&lt;/p&gt;\\n\\n&lt;p&gt;As a human I thus wouldn&amp;#39;t write &amp;quot;model trained exclusively on Poker&amp;quot; and &amp;quot;never saw math&amp;quot;. That&amp;#39;d be highly misleading, especially as the article never mentions anything about a Qwen3 fine-tune - that info was just in a screenshot of a graph.&lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s however something that LLMs are prone to do. The article sounded &amp;quot;write a hyped summary of the paper&amp;quot;-generated to me. In fact, when giving the full paper to Qwen3 32B (thinking) and asking &amp;quot;Is it correct that they trained a model exclusively on Kuhn Poker, the model never saw a single math equation during training, and it did better on math?&amp;quot; it&amp;#39;ll respond with &amp;quot;Yes, the paper confirms that the model was trained exclusively on Kuhn Poker (a zero-sum game) without exposure to any mathematical equations or domain-specific training data during its training process.&amp;quot; - because it took some quotes in the article out of context and disregarded the implications of the existing base model.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2c9w6","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2c9w6/llms_playing_competitive_games_emerge_critical/n3s5s6b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752827188,"author_flair_text":null,"treatment_tags":[],"created_utc":1752827188,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3pmd23","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"TheRealGentlefox","can_mod_post":false,"created_utc":1752789697,"send_replies":true,"parent_id":"t1_n3nwkdt","score":4,"author_fullname":"t2_f471r","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It just seems like a linguistic fart to me. \\"Trained on\\" == \\"Further trained on\\" or \\"Fine-tuned on\\" or \\"Further RL'd on\\".\\n\\nThe listed team is almost entirely Chinese, and that translation isn't easy.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3pmd23","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It just seems like a linguistic fart to me. &amp;quot;Trained on&amp;quot; == &amp;quot;Further trained on&amp;quot; or &amp;quot;Fine-tuned on&amp;quot; or &amp;quot;Further RL&amp;#39;d on&amp;quot;.&lt;/p&gt;\\n\\n&lt;p&gt;The listed team is almost entirely Chinese, and that translation isn&amp;#39;t easy.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2c9w6","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2c9w6/llms_playing_competitive_games_emerge_critical/n3pmd23/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752789697,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3szqbr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Holiday_Sugar9743","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3nydgp","score":1,"author_fullname":"t2_axxs98n5b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Why r ppl still downvotingðŸ˜‚can u read guys","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3szqbr","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why r ppl still downvotingðŸ˜‚can u read guys&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2c9w6","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2c9w6/llms_playing_competitive_games_emerge_critical/n3szqbr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752841673,"author_flair_text":null,"treatment_tags":[],"created_utc":1752841673,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3nydgp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"MarketingNetMind","can_mod_post":false,"created_utc":1752772562,"send_replies":true,"parent_id":"t1_n3nwkdt","score":-7,"author_fullname":"t2_1mz24a41z0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"https://preview.redd.it/dq2l0xsvvgdf1.png?width=2298&amp;format=png&amp;auto=webp&amp;s=497e8d5cfbea6e83386d9fb33ab670e933dca4b1\\n\\nThis is a screenshot from the original paper. \\n\\n&gt;Self-play on Kuhn Poker improves math and general reasoning benchmarks despite never\\n\\n&gt;seeing benchmark related problems.\\n\\nSo our statement **is correct** and the model **was not** trained on math equations.\\n\\nAnd that's exactly why this paper is interesting: very counterintuitive but promising.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3nydgp","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://preview.redd.it/dq2l0xsvvgdf1.png?width=2298&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=497e8d5cfbea6e83386d9fb33ab670e933dca4b1\\"&gt;https://preview.redd.it/dq2l0xsvvgdf1.png?width=2298&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=497e8d5cfbea6e83386d9fb33ab670e933dca4b1&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;This is a screenshot from the original paper. &lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Self-play on Kuhn Poker improves math and general reasoning benchmarks despite never&lt;/p&gt;\\n\\n&lt;p&gt;seeing benchmark related problems.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;So our statement &lt;strong&gt;is correct&lt;/strong&gt; and the model &lt;strong&gt;was not&lt;/strong&gt; trained on math equations.&lt;/p&gt;\\n\\n&lt;p&gt;And that&amp;#39;s exactly why this paper is interesting: very counterintuitive but promising.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2c9w6","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2c9w6/llms_playing_competitive_games_emerge_critical/n3nydgp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752772562,"media_metadata":{"dq2l0xsvvgdf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":23,"x":108,"u":"https://preview.redd.it/dq2l0xsvvgdf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8463d1f893faa22868e8f671c913e3e46d07d173"},{"y":46,"x":216,"u":"https://preview.redd.it/dq2l0xsvvgdf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0427c5fe7762c81cea0a1dab75e538a91891f98e"},{"y":68,"x":320,"u":"https://preview.redd.it/dq2l0xsvvgdf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ff4ea6b94bf9f40887bd0086afed68754f18f6da"},{"y":137,"x":640,"u":"https://preview.redd.it/dq2l0xsvvgdf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f44c52233d223a7cdd57bd841d12fd8b0732c7b9"},{"y":205,"x":960,"u":"https://preview.redd.it/dq2l0xsvvgdf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=72520a628ed2e23ddbcb7acea2dccfb44ce53288"},{"y":231,"x":1080,"u":"https://preview.redd.it/dq2l0xsvvgdf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b8ae5c87e4abfbf06d37ad7aea62fca13ae8675b"}],"s":{"y":492,"x":2298,"u":"https://preview.redd.it/dq2l0xsvvgdf1.png?width=2298&amp;format=png&amp;auto=webp&amp;s=497e8d5cfbea6e83386d9fb33ab670e933dca4b1"},"id":"dq2l0xsvvgdf1"}},"author_flair_text":null,"treatment_tags":[],"collapsed":true,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-7}}],"before":null}},"user_reports":[],"saved":false,"id":"n3nwkdt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Chromix_","can_mod_post":false,"created_utc":1752772074,"send_replies":true,"parent_id":"t3_1m2c9w6","score":6,"author_fullname":"t2_k7w2h","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Direct link to paper: [https://arxiv.org/abs/2506.24119](https://arxiv.org/abs/2506.24119)\\n\\nBased on the paper it seems like self-play can be used to enhance LLM training results, while also reducing training data requirements, yet it isn't the silver-bullet. It's also rather expensive to do (properly).\\n\\nThe linked article by OP is either LLM-written or the author didn't read the paper properly.\\n\\n&gt;The results were striking. An AI model trained exclusively on Kuhn Pokerâ€”**never seeing a single maths equation during training**â€”improved its mathematical reasoning performance by 8.6%\\n\\n(Emphasis is mine). That statement is incorrect. The model was trained on math equations.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3nwkdt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Direct link to paper: &lt;a href=\\"https://arxiv.org/abs/2506.24119\\"&gt;https://arxiv.org/abs/2506.24119&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Based on the paper it seems like self-play can be used to enhance LLM training results, while also reducing training data requirements, yet it isn&amp;#39;t the silver-bullet. It&amp;#39;s also rather expensive to do (properly).&lt;/p&gt;\\n\\n&lt;p&gt;The linked article by OP is either LLM-written or the author didn&amp;#39;t read the paper properly.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;The results were striking. An AI model trained exclusively on Kuhn Pokerâ€”&lt;strong&gt;never seeing a single maths equation during training&lt;/strong&gt;â€”improved its mathematical reasoning performance by 8.6%&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;(Emphasis is mine). That statement is incorrect. The model was trained on math equations.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2c9w6/llms_playing_competitive_games_emerge_critical/n3nwkdt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752772074,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2c9w6","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3rrv85","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MarketingNetMind","can_mod_post":false,"created_utc":1752819522,"send_replies":true,"parent_id":"t1_n3qrx8o","score":2,"author_fullname":"t2_1mz24a41z0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"For academic researchers, itâ€™s difficult to run experiments on larger models. However, we believe the experiments in this paper already provide very important insights for LLM research: self-play can further incentivize an LLMâ€™s reasoning ability. Itâ€™s possible that some closed-source models are already using this approach to improve their performance.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3rrv85","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For academic researchers, itâ€™s difficult to run experiments on larger models. However, we believe the experiments in this paper already provide very important insights for LLM research: self-play can further incentivize an LLMâ€™s reasoning ability. Itâ€™s possible that some closed-source models are already using this approach to improve their performance.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2c9w6","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2c9w6/llms_playing_competitive_games_emerge_critical/n3rrv85/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752819522,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3qrx8o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"relax900","can_mod_post":false,"created_utc":1752803977,"send_replies":true,"parent_id":"t3_1m2c9w6","score":1,"author_fullname":"t2_i6yi2ci18","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"nice paper. the gains for the deepseek distill 7b was not that significant. 2 percent overall, and 1 percent increase in GPQA. it help smaller models, but will it work for larger and more capable models(deepseek R1)?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3qrx8o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;nice paper. the gains for the deepseek distill 7b was not that significant. 2 percent overall, and 1 percent increase in GPQA. it help smaller models, but will it work for larger and more capable models(deepseek R1)?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2c9w6/llms_playing_competitive_games_emerge_critical/n3qrx8o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752803977,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2c9w6","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3speef","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Paradigmind","can_mod_post":false,"created_utc":1752837565,"send_replies":true,"parent_id":"t3_1m2c9w6","score":1,"author_fullname":"t2_6ste18zta","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"So playing with yourself makes you smarter?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3speef","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So playing with yourself makes you smarter?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2c9w6/llms_playing_competitive_games_emerge_critical/n3speef/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752837565,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2c9w6","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3pmgqs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"TheTerrasque","can_mod_post":false,"created_utc":1752789730,"send_replies":true,"parent_id":"t3_1m2c9w6","score":0,"author_fullname":"t2_9uv8v","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Interesting. I wonder if this could be used with some sort of trivia and give 1 point for right answer, -1 for wrong, and 0 for declining to answer. Goal being to reduce hallucinations and \\"confidently incorrect\\" type answers.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3pmgqs","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Interesting. I wonder if this could be used with some sort of trivia and give 1 point for right answer, -1 for wrong, and 0 for declining to answer. Goal being to reduce hallucinations and &amp;quot;confidently incorrect&amp;quot; type answers.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2c9w6/llms_playing_competitive_games_emerge_critical/n3pmgqs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752789730,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2c9w6","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}}]`),i=()=>e.jsx(t,{data:a});export{i as default};
