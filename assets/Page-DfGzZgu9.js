import{j as e}from"./index-DQXiEb7D.js";import{R as l}from"./RedditPostRenderer-BjndLgq8.js";import"./index-B-ILyjT1.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I've recently discovered the wonders of LM Studio, which lets me run models without the CLI headache of OpenWebUI or ollama, and supposedly it supports multi-GPU splitting \\n\\nThe main model I want to use is LLaMA 3.3 70B, ideally Q8, and sometimes fallen Gemma3 27B Q8, but because of scalper scumbags, GPUs are insanely overpriced \\n\\nP40s are actually a pretty good deal, and I want to get 4 of them \\n\\nBecause I use an 8GB GTX1070 for playing games, I'm stuck with CPU only inference, which gives me about 0.4 tok/sec with LLaMA 70B, and about 1 tok/sec on fallen Gemma3 27B (which rapidly drops as context is filled) if I try to do partial GPU offloading, it slows down even more \\n\\nI don't need hundreds of tokens per second, or collosal models, pretty happy with LLaMA 70B (and I'm used to waiting literally 10-15 MINUTES for each reply) would 4 P40s be suitable for what I'm planning to do \\n\\nSome posts here say they work fine for AI, others say they're junk  ","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Are P40s useful for 70B models","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m3kjsm","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.78,"author_flair_background_color":null,"subreddit_type":"public","ups":10,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_pmfowly6n","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":10,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752890771,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve recently discovered the wonders of LM Studio, which lets me run models without the CLI headache of OpenWebUI or ollama, and supposedly it supports multi-GPU splitting &lt;/p&gt;\\n\\n&lt;p&gt;The main model I want to use is LLaMA 3.3 70B, ideally Q8, and sometimes fallen Gemma3 27B Q8, but because of scalper scumbags, GPUs are insanely overpriced &lt;/p&gt;\\n\\n&lt;p&gt;P40s are actually a pretty good deal, and I want to get 4 of them &lt;/p&gt;\\n\\n&lt;p&gt;Because I use an 8GB GTX1070 for playing games, I&amp;#39;m stuck with CPU only inference, which gives me about 0.4 tok/sec with LLaMA 70B, and about 1 tok/sec on fallen Gemma3 27B (which rapidly drops as context is filled) if I try to do partial GPU offloading, it slows down even more &lt;/p&gt;\\n\\n&lt;p&gt;I don&amp;#39;t need hundreds of tokens per second, or collosal models, pretty happy with LLaMA 70B (and I&amp;#39;m used to waiting literally 10-15 MINUTES for each reply) would 4 P40s be suitable for what I&amp;#39;m planning to do &lt;/p&gt;\\n\\n&lt;p&gt;Some posts here say they work fine for AI, others say they&amp;#39;re junk  &lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m3kjsm","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"T-VIRUS999","discussion_type":null,"num_comments":22,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m3kjsm/are_p40s_useful_for_70b_models/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m3kjsm/are_p40s_useful_for_70b_models/","subreddit_subscribers":501232,"created_utc":1752890771,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3yplbk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No-Refrigerator-1672","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3ybvz5","score":4,"author_fullname":"t2_baavelp5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Mi50 got pretty fast memory (1TB/s), so even a single card gives [pretty high token generation speeds](https://www.reddit.com/r/LocalLLaMA/s/PwCp0tMDIM). However, their prefill speeds are wuite slow; usable, but disappointing. So depending on how crucial is it for you to process long context, they could be a wonderful or underwhelming options.","edited":1752914454,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3yplbk","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Mi50 got pretty fast memory (1TB/s), so even a single card gives &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/s/PwCp0tMDIM\\"&gt;pretty high token generation speeds&lt;/a&gt;. However, their prefill speeds are wuite slow; usable, but disappointing. So depending on how crucial is it for you to process long context, they could be a wonderful or underwhelming options.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3kjsm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3kjsm/are_p40s_useful_for_70b_models/n3yplbk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752914164,"author_flair_text":null,"treatment_tags":[],"created_utc":1752914164,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3yx3ch","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3ybvz5","score":0,"author_fullname":"t2_h5utwre7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"There's flash attention rocm.. does it support these?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3yx3ch","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There&amp;#39;s flash attention rocm.. does it support these?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3kjsm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3kjsm/are_p40s_useful_for_70b_models/n3yx3ch/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752918453,"author_flair_text":null,"treatment_tags":[],"created_utc":1752918453,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ybvz5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Willing_Landscape_61","can_mod_post":false,"created_utc":1752906506,"send_replies":true,"parent_id":"t1_n3xeqyf","score":1,"author_fullname":"t2_8lvrytgw","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Even without flash attention?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ybvz5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Even without flash attention?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3kjsm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3kjsm/are_p40s_useful_for_70b_models/n3ybvz5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752906506,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3xgkyt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"T-VIRUS999","can_mod_post":false,"created_utc":1752892129,"send_replies":true,"parent_id":"t1_n3xeqyf","score":0,"author_fullname":"t2_pmfowly6n","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Will those work with LM Studio? Those are an even better deal, but screw Ali, got scammed last time I tried buying something off there","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3xgkyt","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Will those work with LM Studio? Those are an even better deal, but screw Ali, got scammed last time I tried buying something off there&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3kjsm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3kjsm/are_p40s_useful_for_70b_models/n3xgkyt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752892129,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n3xeqyf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ForsookComparison","can_mod_post":false,"created_utc":1752891410,"send_replies":true,"parent_id":"t3_1m3kjsm","score":15,"author_fullname":"t2_on5es7pe3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you're only doing inference, the new meta is buying Alibaba 32GB Mi50's","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3xeqyf","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you&amp;#39;re only doing inference, the new meta is buying Alibaba 32GB Mi50&amp;#39;s&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3kjsm/are_p40s_useful_for_70b_models/n3xeqyf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752891410,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m3kjsm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":15}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3zbhei","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RnRau","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3xgo26","score":0,"author_fullname":"t2_svt5u","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"They gave you the context to ask an AI. Or a google search.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3zbhei","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They gave you the context to ask an AI. Or a google search.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3kjsm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3kjsm/are_p40s_useful_for_70b_models/n3zbhei/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752925667,"author_flair_text":null,"treatment_tags":[],"created_utc":1752925667,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n3xgo26","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"T-VIRUS999","can_mod_post":false,"created_utc":1752892163,"send_replies":true,"parent_id":"t1_n3xeg73","score":-1,"author_fullname":"t2_pmfowly6n","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"No idea what that even is, I have literally zero skill in any sort of CLI","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3xgo26","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No idea what that even is, I have literally zero skill in any sort of CLI&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3kjsm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3kjsm/are_p40s_useful_for_70b_models/n3xgo26/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752892163,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3xeg73","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No-Statement-0001","can_mod_post":false,"created_utc":1752891294,"send_replies":true,"parent_id":"t3_1m3kjsm","score":7,"author_fullname":"t2_11gh93nhos","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"They work fine for 70B models. Use a draft model with speculative decoding and you should get a decent speed up. You’ll want to use llama-server with row split mode to get another speed up.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3xeg73","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They work fine for 70B models. Use a draft model with speculative decoding and you should get a decent speed up. You’ll want to use llama-server with row split mode to get another speed up.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3kjsm/are_p40s_useful_for_70b_models/n3xeg73/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752891294,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m3kjsm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3xrlt7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Ok_Warning2146","can_mod_post":false,"created_utc":1752896697,"send_replies":true,"parent_id":"t3_1m3kjsm","score":6,"author_fullname":"t2_s6sfw4yy","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"70B models are now outperformed by gemma3 27b and qwen3 32b now. Better not to build anything with them in mind.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3xrlt7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;70B models are now outperformed by gemma3 27b and qwen3 32b now. Better not to build anything with them in mind.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3kjsm/are_p40s_useful_for_70b_models/n3xrlt7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752896697,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3kjsm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"50c36eba-fdca-11ee-9735-92a88d7e3b87","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ykwnk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"gerhardmpl","can_mod_post":false,"created_utc":1752911516,"send_replies":true,"parent_id":"t3_1m3kjsm","score":3,"author_fullname":"t2_tlzk7zie","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I am using two P40 with ollama on a Dell R720. With [lama3.3:70b](https://ollama.com/library/llama3.3:70b) and 8k context I get \\\\~4 token/s.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ykwnk","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Ollama"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I am using two P40 with ollama on a Dell R720. With &lt;a href=\\"https://ollama.com/library/llama3.3:70b\\"&gt;lama3.3:70b&lt;/a&gt; and 8k context I get ~4 token/s.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3kjsm/are_p40s_useful_for_70b_models/n3ykwnk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752911516,"author_flair_text":"Ollama","treatment_tags":[],"link_id":"t3_1m3kjsm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3zblq5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RnRau","can_mod_post":false,"created_utc":1752925719,"send_replies":true,"parent_id":"t1_n3y3kab","score":1,"author_fullname":"t2_svt5u","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Is Qwen 2.5 70b outclassed by 32b's nowadays?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3zblq5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Is Qwen 2.5 70b outclassed by 32b&amp;#39;s nowadays?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3kjsm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3kjsm/are_p40s_useful_for_70b_models/n3zblq5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752925719,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3y3kab","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SillyLilBear","can_mod_post":false,"created_utc":1752902178,"send_replies":true,"parent_id":"t3_1m3kjsm","score":2,"author_fullname":"t2_wjjtz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"There are no 70b worth using","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3y3kab","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There are no 70b worth using&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3kjsm/are_p40s_useful_for_70b_models/n3y3kab/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752902178,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3kjsm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3zc9zc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"T-VIRUS999","can_mod_post":false,"created_utc":1752926006,"send_replies":true,"parent_id":"t1_n3ycujk","score":1,"author_fullname":"t2_pmfowly6n","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I tried the kobold AI app previously and every model just spits out gibberish","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3zc9zc","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I tried the kobold AI app previously and every model just spits out gibberish&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3kjsm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3kjsm/are_p40s_useful_for_70b_models/n3zc9zc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752926006,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ycujk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fish312","can_mod_post":false,"created_utc":1752907027,"send_replies":true,"parent_id":"t3_1m3kjsm","score":2,"author_fullname":"t2_mogjd","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Koboldcpp is better","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ycujk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Koboldcpp is better&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3kjsm/are_p40s_useful_for_70b_models/n3ycujk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752907027,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3kjsm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3zc31w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"T-VIRUS999","can_mod_post":false,"created_utc":1752925924,"send_replies":true,"parent_id":"t1_n3yo6gi","score":1,"author_fullname":"t2_pmfowly6n","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What sort of performance do you get out of 24B and what frontend are you using?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3zc31w","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What sort of performance do you get out of 24B and what frontend are you using?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3kjsm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3kjsm/are_p40s_useful_for_70b_models/n3zc31w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752925924,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3yo6gi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FunnyAsparagus1253","can_mod_post":false,"created_utc":1752913351,"send_replies":true,"parent_id":"t3_1m3kjsm","score":2,"author_fullname":"t2_i6c8tay3w","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I have 2 P40s in my rig. I haven’t tried a 70B yet, but doing a rough guess based on 24B (just fine; happy with it) and 120B (pretty slow; kindof usable if you’re not doing anything too fancy), I’d guess that you’d be okay with a 70B.\\n\\nEdit: but yeah, if I was building nowadays I’d get MI50s instead.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3yo6gi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have 2 P40s in my rig. I haven’t tried a 70B yet, but doing a rough guess based on 24B (just fine; happy with it) and 120B (pretty slow; kindof usable if you’re not doing anything too fancy), I’d guess that you’d be okay with a 70B.&lt;/p&gt;\\n\\n&lt;p&gt;Edit: but yeah, if I was building nowadays I’d get MI50s instead.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3kjsm/are_p40s_useful_for_70b_models/n3yo6gi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752913351,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3kjsm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3y4npr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"shing3232","can_mod_post":false,"created_utc":1752902723,"send_replies":true,"parent_id":"t3_1m3kjsm","score":1,"author_fullname":"t2_ze4mg","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"it work but It s not gonna be fast","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3y4npr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;it work but It s not gonna be fast&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3kjsm/are_p40s_useful_for_70b_models/n3y4npr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752902723,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3kjsm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3zdd52","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"T-VIRUS999","can_mod_post":false,"created_utc":1752926460,"send_replies":true,"parent_id":"t1_n3y7rnv","score":1,"author_fullname":"t2_pmfowly6n","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Coherence drops off a cliff with quantization beyond a certain point, I have used Q4 in both, and Fallen Gemma3 27B is dumber in Q4 than Q8 (haven't tried the official Gemma 27B, only this de-censored version) \\n\\n\\nLLaMA 70B is usable at Q4, but is noticably smarter at Q6 in my experience (highest version I can run with 64GB of RAM) and I suspect would be even better at Q8","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3zdd52","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Coherence drops off a cliff with quantization beyond a certain point, I have used Q4 in both, and Fallen Gemma3 27B is dumber in Q4 than Q8 (haven&amp;#39;t tried the official Gemma 27B, only this de-censored version) &lt;/p&gt;\\n\\n&lt;p&gt;LLaMA 70B is usable at Q4, but is noticably smarter at Q6 in my experience (highest version I can run with 64GB of RAM) and I suspect would be even better at Q8&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3kjsm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3kjsm/are_p40s_useful_for_70b_models/n3zdd52/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752926460,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3y7rnv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CheatCodesOfLife","can_mod_post":false,"created_utc":1752904327,"send_replies":true,"parent_id":"t3_1m3kjsm","score":1,"author_fullname":"t2_32el727b","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt;LLaMA 3.3 70B, ideally Q8\\n\\nWhy Q8?\\n\\n&gt; Gemma3 27B Q8\\n\\nTried Q4_0? This model was optimized to run well at Q4. And avoiding the _K would be faster on CPU.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3y7rnv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;LLaMA 3.3 70B, ideally Q8&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Why Q8?&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Gemma3 27B Q8&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Tried Q4_0? This model was optimized to run well at Q4. And avoiding the _K would be faster on CPU.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3kjsm/are_p40s_useful_for_70b_models/n3y7rnv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752904327,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3kjsm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ybu0u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MichaelXie4645","can_mod_post":false,"created_utc":1752906479,"send_replies":true,"parent_id":"t3_1m3kjsm","score":1,"author_fullname":"t2_a06q0mmx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"P40s are near ewaste now as no bf16 native support and no fp16 training either. You can get better performance out of orins and they would support native bf16 and fp16 accel.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ybu0u","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;P40s are near ewaste now as no bf16 native support and no fp16 training either. You can get better performance out of orins and they would support native bf16 and fp16 accel.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3kjsm/are_p40s_useful_for_70b_models/n3ybu0u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752906479,"author_flair_text":"Llama 405B","treatment_tags":[],"link_id":"t3_1m3kjsm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),s=()=>e.jsx(l,{data:a});export{s as default};
