import{j as e}from"./index-BlGsFJYy.js";import{R as t}from"./RedditPostRenderer-B6uvq_Zl.js";import"./index-DDvVtNwD.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"TLDR Personally, I suck at CLI troubleshooting, I realized I will now happily trade away some token speed for a more simple and intuitive UI/UX\\n\\nI'm very new to Linux as well as local LLMs, finally switched over to Linux just last week from Windows 10. I have basically zero CLI experience.\\n\\nFew days ago, I started having trouble with Ollama. One night, I was getting 4 t/s with unsloth's Deepseek R1 0528 684b Q4, then the next day 0.15 t/s... Model generation speeds were painfully slow and inconsistent. Many here on the sub suggested that I switch over from ollama to llama.cpp or ik\\\\_llama.cpp, so I gave both a try.\\n\\nThe performance difference of llama.cpp / ik\\\\_llama.cpp over ollama is absolutely nuts. So running unsloth's Deepseek R1-0528 684B at Q4 (with Threadripper, 512gb DDR4 RAM, and dual 3090s), I got:\\n\\n* **Ollama**: 0.15 t/s - absolutely terrible\\n* **llama.cpp** (through LM Studio): \\\\~4.7 t/s - massive improvement\\n* **ik\\\\_llama.cpp**: \\\\~7.6 t/s!! 60% faster than LM Studio, and literally FIFTY times faster than ollama\\n\\nSounds absolutely amazing, BUT there was a huge catch I didn't know at first.\\n\\nThe learning curve is incredibly steep, especially for a noob like me. I spent WAY more time troubleshooting errors, crashes, scouring online, GH, [r/llocalllama](https://www.reddit.com/r/llocalllama/), asking other users, and hunting for obscure fixes than time actually using the models. I actually copied someone else's ik\\\\_llama.cpp build set up and server run command to use Deepseek 0528, and it ran smoothly. But the moment I try to run any other model, even 20b, 30b or 70b parametermodel, things quickly went downhill. Memory failures, crashes, cryptic error logs. Many hours spent looking for solutions online, or asking CGPT / Deepseek for insight. Sometimes getting lucky with a solution, and other times just giving up altogether. Also hard to optimize for different models with my hardware, as I have no idea what the dozens of flags, commands, and parameters do even after reading the llama-server --help stuff.\\n\\nI realized one important thing that's obvious now but didn't think of earlier. What works for one user doesn't always scale to other users (or noobs like me lol). While many suggested ik\\\\_llama.cpp, there's not always blanket solution that fits all. Perhaps not everyone needs to move to the absolute fastest backend. There's also a ton of great advice out there or troubleshooting tips, but some of it is definitely geared toward power users that understand things like what happens and why it happens when randomparameter=1, when to turn various parameters off, flag this, tensor that, re-build with this flag, CUDA that, offload this here, don't offload that thing in this specific situation. Reading some of the CLI help I found felt like reading another language, felt super lost.\\n\\nOn the flip side, LM Studio was genuinely plug and play. Felt very intuitive, stable, and it just worked out of the box. I didn't encounter any crashes, or error logs to navigate. Practically zero command line stuff after install. Downloading, loading, and swapping models is SO easy in LMS. Front end + back end packaged together. Sure, it's not the fastest, but right now I will take the usability and speed hit over hours of troubleshooting chaos.\\n\\nFor now, I'm probably going to daily drive LM Studio, while slowly working through the steep CLI learning curve on the side. Not an LM Studio ad btw lol. Hopefully one day I can earn my CLI blue belt lol. Thanks for letting me rant.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"How do you guys balance speed versus ease and usability?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lrtv8u","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.89,"author_flair_background_color":null,"subreddit_type":"public","ups":14,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_vct0oav1","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":14,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751664501,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;TLDR Personally, I suck at CLI troubleshooting, I realized I will now happily trade away some token speed for a more simple and intuitive UI/UX&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m very new to Linux as well as local LLMs, finally switched over to Linux just last week from Windows 10. I have basically zero CLI experience.&lt;/p&gt;\\n\\n&lt;p&gt;Few days ago, I started having trouble with Ollama. One night, I was getting 4 t/s with unsloth&amp;#39;s Deepseek R1 0528 684b Q4, then the next day 0.15 t/s... Model generation speeds were painfully slow and inconsistent. Many here on the sub suggested that I switch over from ollama to llama.cpp or ik_llama.cpp, so I gave both a try.&lt;/p&gt;\\n\\n&lt;p&gt;The performance difference of llama.cpp / ik_llama.cpp over ollama is absolutely nuts. So running unsloth&amp;#39;s Deepseek R1-0528 684B at Q4 (with Threadripper, 512gb DDR4 RAM, and dual 3090s), I got:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;strong&gt;Ollama&lt;/strong&gt;: 0.15 t/s - absolutely terrible&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;llama.cpp&lt;/strong&gt; (through LM Studio): ~4.7 t/s - massive improvement&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;ik_llama.cpp&lt;/strong&gt;: ~7.6 t/s!! 60% faster than LM Studio, and literally FIFTY times faster than ollama&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Sounds absolutely amazing, BUT there was a huge catch I didn&amp;#39;t know at first.&lt;/p&gt;\\n\\n&lt;p&gt;The learning curve is incredibly steep, especially for a noob like me. I spent WAY more time troubleshooting errors, crashes, scouring online, GH, &lt;a href=\\"https://www.reddit.com/r/llocalllama/\\"&gt;r/llocalllama&lt;/a&gt;, asking other users, and hunting for obscure fixes than time actually using the models. I actually copied someone else&amp;#39;s ik_llama.cpp build set up and server run command to use Deepseek 0528, and it ran smoothly. But the moment I try to run any other model, even 20b, 30b or 70b parametermodel, things quickly went downhill. Memory failures, crashes, cryptic error logs. Many hours spent looking for solutions online, or asking CGPT / Deepseek for insight. Sometimes getting lucky with a solution, and other times just giving up altogether. Also hard to optimize for different models with my hardware, as I have no idea what the dozens of flags, commands, and parameters do even after reading the llama-server --help stuff.&lt;/p&gt;\\n\\n&lt;p&gt;I realized one important thing that&amp;#39;s obvious now but didn&amp;#39;t think of earlier. What works for one user doesn&amp;#39;t always scale to other users (or noobs like me lol). While many suggested ik_llama.cpp, there&amp;#39;s not always blanket solution that fits all. Perhaps not everyone needs to move to the absolute fastest backend. There&amp;#39;s also a ton of great advice out there or troubleshooting tips, but some of it is definitely geared toward power users that understand things like what happens and why it happens when randomparameter=1, when to turn various parameters off, flag this, tensor that, re-build with this flag, CUDA that, offload this here, don&amp;#39;t offload that thing in this specific situation. Reading some of the CLI help I found felt like reading another language, felt super lost.&lt;/p&gt;\\n\\n&lt;p&gt;On the flip side, LM Studio was genuinely plug and play. Felt very intuitive, stable, and it just worked out of the box. I didn&amp;#39;t encounter any crashes, or error logs to navigate. Practically zero command line stuff after install. Downloading, loading, and swapping models is SO easy in LMS. Front end + back end packaged together. Sure, it&amp;#39;s not the fastest, but right now I will take the usability and speed hit over hours of troubleshooting chaos.&lt;/p&gt;\\n\\n&lt;p&gt;For now, I&amp;#39;m probably going to daily drive LM Studio, while slowly working through the steep CLI learning curve on the side. Not an LM Studio ad btw lol. Hopefully one day I can earn my CLI blue belt lol. Thanks for letting me rant.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lrtv8u","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"sourpatchgrownadults","discussion_type":null,"num_comments":6,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lrtv8u/how_do_you_guys_balance_speed_versus_ease_and/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lrtv8u/how_do_you_guys_balance_speed_versus_ease_and/","subreddit_subscribers":494987,"created_utc":1751664501,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1fnm65","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Evening_Ad6637","can_mod_post":false,"created_utc":1751699411,"send_replies":true,"parent_id":"t1_n1dmtd7","score":2,"author_fullname":"t2_p45er6oo","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Similar here. I'm pretty experienced in cli on Linux, mac and bsd, and I still use llamacpp for experimenting with edge cases. \\n\\nBut the vast majority of the time I only work with lm Studio. \\n\\nLm Studio is great because it has very low barriers to entry for beginners, but still offers a lot of internal customization of llamacpp for power users to tinker with.\\n\\nAnd it's all wrapped up in an extremely stable framework that, at least in my experience, has never crashed.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1fnm65","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Similar here. I&amp;#39;m pretty experienced in cli on Linux, mac and bsd, and I still use llamacpp for experimenting with edge cases. &lt;/p&gt;\\n\\n&lt;p&gt;But the vast majority of the time I only work with lm Studio. &lt;/p&gt;\\n\\n&lt;p&gt;Lm Studio is great because it has very low barriers to entry for beginners, but still offers a lot of internal customization of llamacpp for power users to tinker with.&lt;/p&gt;\\n\\n&lt;p&gt;And it&amp;#39;s all wrapped up in an extremely stable framework that, at least in my experience, has never crashed.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrtv8u","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrtv8u/how_do_you_guys_balance_speed_versus_ease_and/n1fnm65/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751699411,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1dmtd7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"NNN_Throwaway2","can_mod_post":false,"created_utc":1751666538,"send_replies":true,"parent_id":"t3_1lrtv8u","score":8,"author_fullname":"t2_8rrihts9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I have plenty of CLI experience but I still just use LM Studio. No reason not to.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1dmtd7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have plenty of CLI experience but I still just use LM Studio. No reason not to.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrtv8u/how_do_you_guys_balance_speed_versus_ease_and/n1dmtd7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751666538,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrtv8u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1eloj7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"sourpatchgrownadults","can_mod_post":false,"created_utc":1751680467,"send_replies":true,"parent_id":"t1_n1dwhk3","score":1,"author_fullname":"t2_vct0oav1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I will definitely look into that, so I do not have to manually unload, and reload. Like I'm shuffling and rewinding old school VHS tapes or sorting through CD players.\\n\\nYou're right, it's the fine tweaking of the arguments that's gonna make me pull my hair until I'm bald lol","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1eloj7","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I will definitely look into that, so I do not have to manually unload, and reload. Like I&amp;#39;m shuffling and rewinding old school VHS tapes or sorting through CD players.&lt;/p&gt;\\n\\n&lt;p&gt;You&amp;#39;re right, it&amp;#39;s the fine tweaking of the arguments that&amp;#39;s gonna make me pull my hair until I&amp;#39;m bald lol&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrtv8u","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrtv8u/how_do_you_guys_balance_speed_versus_ease_and/n1eloj7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751680467,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1dwhk3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"henfiber","can_mod_post":false,"created_utc":1751670158,"send_replies":true,"parent_id":"t3_1lrtv8u","score":4,"author_fullname":"t2_lw9me25","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"My suggestion would be to try [llama-swap](https://github.com/mostlygeek/llama-swap). You will be able to call whatever runtime you want, provided that it has OpenAI compatible endpoints (which ollama, lm-studio, llama.cpp, ik\\\\_llama.cpp, llamafile, vllm do have) using a relatively simple yaml file.\\n\\nYes, you will still need to experiment with the correct arguments, but when have a working version it will live within this yaml configuration, and you will be able to have variations with a different name and different parameters to try out stuff. They also support macros (reusable strings across definitions). Swapping models and configurations is then automatic similar to ollama.\\n\\nYou can use whatever frontend you like with that since most have OpenAI compatibility. For cli access you could use [llm](https://llm.datasette.io/en/stable/).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1dwhk3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;My suggestion would be to try &lt;a href=\\"https://github.com/mostlygeek/llama-swap\\"&gt;llama-swap&lt;/a&gt;. You will be able to call whatever runtime you want, provided that it has OpenAI compatible endpoints (which ollama, lm-studio, llama.cpp, ik_llama.cpp, llamafile, vllm do have) using a relatively simple yaml file.&lt;/p&gt;\\n\\n&lt;p&gt;Yes, you will still need to experiment with the correct arguments, but when have a working version it will live within this yaml configuration, and you will be able to have variations with a different name and different parameters to try out stuff. They also support macros (reusable strings across definitions). Swapping models and configurations is then automatic similar to ollama.&lt;/p&gt;\\n\\n&lt;p&gt;You can use whatever frontend you like with that since most have OpenAI compatibility. For cli access you could use &lt;a href=\\"https://llm.datasette.io/en/stable/\\"&gt;llm&lt;/a&gt;.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrtv8u/how_do_you_guys_balance_speed_versus_ease_and/n1dwhk3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751670158,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrtv8u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1eeqwr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"novel_market_21","can_mod_post":false,"created_utc":1751677447,"send_replies":true,"parent_id":"t3_1lrtv8u","score":1,"author_fullname":"t2_eqtnew30","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I’m trying to build an almost identical rig! You referenced your run command for 0528 684b. Can you share it please??","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1eeqwr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I’m trying to build an almost identical rig! You referenced your run command for 0528 684b. Can you share it please??&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrtv8u/how_do_you_guys_balance_speed_versus_ease_and/n1eeqwr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751677447,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrtv8u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1eqn9i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Marksta","can_mod_post":false,"created_utc":1751682635,"send_replies":true,"parent_id":"t3_1lrtv8u","score":1,"author_fullname":"t2_559a1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The information is sparce, very often you need to open up the github and check out what the code is doing yourself. So the things like build params or CLI arguments, it's all roughly documented or at least the strings are easy to exact match search in search engines and in the repo.\\n\\nIs it worth doing that for everything? Nah, not really. Deepseek is kind of a stand out thing worth getting going.\\n\\nOtherwise, just follow the lead of others when they post info. If it's good and worth doing, someone is probably talking about it. And lots of personal experimentation. Take the time to figure out those llama.cpp arguments and search them all up, you'll know it good enough by heart in a short time. That's the learning part.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1eqn9i","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The information is sparce, very often you need to open up the github and check out what the code is doing yourself. So the things like build params or CLI arguments, it&amp;#39;s all roughly documented or at least the strings are easy to exact match search in search engines and in the repo.&lt;/p&gt;\\n\\n&lt;p&gt;Is it worth doing that for everything? Nah, not really. Deepseek is kind of a stand out thing worth getting going.&lt;/p&gt;\\n\\n&lt;p&gt;Otherwise, just follow the lead of others when they post info. If it&amp;#39;s good and worth doing, someone is probably talking about it. And lots of personal experimentation. Take the time to figure out those llama.cpp arguments and search them all up, you&amp;#39;ll know it good enough by heart in a short time. That&amp;#39;s the learning part.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrtv8u/how_do_you_guys_balance_speed_versus_ease_and/n1eqn9i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751682635,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrtv8u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(t,{data:l});export{r as default};
