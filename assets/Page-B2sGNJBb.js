import{j as e}from"./index-BOnf-UhU.js";import{R as l}from"./RedditPostRenderer-Ce39qICS.js";import"./index-CDase6VD.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I deployed Llama 3.3-70B for my organization quite a long time ago. I am now thinking of updating it to a newer model since there have been quite a few great new LLM releases recently. However, is there any model that actually performs better than Llama 3.3-70B for general purposes (chat, summarization... basically normal daily office tasks) with more or less the same size? Thanks!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Thinking about updating Llama 3.3-70B","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m6ahsu","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.9,"author_flair_background_color":null,"subreddit_type":"public","ups":17,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_mxles3cs","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":17,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753180144,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I deployed Llama 3.3-70B for my organization quite a long time ago. I am now thinking of updating it to a newer model since there have been quite a few great new LLM releases recently. However, is there any model that actually performs better than Llama 3.3-70B for general purposes (chat, summarization... basically normal daily office tasks) with more or less the same size? Thanks!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m6ahsu","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Only_Emergencies","discussion_type":null,"num_comments":38,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/","subreddit_subscribers":503254,"created_utc":1753180144,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4l7t4c","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Rich_Artist_8327","can_mod_post":false,"created_utc":1753215357,"send_replies":true,"parent_id":"t1_n4i4vql","score":3,"author_fullname":"t2_1jk2ep8a52","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"How much ram these need?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4l7t4c","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How much ram these need?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6ahsu","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/n4l7t4c/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753215357,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n4i4vql","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Admirable-Star7088","can_mod_post":false,"created_utc":1753181734,"send_replies":true,"parent_id":"t3_1m6ahsu","score":15,"author_fullname":"t2_qhlcbiy3k","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Llama 3.3 is, if I'm not mistaken, still the most recent dense \\\\~70b model released. MoE has become more popular currently. They are usually much larger than dense models, but also usually runs faster because of less active parameters.\\n\\nIf your organization has enough RAM/VRAM, you could try some of the following recent popular MoE models:\\n\\n* dots.llm1 (142b, 13b active)\\n* Qwen3-235b (235b, 22b active)\\n* ERNIE-4.5 (300b. 47b active)\\n* Kimi-K2 (1000b, 32b active)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4i4vql","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Llama 3.3 is, if I&amp;#39;m not mistaken, still the most recent dense ~70b model released. MoE has become more popular currently. They are usually much larger than dense models, but also usually runs faster because of less active parameters.&lt;/p&gt;\\n\\n&lt;p&gt;If your organization has enough RAM/VRAM, you could try some of the following recent popular MoE models:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;dots.llm1 (142b, 13b active)&lt;/li&gt;\\n&lt;li&gt;Qwen3-235b (235b, 22b active)&lt;/li&gt;\\n&lt;li&gt;ERNIE-4.5 (300b. 47b active)&lt;/li&gt;\\n&lt;li&gt;Kimi-K2 (1000b, 32b active)&lt;/li&gt;\\n&lt;/ul&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/n4i4vql/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753181734,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6ahsu","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":15}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4jic16","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Only_Emergencies","can_mod_post":false,"created_utc":1753198276,"send_replies":true,"parent_id":"t1_n4iuu2y","score":3,"author_fullname":"t2_mxles3cs","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes, I agree. That would be ideal, but that's not so straightforward in our case. We have stored the conversations in Langfuse, but we don't have the ground truth to be able to properly evaluate them, and users usually don't provide feedback on the responses. We are a small team at the moment doing this, so we don't have the capacity to label some cases.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4jic16","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, I agree. That would be ideal, but that&amp;#39;s not so straightforward in our case. We have stored the conversations in Langfuse, but we don&amp;#39;t have the ground truth to be able to properly evaluate them, and users usually don&amp;#39;t provide feedback on the responses. We are a small team at the moment doing this, so we don&amp;#39;t have the capacity to label some cases.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6ahsu","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/n4jic16/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753198276,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n4iuu2y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"tomz17","can_mod_post":false,"created_utc":1753191511,"send_replies":true,"parent_id":"t3_1m6ahsu","score":11,"author_fullname":"t2_1mhx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"IMHO if it's been \\"deployed for a while,\\" you should have accumulated a nice set of benchmark cases you can run against new models.  Just go through your logs and set up a benchmark suite to evaluate model performance, then throw some of the new models at it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4iuu2y","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;IMHO if it&amp;#39;s been &amp;quot;deployed for a while,&amp;quot; you should have accumulated a nice set of benchmark cases you can run against new models.  Just go through your logs and set up a benchmark suite to evaluate model performance, then throw some of the new models at it.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/n4iuu2y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753191511,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6ahsu","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4jogfp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"raika11182","can_mod_post":false,"created_utc":1753199969,"send_replies":true,"parent_id":"t1_n4i1x31","score":5,"author_fullname":"t2_4xupk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm a huge fan of this model and would ditto this recommendation. Just giving an upvote doesn't capture how nice it is.\\n\\nOne tiny problem with it: As a chatbot, it tends to favor responses that are highly formatted, list-like, and use bullets. It's just a stylistic difference, but a noticeable difference from the 70B it was built off of.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4jogfp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m a huge fan of this model and would ditto this recommendation. Just giving an upvote doesn&amp;#39;t capture how nice it is.&lt;/p&gt;\\n\\n&lt;p&gt;One tiny problem with it: As a chatbot, it tends to favor responses that are highly formatted, list-like, and use bullets. It&amp;#39;s just a stylistic difference, but a noticeable difference from the 70B it was built off of.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6ahsu","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/n4jogfp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753199969,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4oni6r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kaisurniwurer","can_mod_post":false,"created_utc":1753264031,"send_replies":true,"parent_id":"t1_n4omfos","score":1,"author_fullname":"t2_qafso","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Sadly, \\"just\\" 2x3090, so only a quant version comes into play, but it's a good idea. I will try unsloth XL quant and see if it's any better.","edited":1753264587,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n4oni6r","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sadly, &amp;quot;just&amp;quot; 2x3090, so only a quant version comes into play, but it&amp;#39;s a good idea. I will try unsloth XL quant and see if it&amp;#39;s any better.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/n4oni6r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753264031,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6ahsu","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":8,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4omfos","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok_Warning2146","can_mod_post":false,"created_utc":1753263438,"send_replies":true,"parent_id":"t1_n4o73rs","score":1,"author_fullname":"t2_s6sfw4yy","approved_by":null,"mod_note":null,"all_awardings":[],"body":"So in your case, it is actually unusable at any context instead of &gt;8k. If you have the resource, can you try the official fp8 version?\\n\\n[https://huggingface.co/nvidia/Llama-3\\\\_3-Nemotron-Super-49B-v1-FP8](https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1-FP8)","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n4omfos","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So in your case, it is actually unusable at any context instead of &amp;gt;8k. If you have the resource, can you try the official fp8 version?&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1-FP8\\"&gt;https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1-FP8&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m6ahsu","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/n4omfos/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753263438,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4o73rs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kaisurniwurer","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4meatn","score":1,"author_fullname":"t2_qafso","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'm using 70B a lot, and when I saw nemotron, I tried it immediately, since I thought, as someone in the chain said, \\"smaller, faster, better\\" right?\\n\\nIn the first few messages it forgot a lot of the previous responses, even when directly prompted for something specific and hallucinated instead, switched to 70B and got the correct answer, tried mistral too and got the correct answer as well.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n4o73rs","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m using 70B a lot, and when I saw nemotron, I tried it immediately, since I thought, as someone in the chain said, &amp;quot;smaller, faster, better&amp;quot; right?&lt;/p&gt;\\n\\n&lt;p&gt;In the first few messages it forgot a lot of the previous responses, even when directly prompted for something specific and hallucinated instead, switched to 70B and got the correct answer, tried mistral too and got the correct answer as well.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m6ahsu","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/n4o73rs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753254629,"author_flair_text":null,"treatment_tags":[],"created_utc":1753254629,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4meatn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok_Warning2146","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4ix8mb","score":0,"author_fullname":"t2_s6sfw4yy","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I think the same is also true for 3.3 70B and it takes way more VRAM.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n4meatn","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think the same is also true for 3.3 70B and it takes way more VRAM.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m6ahsu","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/n4meatn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753228252,"author_flair_text":null,"treatment_tags":[],"created_utc":1753228252,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n4ix8mb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"kaisurniwurer","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4isjdd","score":4,"author_fullname":"t2_qafso","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Sadly it's true, bad memory shows up in less than 8k context from my experience.","edited":1753209039,"author_flair_css_class":null,"name":"t1_n4ix8mb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sadly it&amp;#39;s true, bad memory shows up in less than 8k context from my experience.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m6ahsu","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/n4ix8mb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753192263,"author_flair_text":null,"collapsed":false,"created_utc":1753192263,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n4isjdd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4ieuo5","score":2,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I've heard lower KV cache requirements of Nemotron come together with bad long context performance.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4isjdd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve heard lower KV cache requirements of Nemotron come together with bad long context performance.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6ahsu","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/n4isjdd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753190769,"author_flair_text":null,"treatment_tags":[],"created_utc":1753190769,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4ieuo5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok_Warning2146","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4i5s09","score":2,"author_fullname":"t2_s6sfw4yy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"And also lower KV cache such that you can run in much higher context","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4ieuo5","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;And also lower KV cache such that you can run in much higher context&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6ahsu","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/n4ieuo5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753185956,"author_flair_text":null,"treatment_tags":[],"created_utc":1753185956,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ka32q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DinoAmino","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4i5s09","score":1,"author_fullname":"t2_j1v7f","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"\\"Better\\" is highly subjective. Totally depends on use case.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4ka32q","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&amp;quot;Better&amp;quot; is highly subjective. Totally depends on use case.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6ahsu","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/n4ka32q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753205896,"author_flair_text":null,"treatment_tags":[],"created_utc":1753205896,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4o8r6t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MaxKruse96","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4nclhi","score":1,"author_fullname":"t2_pfi81","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"[https://www.reddit.com/r/LocalLLaMA/comments/1jhpgum/llama\\\\_33\\\\_70b\\\\_vs\\\\_nemotron\\\\_super\\\\_49b\\\\_based\\\\_on/](https://www.reddit.com/r/LocalLLaMA/comments/1jhpgum/llama_33_70b_vs_nemotron_super_49b_based_on/) for what its worth. generally agree with his benchmarks from personal experience","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4o8r6t","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1jhpgum/llama_33_70b_vs_nemotron_super_49b_based_on/\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1jhpgum/llama_33_70b_vs_nemotron_super_49b_based_on/&lt;/a&gt; for what its worth. generally agree with his benchmarks from personal experience&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6ahsu","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/n4o8r6t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753255549,"author_flair_text":null,"treatment_tags":[],"created_utc":1753255549,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4nclhi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rorowhat","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4i5s09","score":1,"author_fullname":"t2_yq51a","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Any benchmarks that compare this to the 70b?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4nclhi","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Any benchmarks that compare this to the 70b?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6ahsu","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/n4nclhi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753240263,"author_flair_text":null,"treatment_tags":[],"created_utc":1753240263,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4i5s09","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MaxKruse96","can_mod_post":false,"created_utc":1753182146,"send_replies":true,"parent_id":"t1_n4i1x31","score":9,"author_fullname":"t2_pfi81","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"this, its an upgrade directly from llama3.3 70b. smaller, faster, better.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4i5s09","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;this, its an upgrade directly from llama3.3 70b. smaller, faster, better.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6ahsu","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/n4i5s09/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753182146,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4jh4gd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Only_Emergencies","can_mod_post":false,"created_utc":1753197941,"send_replies":true,"parent_id":"t1_n4i1x31","score":1,"author_fullname":"t2_mxles3cs","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Great! Thanks, I will take a look","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4jh4gd","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Great! Thanks, I will take a look&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6ahsu","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/n4jh4gd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753197941,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4i1x31","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Ok_Warning2146","can_mod_post":false,"created_utc":1753180290,"send_replies":true,"parent_id":"t3_1m6ahsu","score":17,"author_fullname":"t2_s6sfw4yy","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Nemotron 49B","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4i1x31","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nemotron 49B&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/n4i1x31/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753180290,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6ahsu","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":17}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4kgf6t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"tarruda","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4jtimp","score":1,"author_fullname":"t2_dphk4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"yes, with Mac Studio M1 Ultra + 128GB RAM. IQ4_XS quant + flash attention lower the RAM requirements to fit 32k context in 125GB VRAM, which can fit in my mac after maxing the amount of VRAM that can be allocated.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4kgf6t","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;yes, with Mac Studio M1 Ultra + 128GB RAM. IQ4_XS quant + flash attention lower the RAM requirements to fit 32k context in 125GB VRAM, which can fit in my mac after maxing the amount of VRAM that can be allocated.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6ahsu","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/n4kgf6t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753207621,"author_flair_text":null,"treatment_tags":[],"created_utc":1753207621,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4jtimp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Only_Emergencies","can_mod_post":false,"created_utc":1753201397,"send_replies":true,"parent_id":"t1_n4js8gq","score":2,"author_fullname":"t2_mxles3cs","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Are you using llama.cpp?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4jtimp","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Are you using llama.cpp?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6ahsu","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/n4jtimp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753201397,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4khyvv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"tarruda","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4kh0dg","score":1,"author_fullname":"t2_dphk4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I just used https://huggingface.co/spaces/SadP0i/GGUF-Model-VRAM-Calculator to calculate, and while a 256GB RAM Mac would fit 256k context (Which is the maximum for Qwen3-235b), it would probably be unusable due to how slow it is for processing long contexts","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4khyvv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I just used &lt;a href=\\"https://huggingface.co/spaces/SadP0i/GGUF-Model-VRAM-Calculator\\"&gt;https://huggingface.co/spaces/SadP0i/GGUF-Model-VRAM-Calculator&lt;/a&gt; to calculate, and while a 256GB RAM Mac would fit 256k context (Which is the maximum for Qwen3-235b), it would probably be unusable due to how slow it is for processing long contexts&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6ahsu","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/n4khyvv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753208050,"author_flair_text":null,"treatment_tags":[],"created_utc":1753208050,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4kh0dg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"tarruda","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4k7qdb","score":2,"author_fullname":"t2_dphk4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm using an M1 ultra with 128GB RAM. While more RAM would allow for larger contexts, I don't recommend it since token processing degrades very quickly on apple silicon.\\n\\nFor example, when I start the conversation, llama-server is outputting around 25 tokens/second, but when context reaches ~10k tokens, speed is lowered to about 10 tokens/second.\\n\\nI think 32k context will already be very slow for practical use, so I don't recommend acquiring a Mac with more RAM for this.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4kh0dg","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m using an M1 ultra with 128GB RAM. While more RAM would allow for larger contexts, I don&amp;#39;t recommend it since token processing degrades very quickly on apple silicon.&lt;/p&gt;\\n\\n&lt;p&gt;For example, when I start the conversation, llama-server is outputting around 25 tokens/second, but when context reaches ~10k tokens, speed is lowered to about 10 tokens/second.&lt;/p&gt;\\n\\n&lt;p&gt;I think 32k context will already be very slow for practical use, so I don&amp;#39;t recommend acquiring a Mac with more RAM for this.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6ahsu","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/n4kh0dg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753207785,"author_flair_text":null,"treatment_tags":[],"created_utc":1753207785,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4k7qdb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Forgot_Password_Dude","can_mod_post":false,"created_utc":1753205263,"send_replies":true,"parent_id":"t1_n4js8gq","score":0,"author_fullname":"t2_g8xg6sut","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"32k context is a bit low though, maybe a 256GB Mac would do better?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4k7qdb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;32k context is a bit low though, maybe a 256GB Mac would do better?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6ahsu","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/n4k7qdb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753205263,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n4js8gq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"tarruda","can_mod_post":false,"created_utc":1753201041,"send_replies":true,"parent_id":"t3_1m6ahsu","score":5,"author_fullname":"t2_dphk4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Qwen3-235B-A22B-Instruct-2507 which was released yesterday is looking amazingly strong in my local tests.\\n\\nTo run at Q4 and 32k context, you will need about 125GB VRAM, but it will have a much faster inference than Llama 3.3 70b","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4js8gq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen3-235B-A22B-Instruct-2507 which was released yesterday is looking amazingly strong in my local tests.&lt;/p&gt;\\n\\n&lt;p&gt;To run at Q4 and 32k context, you will need about 125GB VRAM, but it will have a much faster inference than Llama 3.3 70b&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/n4js8gq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753201041,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6ahsu","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ncruh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rorowhat","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4l8p0b","score":1,"author_fullname":"t2_yq51a","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"But does it beat llama 3.3 70B?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4ncruh","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;But does it beat llama 3.3 70B?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6ahsu","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/n4ncruh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753240333,"author_flair_text":null,"treatment_tags":[],"created_utc":1753240333,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4l8p0b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Rich_Artist_8327","can_mod_post":false,"created_utc":1753215604,"send_replies":true,"parent_id":"t1_n4ktdne","score":1,"author_fullname":"t2_1jk2ep8a52","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I agree that gemma3 is really something special. Google has really done it right. But I pray that they will publish in the future little bit larger models also.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4l8p0b","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I agree that gemma3 is really something special. Google has really done it right. But I pray that they will publish in the future little bit larger models also.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6ahsu","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/n4l8p0b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753215604,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4mlmbk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"vegatx40","can_mod_post":false,"created_utc":1753230724,"send_replies":true,"parent_id":"t1_n4ktdne","score":1,"author_fullname":"t2_18dhiarv40","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I agree it is such a sensational model","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4mlmbk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I agree it is such a sensational model&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6ahsu","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/n4mlmbk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753230724,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4ktdne","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"SidneyFong","can_mod_post":false,"created_utc":1753211264,"send_replies":true,"parent_id":"t3_1m6ahsu","score":4,"author_fullname":"t2_929ppz18","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"gemma-3-27b while kinda small, often punches above its weight. If the larger, recent Chinese MOE models don't fit your needs, you can consider gemma-3.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ktdne","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;gemma-3-27b while kinda small, often punches above its weight. If the larger, recent Chinese MOE models don&amp;#39;t fit your needs, you can consider gemma-3.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/n4ktdne/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753211264,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6ahsu","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"50c36eba-fdca-11ee-9735-92a88d7e3b87","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4jw0n7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Only_Emergencies","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4jp5ok","score":3,"author_fullname":"t2_mxles3cs","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The energy consumption of the Macs are really low, they are really efficient on that sense. Theyre also straightforward to set up, so we can start implementing and iterating on projects without dealing with complex infrastructure.\\n\\nBased on the research we did, just one NVIDIA A100 80GB GPU costs around $30000 and also requires other additional hardware (network switches, power, cooling,... ). As the team grows, probably it makes sense to migrate infrastructure to a more powerful one. But at the moment, the Mac Studios provide a cost-effective solution that allows us to build and experiment with LLMs internally.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4jw0n7","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The energy consumption of the Macs are really low, they are really efficient on that sense. Theyre also straightforward to set up, so we can start implementing and iterating on projects without dealing with complex infrastructure.&lt;/p&gt;\\n\\n&lt;p&gt;Based on the research we did, just one NVIDIA A100 80GB GPU costs around $30000 and also requires other additional hardware (network switches, power, cooling,... ). As the team grows, probably it makes sense to migrate infrastructure to a more powerful one. But at the moment, the Mac Studios provide a cost-effective solution that allows us to build and experiment with LLMs internally.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6ahsu","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/n4jw0n7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753202089,"author_flair_text":null,"treatment_tags":[],"created_utc":1753202089,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n4jp5ok","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"libregrape","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4jmbzc","score":2,"author_fullname":"t2_100x0lil34","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Why does your organization spend so much $$$ on Macs? AFAIK if you build an inference PC for the same money with GPUs it will be much much faster.\\n\\nAlso, why not use LMStudio? I heard it uses some kind of Mac performance magic (maybe it was called MLX) that makes it far faster than llama.cpp.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4jp5ok","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why does your organization spend so much $$$ on Macs? AFAIK if you build an inference PC for the same money with GPUs it will be much much faster.&lt;/p&gt;\\n\\n&lt;p&gt;Also, why not use LMStudio? I heard it uses some kind of Mac performance magic (maybe it was called MLX) that makes it far faster than llama.cpp.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6ahsu","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/n4jp5ok/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753200168,"author_flair_text":null,"treatment_tags":[],"created_utc":1753200168,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ncz2n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rorowhat","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4jmbzc","score":0,"author_fullname":"t2_yq51a","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What org buys macs???? Some marketing firm?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4ncz2n","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What org buys macs???? Some marketing firm?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6ahsu","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/n4ncz2n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753240412,"author_flair_text":null,"treatment_tags":[],"created_utc":1753240412,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n4jmbzc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Only_Emergencies","can_mod_post":false,"created_utc":1753199373,"send_replies":true,"parent_id":"t1_n4i3n68","score":6,"author_fullname":"t2_mxles3cs","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes!\\n\\n\\\\- We are around 70 people in my organisation  \\n\\\\- We work with sensitive data that we can't share with AI Cloud providers such as OpenAI, etc.  \\n\\\\- We have 3x Mac Studios (192GB M2 Ultra)  \\n\\\\- We have acquired 4x new Mac Studios (M3Ultra chip with 32-core CPU, 80core GPU, 32-core NeuralEngine - 512GB unified memory). Waiting for them to be delivered.  \\n\\\\- We are using Ollama to deploy the models but this is not the best efficient way but it was like this when I joined. However, with the new Macs I am planning to replace Ollama with llama.cpp and  experiment with distributing larger models across multiple machines.  \\n\\\\- A Debian VM where OpenwebUI instance is deployed.  \\n\\\\- Another Debian VM where Qdrant is deployed as centralized vector database.   \\n\\\\- We have more use cases that the typical chat UI interface. We have some classification use cases and some general pipelines that run daily.\\n\\nI have to say that our LLM implementation has been quite successful. The main challenge is getting meaningful user feedback, though I suspect this is a common issue across organizations.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4jmbzc","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes!&lt;/p&gt;\\n\\n&lt;p&gt;- We are around 70 people in my organisation&lt;br/&gt;\\n- We work with sensitive data that we can&amp;#39;t share with AI Cloud providers such as OpenAI, etc.&lt;br/&gt;\\n- We have 3x Mac Studios (192GB M2 Ultra)&lt;br/&gt;\\n- We have acquired 4x new Mac Studios (M3Ultra chip with 32-core CPU, 80core GPU, 32-core NeuralEngine - 512GB unified memory). Waiting for them to be delivered.&lt;br/&gt;\\n- We are using Ollama to deploy the models but this is not the best efficient way but it was like this when I joined. However, with the new Macs I am planning to replace Ollama with llama.cpp and  experiment with distributing larger models across multiple machines.&lt;br/&gt;\\n- A Debian VM where OpenwebUI instance is deployed.&lt;br/&gt;\\n- Another Debian VM where Qdrant is deployed as centralized vector database.&lt;br/&gt;\\n- We have more use cases that the typical chat UI interface. We have some classification use cases and some general pipelines that run daily.&lt;/p&gt;\\n\\n&lt;p&gt;I have to say that our LLM implementation has been quite successful. The main challenge is getting meaningful user feedback, though I suspect this is a common issue across organizations.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6ahsu","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/n4jmbzc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753199373,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n4i3n68","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"gerhardmpl","can_mod_post":false,"created_utc":1753181153,"send_replies":true,"parent_id":"t3_1m6ahsu","score":5,"author_fullname":"t2_tlzk7zie","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Not an answer to your question, but could you describe your use case, setup and number of users? Looks like you are using that setup for some time and it would be great if you could share your experience running LLMs in a company / organisation.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4i3n68","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Ollama"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not an answer to your question, but could you describe your use case, setup and number of users? Looks like you are using that setup for some time and it would be great if you could share your experience running LLMs in a company / organisation.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/n4i3n68/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753181153,"author_flair_text":"Ollama","treatment_tags":[],"link_id":"t3_1m6ahsu","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4loog2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Significant_Post8359","can_mod_post":false,"created_utc":1753220134,"send_replies":true,"parent_id":"t3_1m6ahsu","score":2,"author_fullname":"t2_1lbnsmc47m","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Do not update in a production environment. You need a test environment to make sure it wont create a big problem.\\n\\nI wanted to try a new model and to do so I had to update Ollama. After the update, llama would go into an infinite hallucination loop. \\n\\nLessons learned. Dont update prod without testing first. Consider other options besides Ollama for production systems.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4loog2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Do not update in a production environment. You need a test environment to make sure it wont create a big problem.&lt;/p&gt;\\n\\n&lt;p&gt;I wanted to try a new model and to do so I had to update Ollama. After the update, llama would go into an infinite hallucination loop. &lt;/p&gt;\\n\\n&lt;p&gt;Lessons learned. Dont update prod without testing first. Consider other options besides Ollama for production systems.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/n4loog2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753220134,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6ahsu","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4izaxv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"vasileer","can_mod_post":false,"created_utc":1753192884,"send_replies":true,"parent_id":"t3_1m6ahsu","score":5,"author_fullname":"t2_730bgdulm","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"mistral-small-3.2 is great, and about \\\\~3x smaller (24B vs 70B)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4izaxv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;mistral-small-3.2 is great, and about ~3x smaller (24B vs 70B)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/n4izaxv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753192884,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6ahsu","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4lhdeg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"My_Unbiased_Opinion","can_mod_post":false,"created_utc":1753217995,"send_replies":true,"parent_id":"t3_1m6ahsu","score":2,"author_fullname":"t2_esiyl0yb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Take a look at Mistral 3.2 24B. I actually find it a nice jack of all trades and it's not highly censored. Also great at vision so you can expand your use case. Usually larger models have better world knowledge, but the Mistral models are surprisingly good at coding AND world knowledge for their size.\\n\\n\\nNemotron 49B is also solid too. I personally would avoid Gemma 3 27B, I find it hallucinates way too much.\\n\\n\\nYes, I do find Mistral 3.2 24B overall better than even Llama 3.3 70B.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4lhdeg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Take a look at Mistral 3.2 24B. I actually find it a nice jack of all trades and it&amp;#39;s not highly censored. Also great at vision so you can expand your use case. Usually larger models have better world knowledge, but the Mistral models are surprisingly good at coding AND world knowledge for their size.&lt;/p&gt;\\n\\n&lt;p&gt;Nemotron 49B is also solid too. I personally would avoid Gemma 3 27B, I find it hallucinates way too much.&lt;/p&gt;\\n\\n&lt;p&gt;Yes, I do find Mistral 3.2 24B overall better than even Llama 3.3 70B.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/n4lhdeg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753217995,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6ahsu","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ixyvb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kaisurniwurer","can_mod_post":false,"created_utc":1753192484,"send_replies":true,"parent_id":"t3_1m6ahsu","score":1,"author_fullname":"t2_qafso","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If I were to try changing 70B Nevoria at IQ_4q_xs to a newer model I would try the new mistral at high quant.\\n\\nDidn't have time to bite in yet, but 3.2 mistral seems cool, and at higher quant you get more precise and factual answers. Also it seems to handle context better than LLama 3.3 70B.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ixyvb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If I were to try changing 70B Nevoria at IQ_4q_xs to a newer model I would try the new mistral at high quant.&lt;/p&gt;\\n\\n&lt;p&gt;Didn&amp;#39;t have time to bite in yet, but 3.2 mistral seems cool, and at higher quant you get more precise and factual answers. Also it seems to handle context better than LLama 3.3 70B.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/n4ixyvb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753192484,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6ahsu","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4jo0ds","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"tomkowyreddit","can_mod_post":false,"created_utc":1753199843,"send_replies":true,"parent_id":"t3_1m6ahsu","score":1,"author_fullname":"t2_47cffao7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'd say wait a few months. With switching the model you can piss off users with new style od answers and performance upgrade will not be significant.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4jo0ds","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;d say wait a few months. With switching the model you can piss off users with new style od answers and performance upgrade will not be significant.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/n4jo0ds/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753199843,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6ahsu","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),o=()=>e.jsx(l,{data:a});export{o as default};
