import{j as e}from"./index-xfnGEtuL.js";import{R as l}from"./RedditPostRenderer-DAZRIZZK.js";import"./index-BaNn5-RR.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hey everyone,\\n\\nI’m building a MOE optimized llm inference rig.\\n\\nMy plans currently are \\nGPU: 2x 5090’s (FE’s I got msrp from Best Buy)\\nCPU: threadripper 7000 pro series\\nMotherboard: trx50 or wrx 90\\nMemory: 512gb ddr5\\nCase: ideally rack mountable, not sure\\n\\nMy performance target is a min of 20 t/s generation with DEEPSEEK R1 5028 @q4 with full 128k context\\n\\nAny suggestions or thoughts? ","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Building MOE inference Optimized workstation with 2 5090’s","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lsipdy","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.17,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_eqtnew30","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":true,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751745746,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\\n\\n&lt;p&gt;I’m building a MOE optimized llm inference rig.&lt;/p&gt;\\n\\n&lt;p&gt;My plans currently are \\nGPU: 2x 5090’s (FE’s I got msrp from Best Buy)\\nCPU: threadripper 7000 pro series\\nMotherboard: trx50 or wrx 90\\nMemory: 512gb ddr5\\nCase: ideally rack mountable, not sure&lt;/p&gt;\\n\\n&lt;p&gt;My performance target is a min of 20 t/s generation with DEEPSEEK R1 5028 @q4 with full 128k context&lt;/p&gt;\\n\\n&lt;p&gt;Any suggestions or thoughts? &lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lsipdy","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"novel_market_21","discussion_type":null,"num_comments":9,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lsipdy/building_moe_inference_optimized_workstation_with/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lsipdy/building_moe_inference_optimized_workstation_with/","subreddit_subscribers":494986,"created_utc":1751745746,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1j0g8v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FullOf_Bad_Ideas","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1iyg8g","score":5,"author_fullname":"t2_9s7pmakgx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That's not how experts work, you don't know which expert to choose before it's chosen, so you won't be able to keep them loaded in fast VRAM.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1j0g8v","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s not how experts work, you don&amp;#39;t know which expert to choose before it&amp;#39;s chosen, so you won&amp;#39;t be able to keep them loaded in fast VRAM.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsipdy","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsipdy/building_moe_inference_optimized_workstation_with/n1j0g8v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751747798,"author_flair_text":null,"treatment_tags":[],"created_utc":1751747798,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1izftj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Threatening-Silence-","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1iyg8g","score":3,"author_fullname":"t2_15wqsifdjf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"If you're planning on using llama cpp like me, all I can say is that there's almost no traffic across my pcie bus at inference time. For prompt processing yes, it maxes out the bus. For inference, it's almost nothing, like a few hundred MiB at most (pipeline parallel with partial offload to RAM).\\n\\nYour bottleneck will almost certainly be your system ram. Mine benches at 94GB/s. You'll have more with your build but I wouldn't expect miracles tbh.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1izftj","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you&amp;#39;re planning on using llama cpp like me, all I can say is that there&amp;#39;s almost no traffic across my pcie bus at inference time. For prompt processing yes, it maxes out the bus. For inference, it&amp;#39;s almost nothing, like a few hundred MiB at most (pipeline parallel with partial offload to RAM).&lt;/p&gt;\\n\\n&lt;p&gt;Your bottleneck will almost certainly be your system ram. Mine benches at 94GB/s. You&amp;#39;ll have more with your build but I wouldn&amp;#39;t expect miracles tbh.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsipdy","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsipdy/building_moe_inference_optimized_workstation_with/n1izftj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751747469,"author_flair_text":null,"treatment_tags":[],"created_utc":1751747469,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n1iyg8g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"novel_market_21","can_mod_post":false,"created_utc":1751747145,"send_replies":true,"parent_id":"t1_n1iwuel","score":1,"author_fullname":"t2_eqtnew30","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks that’s super helpful. Would pcie 5 + actual speed of 5090 not constitute near doubling it? My thought was once the expert get loaded it’s akin to a 42b dense model being inferred on?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1iyg8g","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks that’s super helpful. Would pcie 5 + actual speed of 5090 not constitute near doubling it? My thought was once the expert get loaded it’s akin to a 42b dense model being inferred on?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"link_id":"t3_1lsipdy","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsipdy/building_moe_inference_optimized_workstation_with/n1iyg8g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751747145,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1iwuel","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Threatening-Silence-","can_mod_post":false,"created_utc":1751746625,"send_replies":true,"parent_id":"t3_1lsipdy","score":2,"author_fullname":"t2_15wqsifdjf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I don't think you're gonna hit 20 tps. \\n\\nI have 9x 3090s and I get 8.5 tps with Q3_K_XL quant at 85k context. \\n\\nYou are probably looking at something more akin to my speeds.\\n\\nHere are my specs:\\n\\nhttps://www.reddit.com/r/LocalLLaMA/s/vnExqq1ppe","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1iwuel","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t think you&amp;#39;re gonna hit 20 tps. &lt;/p&gt;\\n\\n&lt;p&gt;I have 9x 3090s and I get 8.5 tps with Q3_K_XL quant at 85k context. &lt;/p&gt;\\n\\n&lt;p&gt;You are probably looking at something more akin to my speeds.&lt;/p&gt;\\n\\n&lt;p&gt;Here are my specs:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/s/vnExqq1ppe\\"&gt;https://www.reddit.com/r/LocalLLaMA/s/vnExqq1ppe&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsipdy/building_moe_inference_optimized_workstation_with/n1iwuel/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751746625,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lsipdy","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1iwe5f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"novel_market_21","can_mod_post":false,"created_utc":1751746476,"send_replies":true,"parent_id":"t1_n1ivoqt","score":1,"author_fullname":"t2_eqtnew30","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yup. My goal is to spend under 5k on the non GPU parts I’m trying to offload to the GPU","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1iwe5f","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yup. My goal is to spend under 5k on the non GPU parts I’m trying to offload to the GPU&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"link_id":"t3_1lsipdy","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsipdy/building_moe_inference_optimized_workstation_with/n1iwe5f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751746476,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1ivoqt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"nonerequired_","can_mod_post":false,"created_utc":1751746244,"send_replies":true,"parent_id":"t3_1lsipdy","score":1,"author_fullname":"t2_dn63q2ky","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Target is quite high I think","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ivoqt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Target is quite high I think&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsipdy/building_moe_inference_optimized_workstation_with/n1ivoqt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751746244,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lsipdy","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1jbgue","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"un_passant","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1j83tl","score":1,"author_fullname":"t2_7rqtc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Because high end gamer GPU were too competitive with the pricey datacenter GPUs, NVidia crippled their ability to be used for inference in a multi-GPU setup by disabling P2P communication at the driver level for the 4090. A hacked driver by geohot enables the P2P for 4090, but I'm not sure such a driver exist / is possible for the 5090, which would reduce their performance for fine tuning.\\n\\nA shame really.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1jbgue","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Because high end gamer GPU were too competitive with the pricey datacenter GPUs, NVidia crippled their ability to be used for inference in a multi-GPU setup by disabling P2P communication at the driver level for the 4090. A hacked driver by geohot enables the P2P for 4090, but I&amp;#39;m not sure such a driver exist / is possible for the 5090, which would reduce their performance for fine tuning.&lt;/p&gt;\\n\\n&lt;p&gt;A shame really.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsipdy","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsipdy/building_moe_inference_optimized_workstation_with/n1jbgue/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751751475,"author_flair_text":null,"treatment_tags":[],"created_utc":1751751475,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1j83tl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"novel_market_21","can_mod_post":false,"created_utc":1751750318,"send_replies":true,"parent_id":"t1_n1j7ej2","score":1,"author_fullname":"t2_eqtnew30","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks for the input! Can you clarify a bit more the context for me to look into?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1j83tl","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for the input! Can you clarify a bit more the context for me to look into?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"link_id":"t3_1lsipdy","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsipdy/building_moe_inference_optimized_workstation_with/n1j83tl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751750318,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1j7ej2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"un_passant","can_mod_post":false,"created_utc":1751750080,"send_replies":true,"parent_id":"t3_1lsipdy","score":1,"author_fullname":"t2_7rqtc","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'm just worried about the P2P situation for 5090, but it should matter much for inference.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1j7ej2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m just worried about the P2P situation for 5090, but it should matter much for inference.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsipdy/building_moe_inference_optimized_workstation_with/n1j7ej2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751750080,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lsipdy","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),o=()=>e.jsx(l,{data:t});export{o as default};
