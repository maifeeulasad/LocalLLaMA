import{j as e}from"./index-DACS7Nh6.js";import{R as t}from"./RedditPostRenderer-Dqa1NZuX.js";import"./index-DiMIVQx4.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"What kind of performance can I expect when using 4× RTX 5090s with vLLM in high-batch scenarios, serving many concurrent users?\\n\\nI’ve tried looking for benchmarks, but most of them use \`batch_size = 1\`, which doesn’t reflect my use case.  \\nI read that throughput can scale up to 20× when using batching (&gt;128) - assuming there are no VRAM limitations - but I’m not sure how reliable that estimate is.\\n\\nAnyone have real-world numbers or experience to share?\\n\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"RTX 5090 performance with vLLM and batching?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m0pn5c","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.73,"author_flair_background_color":null,"subreddit_type":"public","ups":5,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_pv1nb9469","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":5,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752604111,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;What kind of performance can I expect when using 4× RTX 5090s with vLLM in high-batch scenarios, serving many concurrent users?&lt;/p&gt;\\n\\n&lt;p&gt;I’ve tried looking for benchmarks, but most of them use &lt;code&gt;batch_size = 1&lt;/code&gt;, which doesn’t reflect my use case.&lt;br/&gt;\\nI read that throughput can scale up to 20× when using batching (&amp;gt;128) - assuming there are no VRAM limitations - but I’m not sure how reliable that estimate is.&lt;/p&gt;\\n\\n&lt;p&gt;Anyone have real-world numbers or experience to share?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m0pn5c","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"GabryIta","discussion_type":null,"num_comments":12,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m0pn5c/rtx_5090_performance_with_vllm_and_batching/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m0pn5c/rtx_5090_performance_with_vllm_and_batching/","subreddit_subscribers":499773,"created_utc":1752604111,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ceuhq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Afternoon_4260","can_mod_post":false,"created_utc":1752618085,"send_replies":true,"parent_id":"t1_n3cbtav","score":1,"author_fullname":"t2_cj9kap4bx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Ok thanks a lot for your time","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ceuhq","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ok thanks a lot for your time&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0pn5c/rtx_5090_performance_with_vllm_and_batching/n3ceuhq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752618085,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m0pn5c","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":9,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3cbtav","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Capable-Ad-7494","can_mod_post":false,"created_utc":1752617054,"send_replies":true,"parent_id":"t1_n3c6srv","score":1,"author_fullname":"t2_9so78ol2","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Nope, each user is limited in how much context they use when you define max sequence length.\\n\\nAnd yes, afaik. except i’m fairly sure if it gets swapped by vllm natively it’s getting recomputed regardless","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3cbtav","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nope, each user is limited in how much context they use when you define max sequence length.&lt;/p&gt;\\n\\n&lt;p&gt;And yes, afaik. except i’m fairly sure if it gets swapped by vllm natively it’s getting recomputed regardless&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0pn5c/rtx_5090_performance_with_vllm_and_batching/n3cbtav/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752617054,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0pn5c","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":8,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3c6srv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Afternoon_4260","can_mod_post":false,"created_utc":1752615516,"send_replies":true,"parent_id":"t1_n3c4cjh","score":1,"author_fullname":"t2_cj9kap4bx","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ok thanks for that comprehensive answer. Last one, the users can have different size ctx?\\n\\n&gt;That entire conversation that was dropped will have to be recomputed when there's more available kv cache.\\n\\nIf you don't cache swap it or cache it to cpu with lm cache right?","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n3c6srv","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ok thanks for that comprehensive answer. Last one, the users can have different size ctx?&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;That entire conversation that was dropped will have to be recomputed when there&amp;#39;s more available kv cache.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;If you don&amp;#39;t cache swap it or cache it to cpu with lm cache right?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m0pn5c","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0pn5c/rtx_5090_performance_with_vllm_and_batching/n3c6srv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752615516,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3c4cjh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Capable-Ad-7494","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3c1ysy","score":1,"author_fullname":"t2_9so78ol2","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"So, active users in vllm are dynamic. You can have 5 active users, 1 active user, or 300 active users. The engine will let them use as much as you provide, for the most part.\\n\\nWhen I say limit how much context each one gets, I mean limiting the maximum sequence length the llm allows each request to use, to limit overuse by one user. So all users can use 128k context, or only 64k to double maximum concurrent users at once, but those people trying to go over that may run into errors when sending requests that go over the context limit.\\n\\nAnd yeah, when vllm runs out of kv cache because user context &gt; available context, it will put that canceled request in a queue and dump all of the kv cache needed for that request to allow existing requests to finish. That entire conversation that was dropped will have to be recomputed when there's more available kv cache. This is afaik first come first serve, iirc. don't quote me on that.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n3c4cjh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So, active users in vllm are dynamic. You can have 5 active users, 1 active user, or 300 active users. The engine will let them use as much as you provide, for the most part.&lt;/p&gt;\\n\\n&lt;p&gt;When I say limit how much context each one gets, I mean limiting the maximum sequence length the llm allows each request to use, to limit overuse by one user. So all users can use 128k context, or only 64k to double maximum concurrent users at once, but those people trying to go over that may run into errors when sending requests that go over the context limit.&lt;/p&gt;\\n\\n&lt;p&gt;And yeah, when vllm runs out of kv cache because user context &amp;gt; available context, it will put that canceled request in a queue and dump all of the kv cache needed for that request to allow existing requests to finish. That entire conversation that was dropped will have to be recomputed when there&amp;#39;s more available kv cache. This is afaik first come first serve, iirc. don&amp;#39;t quote me on that.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m0pn5c","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0pn5c/rtx_5090_performance_with_vllm_and_batching/n3c4cjh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752614787,"author_flair_text":null,"treatment_tags":[],"created_utc":1752614787,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3c1ysy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Afternoon_4260","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3bxtso","score":1,"author_fullname":"t2_cj9kap4bx","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ho yeah sorry I was really off the mark thinking fp16.\\n\\nSo you modified your answer and thanks it clarified some point\\n\\nSo each user can have a different amount of kv cache and you can change the number of active user in runtime? I thought it was more strict.\\n\\nAlso does it automatically put your request in a queue when oom? You were speaking about swapping kv cache and lm cache that \\"swaps\\" it to cpu (system ram), I guess that was for inactive users?","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3c1ysy","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ho yeah sorry I was really off the mark thinking fp16.&lt;/p&gt;\\n\\n&lt;p&gt;So you modified your answer and thanks it clarified some point&lt;/p&gt;\\n\\n&lt;p&gt;So each user can have a different amount of kv cache and you can change the number of active user in runtime? I thought it was more strict.&lt;/p&gt;\\n\\n&lt;p&gt;Also does it automatically put your request in a queue when oom? You were speaking about swapping kv cache and lm cache that &amp;quot;swaps&amp;quot; it to cpu (system ram), I guess that was for inactive users?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m0pn5c","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0pn5c/rtx_5090_performance_with_vllm_and_batching/n3c1ysy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752614083,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752614083,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3bxtso","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Capable-Ad-7494","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3bwipp","score":1,"author_fullname":"t2_9so78ol2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"VLLM doesn't share context between users. It just has a fixed amount of memory to distribute among all active users, so you need to balance how many users you serve versus how much context each one gets.\\n\\nand AWQ is a 4 bit format,","edited":false,"author_flair_css_class":null,"name":"t1_n3bxtso","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;VLLM doesn&amp;#39;t share context between users. It just has a fixed amount of memory to distribute among all active users, so you need to balance how many users you serve versus how much context each one gets.&lt;/p&gt;\\n\\n&lt;p&gt;and AWQ is a 4 bit format,&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m0pn5c","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0pn5c/rtx_5090_performance_with_vllm_and_batching/n3bxtso/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752612904,"author_flair_text":null,"collapsed":false,"created_utc":1752612904,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3c13f0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Afternoon_4260","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3bzqht","score":1,"author_fullname":"t2_cj9kap4bx","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Nah I'm not but he has a relevant question of mine lol","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3c13f0","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nah I&amp;#39;m not but he has a relevant question of mine lol&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m0pn5c","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0pn5c/rtx_5090_performance_with_vllm_and_batching/n3c13f0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752613830,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752613830,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3bzqht","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Capable-Ad-7494","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3bwipp","score":1,"author_fullname":"t2_9so78ol2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"and i also thought u were op :P","edited":false,"author_flair_css_class":null,"name":"t1_n3bzqht","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;and i also thought u were op :P&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m0pn5c","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0pn5c/rtx_5090_performance_with_vllm_and_batching/n3bzqht/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752613439,"author_flair_text":null,"collapsed":false,"created_utc":1752613439,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3bwipp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Afternoon_4260","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3bvbg4","score":0,"author_fullname":"t2_cj9kap4bx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'm sorry that really tells me nothing except that I can saturate a 5090 with a (supposedly fp16?) 14b and 50k ctx\\n\\nSo vllm has some kind of shared context? I thought it was more strict","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3bwipp","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m sorry that really tells me nothing except that I can saturate a 5090 with a (supposedly fp16?) 14b and 50k ctx&lt;/p&gt;\\n\\n&lt;p&gt;So vllm has some kind of shared context? I thought it was more strict&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0pn5c","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0pn5c/rtx_5090_performance_with_vllm_and_batching/n3bwipp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752612542,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752612542,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n3bvbg4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Capable-Ad-7494","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3bsab3","score":2,"author_fullname":"t2_9so78ol2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"https://preview.redd.it/upzp3xecn3df1.png?width=1466&amp;format=png&amp;auto=webp&amp;s=c0bc11d113f046b286a948f51dc1d16776e6ee53\\n\\ntaken from the vllm discord single 5090, max requests 45, qwen 14b awq, 1000 prompt tokens, 100 output tokens. your biggest worry with 4 5090's, and multiple users, is how much context your willing to give maximally per user, in a fully saturated environment, so each user is using about as much as they can take, or alternatively, how long your willing for each user to wait since vllm will swap kv-cache if it reaches the limitI would recommend checking out lm cache, since its a decent alternative to offloading kv cache to cpu and attempting to mitigate it.\\n\\nMy experience, you shouldn't really have too many issues with vllm and 4 5090's, since you have tensorparallel, and i wouldnt recommend gguf, Use autoround and quant everything you want to use with auto-round-best after using and evaluating a random 4bit  gptq-awq variant you find online.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3bvbg4","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://preview.redd.it/upzp3xecn3df1.png?width=1466&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c0bc11d113f046b286a948f51dc1d16776e6ee53\\"&gt;https://preview.redd.it/upzp3xecn3df1.png?width=1466&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c0bc11d113f046b286a948f51dc1d16776e6ee53&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;taken from the vllm discord single 5090, max requests 45, qwen 14b awq, 1000 prompt tokens, 100 output tokens. your biggest worry with 4 5090&amp;#39;s, and multiple users, is how much context your willing to give maximally per user, in a fully saturated environment, so each user is using about as much as they can take, or alternatively, how long your willing for each user to wait since vllm will swap kv-cache if it reaches the limitI would recommend checking out lm cache, since its a decent alternative to offloading kv cache to cpu and attempting to mitigate it.&lt;/p&gt;\\n\\n&lt;p&gt;My experience, you shouldn&amp;#39;t really have too many issues with vllm and 4 5090&amp;#39;s, since you have tensorparallel, and i wouldnt recommend gguf, Use autoround and quant everything you want to use with auto-round-best after using and evaluating a random 4bit  gptq-awq variant you find online.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0pn5c","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0pn5c/rtx_5090_performance_with_vllm_and_batching/n3bvbg4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752612209,"media_metadata":{"upzp3xecn3df1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":23,"x":108,"u":"https://preview.redd.it/upzp3xecn3df1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b1f83d3f187564859892db4d09cd8b49180284c1"},{"y":47,"x":216,"u":"https://preview.redd.it/upzp3xecn3df1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=cdb9d76dc54d2a2e10ce64d75e5dd55393c9e5ec"},{"y":70,"x":320,"u":"https://preview.redd.it/upzp3xecn3df1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e8f6846980921e22b7afd478bad0ba9a57ef8390"},{"y":141,"x":640,"u":"https://preview.redd.it/upzp3xecn3df1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9cbd2922bf0e0c3224f5b725b26785fbd9237cc5"},{"y":212,"x":960,"u":"https://preview.redd.it/upzp3xecn3df1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7941f9964da2fe741fc1b0e79d76c30868c1b84c"},{"y":239,"x":1080,"u":"https://preview.redd.it/upzp3xecn3df1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=136f746e6b4a306978446896c7ce3cc4ed1490b4"}],"s":{"y":325,"x":1466,"u":"https://preview.redd.it/upzp3xecn3df1.png?width=1466&amp;format=png&amp;auto=webp&amp;s=c0bc11d113f046b286a948f51dc1d16776e6ee53"},"id":"upzp3xecn3df1"}},"author_flair_text":null,"treatment_tags":[],"created_utc":1752612209,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3bsab3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Afternoon_4260","can_mod_post":false,"created_utc":1752611368,"send_replies":true,"parent_id":"t1_n3bb8g3","score":0,"author_fullname":"t2_cj9kap4bx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Mistral small 64k ctx a q8 would be great, else q5? 🤷  \\nElse a L3.1 70b I guess","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3bsab3","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Mistral small 64k ctx a q8 would be great, else q5? 🤷&lt;br/&gt;\\nElse a L3.1 70b I guess&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0pn5c","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0pn5c/rtx_5090_performance_with_vllm_and_batching/n3bsab3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752611368,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n3bb8g3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"nivvis","can_mod_post":false,"created_utc":1752606553,"send_replies":true,"parent_id":"t3_1m0pn5c","score":1,"author_fullname":"t2_39blx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What model family are you looking to run? What context size? (significant limiting factor)\\n\\nI don’t have any hard numbers for you, but iirc for most models my 5090 generally sits fairly idle during inference (eg 15-20% usage) so I’m sure there’s lots of room for batching. This is with llamacpp though so ymmv. (have neglected my PyTorch tooling until I have time to wrangle driver incompatibilities :’( .. )","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3bb8g3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What model family are you looking to run? What context size? (significant limiting factor)&lt;/p&gt;\\n\\n&lt;p&gt;I don’t have any hard numbers for you, but iirc for most models my 5090 generally sits fairly idle during inference (eg 15-20% usage) so I’m sure there’s lots of room for batching. This is with llamacpp though so ymmv. (have neglected my PyTorch tooling until I have time to wrangle driver incompatibilities :’( .. )&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0pn5c/rtx_5090_performance_with_vllm_and_batching/n3bb8g3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752606553,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0pn5c","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),o=()=>e.jsx(t,{data:a});export{o as default};
