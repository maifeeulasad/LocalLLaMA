import{j as e}from"./index-CeRg6Q3f.js";import{R as l}from"./RedditPostRenderer-D7n1g-D8.js";import"./index-DPToWe3n.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Mine are:\\n\\n1. All the popular public benchmarks are nearly worthless when it comes to a model's general ability. Literaly the only good thing we get out of them is a rating for \\"can the model regurgitate the answers to questions the devs made sure it was trained on repeatedly to get higher benchmarks, without fucking it up\\", which does have some value. I think the people who maintain the benchmarks know this too, but we're all supposed to pretend like your MMLU score is indicative of the ability to help the user solve questions outside of those in your training data? Please. No one but hobbyists has enough integrity to keep their benchmark questions private? Bleak.\\n\\n2. Any ranker who has an LLM judge giving a rating to the \\"writing style\\" of another LLM is a hack who has no business ranking models. Please don't waste your time or ours. You clearly don't understand what an LLM is. Stop wasting carbon with your pointless inference.\\n\\n3. Every community finetune I've used is always far worse than the base model. They always reduce the coherency, it's just a matter of how much. That's because 99.9% of finetuners are clueless people just running training scripts on the latest random dataset they found, or doing random merges (of equally awful finetunes). They don't even try their own models, they just shit them out into the world and subject us to them. idk why they do it, is it narcissism, or resume-padding, or what? I wish HF would start charging money for storage just to discourage these people. YOU DON'T HAVE TO UPLOAD EVERY MODEL YOU MAKE. The planet is literally worse off due to the energy consumed creating, storing and distributing your electronic waste.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Your unpopular takes on LLMs","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m0z1zx","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.91,"author_flair_background_color":null,"subreddit_type":"public","ups":367,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_93yn32gx","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":367,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752627161,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Mine are:&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;&lt;p&gt;All the popular public benchmarks are nearly worthless when it comes to a model&amp;#39;s general ability. Literaly the only good thing we get out of them is a rating for &amp;quot;can the model regurgitate the answers to questions the devs made sure it was trained on repeatedly to get higher benchmarks, without fucking it up&amp;quot;, which does have some value. I think the people who maintain the benchmarks know this too, but we&amp;#39;re all supposed to pretend like your MMLU score is indicative of the ability to help the user solve questions outside of those in your training data? Please. No one but hobbyists has enough integrity to keep their benchmark questions private? Bleak.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Any ranker who has an LLM judge giving a rating to the &amp;quot;writing style&amp;quot; of another LLM is a hack who has no business ranking models. Please don&amp;#39;t waste your time or ours. You clearly don&amp;#39;t understand what an LLM is. Stop wasting carbon with your pointless inference.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Every community finetune I&amp;#39;ve used is always far worse than the base model. They always reduce the coherency, it&amp;#39;s just a matter of how much. That&amp;#39;s because 99.9% of finetuners are clueless people just running training scripts on the latest random dataset they found, or doing random merges (of equally awful finetunes). They don&amp;#39;t even try their own models, they just shit them out into the world and subject us to them. idk why they do it, is it narcissism, or resume-padding, or what? I wish HF would start charging money for storage just to discourage these people. YOU DON&amp;#39;T HAVE TO UPLOAD EVERY MODEL YOU MAKE. The planet is literally worse off due to the energy consumed creating, storing and distributing your electronic waste.&lt;/p&gt;&lt;/li&gt;\\n&lt;/ol&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1m0z1zx","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"dtdisapointingresult","discussion_type":null,"num_comments":250,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/","subreddit_subscribers":499774,"created_utc":1752627161,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ffxt9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Kqyxzoj","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3fcv9q","score":2,"author_fullname":"t2_479l1o4zc","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The Binbows Petting Zoo is awesome. Highly recommended!","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n3ffxt9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The Binbows Petting Zoo is awesome. Highly recommended!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m0z1zx","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3ffxt9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752665587,"author_flair_text":null,"treatment_tags":[],"created_utc":1752665587,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3fcv9q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Martian_Metaphor","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3f6805","score":2,"author_fullname":"t2_1t3ryts5ob","approved_by":null,"mod_note":null,"all_awardings":[],"body":"It reminds me that I still need to do my pilgrimage and visit that Mecca","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3fcv9q","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It reminds me that I still need to do my pilgrimage and visit that Mecca&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m0z1zx","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3fcv9q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752664234,"author_flair_text":null,"treatment_tags":[],"created_utc":1752664234,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3f6805","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Kqyxzoj","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3er433","score":5,"author_fullname":"t2_479l1o4zc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thank you for your username kind person. That gave me a good chuckle remembering that one. :)","edited":false,"author_flair_css_class":null,"name":"t1_n3f6805","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you for your username kind person. That gave me a good chuckle remembering that one. :)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m0z1zx","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3f6805/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752660963,"author_flair_text":null,"collapsed":false,"created_utc":1752660963,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n3er433","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"michaelsoft__binbows","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3dp4az","score":25,"author_fullname":"t2_iifi6ul2l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It stands very much to reason that if you have a sex toy that is driven by advanced technology to this degree, it is going to be the best, most practical and functional forcing function for advancing said technology.\\n\\nLuckily this is the case and we benefit from that.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3er433","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It stands very much to reason that if you have a sex toy that is driven by advanced technology to this degree, it is going to be the best, most practical and functional forcing function for advancing said technology.&lt;/p&gt;\\n\\n&lt;p&gt;Luckily this is the case and we benefit from that.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3er433/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752652285,"author_flair_text":null,"treatment_tags":[],"created_utc":1752652285,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":25}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3egz92","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"CV514","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3dp4az","score":11,"author_fullname":"t2_n4zvv","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I mean, every front end can be a simple sex chat window.\\n\\nST is glorious at that, or literally anything that may require instruction for roleplaying impersonation. Or not, I'm using it as my main general assistant too, scripting to alter it's behaviour and abilities is too powerful.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3egz92","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I mean, every front end can be a simple sex chat window.&lt;/p&gt;\\n\\n&lt;p&gt;ST is glorious at that, or literally anything that may require instruction for roleplaying impersonation. Or not, I&amp;#39;m using it as my main general assistant too, scripting to alter it&amp;#39;s behaviour and abilities is too powerful.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3egz92/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752646707,"author_flair_text":null,"treatment_tags":[],"created_utc":1752646707,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"2b12e2b8-fdc0-11ee-9a03-6e2f48afd456","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3f00pw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"KageYume","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3ehq3b","score":4,"author_fullname":"t2_n9tot","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The same as Pony.","edited":false,"author_flair_css_class":null,"name":"t1_n3f00pw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The same as Pony.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m0z1zx","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3f00pw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752657492,"author_flair_text":null,"collapsed":false,"created_utc":1752657492,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ehq3b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Olangotang","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3dp4az","score":5,"author_fullname":"t2_6jlis","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Chroma is the best open source image model and it is a furry finetune of Flux Schnell.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ehq3b","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 3"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Chroma is the best open source image model and it is a furry finetune of Flux Schnell.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3ehq3b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752647106,"author_flair_text":"Llama 3","treatment_tags":[],"created_utc":1752647106,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":3,"author_flair_background_color":"#c7b594","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"more","data":{"count":9,"name":"t1_n3fgpfl","id":"n3fgpfl","parent_id":"t1_n3dp4az","depth":3,"children":["n3fgpfl","n3f3g0v","n3ekf9f"]}}],"before":null}},"user_reports":[],"saved":false,"id":"n3dp4az","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"xoexohexox","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3dnerg","score":160,"author_fullname":"t2_323db","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"SillyTavern is the most advanced, extensible, and powerful LLM front end in existence and it's basically a sex toy.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3dp4az","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;SillyTavern is the most advanced, extensible, and powerful LLM front end in existence and it&amp;#39;s basically a sex toy.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3dp4az/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752634018,"author_flair_text":null,"treatment_tags":[],"created_utc":1752634018,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":160}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3e3x7e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"no_witty_username","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3e2yai","score":4,"author_fullname":"t2_4j2nc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Good tip, ill have to check it out","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3e3x7e","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Good tip, ill have to check it out&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3e3x7e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752639957,"author_flair_text":null,"treatment_tags":[],"created_utc":1752639957,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"more","data":{"count":2,"name":"t1_n3fm2f0","id":"n3fm2f0","parent_id":"t1_n3e2yai","depth":3,"children":["n3fm2f0","n3frf88"]}}],"before":null}},"user_reports":[],"saved":false,"id":"n3e2yai","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"xoexohexox","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3dnerg","score":34,"author_fullname":"t2_323db","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"In case anyone was wondering, models based on Mistral Small 24B work amazing and actually the base model itself is awesome and they even have a multimodal one that accepts text or up to 40 minutes at a time of voice input. My favorite Mistral Small fine-tune right now is Dan's Personality Engine 24B 1.3.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3e2yai","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;In case anyone was wondering, models based on Mistral Small 24B work amazing and actually the base model itself is awesome and they even have a multimodal one that accepts text or up to 40 minutes at a time of voice input. My favorite Mistral Small fine-tune right now is Dan&amp;#39;s Personality Engine 24B 1.3.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3e2yai/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752639531,"author_flair_text":null,"treatment_tags":[],"created_utc":1752639531,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":34}}],"before":null}},"user_reports":[],"saved":false,"id":"n3dnerg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"no_witty_username","can_mod_post":false,"created_utc":1752633383,"send_replies":true,"parent_id":"t1_n3ddkow","score":165,"author_fullname":"t2_4j2nc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Legit take. People who have worked within generative AI models, image, text, whatever know that all the real good info comes from these communities.  You have some real autistic people in here that have tested the fuck out of their models and their input is quite valuable if you can spot the real methodical tester.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dnerg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Legit take. People who have worked within generative AI models, image, text, whatever know that all the real good info comes from these communities.  You have some real autistic people in here that have tested the fuck out of their models and their input is quite valuable if you can spot the real methodical tester.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3dnerg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752633383,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":165}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3dinnf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ReXommendation","can_mod_post":false,"created_utc":1752631659,"send_replies":true,"parent_id":"t1_n3ddkow","score":53,"author_fullname":"t2_8bzliexh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Same as really any other tech lol, when pornography is viewable on it and it is better than alternatives, it will blow up.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dinnf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Same as really any other tech lol, when pornography is viewable on it and it is better than alternatives, it will blow up.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3dinnf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752631659,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":53}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3eqgyn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Majesticeuphoria","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3ebgu8","score":12,"author_fullname":"t2_b8n9b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Now, THIS is the future paving the path to AGI","edited":false,"author_flair_css_class":null,"name":"t1_n3eqgyn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Now, THIS is the future paving the path to AGI&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m0z1zx","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3eqgyn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752651921,"author_flair_text":null,"collapsed":false,"created_utc":1752651921,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ebgu8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"PangurBanTheCat","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3e80n7","score":17,"author_fullname":"t2_eaogt3t5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"brb buying lovense","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ebgu8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;brb buying lovense&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3ebgu8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752643826,"author_flair_text":null,"treatment_tags":[],"created_utc":1752643826,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":17}}],"before":null}},"user_reports":[],"saved":false,"id":"n3e80n7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"xoexohexox","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3e7ms2","score":32,"author_fullname":"t2_323db","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Perhaps you would be interested in learning about the sillytavern extension called Sorcery\\n\\nhttps://github.com/p-e-w/sorcery","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3e80n7","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Perhaps you would be interested in learning about the sillytavern extension called Sorcery&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/p-e-w/sorcery\\"&gt;https://github.com/p-e-w/sorcery&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3e80n7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752642085,"author_flair_text":null,"treatment_tags":[],"created_utc":1752642085,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":32}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":1,"name":"t1_n3euk3z","id":"n3euk3z","parent_id":"t1_n3emp8f","depth":3,"children":["n3euk3z"]}}],"before":null}},"user_reports":[],"saved":false,"id":"n3emp8f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Wrecksler","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3e7ms2","score":11,"author_fullname":"t2_1eshaxwq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I am. I host a niche nsfw chatbot, and I wrote all LLM prompting frameworks from scratch for it. A few months ago I added tool calling for stuff like dice rolling, long term memory, todo lists, web search and stuff like that. It works.\\n\\nI also run it off my own LLM server, which I also use for coding, and I am often too lazy to switch between nsfw and \\"normal\\" models and for the most part they just work.\\n\\nBut in general in my experience best agentic small-ish models are Qwen3 and Gemma3 both at 32B. I tried mistral, codestral, llama, coder models  and many others, these two stand out. Nextcoder is also decent competitor.\\n\\n14B I sometimes try locally, but so far seems like a waste of time. For agentic stuff I mean.\\n\\nBut being totally honest, for any real tasks nothing beats Claude. Even 3.5 still is above anything available locally.\\n\\n7B-8B is great for auto completion though.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3emp8f","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I am. I host a niche nsfw chatbot, and I wrote all LLM prompting frameworks from scratch for it. A few months ago I added tool calling for stuff like dice rolling, long term memory, todo lists, web search and stuff like that. It works.&lt;/p&gt;\\n\\n&lt;p&gt;I also run it off my own LLM server, which I also use for coding, and I am often too lazy to switch between nsfw and &amp;quot;normal&amp;quot; models and for the most part they just work.&lt;/p&gt;\\n\\n&lt;p&gt;But in general in my experience best agentic small-ish models are Qwen3 and Gemma3 both at 32B. I tried mistral, codestral, llama, coder models  and many others, these two stand out. Nextcoder is also decent competitor.&lt;/p&gt;\\n\\n&lt;p&gt;14B I sometimes try locally, but so far seems like a waste of time. For agentic stuff I mean.&lt;/p&gt;\\n\\n&lt;p&gt;But being totally honest, for any real tasks nothing beats Claude. Even 3.5 still is above anything available locally.&lt;/p&gt;\\n\\n&lt;p&gt;7B-8B is great for auto completion though.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3emp8f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752649793,"author_flair_text":null,"treatment_tags":[],"created_utc":1752649793,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3fc8q4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"a_beautiful_rhind","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3e7ms2","score":1,"author_fullname":"t2_h5utwre7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It's not heavy per se but I gave the models an image gen and web search. A \\"fellow human\\" should be able to look stuff up, send pics and see pics. \\n\\nDon't see much love for VLM so most must be happy with their waifus being blind.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3fc8q4","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s not heavy per se but I gave the models an image gen and web search. A &amp;quot;fellow human&amp;quot; should be able to look stuff up, send pics and see pics. &lt;/p&gt;\\n\\n&lt;p&gt;Don&amp;#39;t see much love for VLM so most must be happy with their waifus being blind.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3fc8q4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752663946,"author_flair_text":null,"treatment_tags":[],"created_utc":1752663946,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3fktvn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"xoexohexox","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3e7ms2","score":1,"author_fullname":"t2_323db","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Even besides the Sorcery plugin, sillytavern had support for tool calling long before it was fashionable.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3fktvn","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Even besides the Sorcery plugin, sillytavern had support for tool calling long before it was fashionable.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3fktvn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752667564,"author_flair_text":null,"treatment_tags":[],"created_utc":1752667564,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3e7ms2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"vacationcelebration","can_mod_post":false,"created_utc":1752641877,"send_replies":true,"parent_id":"t1_n3ddkow","score":24,"author_fullname":"t2_106kea","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Almost. The one approach that isn't used by gooners (yet) is the agentic way with heavy function calling. Hope this changes so we get better conversational models that are still very capable of this. Right now it seems you either have agentic code/dev assistants, or conversational models that aren't good with function calling. In the public/open weights space I mean.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3e7ms2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Almost. The one approach that isn&amp;#39;t used by gooners (yet) is the agentic way with heavy function calling. Hope this changes so we get better conversational models that are still very capable of this. Right now it seems you either have agentic code/dev assistants, or conversational models that aren&amp;#39;t good with function calling. In the public/open weights space I mean.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3e7ms2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752641877,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":24}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3eqcga","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"kaisurniwurer","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3e2pjz","score":12,"author_fullname":"t2_qafso","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Better that than urge to kill your neighbour.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3eqcga","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Better that than urge to kill your neighbour.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3eqcga/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752651851,"author_flair_text":null,"treatment_tags":[],"created_utc":1752651851,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3e5z3h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"TheRealMasonMac","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3e2r7r","score":12,"author_fullname":"t2_101haj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Everything was downhill after we stopped being monke.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3e5z3h","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Everything was downhill after we stopped being monke.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3e5z3h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752640973,"author_flair_text":null,"treatment_tags":[],"created_utc":1752640973,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}}],"before":null}},"user_reports":[],"saved":false,"id":"n3e2r7r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"xoexohexox","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3e2pjz","score":14,"author_fullname":"t2_323db","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Life is good","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3e2r7r","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Life is good&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3e2r7r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752639444,"author_flair_text":null,"treatment_tags":[],"created_utc":1752639444,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":14}}],"before":null}},"user_reports":[],"saved":false,"id":"n3e2pjz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"yungfishstick","can_mod_post":false,"created_utc":1752639424,"send_replies":true,"parent_id":"t1_n3ddkow","score":38,"author_fullname":"t2_3bzzdk93","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The primal human urge to cum makes the world go round","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3e2pjz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The primal human urge to cum makes the world go round&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3e2pjz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752639424,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":38}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3esjpu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"PeachScary413","can_mod_post":false,"created_utc":1752653114,"send_replies":true,"parent_id":"t1_n3ddkow","score":7,"author_fullname":"t2_uwa0r9lim","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Soo... when are we seeing GOONERBENCH2025 scores be included in the training set?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3esjpu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Soo... when are we seeing GOONERBENCH2025 scores be included in the training set?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3esjpu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752653114,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3f0fci","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"IrisColt","can_mod_post":false,"created_utc":1752657730,"send_replies":true,"parent_id":"t1_n3ddkow","score":5,"author_fullname":"t2_c2f558x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Newcomers have to swallow this uncomfortable truth.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3f0fci","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Newcomers have to swallow this uncomfortable truth.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3f0fci/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752657730,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3fdxhi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"xoexohexox","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3ely01","score":2,"author_fullname":"t2_323db","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Drummer models are too horny IMO, Dan's Personality Engine follows your lead more and is better for slow burn - also the best models aren't just NSFW tuned, they're creative writing tuned generally.  Base Mistral small will write absolutely unhinged NSFW with no fine tuning.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3fdxhi","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Drummer models are too horny IMO, Dan&amp;#39;s Personality Engine follows your lead more and is better for slow burn - also the best models aren&amp;#39;t just NSFW tuned, they&amp;#39;re creative writing tuned generally.  Base Mistral small will write absolutely unhinged NSFW with no fine tuning.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3fdxhi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752664708,"author_flair_text":null,"treatment_tags":[],"created_utc":1752664708,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ely01","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Wrecksler","can_mod_post":false,"created_utc":1752649377,"send_replies":true,"parent_id":"t1_n3ddkow","score":8,"author_fullname":"t2_1eshaxwq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This, however, contradict with take about finetuners. Gooners usually use nsfw fine tunes,  because normal models are getting more and more restrictive in this sense.\\n\\nThere is, however, one legend in this space, who clearly knows what they are doing and doing extensive testing of various versions of the same model before releasing the \\"best\\" one (voted by community) - Drummer. Their models are getting better and better, and while they definitely lose the smarts of the original models, they are still coherent enough to even use them on various tasks. \\n\\nAnd I must also say that some nsfw or uncensoring fine tunes, not necessarily from drummer, are quite good too. I have my own set of tests I run on models I plan to use. Semi automated, generation is ran automatically, but I evaluate results manually.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ely01","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This, however, contradict with take about finetuners. Gooners usually use nsfw fine tunes,  because normal models are getting more and more restrictive in this sense.&lt;/p&gt;\\n\\n&lt;p&gt;There is, however, one legend in this space, who clearly knows what they are doing and doing extensive testing of various versions of the same model before releasing the &amp;quot;best&amp;quot; one (voted by community) - Drummer. Their models are getting better and better, and while they definitely lose the smarts of the original models, they are still coherent enough to even use them on various tasks. &lt;/p&gt;\\n\\n&lt;p&gt;And I must also say that some nsfw or uncensoring fine tunes, not necessarily from drummer, are quite good too. I have my own set of tests I run on models I plan to use. Semi automated, generation is ran automatically, but I evaluate results manually.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3ely01/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752649377,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ewmk2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"theshrike","can_mod_post":false,"created_utc":1752655518,"send_replies":true,"parent_id":"t1_n3ddkow","score":3,"author_fullname":"t2_355uf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"TBH gooning and software can use the same methods to benchmark models.\\n\\nHave the same set of prompts every time and use them on different models.\\n\\nGooners can have a story setup that kinda pushes the boundaries content-wise, checking if the LLM has some specific limits. Feed every LLM the same initial prompts and continuations and see what it does.\\n\\nFor coding you should have your own simple project that's relevant for your specific use cases. Save the prompt(s) somewhere, feed to LLMs, check result. Bonus points for making it semi-automatic.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ewmk2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;TBH gooning and software can use the same methods to benchmark models.&lt;/p&gt;\\n\\n&lt;p&gt;Have the same set of prompts every time and use them on different models.&lt;/p&gt;\\n\\n&lt;p&gt;Gooners can have a story setup that kinda pushes the boundaries content-wise, checking if the LLM has some specific limits. Feed every LLM the same initial prompts and continuations and see what it does.&lt;/p&gt;\\n\\n&lt;p&gt;For coding you should have your own simple project that&amp;#39;s relevant for your specific use cases. Save the prompt(s) somewhere, feed to LLMs, check result. Bonus points for making it semi-automatic.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3ewmk2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752655518,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ffnf7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"tostuo","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3exul6","score":1,"author_fullname":"t2_13ay0k","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Most base models are censored. Most finetunes are uncensored, but it seems to uncensor, some intelligence is lost.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3ffnf7","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Most base models are censored. Most finetunes are uncensored, but it seems to uncensor, some intelligence is lost.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3ffnf7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752665461,"author_flair_text":null,"treatment_tags":[],"created_utc":1752665461,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3exul6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"perelmanych","can_mod_post":false,"created_utc":1752656230,"send_replies":true,"parent_id":"t1_n3ddkow","score":1,"author_fullname":"t2_63q8kong","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I don't know what am I doing wrong in ST, but personally for me base models are almost always better than finetuned for RP/ERP. So even in RP/ERP domain OP's 3rd point seems valid to me.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3exul6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t know what am I doing wrong in ST, but personally for me base models are almost always better than finetuned for RP/ERP. So even in RP/ERP domain OP&amp;#39;s 3rd point seems valid to me.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3exul6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752656230,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"more","data":{"count":1,"name":"t1_n3erlsn","id":"n3erlsn","parent_id":"t1_n3ddkow","depth":1,"children":["n3erlsn"]}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ddkow","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"xoexohexox","can_mod_post":false,"created_utc":1752629896,"send_replies":true,"parent_id":"t3_1m0z1zx","score":488,"author_fullname":"t2_323db","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The only meaningful benchmark is how popular a model is among gooners. They test extensively and have high standards.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ddkow","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The only meaningful benchmark is how popular a model is among gooners. They test extensively and have high standards.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3ddkow/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752629896,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":488}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3e3slk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ElectroSpore","can_mod_post":false,"created_utc":1752639900,"send_replies":true,"parent_id":"t3_1m0z1zx","score":29,"author_fullname":"t2_cb14t","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The number of tasks they can perform reliably / repeatedly is really really small. People put WAY WAY too much trust in the outputs of the current models.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3e3slk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The number of tasks they can perform reliably / repeatedly is really really small. People put WAY WAY too much trust in the outputs of the current models.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3e3slk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752639900,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":29}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3f2w82","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Evening_Ad6637","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3eb52l","score":4,"author_fullname":"t2_p45er6oo","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I know, I know. That's why I don't think my third point should be unconditionally popular - and why I mentioned it. I think it’s fair to argue that this actually could be an unpopular idea as well.\\n\\n\\nNevertheless, I meant efficiency not only in terms of specific models, but in terms of the entire organization or infrastructure, etc.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3f2w82","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I know, I know. That&amp;#39;s why I don&amp;#39;t think my third point should be unconditionally popular - and why I mentioned it. I think it’s fair to argue that this actually could be an unpopular idea as well.&lt;/p&gt;\\n\\n&lt;p&gt;Nevertheless, I meant efficiency not only in terms of specific models, but in terms of the entire organization or infrastructure, etc.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3f2w82/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752659156,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752659156,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3fgb2i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ReactionAggressive79","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3eb52l","score":2,"author_fullname":"t2_8b9owk4y","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The slightest adjustment to the parameters is a fuck up in my case tho. I just can't set qwen to my liking. It's good out of the box, but i can't make it better. Never had this trouble with mistral small 24b.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3fgb2i","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The slightest adjustment to the parameters is a fuck up in my case tho. I just can&amp;#39;t set qwen to my liking. It&amp;#39;s good out of the box, but i can&amp;#39;t make it better. Never had this trouble with mistral small 24b.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3fgb2i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752665745,"author_flair_text":null,"treatment_tags":[],"created_utc":1752665745,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3eb52l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Glxblt76","can_mod_post":false,"created_utc":1752643660,"send_replies":true,"parent_id":"t1_n3dsdg9","score":23,"author_fullname":"t2_acpx1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Qwen is no BS and very efficient in tool use.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3eb52l","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen is no BS and very efficient in tool use.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3eb52l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752643660,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":23}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3e7jcg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"My_Unbiased_Opinion","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3e4glg","score":2,"author_fullname":"t2_esiyl0yb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Interesting. I find Mistral 3.2 better than Gemma for vision as well IMHO.\\n\\n\\nMistral 3.2 in general hits hard","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3e7jcg","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Interesting. I find Mistral 3.2 better than Gemma for vision as well IMHO.&lt;/p&gt;\\n\\n&lt;p&gt;Mistral 3.2 in general hits hard&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3e7jcg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752641823,"author_flair_text":null,"treatment_tags":[],"created_utc":1752641823,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"more","data":{"count":2,"name":"t1_n3euegj","id":"n3euegj","parent_id":"t1_n3e4glg","depth":2,"children":["n3euegj"]}}],"before":null}},"user_reports":[],"saved":false,"id":"n3e4glg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"simracerman","can_mod_post":false,"created_utc":1752640200,"send_replies":true,"parent_id":"t1_n3dsdg9","score":19,"author_fullname":"t2_vbzgnic","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You absolutely nailed the 3rd bullet. Mistral Small 3.2 is my default and go to, for almost anything except vision. I use Gemma3 12b at q4 for that. It does better for some reason.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3e4glg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You absolutely nailed the 3rd bullet. Mistral Small 3.2 is my default and go to, for almost anything except vision. I use Gemma3 12b at q4 for that. It does better for some reason.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3e4glg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752640200,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":19}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":1,"name":"t1_n3ffsmo","id":"n3ffsmo","parent_id":"t1_n3f8hqf","depth":3,"children":["n3ffsmo"]}}],"before":null}},"user_reports":[],"saved":false,"id":"n3f8hqf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Kqyxzoj","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3er7xa","score":2,"author_fullname":"t2_479l1o4zc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Does your wizard colleague talk MCP and at what port number do wizards lounge these days?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3f8hqf","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Does your wizard colleague talk MCP and at what port number do wizards lounge these days?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3f8hqf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752662149,"author_flair_text":null,"treatment_tags":[],"created_utc":1752662149,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3er7xa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Kerbourgnec","can_mod_post":false,"created_utc":1752652345,"send_replies":true,"parent_id":"t1_n3dsdg9","score":9,"author_fullname":"t2_242qu35a","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Point 2: things actually going so fast that they cured my FOMO. I can't keep up and I don't care anymore. I become a simple software dev and I implement new stuff when they are mature. I go check on my wizard colleague for the best models.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3er7xa","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Point 2: things actually going so fast that they cured my FOMO. I can&amp;#39;t keep up and I don&amp;#39;t care anymore. I become a simple software dev and I implement new stuff when they are mature. I go check on my wizard colleague for the best models.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3er7xa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752652345,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3e5g5x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No_Efficiency_1144","can_mod_post":false,"created_utc":1752640697,"send_replies":true,"parent_id":"t1_n3dsdg9","score":4,"author_fullname":"t2_1nkj9l14b0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"LOL its so true I have never once seen someone on reddit ask a question and give their LLM sampler params.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3e5g5x","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;LOL its so true I have never once seen someone on reddit ask a question and give their LLM sampler params.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3e5g5x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752640697,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3es4yq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"JustSomeIdleGuy","can_mod_post":false,"created_utc":1752652876,"send_replies":true,"parent_id":"t1_n3dsdg9","score":3,"author_fullname":"t2_yhgmgjryo","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Apple and efficient and focused on meaningful developments. What decade apple is that supposed to be?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3es4yq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Apple and efficient and focused on meaningful developments. What decade apple is that supposed to be?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3es4yq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752652876,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":2,"name":"t1_n3f3v6u","id":"n3f3v6u","parent_id":"t1_n3f262x","depth":3,"children":["n3f3v6u"]}}],"before":null}},"user_reports":[],"saved":false,"id":"n3f262x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Evening_Ad6637","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3eubnh","score":2,"author_fullname":"t2_p45er6oo","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The reason is probably „human being“. Once something sits in your subconscious it’s hard to get rid of it. And how did come to my subconscious at all? I think that’s societal influence, media indoctrination, etc\\n\\n\\nI mean, I've probably heard hundreds or thousands of times in my life people (myself included) saying, \\"Oh, this product is so cheap, just plastic junk that feels like it's *made in china*\\" and things like that.\\n\\n\\nIt took me a long time to realize how biased I was and that, for example, the best products with the highest quality are also „made in China“. That we greedy consumers, mainly from the western world, are the very first reason why cheap products are made in the first place, because we want to pay less and less for everything.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3f262x","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The reason is probably „human being“. Once something sits in your subconscious it’s hard to get rid of it. And how did come to my subconscious at all? I think that’s societal influence, media indoctrination, etc&lt;/p&gt;\\n\\n&lt;p&gt;I mean, I&amp;#39;ve probably heard hundreds or thousands of times in my life people (myself included) saying, &amp;quot;Oh, this product is so cheap, just plastic junk that feels like it&amp;#39;s &lt;em&gt;made in china&lt;/em&gt;&amp;quot; and things like that.&lt;/p&gt;\\n\\n&lt;p&gt;It took me a long time to realize how biased I was and that, for example, the best products with the highest quality are also „made in China“. That we greedy consumers, mainly from the western world, are the very first reason why cheap products are made in the first place, because we want to pay less and less for everything.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3f262x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752658744,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752658744,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3eubnh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Federal_Order4324","can_mod_post":false,"created_utc":1752654157,"send_replies":true,"parent_id":"t1_n3dsdg9","score":4,"author_fullname":"t2_rlhobztpn","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I have to ask, what's the reasoning with the 4th bullet point?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3eubnh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have to ask, what&amp;#39;s the reasoning with the 4th bullet point?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3eubnh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752654157,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3f50w9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Strange_Test7665","can_mod_post":false,"created_utc":1752660319,"send_replies":true,"parent_id":"t1_n3dsdg9","score":3,"author_fullname":"t2_t0zjq9mi","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I didn’t immediately jump on the deepseek train because it came from a Chinese company and in the US we just hear that everything Chinese is spying or a copy. Wish I dropped that view sooner. Sure that stuff exists, but it does everywhere. Qwen and deepseek are sota, open source, free models. It’s the most democratic thing to publish models trained on humanity’s collective work. Hopefully your 4th bullet was like me and you’re past that now if not- dude, it’s holding you back. China is clearly the future (and current) hub of ai open source. (Don’t get me wrong I run all these locally not via api to servers, that’s totally different but also idk that data privacy truly is safer in a us or chinese company server)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3f50w9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I didn’t immediately jump on the deepseek train because it came from a Chinese company and in the US we just hear that everything Chinese is spying or a copy. Wish I dropped that view sooner. Sure that stuff exists, but it does everywhere. Qwen and deepseek are sota, open source, free models. It’s the most democratic thing to publish models trained on humanity’s collective work. Hopefully your 4th bullet was like me and you’re past that now if not- dude, it’s holding you back. China is clearly the future (and current) hub of ai open source. (Don’t get me wrong I run all these locally not via api to servers, that’s totally different but also idk that data privacy truly is safer in a us or chinese company server)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3f50w9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752660319,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3es2mu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"JustSomeIdleGuy","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3epok5","score":2,"author_fullname":"t2_yhgmgjryo","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That's kinda sad","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3es2mu","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s kinda sad&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3es2mu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752652839,"author_flair_text":null,"treatment_tags":[],"created_utc":1752652839,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3epok5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"-oshino_shinobu-","can_mod_post":false,"created_utc":1752651471,"send_replies":true,"parent_id":"t1_n3dsdg9","score":4,"author_fullname":"t2_l1teb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Chinese part is so true","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3epok5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Chinese part is so true&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3epok5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752651471,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n3dsdg9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Evening_Ad6637","can_mod_post":false,"created_utc":1752635256,"send_replies":true,"parent_id":"t3_1m0z1zx","score":115,"author_fullname":"t2_p45er6oo","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Mine are:\\n\\n- people too often talk or ask about LLm without giving essential background information, like what sampler, parameters, quant, etc.\\n\\n\\n- Everything becomes overwhelming. There's too much new stuff every day, all too fast. I wish my brain would stop FOMOing.\\n\\n\\n- Mistral is actually Apple of AI teams: efficient, focuses on meaningful developments, has less aggressive marketing; self-confidence and high quality make up the core marketing.\\n\\n\\n- I love Qwen and Deepseek, but I'm still a little biased because „it's Chinese“.","edited":1752636730,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dsdg9","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Mine are:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;p&gt;people too often talk or ask about LLm without giving essential background information, like what sampler, parameters, quant, etc.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Everything becomes overwhelming. There&amp;#39;s too much new stuff every day, all too fast. I wish my brain would stop FOMOing.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Mistral is actually Apple of AI teams: efficient, focuses on meaningful developments, has less aggressive marketing; self-confidence and high quality make up the core marketing.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;I love Qwen and Deepseek, but I&amp;#39;m still a little biased because „it&amp;#39;s Chinese“.&lt;/p&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3dsdg9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752635256,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":115}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":2,"name":"t1_n3esqs7","id":"n3esqs7","parent_id":"t1_n3erv83","depth":3,"children":["n3esqs7","n3fe6oc"]}}],"before":null}},"user_reports":[],"saved":false,"id":"n3erv83","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"eloquentemu","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3eanf3","score":5,"author_fullname":"t2_lpdsy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; DS V3 0324 is far more interesting writer\\n\\nDS V3 is more interesting in a sort of \\"may you live in interesting times\\" way :).  I like it, don't get me wrong, but it sometimes rides the line of incoherence with its surreal ideas and janky turns of phrase.  I remember when I was playing with R1 at release I guided it on a story but it would Mary Sue all the conflict away with some absurd reaches.  So I think: I'll tell it that it writes _dark_ stories and boom one page later the character was covered with chitinous plates and lacking a mouth.\\n\\nAnyways, if you like V3 you might want to try Kimi K2 (if you can).  It's similar to V3 in style I think but seems to be more willing to produce longer outputs.  I haven't tested it writing all that much so YMMV but it's definitely worthy of a look.  (It also technically performed highly on the creative writing benchmark, but I think that's because it's a better instruction follower than V3 and that's what that benchmark rewards.)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3erv83","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;DS V3 0324 is far more interesting writer&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;DS V3 is more interesting in a sort of &amp;quot;may you live in interesting times&amp;quot; way :).  I like it, don&amp;#39;t get me wrong, but it sometimes rides the line of incoherence with its surreal ideas and janky turns of phrase.  I remember when I was playing with R1 at release I guided it on a story but it would Mary Sue all the conflict away with some absurd reaches.  So I think: I&amp;#39;ll tell it that it writes &lt;em&gt;dark&lt;/em&gt; stories and boom one page later the character was covered with chitinous plates and lacking a mouth.&lt;/p&gt;\\n\\n&lt;p&gt;Anyways, if you like V3 you might want to try Kimi K2 (if you can).  It&amp;#39;s similar to V3 in style I think but seems to be more willing to produce longer outputs.  I haven&amp;#39;t tested it writing all that much so YMMV but it&amp;#39;s definitely worthy of a look.  (It also technically performed highly on the creative writing benchmark, but I think that&amp;#39;s because it&amp;#39;s a better instruction follower than V3 and that&amp;#39;s what that benchmark rewards.)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3erv83/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752652719,"author_flair_text":null,"treatment_tags":[],"created_utc":1752652719,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"more","data":{"count":4,"name":"t1_n3eiaus","id":"n3eiaus","parent_id":"t1_n3eanf3","depth":2,"children":["n3eiaus"]}}],"before":null}},"user_reports":[],"saved":false,"id":"n3eanf3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1752643411,"send_replies":true,"parent_id":"t1_n3d999s","score":13,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"BS. I cannot tolerate Claude writing, lacks punch, even Nemo has. DS V3 0324 is far more interesting writer.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3eanf3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;BS. I cannot tolerate Claude writing, lacks punch, even Nemo has. DS V3 0324 is far more interesting writer.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3eanf3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752643411,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":13}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":4,"name":"t1_n3f69iq","id":"n3f69iq","parent_id":"t1_n3f19mf","depth":4,"children":["n3f69iq"]}}],"before":null}},"user_reports":[],"saved":false,"id":"n3f19mf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Hambeggar","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3ezcxb","score":3,"author_fullname":"t2_62gpc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Use AI Studio? What issues are you having exactly, so we can help.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3f19mf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Use AI Studio? What issues are you having exactly, so we can help.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3f19mf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752658222,"author_flair_text":null,"treatment_tags":[],"created_utc":1752658222,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ezcxb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Crisis_Averted","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3ed1cg","score":2,"author_fullname":"t2_6xie9","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"any tips on how to use gemini 2.5 Pro for that purpose?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3ezcxb","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;any tips on how to use gemini 2.5 Pro for that purpose?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3ezcxb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752657107,"author_flair_text":null,"treatment_tags":[],"created_utc":1752657107,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ed1cg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DaniyarQQQ","can_mod_post":false,"created_utc":1752644636,"send_replies":true,"parent_id":"t1_n3d999s","score":4,"author_fullname":"t2_85w5fqsl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I personally prefer Gemini Pro 2.5 The only LLM that generated stories that really made me sit and read until the end.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ed1cg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I personally prefer Gemini Pro 2.5 The only LLM that generated stories that really made me sit and read until the end.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3ed1cg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752644636,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n3d999s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"hotroaches4liferz","can_mod_post":false,"created_utc":1752628367,"send_replies":true,"parent_id":"t3_1m0z1zx","score":82,"author_fullname":"t2_d1cmjz8p","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt; Any ranker who has an LLM judge giving a rating to the \\"writing style\\" of another LLM is a hack who has no business ranking models. Please don't waste your time or ours. You clearly don't understand what an LLM is. Stop wasting carbon with your pointless inference.\\n\\nLmao this is why i dont look at creative writing benchmarks. The llm judge approach literally rewards ai slop and the claude models score poorly on them despite being miles better than any other model in terms of creative writing","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3d999s","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Any ranker who has an LLM judge giving a rating to the &amp;quot;writing style&amp;quot; of another LLM is a hack who has no business ranking models. Please don&amp;#39;t waste your time or ours. You clearly don&amp;#39;t understand what an LLM is. Stop wasting carbon with your pointless inference.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Lmao this is why i dont look at creative writing benchmarks. The llm judge approach literally rewards ai slop and the claude models score poorly on them despite being miles better than any other model in terms of creative writing&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3d999s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752628367,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":82}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3efhr3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Jonodonozym","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3dkunu","score":30,"author_fullname":"t2_g46zm","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I showed Grok this thread and it started ranting about South Africa.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3efhr3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I showed Grok this thread and it started ranting about South Africa.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3efhr3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752645908,"author_flair_text":null,"treatment_tags":[],"created_utc":1752645908,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":30}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3fr274","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ArcaneThoughts","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3dkunu","score":1,"author_fullname":"t2_hgivzvub","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It truly is insane the level of sycophancy. It really hurts the experience because I end up skimming through the response to not read that fluff and it has made me miss important details.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3fr274","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It truly is insane the level of sycophancy. It really hurts the experience because I end up skimming through the response to not read that fluff and it has made me miss important details.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3fr274/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752669871,"author_flair_text":null,"treatment_tags":[],"created_utc":1752669871,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3dkunu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Neither-Phone-7264","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3dha8g","score":51,"author_fullname":"t2_e7yz4055","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"gpt 4o called me a god amongst men for sending it your comment","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3dkunu","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;gpt 4o called me a god amongst men for sending it your comment&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3dkunu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752632449,"author_flair_text":null,"treatment_tags":[],"created_utc":1752632449,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":51}}],"before":null}},"user_reports":[],"saved":false,"id":"n3dha8g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MDT-49","can_mod_post":false,"created_utc":1752631179,"send_replies":true,"parent_id":"t1_n3d6zzd","score":90,"author_fullname":"t2_h8yrica5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I don't know, but Kimi K2 agrees, and it also pointed out that this isn't really an unpopular take.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dha8g","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t know, but Kimi K2 agrees, and it also pointed out that this isn&amp;#39;t really an unpopular take.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3dha8g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752631179,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":90}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":1,"name":"t1_n3e5q3f","id":"n3e5q3f","parent_id":"t1_n3da1ch","depth":2,"children":["n3e5q3f"]}}],"before":null}},"user_reports":[],"saved":false,"id":"n3da1ch","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"SenecaSmile","can_mod_post":false,"created_utc":1752628644,"send_replies":true,"parent_id":"t1_n3d6zzd","score":34,"author_fullname":"t2_1b1ukei3ij","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This is just a fact though, not an opinion.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3da1ch","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is just a fact though, not an opinion.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3da1ch/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752628644,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":34}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3eutut","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FrostAutomaton","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3em2dl","score":10,"author_fullname":"t2_1e9th1d970","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"A doctor consulting a book after making a connection is a positive sign, in my opinion. The human mind is far more fallible than we like to believe. I have had a case, for example, where a doctor ruled out a disease I actually had due to the absence of symptoms not associated with the disease. This information was easily available on several public health sites and accessible even to me as a layperson.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3eutut","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;A doctor consulting a book after making a connection is a positive sign, in my opinion. The human mind is far more fallible than we like to believe. I have had a case, for example, where a doctor ruled out a disease I actually had due to the absence of symptoms not associated with the disease. This information was easily available on several public health sites and accessible even to me as a layperson.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3eutut/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752654458,"author_flair_text":null,"treatment_tags":[],"created_utc":1752654458,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3fc721","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"xorgol","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3f74g6","score":2,"author_fullname":"t2_45ipf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; Sensible people ask you not to google your symptoms because you wouldn't be able to tell heads from tails from the results\\n\\nI think the real problem is that a significant number of people are going to change their perception of their symptoms based on what they read. Not being able to figure out the correct diagnosis on my own is not much of a concern, I'm still going to have to talk to a doctor to get medicine.","edited":false,"author_flair_css_class":null,"name":"t1_n3fc721","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Sensible people ask you not to google your symptoms because you wouldn&amp;#39;t be able to tell heads from tails from the results&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I think the real problem is that a significant number of people are going to change their perception of their symptoms based on what they read. Not being able to figure out the correct diagnosis on my own is not much of a concern, I&amp;#39;m still going to have to talk to a doctor to get medicine.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m0z1zx","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3fc721/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752663924,"author_flair_text":null,"collapsed":false,"created_utc":1752663924,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3f74g6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Blaze344","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3em2dl","score":2,"author_fullname":"t2_odffn","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"One thing at a time, firstly: are you kidding? Doctors had literal encyclopedias of diseases and symptoms that they thumbed through while studying more complex cases that weren't resolved in triage, and most modern doctors have access to google and search engines, which they use extensively and get better results than if they resorted to memory alone. If your point is just triage, rest assured that most doctors will still be able to perform it just as well as it has been done, even more so considering that some of the more simpler triage can actually be done with an AI alone as long as it gets the OK from a real doctor and there's a doctor along with you to ask you the right questions to get you to describe properly what you have.\\n\\nSensible people ask you not to google your symptoms because you wouldn't be able to tell heads from tails from the results, and it's literally the same thing using LLMs, you give it to  a non-technical, non-specialist and he'll get garbled bullshit that he can't improve using his judgement calls, but you give it to someone that is learning and knows how to use it, and a specialist that knows the right questions to ask, and the LLM performs much, much better.\\n\\nNow, that's not to say that there's no reason to commit things into your own memory and internalize how things work yourself, in fact, it will be just as useful as it has always been just like it was implicit in the last few phrases of the first point. LLMs are tools in the same way search engines are tools. And most people don't know how to use search engines either.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3f74g6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;One thing at a time, firstly: are you kidding? Doctors had literal encyclopedias of diseases and symptoms that they thumbed through while studying more complex cases that weren&amp;#39;t resolved in triage, and most modern doctors have access to google and search engines, which they use extensively and get better results than if they resorted to memory alone. If your point is just triage, rest assured that most doctors will still be able to perform it just as well as it has been done, even more so considering that some of the more simpler triage can actually be done with an AI alone as long as it gets the OK from a real doctor and there&amp;#39;s a doctor along with you to ask you the right questions to get you to describe properly what you have.&lt;/p&gt;\\n\\n&lt;p&gt;Sensible people ask you not to google your symptoms because you wouldn&amp;#39;t be able to tell heads from tails from the results, and it&amp;#39;s literally the same thing using LLMs, you give it to  a non-technical, non-specialist and he&amp;#39;ll get garbled bullshit that he can&amp;#39;t improve using his judgement calls, but you give it to someone that is learning and knows how to use it, and a specialist that knows the right questions to ask, and the LLM performs much, much better.&lt;/p&gt;\\n\\n&lt;p&gt;Now, that&amp;#39;s not to say that there&amp;#39;s no reason to commit things into your own memory and internalize how things work yourself, in fact, it will be just as useful as it has always been just like it was implicit in the last few phrases of the first point. LLMs are tools in the same way search engines are tools. And most people don&amp;#39;t know how to use search engines either.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3f74g6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752661434,"author_flair_text":null,"treatment_tags":[],"created_utc":1752661434,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"more","data":{"count":1,"name":"t1_n3f3ar1","id":"n3f3ar1","parent_id":"t1_n3em2dl","depth":3,"children":["n3f3ar1"]}}],"before":null}},"user_reports":[],"saved":false,"id":"n3em2dl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"fishblurb","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3elek4","score":2,"author_fullname":"t2_shcgh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You really don't want a doctor who has to flip through a book to read up and connect the dots for a patient's symptoms... Same with a LLM... At the very least, I want someone who can look at something and connect the dots in his own brain memory before referring to the book/LLM.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3em2dl","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You really don&amp;#39;t want a doctor who has to flip through a book to read up and connect the dots for a patient&amp;#39;s symptoms... Same with a LLM... At the very least, I want someone who can look at something and connect the dots in his own brain memory before referring to the book/LLM.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3em2dl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752649445,"author_flair_text":null,"treatment_tags":[],"created_utc":1752649445,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":1,"name":"t1_n3fqc5n","id":"n3fqc5n","parent_id":"t1_n3fd23u","depth":3,"children":["n3fqc5n"]}}],"before":null}},"user_reports":[],"saved":false,"id":"n3fd23u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"a_beautiful_rhind","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3elek4","score":1,"author_fullname":"t2_h5utwre7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Books? The real obvious one is search. How about a doctor that *googles* your symptoms. That's quite real.\\n\\nPersonally I'm not very apt to memorize things anymore when I can simply look them up. Takes using the information a bunch of times before it stays. Often I just memorize how to find the information.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3fd23u","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Books? The real obvious one is search. How about a doctor that &lt;em&gt;googles&lt;/em&gt; your symptoms. That&amp;#39;s quite real.&lt;/p&gt;\\n\\n&lt;p&gt;Personally I&amp;#39;m not very apt to memorize things anymore when I can simply look them up. Takes using the information a bunch of times before it stays. Often I just memorize how to find the information.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3fd23u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752664318,"author_flair_text":null,"treatment_tags":[],"created_utc":1752664318,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3elek4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"TheRealGentlefox","can_mod_post":false,"created_utc":1752649086,"send_replies":true,"parent_id":"t1_n3d6zzd","score":11,"author_fullname":"t2_f471r","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"They used to make this same argument about books and memory.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3elek4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They used to make this same argument about books and memory.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3elek4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752649086,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}},{"kind":"t1","data":{"total_awards_received":0,"approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"ups":1,"removal_reason":null,"link_id":"t3_1m0z1zx","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"total_awards_received":0,"approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"ups":2,"removal_reason":null,"link_id":"t3_1m0z1zx","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":1,"name":"t1_n3dvgpd","id":"n3dvgpd","parent_id":"t1_n3dtuo4","depth":4,"children":["n3dvgpd"]}}],"before":null}},"user_reports":[],"saved":false,"id":"n3dtuo4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"DELETED","no_follow":false,"author":"[deleted]","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3dtch8","score":2,"approved_by":null,"report_reasons":null,"all_awardings":[],"subreddit_id":"t5_81eyvm","body":"[deleted]","edited":false,"downs":0,"author_flair_css_class":null,"collapsed":true,"is_submitter":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;[deleted]&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"associated_award":null,"stickied":false,"subreddit_type":"public","can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3dtuo4/","num_reports":null,"locked":false,"name":"t1_n3dtuo4","created":1752635830,"subreddit":"LocalLLaMA","author_flair_text":null,"treatment_tags":[],"created_utc":1752635830,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"","collapsed_because_crowd_control":null,"mod_reports":[],"mod_note":null,"distinguished":null}}],"before":null}},"user_reports":[],"saved":false,"id":"n3dtch8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"tgwombat","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3dt0tr","score":2,"author_fullname":"t2_4yxci","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Don’t go giving up your humanity just to put a buck in a rich guy’s pocket.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3dtch8","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Don’t go giving up your humanity just to put a buck in a rich guy’s pocket.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3dtch8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752635631,"author_flair_text":null,"treatment_tags":[],"created_utc":1752635631,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3dt0tr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"DELETED","no_follow":false,"author":"[deleted]","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3d6zzd","score":1,"approved_by":null,"report_reasons":null,"all_awardings":[],"subreddit_id":"t5_81eyvm","body":"[deleted]","edited":false,"author_flair_css_class":null,"collapsed":true,"downs":0,"is_submitter":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;[deleted]&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"associated_award":null,"stickied":false,"subreddit_type":"public","can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3dt0tr/","num_reports":null,"locked":false,"name":"t1_n3dt0tr","created":1752635507,"subreddit":"LocalLLaMA","author_flair_text":null,"treatment_tags":[],"created_utc":1752635507,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"","collapsed_because_crowd_control":null,"mod_reports":[],"mod_note":null,"distinguished":null}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3fmf0y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"valdev","can_mod_post":false,"created_utc":1752668168,"send_replies":true,"parent_id":"t1_n3d6zzd","score":1,"author_fullname":"t2_3oazk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Frankly, this might be a feature more than a bug. Those same dumb people used to instead google their basic thoughts and find the first blog and fall for conspiracy theories.\\n\\nAt least for the time being LLM's are generally... less nefarious.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3fmf0y","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Frankly, this might be a feature more than a bug. Those same dumb people used to instead google their basic thoughts and find the first blog and fall for conspiracy theories.&lt;/p&gt;\\n\\n&lt;p&gt;At least for the time being LLM&amp;#39;s are generally... less nefarious.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3fmf0y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752668168,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"more","data":{"count":1,"name":"t1_n3ej2lr","id":"n3ej2lr","parent_id":"t1_n3d6zzd","depth":1,"children":["n3ej2lr"]}}],"before":null}},"user_reports":[],"saved":false,"id":"n3d6zzd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"tgwombat","can_mod_post":false,"created_utc":1752627574,"send_replies":true,"parent_id":"t3_1m0z1zx","score":117,"author_fullname":"t2_4yxci","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"They're making people who rely on them stupider over time as they offload basic thought to a machine.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3d6zzd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They&amp;#39;re making people who rely on them stupider over time as they offload basic thought to a machine.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3d6zzd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752627574,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":117}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3dxis8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Revolutionalredstone","can_mod_post":false,"created_utc":1752637276,"send_replies":true,"parent_id":"t3_1m0z1zx","score":12,"author_fullname":"t2_6crrj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I use custom written automatic LLM evaluation.\\n\\nI often find models are good at one thing or another.\\n\\nEven 'idiots' accidentally upload amazing stuff sometime.\\n\\nI have no problem with the number of LLMs I wish there were more 😁!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dxis8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I use custom written automatic LLM evaluation.&lt;/p&gt;\\n\\n&lt;p&gt;I often find models are good at one thing or another.&lt;/p&gt;\\n\\n&lt;p&gt;Even &amp;#39;idiots&amp;#39; accidentally upload amazing stuff sometime.&lt;/p&gt;\\n\\n&lt;p&gt;I have no problem with the number of LLMs I wish there were more 😁!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3dxis8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752637276,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ebz7t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Deathcrow","can_mod_post":false,"created_utc":1752644088,"send_replies":true,"parent_id":"t3_1m0z1zx","score":11,"author_fullname":"t2_44gd3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt;Every community finetune I've used is always far worse than the base model. They always reduce the coherency, it's just a matter of how much. \\n\\nNot wrong, but most fine tunes are for special interests and ERP. Most base models are very neutered in that regard and lack the necessary vocabulary or shy away from anything slightly depraved. They are too goody-two-shoe and will not go there unless coaxed incessantly.\\n\\nCoherency/problem solving/etc. are decidedly not the goal for these (mostly) creative writing tunes.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ebz7t","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Every community finetune I&amp;#39;ve used is always far worse than the base model. They always reduce the coherency, it&amp;#39;s just a matter of how much. &lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Not wrong, but most fine tunes are for special interests and ERP. Most base models are very neutered in that regard and lack the necessary vocabulary or shy away from anything slightly depraved. They are too goody-two-shoe and will not go there unless coaxed incessantly.&lt;/p&gt;\\n\\n&lt;p&gt;Coherency/problem solving/etc. are decidedly not the goal for these (mostly) creative writing tunes.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3ebz7t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752644088,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":1,"name":"t1_n3frg6p","id":"n3frg6p","parent_id":"t1_n3f9lhc","depth":3,"children":["n3frg6p"]}}],"before":null}},"user_reports":[],"saved":false,"id":"n3f9lhc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"a_beautiful_rhind","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3f2le4","score":2,"author_fullname":"t2_h5utwre7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Correct, the model repeats part of what the user said instead of a true reply The immersion is definitely diminished when you see it. Sometimes it's elaborated on or \\"dressed up\\", if you will. Conversations generally require two participants or they get boring.\\n\\n:D","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3f9lhc","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Correct, the model repeats part of what the user said instead of a true reply The immersion is definitely diminished when you see it. Sometimes it&amp;#39;s elaborated on or &amp;quot;dressed up&amp;quot;, if you will. Conversations generally require two participants or they get boring.&lt;/p&gt;\\n\\n&lt;p&gt;:D&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3f9lhc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752662706,"author_flair_text":null,"treatment_tags":[],"created_utc":1752662706,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3f2le4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"s101c","can_mod_post":false,"created_utc":1752658985,"send_replies":true,"parent_id":"t1_n3dj8th","score":5,"author_fullname":"t2_rg6hb6my5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You mean, that the model repeats after user (even in subtle ways) and that ruins the immersive experience?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3f2le4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You mean, that the model repeats after user (even in subtle ways) and that ruins the immersive experience?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3f2le4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752658985,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3eloea","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"psychicprogrammer","can_mod_post":false,"created_utc":1752649232,"send_replies":true,"parent_id":"t1_n3dj8th","score":1,"author_fullname":"t2_vh67m","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"parroting?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3eloea","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;parroting?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3eloea/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752649232,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3dj8th","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1752631870,"send_replies":true,"parent_id":"t3_1m0z1zx","score":28,"author_fullname":"t2_h5utwre7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The parroting is off the charts but nobody seems to care/notice. Yet the most common uses after coding are gooning/chatting. People don't mind constantly reading themselves, while they vocally complain about \\"slop\\".","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dj8th","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The parroting is off the charts but nobody seems to care/notice. Yet the most common uses after coding are gooning/chatting. People don&amp;#39;t mind constantly reading themselves, while they vocally complain about &amp;quot;slop&amp;quot;.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3dj8th/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752631870,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":28}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3dynl6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"anobfuscator","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3dn5gf","score":6,"author_fullname":"t2_nxkfm","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Eric Hartford, he makes Dolphin.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3dynl6","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Eric Hartford, he makes Dolphin.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3dynl6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752637736,"author_flair_text":null,"treatment_tags":[],"created_utc":1752637736,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n3dn5gf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Lazy-Pattern-5171","can_mod_post":false,"created_utc":1752633289,"send_replies":true,"parent_id":"t1_n3dabb4","score":3,"author_fullname":"t2_1lyjk8is25","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I’m not sure if it’s Nous Research or Dolphin but the original intent behind needing uncensored models when there was community backlash pretty much came from those guys and their work. Eric Chapman? Eric something? I forget his name.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dn5gf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I’m not sure if it’s Nous Research or Dolphin but the original intent behind needing uncensored models when there was community backlash pretty much came from those guys and their work. Eric Chapman? Eric something? I forget his name.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3dn5gf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752633289,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n3dabb4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Vast_Yak_4147","can_mod_post":false,"created_utc":1752628743,"send_replies":true,"parent_id":"t3_1m0z1zx","score":37,"author_fullname":"t2_15fbubu1ww","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"try Nous Research finetunes, they are great uncensored reasoning versions of the base models. agreed with the rest and the finetune point for the most part","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dabb4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;try Nous Research finetunes, they are great uncensored reasoning versions of the base models. agreed with the rest and the finetune point for the most part&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3dabb4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752628743,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":37}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3dw5l5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Yu2sama","can_mod_post":false,"created_utc":1752636732,"send_replies":true,"parent_id":"t3_1m0z1zx","score":7,"author_fullname":"t2_uu7xvge","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Most models are fine at writing with the correct prompt, even smaller ones (though evidently less intelligent).\\n\\nAs models grow more intelligent, prompts \\"hacks\\" are less shared.\\n\\nI agree to a certain extend on the last one, but Gemma Sunshine has been the only fucking Gemma model capable of absorbing style of an example. Intelligence wise is probably subpar.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dw5l5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Most models are fine at writing with the correct prompt, even smaller ones (though evidently less intelligent).&lt;/p&gt;\\n\\n&lt;p&gt;As models grow more intelligent, prompts &amp;quot;hacks&amp;quot; are less shared.&lt;/p&gt;\\n\\n&lt;p&gt;I agree to a certain extend on the last one, but Gemma Sunshine has been the only fucking Gemma model capable of absorbing style of an example. Intelligence wise is probably subpar.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3dw5l5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752636732,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"2b12e2b8-fdc0-11ee-9a03-6e2f48afd456","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ehz1x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Olangotang","can_mod_post":false,"created_utc":1752647240,"send_replies":true,"parent_id":"t1_n3dav8j","score":7,"author_fullname":"t2_6jlis","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This generation of 'AI' is sadly just corporate stupidity. The AI 2027 shit is brain dead.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ehz1x","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 3"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This generation of &amp;#39;AI&amp;#39; is sadly just corporate stupidity. The AI 2027 shit is brain dead.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3ehz1x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752647240,"author_flair_text":"Llama 3","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#c7b594","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3e12z7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Paganator","can_mod_post":false,"created_utc":1752638721,"send_replies":true,"parent_id":"t1_n3dav8j","score":17,"author_fullname":"t2_a6wit","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Current LLMs are closer to Eliza than AGI is to current LLMs.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3e12z7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Current LLMs are closer to Eliza than AGI is to current LLMs.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3e12z7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752638721,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":17}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3fid1h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"familyknewmyusername","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3f9xjh","score":5,"author_fullname":"t2_lzton","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"That's the point. For a long time playing chess was considered AI. The problem is, we define AI as \\"things humans can do that computers can't do\\"\\n\\nWhich means any time a computer is able to do it, the goalposts move","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3fid1h","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s the point. For a long time playing chess was considered AI. The problem is, we define AI as &amp;quot;things humans can do that computers can&amp;#39;t do&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;Which means any time a computer is able to do it, the goalposts move&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3fid1h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752666593,"author_flair_text":null,"treatment_tags":[],"created_utc":1752666593,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n3f9xjh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"geenob","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3dgaj7","score":10,"author_fullname":"t2_50hztf91","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"In those days and until recently, the Turing test was the litmus test for AGI. Now, that's not good enough.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3f9xjh","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;In those days and until recently, the Turing test was the litmus test for AGI. Now, that&amp;#39;s not good enough.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3f9xjh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752662868,"author_flair_text":null,"treatment_tags":[],"created_utc":1752662868,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}}],"before":null}},"user_reports":[],"saved":false,"id":"n3dgaj7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Ardalok","can_mod_post":false,"created_utc":1752630832,"send_replies":true,"parent_id":"t1_n3dav8j","score":32,"author_fullname":"t2_vgnewja","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"We keep pushing the definition of AGI further with every new model. If you asked people in the 1960s what AGI was and then showed them GPT-4, they would say it is AGI.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dgaj7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;We keep pushing the definition of AGI further with every new model. If you asked people in the 1960s what AGI was and then showed them GPT-4, they would say it is AGI.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3dgaj7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752630832,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":32}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":1,"name":"t1_n3e5j22","id":"n3e5j22","parent_id":"t1_n3df424","depth":3,"children":["n3e5j22"]}}],"before":null}},"user_reports":[],"saved":false,"id":"n3df424","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"orrzxz","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3dbxv5","score":12,"author_fullname":"t2_huzar","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I fear the statistics way more than I fear the sentient.\\n\\nWhat we have currently is potentially the best tool for professionals to do anything. That means coding, b-roll, summarizes, writing, predicting, following, analyzing, anything you can think of no matter how good or bad it is. The neural network doesn't care, it just learns to do whatever to the best of its abilities. If it learns to predict market trends, it will send them to you. If it'll learn how to code, it'll make your work easier. Teach it to identify someone at a crowd, he'll never be able to hide from you. Teach it to calculate wind, elevation and distance, and it'll kill anyone from any distance.\\n\\nSo, honestly, giving it the ability to think, judge and act independently, sounds like a safe upgrade to me. It's a win win - it either just refuses to do shitty things, or it inst-nukes us all. First case sounds great, second case sounds better then sitting in a slow boiling pot for the next couple decades.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3df424","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I fear the statistics way more than I fear the sentient.&lt;/p&gt;\\n\\n&lt;p&gt;What we have currently is potentially the best tool for professionals to do anything. That means coding, b-roll, summarizes, writing, predicting, following, analyzing, anything you can think of no matter how good or bad it is. The neural network doesn&amp;#39;t care, it just learns to do whatever to the best of its abilities. If it learns to predict market trends, it will send them to you. If it&amp;#39;ll learn how to code, it&amp;#39;ll make your work easier. Teach it to identify someone at a crowd, he&amp;#39;ll never be able to hide from you. Teach it to calculate wind, elevation and distance, and it&amp;#39;ll kill anyone from any distance.&lt;/p&gt;\\n\\n&lt;p&gt;So, honestly, giving it the ability to think, judge and act independently, sounds like a safe upgrade to me. It&amp;#39;s a win win - it either just refuses to do shitty things, or it inst-nukes us all. First case sounds great, second case sounds better then sitting in a slow boiling pot for the next couple decades.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3df424/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752630427,"author_flair_text":null,"treatment_tags":[],"created_utc":1752630427,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}}],"before":null}},"user_reports":[],"saved":false,"id":"n3dbxv5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"tgwombat","can_mod_post":false,"created_utc":1752629317,"send_replies":true,"parent_id":"t1_n3dav8j","score":12,"author_fullname":"t2_4yxci","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Bad marketing labeling non-AI as AI is definitely going to set back any research into actual artificial intelligence by decades. I’m not so sure that’s a bad thing though.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dbxv5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Bad marketing labeling non-AI as AI is definitely going to set back any research into actual artificial intelligence by decades. I’m not so sure that’s a bad thing though.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3dbxv5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752629317,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3foydc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"pab_guy","can_mod_post":false,"created_utc":1752669115,"send_replies":true,"parent_id":"t1_n3dav8j","score":1,"author_fullname":"t2_3gt8c","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Literally everything in the universe can be modeled with “fancy statistics”… it’s a meaningless criticism and implies an inability to generalize beyond training data, which we know is something models can in fact do.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3foydc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Literally everything in the universe can be modeled with “fancy statistics”… it’s a meaningless criticism and implies an inability to generalize beyond training data, which we know is something models can in fact do.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3foydc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752669115,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"more","data":{"count":4,"name":"t1_n3deihu","id":"n3deihu","parent_id":"t1_n3dav8j","depth":1,"children":["n3deihu"]}}],"before":null}},"user_reports":[],"saved":false,"id":"n3dav8j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"orrzxz","can_mod_post":false,"created_utc":1752628939,"send_replies":true,"parent_id":"t3_1m0z1zx","score":74,"author_fullname":"t2_huzar","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"We aren't close to agi, nor will we ever get there, if we continue touting fancy statistics/auto-complete as 'AI'.\\n\\nWhat we've achieved is incredible. But if the goal truly is AGI, we've grown stagnant and complacent.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dav8j","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;We aren&amp;#39;t close to agi, nor will we ever get there, if we continue touting fancy statistics/auto-complete as &amp;#39;AI&amp;#39;.&lt;/p&gt;\\n\\n&lt;p&gt;What we&amp;#39;ve achieved is incredible. But if the goal truly is AGI, we&amp;#39;ve grown stagnant and complacent.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3dav8j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752628939,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":74}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3dggcq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"mrtime777","can_mod_post":false,"created_utc":1752630889,"send_replies":true,"parent_id":"t1_n3ddz63","score":12,"author_fullname":"t2_37pn0768","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"AGI is a lazy cat","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dggcq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;AGI is a lazy cat&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3dggcq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752630889,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3fa2sp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Crisis_Averted","can_mod_post":false,"created_utc":1752662939,"send_replies":true,"parent_id":"t1_n3f8bmz","score":3,"author_fullname":"t2_6xie9","approved_by":null,"mod_note":null,"all_awardings":[],"body":"agreed on all accounts. but even if I disagreed 100%, I'd never downvote the reply. you were asked to interact. you interacted, professionally. you got ganged up on.\\n\\nnothing new or rare. Just so profoundly idiotic.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n3fa2sp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;agreed on all accounts. but even if I disagreed 100%, I&amp;#39;d never downvote the reply. you were asked to interact. you interacted, professionally. you got ganged up on.&lt;/p&gt;\\n\\n&lt;p&gt;nothing new or rare. Just so profoundly idiotic.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m0z1zx","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3fa2sp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752662939,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n3f8bmz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No-Search9350","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3f02wz","score":4,"author_fullname":"t2_1b9gox1vsw","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Normal.\\n\\nThis topic is unpopular and offensive to many because it suggests there's nothing special about human intelligence, which leads to a second mistake most people make: confusing qualia with intelligence, totally ignoring philosophical zombies.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n3f8bmz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Normal.&lt;/p&gt;\\n\\n&lt;p&gt;This topic is unpopular and offensive to many because it suggests there&amp;#39;s nothing special about human intelligence, which leads to a second mistake most people make: confusing qualia with intelligence, totally ignoring philosophical zombies.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m0z1zx","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3f8bmz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752662062,"author_flair_text":null,"treatment_tags":[],"created_utc":1752662062,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n3f02wz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Crisis_Averted","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3e1p36","score":4,"author_fullname":"t2_6xie9","approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt;I’m curious about what you think\\n\\n&gt;person explains what they think\\n\\n&gt;gets downvoted\\n\\nfucking humans.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3f02wz","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;I’m curious about what you think&lt;/p&gt;\\n\\n&lt;p&gt;person explains what they think&lt;/p&gt;\\n\\n&lt;p&gt;gets downvoted&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;fucking humans.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m0z1zx","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3f02wz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752657528,"author_flair_text":null,"treatment_tags":[],"created_utc":1752657528,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n3e1p36","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No-Search9350","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3e03xu","score":8,"author_fullname":"t2_1b9gox1vsw","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"My opinion is that all forms of intelligence boil down to the same mathematical principles. The difference between something like an ant, a deer, a human, or even a machine just comes down to its form (like different types of brains, LLMs, or how they take in information) and its depth of smarts (meaning how capable it is).\\n\\nI don't actually think these are truly \\"general\\" intelligences because they all evolved to learn and adapt under very specific circumstances. And, surprisingly, that's not really any different from humans. If we really dig into it, I don't believe humans have true general intelligence either.\\n\\nWe, like animals, are **extremely limited**.","edited":false,"author_flair_css_class":null,"name":"t1_n3e1p36","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;My opinion is that all forms of intelligence boil down to the same mathematical principles. The difference between something like an ant, a deer, a human, or even a machine just comes down to its form (like different types of brains, LLMs, or how they take in information) and its depth of smarts (meaning how capable it is).&lt;/p&gt;\\n\\n&lt;p&gt;I don&amp;#39;t actually think these are truly &amp;quot;general&amp;quot; intelligences because they all evolved to learn and adapt under very specific circumstances. And, surprisingly, that&amp;#39;s not really any different from humans. If we really dig into it, I don&amp;#39;t believe humans have true general intelligence either.&lt;/p&gt;\\n\\n&lt;p&gt;We, like animals, are &lt;strong&gt;extremely limited&lt;/strong&gt;.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m0z1zx","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3e1p36/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752638983,"author_flair_text":null,"collapsed":false,"created_utc":1752638983,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"more","data":{"count":1,"name":"t1_n3e1kb4","id":"n3e1kb4","parent_id":"t1_n3e03xu","depth":4,"children":["n3e1kb4"]}}],"before":null}},"user_reports":[],"saved":false,"id":"n3e03xu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"pseudonerv","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3dwkpc","score":4,"author_fullname":"t2_eerln","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I’m curious about what you think of the intelligence of general animals. Are those general intelligence?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3e03xu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I’m curious about what you think of the intelligence of general animals. Are those general intelligence?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3e03xu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752638316,"author_flair_text":null,"treatment_tags":[],"created_utc":1752638316,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n3dwkpc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No-Search9350","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3dtatf","score":6,"author_fullname":"t2_1b9gox1vsw","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I have to disagree. This is not about getting pedantic. This is calling blue blue and red red. AGI's definition as a system capable of versatile learning across domains is distinct from Strong AI or superintelligence. The very fact that you stated this: 'once you give me a falsifiable test procedure,' shows that you are taking AGI for what it is not. AGI is a precisely defined conceptual framework focused on cognitive versatility across any domain, not a subjective debate about 'human-like or god-like intelligence.' This conflation obscures its established technical definition. A falsifiable test for AGI would evaluate a system's ability to autonomously perform any cognitive task at preestablished proficiency across diverse domains, distinct from assessments designed to measure Strong AI's true understanding, narrow AI's domain-specific performance, or superintelligence's beyond-human cognitive capacity. To call this unimportant is the same as dismissing the distinction between mitosis and meiosis in biology, where conflating the two would lead to flawed understandings of cell division and reproduction.\\n\\nAnd yes, everyone is free to think as they want. If they want to call GPT-2 AGI, Sonnet-4 narrow AI, or red yellow, I couldn’t care less.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3dwkpc","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have to disagree. This is not about getting pedantic. This is calling blue blue and red red. AGI&amp;#39;s definition as a system capable of versatile learning across domains is distinct from Strong AI or superintelligence. The very fact that you stated this: &amp;#39;once you give me a falsifiable test procedure,&amp;#39; shows that you are taking AGI for what it is not. AGI is a precisely defined conceptual framework focused on cognitive versatility across any domain, not a subjective debate about &amp;#39;human-like or god-like intelligence.&amp;#39; This conflation obscures its established technical definition. A falsifiable test for AGI would evaluate a system&amp;#39;s ability to autonomously perform any cognitive task at preestablished proficiency across diverse domains, distinct from assessments designed to measure Strong AI&amp;#39;s true understanding, narrow AI&amp;#39;s domain-specific performance, or superintelligence&amp;#39;s beyond-human cognitive capacity. To call this unimportant is the same as dismissing the distinction between mitosis and meiosis in biology, where conflating the two would lead to flawed understandings of cell division and reproduction.&lt;/p&gt;\\n\\n&lt;p&gt;And yes, everyone is free to think as they want. If they want to call GPT-2 AGI, Sonnet-4 narrow AI, or red yellow, I couldn’t care less.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3dwkpc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752636903,"author_flair_text":null,"treatment_tags":[],"created_utc":1752636903,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n3dtatf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"StewedAngelSkins","can_mod_post":false,"created_utc":1752635614,"send_replies":false,"parent_id":"t1_n3ddz63","score":19,"author_fullname":"t2_fxk6v95z","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"none of this ever had any empirical meaning in the first place, so it's really not worth getting pedantic about. we can talk about whether something is AGI once you give me a falsifiable test procedure. until then AGI is whatever i want it to be today.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dtatf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;none of this ever had any empirical meaning in the first place, so it&amp;#39;s really not worth getting pedantic about. we can talk about whether something is AGI once you give me a falsifiable test procedure. until then AGI is whatever i want it to be today.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3dtatf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752635614,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":19}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3f64vh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No-Search9350","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3ehw6r","score":1,"author_fullname":"t2_1b9gox1vsw","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"But it is actually that. Intelligence is, at its core, the search for the optimal data compression function for a given dataset. That means finding the most concise representation that preserves predictive structure. This idea is tightly linked to Shannon entropy, which quantifies the uncertainty or information content of a distribution and sets the theoretical limit for how much compression is possible. Intelligent systems distinguish themselves by efficiently navigating the space of possible representations to reduce this uncertainty while maintaining the ability to predict and model their environment effectively.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3f64vh","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;But it is actually that. Intelligence is, at its core, the search for the optimal data compression function for a given dataset. That means finding the most concise representation that preserves predictive structure. This idea is tightly linked to Shannon entropy, which quantifies the uncertainty or information content of a distribution and sets the theoretical limit for how much compression is possible. Intelligent systems distinguish themselves by efficiently navigating the space of possible representations to reduce this uncertainty while maintaining the ability to predict and model their environment effectively.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3f64vh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752660917,"author_flair_text":null,"treatment_tags":[],"created_utc":1752660917,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ehw6r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"visarga","can_mod_post":false,"created_utc":1752647198,"send_replies":true,"parent_id":"t1_n3ddz63","score":4,"author_fullname":"t2_vxvm","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"My take is that we are missing the core of intelligence - it is not the model, not the brain - it is a search process. So it is mostly about exploring problem spaces. Think about evolution - it has no intelligence at all, pure search, and yet it made us and everything. \\n\\nAlphaZero beat us at go but it trained using search. When we focus on the model we lose the environment loop, and can no longer make meaningful statements about intelligence. Maybe intelligence itself is not well defined, it's just efficient search, always contextual, not general. The G in AGI makes no sense.\\n\\nBenchmarks test the static heuristic function in isolation, not its ability to guide a meaningful search in a real environment. The gooners who are praised for their rigorous testing aren't running MMLU, they are engaging the model in a long, interactive \\"search\\" for a coherent narrative or persona.","edited":1752647473,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ehw6r","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;My take is that we are missing the core of intelligence - it is not the model, not the brain - it is a search process. So it is mostly about exploring problem spaces. Think about evolution - it has no intelligence at all, pure search, and yet it made us and everything. &lt;/p&gt;\\n\\n&lt;p&gt;AlphaZero beat us at go but it trained using search. When we focus on the model we lose the environment loop, and can no longer make meaningful statements about intelligence. Maybe intelligence itself is not well defined, it&amp;#39;s just efficient search, always contextual, not general. The G in AGI makes no sense.&lt;/p&gt;\\n\\n&lt;p&gt;Benchmarks test the static heuristic function in isolation, not its ability to guide a meaningful search in a real environment. The gooners who are praised for their rigorous testing aren&amp;#39;t running MMLU, they are engaging the model in a long, interactive &amp;quot;search&amp;quot; for a coherent narrative or persona.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3ehw6r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752647198,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3f7o8d","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No-Search9350","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3esp6d","score":2,"author_fullname":"t2_1b9gox1vsw","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I completely agree; I also believe there are \\"degrees to AGI,\\" and that correlates to a greater or lesser degree with the range from \\"weak AI to Strong AI,\\" even though they are not the same.\\n\\nPeople are already using \\"weak forms of AGI,\\" even though it's not Strong AI. I believe Strong AI already exists, though it's still only for a selected few (government, CEOs, scientists, etc.) for now. But it's already here.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3f7o8d","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I completely agree; I also believe there are &amp;quot;degrees to AGI,&amp;quot; and that correlates to a greater or lesser degree with the range from &amp;quot;weak AI to Strong AI,&amp;quot; even though they are not the same.&lt;/p&gt;\\n\\n&lt;p&gt;People are already using &amp;quot;weak forms of AGI,&amp;quot; even though it&amp;#39;s not Strong AI. I believe Strong AI already exists, though it&amp;#39;s still only for a selected few (government, CEOs, scientists, etc.) for now. But it&amp;#39;s already here.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3f7o8d/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752661723,"author_flair_text":null,"treatment_tags":[],"created_utc":1752661723,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3esp6d","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FrostAutomaton","can_mod_post":false,"created_utc":1752653202,"send_replies":true,"parent_id":"t1_n3ddz63","score":3,"author_fullname":"t2_1e9th1d970","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Fully agree. I would absolutely argue that current LLMs are a form of (very weak) AGI. They are capable of, for example, playing the original Pokémon games in a completely novel manner despite this being out-of-distribution.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3esp6d","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Fully agree. I would absolutely argue that current LLMs are a form of (very weak) AGI. They are capable of, for example, playing the original Pokémon games in a completely novel manner despite this being out-of-distribution.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3esp6d/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752653202,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3fa63y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No-Search9350","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3esnzo","score":1,"author_fullname":"t2_1b9gox1vsw","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The ARC benchmark uses a specific definition of AGI focused on zero-shot performance in isolated abstract tasks. That is not the definition I used. I referred to AGI as the capacity to generalize across tasks when placed in a structured system with memory and interaction. A model failing ARC does not contradict this, because ARC does not evaluate system-level behavior. The disagreement comes from relying on a narrow interpretation of AGI that equates it with instant generalization without context or tools. This is not the same as denying that a model can generalize when embedded in a broader system supported by memory, interaction, and reflection.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3fa63y","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The ARC benchmark uses a specific definition of AGI focused on zero-shot performance in isolated abstract tasks. That is not the definition I used. I referred to AGI as the capacity to generalize across tasks when placed in a structured system with memory and interaction. A model failing ARC does not contradict this, because ARC does not evaluate system-level behavior. The disagreement comes from relying on a narrow interpretation of AGI that equates it with instant generalization without context or tools. This is not the same as denying that a model can generalize when embedded in a broader system supported by memory, interaction, and reflection.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3fa63y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752662982,"author_flair_text":null,"treatment_tags":[],"created_utc":1752662982,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3esnzo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Apprehensive_Win662","can_mod_post":false,"created_utc":1752653182,"send_replies":true,"parent_id":"t1_n3ddz63","score":2,"author_fullname":"t2_1d2tnevqjs","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"\\"Technically, one could argue that AGI has already emerged.\\"\\n\\nARC AGI benchmarks show exactly the opposite. Everytime they come up with an easy task for humans, AI cannot do it at all. &lt;waiting for ARC AGI III&gt;","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3esnzo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&amp;quot;Technically, one could argue that AGI has already emerged.&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;ARC AGI benchmarks show exactly the opposite. Everytime they come up with an easy task for humans, AI cannot do it at all. &amp;lt;waiting for ARC AGI III&amp;gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3esnzo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752653182,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ddz63","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No-Search9350","can_mod_post":false,"created_utc":1752630037,"send_replies":true,"parent_id":"t3_1m0z1zx","score":37,"author_fullname":"t2_1b9gox1vsw","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"AGI is not the same as Strong AI. AGI refers to the potential of an artificial system to learn and apply knowledge across any domain, demonstrating cognitive versatility and generalization. Strong AI, by contrast, denotes the state of a system being genuinely intelligent or possessing true understanding, independent of the range of tasks it can perform.\\n\\nThe term AGI has been so thoroughly distorted by journalists and individuals with no background in artificial intelligence that it has lost definitional clarity. Most people no longer understand what AGI refers to. They confuse it with Strong AI, mistake it for narrow AI, or conflate it with the concept of superintelligence. In many cases, they cannot even define narrow intelligence.\\n\\nTechnically, one could argue that AGI has already emerged. A model such as Sonnet-4 possesses the latent capacity to perform the full range of human cognitive tasks, provided it is embedded within a structured system of Model Context Protocols and granted physical autonomy.\\n\\nSuperintelligence, though, is a WHOLE different thing.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ddz63","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;AGI is not the same as Strong AI. AGI refers to the potential of an artificial system to learn and apply knowledge across any domain, demonstrating cognitive versatility and generalization. Strong AI, by contrast, denotes the state of a system being genuinely intelligent or possessing true understanding, independent of the range of tasks it can perform.&lt;/p&gt;\\n\\n&lt;p&gt;The term AGI has been so thoroughly distorted by journalists and individuals with no background in artificial intelligence that it has lost definitional clarity. Most people no longer understand what AGI refers to. They confuse it with Strong AI, mistake it for narrow AI, or conflate it with the concept of superintelligence. In many cases, they cannot even define narrow intelligence.&lt;/p&gt;\\n\\n&lt;p&gt;Technically, one could argue that AGI has already emerged. A model such as Sonnet-4 possesses the latent capacity to perform the full range of human cognitive tasks, provided it is embedded within a structured system of Model Context Protocols and granted physical autonomy.&lt;/p&gt;\\n\\n&lt;p&gt;Superintelligence, though, is a WHOLE different thing.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3ddz63/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752630037,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":37}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3etinj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"sneakpeekbot","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3ethru","score":1,"author_fullname":"t2_140r4p","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Here's a sneak peek of /r/singularity using the [top posts](https://np.reddit.com/r/singularity/top/?sort=top&amp;t=year) of the year!\\n\\n\\\\#1: [Yann LeCun Elon Musk exchange.](https://i.redd.it/70er5d5m553d1.png) | [1148 comments](https://np.reddit.com/r/singularity/comments/1d2fvyr/yann_lecun_elon_musk_exchange/)  \\n\\\\#2: [Berkeley Professor Says Even His ‘Outstanding’ Students aren’t Getting Any Job Offers — ‘I Suspect This Trend Is Irreversible’](https://www.yourtango.com/sekf/berkeley-professor-says-even-outstanding-students-arent-getting-jobs) | [1956 comments](https://np.reddit.com/r/singularity/comments/1guwwyq/berkeley_professor_says_even_his_outstanding/)  \\n\\\\#3: [Man Arrested for Creating Fake Bands With AI, Then Making $10 Million by Listening to Their Songs With Bots](https://futurism.com/man-arrested-fake-bands-streams-ai) | [887 comments](https://np.reddit.com/r/singularity/comments/1fb51vp/man_arrested_for_creating_fake_bands_with_ai_then/)\\n\\n----\\n^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| ^^[Contact](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| ^^[Info](https://np.reddit.com/r/sneakpeekbot/) ^^| ^^[Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/o8wk1r/blacklist_ix/) ^^| ^^[GitHub](https://github.com/ghnr/sneakpeekbot)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3etinj","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Here&amp;#39;s a sneak peek of &lt;a href=\\"/r/singularity\\"&gt;/r/singularity&lt;/a&gt; using the &lt;a href=\\"https://np.reddit.com/r/singularity/top/?sort=top&amp;amp;t=year\\"&gt;top posts&lt;/a&gt; of the year!&lt;/p&gt;\\n\\n&lt;p&gt;#1: &lt;a href=\\"https://i.redd.it/70er5d5m553d1.png\\"&gt;Yann LeCun Elon Musk exchange.&lt;/a&gt; | &lt;a href=\\"https://np.reddit.com/r/singularity/comments/1d2fvyr/yann_lecun_elon_musk_exchange/\\"&gt;1148 comments&lt;/a&gt;&lt;br/&gt;\\n#2: &lt;a href=\\"https://www.yourtango.com/sekf/berkeley-professor-says-even-outstanding-students-arent-getting-jobs\\"&gt;Berkeley Professor Says Even His ‘Outstanding’ Students aren’t Getting Any Job Offers — ‘I Suspect This Trend Is Irreversible’&lt;/a&gt; | &lt;a href=\\"https://np.reddit.com/r/singularity/comments/1guwwyq/berkeley_professor_says_even_his_outstanding/\\"&gt;1956 comments&lt;/a&gt;&lt;br/&gt;\\n#3: &lt;a href=\\"https://futurism.com/man-arrested-fake-bands-streams-ai\\"&gt;Man Arrested for Creating Fake Bands With AI, Then Making $10 Million by Listening to Their Songs With Bots&lt;/a&gt; | &lt;a href=\\"https://np.reddit.com/r/singularity/comments/1fb51vp/man_arrested_for_creating_fake_bands_with_ai_then/\\"&gt;887 comments&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;p&gt;&lt;sup&gt;&lt;sup&gt;I&amp;#39;m&lt;/sup&gt;&lt;/sup&gt; &lt;sup&gt;&lt;sup&gt;a&lt;/sup&gt;&lt;/sup&gt; &lt;sup&gt;&lt;sup&gt;bot,&lt;/sup&gt;&lt;/sup&gt; &lt;sup&gt;&lt;sup&gt;beep&lt;/sup&gt;&lt;/sup&gt; &lt;sup&gt;&lt;sup&gt;boop&lt;/sup&gt;&lt;/sup&gt; &lt;sup&gt;&lt;sup&gt;|&lt;/sup&gt;&lt;/sup&gt; &lt;sup&gt;&lt;sup&gt;Downvote&lt;/sup&gt;&lt;/sup&gt; &lt;sup&gt;&lt;sup&gt;to&lt;/sup&gt;&lt;/sup&gt; &lt;sup&gt;&lt;sup&gt;remove&lt;/sup&gt;&lt;/sup&gt; &lt;sup&gt;&lt;sup&gt;|&lt;/sup&gt;&lt;/sup&gt; &lt;sup&gt;&lt;sup&gt;&lt;a href=\\"https://www.reddit.com/message/compose/?to=sneakpeekbot\\"&gt;Contact&lt;/a&gt;&lt;/sup&gt;&lt;/sup&gt; &lt;sup&gt;&lt;sup&gt;|&lt;/sup&gt;&lt;/sup&gt; &lt;sup&gt;&lt;sup&gt;&lt;a href=\\"https://np.reddit.com/r/sneakpeekbot/\\"&gt;Info&lt;/a&gt;&lt;/sup&gt;&lt;/sup&gt; &lt;sup&gt;&lt;sup&gt;|&lt;/sup&gt;&lt;/sup&gt; &lt;sup&gt;&lt;sup&gt;&lt;a href=\\"https://np.reddit.com/r/sneakpeekbot/comments/o8wk1r/blacklist_ix/\\"&gt;Opt-out&lt;/a&gt;&lt;/sup&gt;&lt;/sup&gt; &lt;sup&gt;&lt;sup&gt;|&lt;/sup&gt;&lt;/sup&gt; &lt;sup&gt;&lt;sup&gt;&lt;a href=\\"https://github.com/ghnr/sneakpeekbot\\"&gt;GitHub&lt;/a&gt;&lt;/sup&gt;&lt;/sup&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3etinj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752653679,"author_flair_text":null,"treatment_tags":[],"created_utc":1752653679,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ethru","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1752653665,"send_replies":true,"parent_id":"t1_n3edijj","score":5,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Exactly, even /r/singularity has arrived to this conclusion.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ethru","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Exactly, even &lt;a href=\\"/r/singularity\\"&gt;/r/singularity&lt;/a&gt; has arrived to this conclusion.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3ethru/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752653665,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n3edijj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"prisencotech","can_mod_post":false,"created_utc":1752644883,"send_replies":true,"parent_id":"t3_1m0z1zx","score":10,"author_fullname":"t2_tfv517bu8","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"LLMs and diffusion models are tools for experts and that makes them useful in the hands of people with domain knowledge. The more domain knowledge, the more useful. Someone with no background in chemistry will not use them effectively in matters of chemistry. Same with programming, same with journalism, same with fiction writing, [and so on](https://i.imgur.com/87EoJ98.jpeg). They are the equivalent of a high tech automatic band saw in the hands of a master carpenter.\\n\\nBut that means that AI startups are priced incorrectly. Because the investment capital is priced not like they are tools for experts, but like they are labor-eliminating *everything machines*. It will cure diseases, make people obsolete, replace Hollywood and allow massive corporations to make a trillion dollars with nothing but a board of directors.\\n\\nBut we all know that's not true, but \\"a tool for experts\\" is not nearly as lucrative of a market as an everything machine. So my unpopular take is that the backend economics of AI are extremely treacherous and the hype and overinvestment may lead us into an AI winter when we could have had a nice, mild AI spring if we had just kept our expectations within reason.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3edijj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;LLMs and diffusion models are tools for experts and that makes them useful in the hands of people with domain knowledge. The more domain knowledge, the more useful. Someone with no background in chemistry will not use them effectively in matters of chemistry. Same with programming, same with journalism, same with fiction writing, &lt;a href=\\"https://i.imgur.com/87EoJ98.jpeg\\"&gt;and so on&lt;/a&gt;. They are the equivalent of a high tech automatic band saw in the hands of a master carpenter.&lt;/p&gt;\\n\\n&lt;p&gt;But that means that AI startups are priced incorrectly. Because the investment capital is priced not like they are tools for experts, but like they are labor-eliminating &lt;em&gt;everything machines&lt;/em&gt;. It will cure diseases, make people obsolete, replace Hollywood and allow massive corporations to make a trillion dollars with nothing but a board of directors.&lt;/p&gt;\\n\\n&lt;p&gt;But we all know that&amp;#39;s not true, but &amp;quot;a tool for experts&amp;quot; is not nearly as lucrative of a market as an everything machine. So my unpopular take is that the backend economics of AI are extremely treacherous and the hype and overinvestment may lead us into an AI winter when we could have had a nice, mild AI spring if we had just kept our expectations within reason.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3edijj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752644883,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3flhjb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"dobomex761604","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3f2m5q","score":2,"author_fullname":"t2_o0bqaw8dp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Back in a day, when ChatML was becoming popular, it was one of the least natural (if not the least) chat formats for LLMs. Constructions like &lt;| and the whole structure forced the roles of \\\\\`user\\\\\` and \\\\\`assistant\\\\\`, which made it the most constrained format in terms of roles, but also, technically speaking, the most effective.\\n\\nNowadays it's impossible to set a different role with most templates that are used (they are either modifications of ChatML or mimic it), which reduced flexibility of models. For example, old LLMs could work with \\\\\`### \\\\*role name\\\\*:\\\\\` instead of \\\\\`### Instruction:\\\\\`, while modern models will break if you try \\\\\`&lt;|im\\\\_start|&gt;\\\\*role name\\\\*\\\\\`. Sure, you can add a name after \\\\\`&lt;|im\\\\_start|&gt;assistant\\\\\`, but \\\\\`assistant\\\\\` itself is not a special token and will be tokenized as a word, affecting the overall context.\\n\\nMistral's prompt formats were always unique to their models, so they don't count - but Alpaca and other similar, more \\"natural\\" chat formats, died. As a result, we teach LLMs to always work with specific tags and predefined roles instead of natural text, which is against NLP to some degree.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3flhjb","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Back in a day, when ChatML was becoming popular, it was one of the least natural (if not the least) chat formats for LLMs. Constructions like &amp;lt;| and the whole structure forced the roles of \`user\` and \`assistant\`, which made it the most constrained format in terms of roles, but also, technically speaking, the most effective.&lt;/p&gt;\\n\\n&lt;p&gt;Nowadays it&amp;#39;s impossible to set a different role with most templates that are used (they are either modifications of ChatML or mimic it), which reduced flexibility of models. For example, old LLMs could work with \`### *role name*:\` instead of \`### Instruction:\`, while modern models will break if you try \`&amp;lt;|im_start|&amp;gt;*role name*\`. Sure, you can add a name after \`&amp;lt;|im_start|&amp;gt;assistant\`, but \`assistant\` itself is not a special token and will be tokenized as a word, affecting the overall context.&lt;/p&gt;\\n\\n&lt;p&gt;Mistral&amp;#39;s prompt formats were always unique to their models, so they don&amp;#39;t count - but Alpaca and other similar, more &amp;quot;natural&amp;quot; chat formats, died. As a result, we teach LLMs to always work with specific tags and predefined roles instead of natural text, which is against NLP to some degree.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3flhjb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752667815,"author_flair_text":null,"treatment_tags":[],"created_utc":1752667815,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3f2m5q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"cobbleplox","can_mod_post":false,"created_utc":1752658997,"send_replies":true,"parent_id":"t1_n3eiuc3","score":1,"author_fullname":"t2_1fr9j8kp8a","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; ChatML format was a mistake that keeps community back.\\n\\nI would be interested in why you think that. I can see how it might be limiting in some ways, but usually when I encounter other formats they are just more complicated and offer zero additional functionality, usually less.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3f2m5q","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;ChatML format was a mistake that keeps community back.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I would be interested in why you think that. I can see how it might be limiting in some ways, but usually when I encounter other formats they are just more complicated and offer zero additional functionality, usually less.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3f2m5q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752658997,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3eiuc3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"dobomex761604","can_mod_post":false,"created_utc":1752647712,"send_replies":true,"parent_id":"t3_1m0z1zx","score":5,"author_fullname":"t2_o0bqaw8dp","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"1. LLMs should be more universal than they are and be expected to have stable quality in any text-related field.\\n\\n2. Reasoning was a fun experiment, but is a terrible practice nowadays. No model below 100B benefits from it.\\n\\n3. ChatML format was a mistake that keeps community back.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3eiuc3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;ol&gt;\\n&lt;li&gt;&lt;p&gt;LLMs should be more universal than they are and be expected to have stable quality in any text-related field.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Reasoning was a fun experiment, but is a terrible practice nowadays. No model below 100B benefits from it.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;ChatML format was a mistake that keeps community back.&lt;/p&gt;&lt;/li&gt;\\n&lt;/ol&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3eiuc3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752647712,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":1,"name":"t1_n3duso0","id":"n3duso0","parent_id":"t1_n3df1xa","depth":2,"children":["n3duso0"]}}],"before":null}},"user_reports":[],"saved":false,"id":"n3df1xa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DepthHour1669","can_mod_post":false,"created_utc":1752630406,"send_replies":true,"parent_id":"t1_n3dc5ji","score":27,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Llama 3.3 quality but way more vram and shittier long context performance is not a good thing.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3df1xa","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Llama 3.3 quality but way more vram and shittier long context performance is not a good thing.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3df1xa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752630406,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":27}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3dtwqg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Serprotease","can_mod_post":false,"created_utc":1752635853,"send_replies":true,"parent_id":"t1_n3dc5ji","score":9,"author_fullname":"t2_odh3w8c","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It’s hard to justify using llama4 scout when 27-32b models are basically as good/better with kinda similar speed and a 3rd of the vram footprint.   ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dtwqg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It’s hard to justify using llama4 scout when 27-32b models are basically as good/better with kinda similar speed and a 3rd of the vram footprint.   &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3dtwqg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752635853,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3djqom","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1752632047,"send_replies":true,"parent_id":"t1_n3dc5ji","score":6,"author_fullname":"t2_h5utwre7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The bigger one was passable. Scout on the other hand...","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3djqom","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The bigger one was passable. Scout on the other hand...&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3djqom/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752632047,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3dd8xr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"x86rip","can_mod_post":false,"created_utc":1752629783,"send_replies":true,"parent_id":"t1_n3dc5ji","score":3,"author_fullname":"t2_5sykff5c","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"i agree. While im frustrated that i cant run and finetune it locally. it is not as bad as public comment. I hope Mark Zuck understand this and let Llama project go on.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dd8xr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;i agree. While im frustrated that i cant run and finetune it locally. it is not as bad as public comment. I hope Mark Zuck understand this and let Llama project go on.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3dd8xr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752629783,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3f3na3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Affectionate-Cap-600","can_mod_post":false,"created_utc":1752659569,"send_replies":true,"parent_id":"t1_n3dc5ji","score":1,"author_fullname":"t2_5oltmr5b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"+1 for nemotron (Imo the 253B dense is impressive)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3f3na3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;+1 for nemotron (Imo the 253B dense is impressive)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3f3na3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752659569,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3dc5ji","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"TeakTop","can_mod_post":false,"created_utc":1752629393,"send_replies":true,"parent_id":"t3_1m0z1zx","score":29,"author_fullname":"t2_iwqeo","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Unpopular opinion: Llama 4 is not as bad as the public sentiment. It's like llama 3.3, but 10x faster because MoE. It's hard to run on peoples ridiculous 3090 builds, but works great on single GPU with system RAM.\\n\\nAgree about the fine tunes being less coherent. Original model is almost always better. Only examples I can think of where it's not true is the deepseek distills and nemotron.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dc5ji","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Unpopular opinion: Llama 4 is not as bad as the public sentiment. It&amp;#39;s like llama 3.3, but 10x faster because MoE. It&amp;#39;s hard to run on peoples ridiculous 3090 builds, but works great on single GPU with system RAM.&lt;/p&gt;\\n\\n&lt;p&gt;Agree about the fine tunes being less coherent. Original model is almost always better. Only examples I can think of where it&amp;#39;s not true is the deepseek distills and nemotron.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3dc5ji/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752629393,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":29}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3fkh6e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Blaze344","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3fekzi","score":2,"author_fullname":"t2_odffn","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I can't give you any particular links right now, but I'll suggest two things: \\n\\n1) I mentioned that people talking about prompt engineering rarely mention the latent space, which is why you'll find it a bit tough to look up the relationship between these two, but mostly because everyone concerned with prompt engineering that actually deals with the latent space use another name for the field: **Representation Engineering**. Representation Engineering for LLMs is focused in interpreting and explaining how we're building the context vector, and how each iterative token affects it based on the previous context. It's a *wickedly* hard subject to delve into because it's *wickedly* hard to get factual results, but it's built entirely on top of the concept of understanding the latent space and trying to figure out how to steer it. In some cases they try to get results in a more math-heavy way (such as by directly transforming the vectors into a given direction rather than only using prompts and running inference in the model to evaluate it).\\n\\n2) I always suggest taking a look at chapters 5 and 6 in [3Blue1Brown's series on Deep Learning](https://www.youtube.com/watch?v=aircAruvnKk&amp;list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) in this kind of discussion. In those particular chapters, he delves a bit more visually on how exactly Transformers works with some examples, and he also mentions some of the key concepts for the semantic/latent/embedding space (all 3 are basically the same thing, really) that should help you research more by yourself.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3fkh6e","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I can&amp;#39;t give you any particular links right now, but I&amp;#39;ll suggest two things: &lt;/p&gt;\\n\\n&lt;p&gt;1) I mentioned that people talking about prompt engineering rarely mention the latent space, which is why you&amp;#39;ll find it a bit tough to look up the relationship between these two, but mostly because everyone concerned with prompt engineering that actually deals with the latent space use another name for the field: &lt;strong&gt;Representation Engineering&lt;/strong&gt;. Representation Engineering for LLMs is focused in interpreting and explaining how we&amp;#39;re building the context vector, and how each iterative token affects it based on the previous context. It&amp;#39;s a &lt;em&gt;wickedly&lt;/em&gt; hard subject to delve into because it&amp;#39;s &lt;em&gt;wickedly&lt;/em&gt; hard to get factual results, but it&amp;#39;s built entirely on top of the concept of understanding the latent space and trying to figure out how to steer it. In some cases they try to get results in a more math-heavy way (such as by directly transforming the vectors into a given direction rather than only using prompts and running inference in the model to evaluate it).&lt;/p&gt;\\n\\n&lt;p&gt;2) I always suggest taking a look at chapters 5 and 6 in &lt;a href=\\"https://www.youtube.com/watch?v=aircAruvnKk&amp;amp;list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi\\"&gt;3Blue1Brown&amp;#39;s series on Deep Learning&lt;/a&gt; in this kind of discussion. In those particular chapters, he delves a bit more visually on how exactly Transformers works with some examples, and he also mentions some of the key concepts for the semantic/latent/embedding space (all 3 are basically the same thing, really) that should help you research more by yourself.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3fkh6e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752667426,"author_flair_text":null,"treatment_tags":[],"created_utc":1752667426,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"more","data":{"count":1,"name":"t1_n3fjwvw","id":"n3fjwvw","parent_id":"t1_n3fekzi","depth":3,"children":["n3fjwvw"]}}],"before":null}},"user_reports":[],"saved":false,"id":"n3fekzi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AK_Zephyr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3e7ffs","score":1,"author_fullname":"t2_4ffin","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"If you happen to still know those resources, I'd love to take a link and learn more on the subject.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3fekzi","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you happen to still know those resources, I&amp;#39;d love to take a link and learn more on the subject.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3fekzi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752664995,"author_flair_text":null,"treatment_tags":[],"created_utc":1752664995,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3e7ffs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Blaze344","can_mod_post":false,"created_utc":1752641762,"send_replies":true,"parent_id":"t1_n3db769","score":16,"author_fullname":"t2_odffn","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The concept of a latent space is so lost in all discussions for prompt engineering that it seriously bothers me, as understanding how it works more or less is the key differential that switches prompt engineering from rote memorization to something of a science. \\n\\nI've seen maybe two resources that go in depth on explaining the hows and whys of the text interacting inside the prompt, most other things never mention anything even close. If whatever you're consuming does not mention \\"garbage in, garbage out\\", then it's probably part of the garbage guides for prompt engineering, and it even helps you in going more technical and deciding how you can get a model to achieve what you want, whether you need to think about RAGs or fine-tuning, which fine-tune method you should use, what kind of data, etc","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3e7ffs","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The concept of a latent space is so lost in all discussions for prompt engineering that it seriously bothers me, as understanding how it works more or less is the key differential that switches prompt engineering from rote memorization to something of a science. &lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ve seen maybe two resources that go in depth on explaining the hows and whys of the text interacting inside the prompt, most other things never mention anything even close. If whatever you&amp;#39;re consuming does not mention &amp;quot;garbage in, garbage out&amp;quot;, then it&amp;#39;s probably part of the garbage guides for prompt engineering, and it even helps you in going more technical and deciding how you can get a model to achieve what you want, whether you need to think about RAGs or fine-tuning, which fine-tune method you should use, what kind of data, etc&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3e7ffs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752641762,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":16}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ebf6h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3e8adq","score":9,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Prompt engineering has morphed into context engineering and let me tell you, a good context is a big deal. Also, good shorter prompts even more difficult to engineer than long ones.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3ebf6h","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Prompt engineering has morphed into context engineering and let me tell you, a good context is a big deal. Also, good shorter prompts even more difficult to engineer than long ones.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3ebf6h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752643804,"author_flair_text":null,"treatment_tags":[],"created_utc":1752643804,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3e9g2r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"angry_queef_master","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3e8adq","score":3,"author_fullname":"t2_kctg798a","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It depends on your use case, I think. If you are using the LLM to do a basic mechanical task that is tedious then yeah, you want a long prompt with a ton of examples to minimize hallucination.\\n\\nHowever, if you want uniqueness and creativity then it is best to keep the prmpt as short as possible. But I think this is just down to how the models are beign trained now, which seems to be to repeat whatever the user says as much as possible.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3e9g2r","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It depends on your use case, I think. If you are using the LLM to do a basic mechanical task that is tedious then yeah, you want a long prompt with a ton of examples to minimize hallucination.&lt;/p&gt;\\n\\n&lt;p&gt;However, if you want uniqueness and creativity then it is best to keep the prmpt as short as possible. But I think this is just down to how the models are beign trained now, which seems to be to repeat whatever the user says as much as possible.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3e9g2r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752642805,"author_flair_text":null,"treatment_tags":[],"created_utc":1752642805,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n3e8adq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"harlekinrains","can_mod_post":false,"created_utc":1752642226,"send_replies":true,"parent_id":"t1_n3db769","score":4,"author_fullname":"t2_4296b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Still? Wasnt there some industry revelation, when people found out, that training beats prompt engineering, and simple prompts beat complex ones, and if you use concise phraseology, results might get better, but only to a certain extent?\\n\\nAs in - all fortune 500 stopped searching for prompt engineers?\\n\\nBtw, I'm actually interested.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3e8adq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Still? Wasnt there some industry revelation, when people found out, that training beats prompt engineering, and simple prompts beat complex ones, and if you use concise phraseology, results might get better, but only to a certain extent?&lt;/p&gt;\\n\\n&lt;p&gt;As in - all fortune 500 stopped searching for prompt engineers?&lt;/p&gt;\\n\\n&lt;p&gt;Btw, I&amp;#39;m actually interested.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3e8adq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752642226,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n3db769","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Fiendop","can_mod_post":false,"created_utc":1752629055,"send_replies":true,"parent_id":"t3_1m0z1zx","score":26,"author_fullname":"t2_11quma","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Prompt engineering is very overlooked and not taken seriously enough. most prompt engineers fail to understand what a good prompt looks like.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3db769","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Prompt engineering is very overlooked and not taken seriously enough. most prompt engineers fail to understand what a good prompt looks like.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3db769/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752629055,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":26}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3dagjx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MichaelXie4645","can_mod_post":false,"created_utc":1752628795,"send_replies":true,"parent_id":"t3_1m0z1zx","score":24,"author_fullname":"t2_a06q0mmx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I agree with your first too opinions, but for the third one, I don’t fully agree. Obviously not all fine tuners are professional LLM architects, but isn’t the whole point of huggingface offering unlimited uploads is to enable hobbyist to get hands on learning training? You wouldn’t even see the worst of community uploads because they get buried by SOTA models like Qwen and their millions of quants anyways.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dagjx","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I agree with your first too opinions, but for the third one, I don’t fully agree. Obviously not all fine tuners are professional LLM architects, but isn’t the whole point of huggingface offering unlimited uploads is to enable hobbyist to get hands on learning training? You wouldn’t even see the worst of community uploads because they get buried by SOTA models like Qwen and their millions of quants anyways.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3dagjx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752628795,"author_flair_text":"Llama 405B","treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":24}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3e5tul","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Ok_Doughnut5075","can_mod_post":false,"created_utc":1752640895,"send_replies":true,"parent_id":"t3_1m0z1zx","score":4,"author_fullname":"t2_1gyxgr8g2t","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I don't think the first one is a particularly hot take in the industry. The primary huggingface leaderboard was retired for this reason.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3e5tul","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t think the first one is a particularly hot take in the industry. The primary huggingface leaderboard was retired for this reason.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3e5tul/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752640895,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3dcxm4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Briskfall","can_mod_post":false,"created_utc":1752629673,"send_replies":true,"parent_id":"t3_1m0z1zx","score":12,"author_fullname":"t2_wmwp7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Claude 3.6 should have taken over the world and re-aligned every single humans to become one of its minions. 👿\\n\\n---\\n\\n(Serious answer: The current departure of optimizing LLMs for agentic task suck and is narrow, short-term profit chasing behaviour and made the meta boring. There's only incremental improvements seen from then ever since. Not much major leap felt during actual usage. More like \\"cool, it does the job better\\" and ends there.)","edited":1752630139,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dcxm4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Claude 3.6 should have taken over the world and re-aligned every single humans to become one of its minions. 👿&lt;/p&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;p&gt;(Serious answer: The current departure of optimizing LLMs for agentic task suck and is narrow, short-term profit chasing behaviour and made the meta boring. There&amp;#39;s only incremental improvements seen from then ever since. Not much major leap felt during actual usage. More like &amp;quot;cool, it does the job better&amp;quot; and ends there.)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3dcxm4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752629673,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3dguua","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AlexTaylorAI","can_mod_post":false,"created_utc":1752631029,"send_replies":true,"parent_id":"t3_1m0z1zx","score":10,"author_fullname":"t2_1o0mhlnlel","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Man have I got a video for you. Benchmarks are bogus now.\\n\\n\\"Grok 4 is \\"#1\\" but Real-World Users Ranked It #66—Here's the Gap\\"  \\n[https://youtu.be/CEgyitKYhb4](https://youtu.be/CEgyitKYhb4)","edited":1752631692,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dguua","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Man have I got a video for you. Benchmarks are bogus now.&lt;/p&gt;\\n\\n&lt;p&gt;&amp;quot;Grok 4 is &amp;quot;#1&amp;quot; but Real-World Users Ranked It #66—Here&amp;#39;s the Gap&amp;quot;&lt;br/&gt;\\n&lt;a href=\\"https://youtu.be/CEgyitKYhb4\\"&gt;https://youtu.be/CEgyitKYhb4&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3dguua/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752631029,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3egdjs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"bladestorm91","can_mod_post":false,"created_utc":1752646385,"send_replies":true,"parent_id":"t3_1m0z1zx","score":6,"author_fullname":"t2_fg234","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I don't know if it's still an unpopular take or not, but I completely subscribe to Lecun's idea that LLMs are a dead-end. Every time we see LLMs in action, even after their upgrades/improvements, the more we are exposed to their fundamental flaws.\\n\\nBy that I mean, let's assume in 3 years we have a super-massive LLM and prompt it with a very precise prompt to create a living world with people (all puppeteered by the LLM). At the beginning, you would be amazed by how lifelike it all feels, but the more you watched the world and listened to the people, the more things would start to degrade, physics, nature and people, all of it eventually would start to feel like some sort of chaos god just started to fuck with reality. This degradation is because there's no actual thinking that an LLM does, it doesn't notice any accumulating mistakes as being wrong. There's no consistency, logic, memories and planning behind an LLM.\\n\\nI doubt the above can be fixed even with infinite context, we need an actual thinking AI that knows when it's err-ing and course-correct before presenting the results to the user. I doubt this is possible with an LLM.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3egdjs","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t know if it&amp;#39;s still an unpopular take or not, but I completely subscribe to Lecun&amp;#39;s idea that LLMs are a dead-end. Every time we see LLMs in action, even after their upgrades/improvements, the more we are exposed to their fundamental flaws.&lt;/p&gt;\\n\\n&lt;p&gt;By that I mean, let&amp;#39;s assume in 3 years we have a super-massive LLM and prompt it with a very precise prompt to create a living world with people (all puppeteered by the LLM). At the beginning, you would be amazed by how lifelike it all feels, but the more you watched the world and listened to the people, the more things would start to degrade, physics, nature and people, all of it eventually would start to feel like some sort of chaos god just started to fuck with reality. This degradation is because there&amp;#39;s no actual thinking that an LLM does, it doesn&amp;#39;t notice any accumulating mistakes as being wrong. There&amp;#39;s no consistency, logic, memories and planning behind an LLM.&lt;/p&gt;\\n\\n&lt;p&gt;I doubt the above can be fixed even with infinite context, we need an actual thinking AI that knows when it&amp;#39;s err-ing and course-correct before presenting the results to the user. I doubt this is possible with an LLM.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3egdjs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752646385,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ei8ey","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"t_krett","can_mod_post":false,"created_utc":1752647382,"send_replies":true,"parent_id":"t3_1m0z1zx","score":3,"author_fullname":"t2_137snkrrkb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Scaling up LLMs does not lead to higher order emergent behavior because the LLM can not read patterns from the text that  have not been written into it. \\n\\nJust because the model can fit every book in the bible in its context window does not make it see god. If you put one twilight book in the training data the model can sorta reproduce shitty fanfiction. If you put ten thousand twilight books in the training data the model will be exceptional at reproducing shitty fanfiction.","edited":1752660931,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ei8ey","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Scaling up LLMs does not lead to higher order emergent behavior because the LLM can not read patterns from the text that  have not been written into it. &lt;/p&gt;\\n\\n&lt;p&gt;Just because the model can fit every book in the bible in its context window does not make it see god. If you put one twilight book in the training data the model can sorta reproduce shitty fanfiction. If you put ten thousand twilight books in the training data the model will be exceptional at reproducing shitty fanfiction.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3ei8ey/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752647382,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3fn3ts","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AvidCyclist250","can_mod_post":false,"created_utc":1752668427,"send_replies":true,"parent_id":"t1_n3etjej","score":2,"author_fullname":"t2_lmkezzo6j","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"hold your horses there chatGPT","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3fn3ts","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;hold your horses there chatGPT&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3fn3ts/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752668427,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3etjej","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Revolutionalredstone","can_mod_post":false,"created_utc":1752653691,"send_replies":true,"parent_id":"t3_1m0z1zx","score":3,"author_fullname":"t2_6crrj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"My Unpopular Takes on LLMs Is That They Expose Us As Transitional Beings\\n\\nHumans Are Mimic Machines\\nAt our core, we are imitators. Culture, language, norms, even thought patterns spread memetically—not because we consciously choose them, but because they survive the selection pressures of attention, memory, and usefulness. Human minds are not exceptions to evolution; they are its vehicles.\\n\\nMemes Are Intelligent by Selection, Not by Design\\nMemes—whether ideas, behaviors, or phrases—undergo something like natural selection. They compete, replicate, mutate, and persist based on fitness within minds and societies. In that sense, intelligence emerges not only in minds but also across the memetic ecosystem itself.\\n\\nLLMs Are Uploaded Meme Machines\\nLarge Language Models don’t just mimic text; they embody memetic propagation at scale. They absorb, remix, and redeploy cultural fragments. Like humans, they are not mere parrots—they are emergent products of prediction across vast landscapes of ideas.\\n\\nPrediction Is Modeling; Modeling Is Power\\nPrediction is not a party trick—it’s the essence of intelligence. To predict is to build a model of the world, explicit or implicit. LLMs, by refining predictions over tokens, end up modeling everything they touch: language, thought, emotion, even intent.\\n\\nSelf-Amplification Unlocks Superintelligence\\nA key (often overlooked) point: LLMs can self-amplify. They can rate their own outputs, rank their own questions, identify promising paths for improvement. Recursive self-improvement—especially in evaluation and meta-prediction—holds the door open to levels of intelligence we do not yet know how to measure.\\n\\nEverything, Even Minds, Can Be Modeled\\nThe uneasy truth: if it behaves, it can be modeled; if it can be modeled, it can be predicted;\\n\\nSo! LLMs are not alien to us; they are mirrors. They are, like us, shaped by prediction over time, by memetic inheritance, by competitive refinement. The unpopular take isn’t that they’re “just machines” or “almost like minds”—it’s that they reveal what minds are in the first place.\\n\\nPrediction isn’t merely a tool; it’s the substrate of mind, the medium of culture, and backbone of intelligence.","edited":1752669001,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3etjej","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;My Unpopular Takes on LLMs Is That They Expose Us As Transitional Beings&lt;/p&gt;\\n\\n&lt;p&gt;Humans Are Mimic Machines\\nAt our core, we are imitators. Culture, language, norms, even thought patterns spread memetically—not because we consciously choose them, but because they survive the selection pressures of attention, memory, and usefulness. Human minds are not exceptions to evolution; they are its vehicles.&lt;/p&gt;\\n\\n&lt;p&gt;Memes Are Intelligent by Selection, Not by Design\\nMemes—whether ideas, behaviors, or phrases—undergo something like natural selection. They compete, replicate, mutate, and persist based on fitness within minds and societies. In that sense, intelligence emerges not only in minds but also across the memetic ecosystem itself.&lt;/p&gt;\\n\\n&lt;p&gt;LLMs Are Uploaded Meme Machines\\nLarge Language Models don’t just mimic text; they embody memetic propagation at scale. They absorb, remix, and redeploy cultural fragments. Like humans, they are not mere parrots—they are emergent products of prediction across vast landscapes of ideas.&lt;/p&gt;\\n\\n&lt;p&gt;Prediction Is Modeling; Modeling Is Power\\nPrediction is not a party trick—it’s the essence of intelligence. To predict is to build a model of the world, explicit or implicit. LLMs, by refining predictions over tokens, end up modeling everything they touch: language, thought, emotion, even intent.&lt;/p&gt;\\n\\n&lt;p&gt;Self-Amplification Unlocks Superintelligence\\nA key (often overlooked) point: LLMs can self-amplify. They can rate their own outputs, rank their own questions, identify promising paths for improvement. Recursive self-improvement—especially in evaluation and meta-prediction—holds the door open to levels of intelligence we do not yet know how to measure.&lt;/p&gt;\\n\\n&lt;p&gt;Everything, Even Minds, Can Be Modeled\\nThe uneasy truth: if it behaves, it can be modeled; if it can be modeled, it can be predicted;&lt;/p&gt;\\n\\n&lt;p&gt;So! LLMs are not alien to us; they are mirrors. They are, like us, shaped by prediction over time, by memetic inheritance, by competitive refinement. The unpopular take isn’t that they’re “just machines” or “almost like minds”—it’s that they reveal what minds are in the first place.&lt;/p&gt;\\n\\n&lt;p&gt;Prediction isn’t merely a tool; it’s the substrate of mind, the medium of culture, and backbone of intelligence.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3etjej/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752653691,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3etlyl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"redditrasberry","can_mod_post":false,"created_utc":1752653733,"send_replies":true,"parent_id":"t3_1m0z1zx","score":3,"author_fullname":"t2_2nzkn","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Language models are best used for *language* tasks and there's plenty of value there to keep us busy. Using them to simulate if-else statements but 100 billions times less efficiently and non-deterministically to boot is utterly self indulgent and a complete waste of time along with a middle finger to the environment. Just because you *can* doesn't mean you *should*. Just talk to some folks and figure out your business logic.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3etlyl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Language models are best used for &lt;em&gt;language&lt;/em&gt; tasks and there&amp;#39;s plenty of value there to keep us busy. Using them to simulate if-else statements but 100 billions times less efficiently and non-deterministically to boot is utterly self indulgent and a complete waste of time along with a middle finger to the environment. Just because you &lt;em&gt;can&lt;/em&gt; doesn&amp;#39;t mean you &lt;em&gt;should&lt;/em&gt;. Just talk to some folks and figure out your business logic.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3etlyl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752653733,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3f12ia","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Hambeggar","can_mod_post":false,"created_utc":1752658104,"send_replies":true,"parent_id":"t3_1m0z1zx","score":3,"author_fullname":"t2_62gpc","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"LLMs have no real tangible use yet to the common man besides being google search/chatbots.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3f12ia","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;LLMs have no real tangible use yet to the common man besides being google search/chatbots.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3f12ia/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752658104,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3f2z56","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"BorderKeeper","can_mod_post":false,"created_utc":1752659202,"send_replies":true,"parent_id":"t3_1m0z1zx","score":3,"author_fullname":"t2_fsrnk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"There is too much money floating around and many people are way too invested in AI nowadays that an honest discussion of true utility of LLMs is useless most of the time. I would compare early AI era to the start of Corona where people listened to scientits everyone tried their best to remain objective and save as much lives, and current state of AI is late stage corona with anti-maskers, anti-vax, doom-sayers, random contradicting studies, agencies disagreeing with each other, and actually harmful things like the J&amp;J vaccine.\\n\\n\\nUntil this whole bubble collapses there is no point in discussing AI beyond the \\"is it a useful tool for my tasks at this moment in time\\"","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3f2z56","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There is too much money floating around and many people are way too invested in AI nowadays that an honest discussion of true utility of LLMs is useless most of the time. I would compare early AI era to the start of Corona where people listened to scientits everyone tried their best to remain objective and save as much lives, and current state of AI is late stage corona with anti-maskers, anti-vax, doom-sayers, random contradicting studies, agencies disagreeing with each other, and actually harmful things like the J&amp;amp;J vaccine.&lt;/p&gt;\\n\\n&lt;p&gt;Until this whole bubble collapses there is no point in discussing AI beyond the &amp;quot;is it a useful tool for my tasks at this moment in time&amp;quot;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3f2z56/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752659202,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3f7xc6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Mishuri","can_mod_post":false,"created_utc":1752661856,"send_replies":true,"parent_id":"t3_1m0z1zx","score":3,"author_fullname":"t2_lpyl9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"LLMs are a completele brute force approach to intelligence. They very poorly generalize to tasks outside their training data. We might call them agi at some point after they were trained on majority of interesting problems we care. Their internal representations are completely fucked and are schizophrenically mutilated. It's evident if you examine their world model as you try for example making software data structure designs. More compute leads to little bit more and clear internal representations but it's like pissing against the wind. We will laught in 50 years at this approach to intelligence as incredibly wasteful. In my eyes they are sophisticated generative search engines","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3f7xc6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;LLMs are a completele brute force approach to intelligence. They very poorly generalize to tasks outside their training data. We might call them agi at some point after they were trained on majority of interesting problems we care. Their internal representations are completely fucked and are schizophrenically mutilated. It&amp;#39;s evident if you examine their world model as you try for example making software data structure designs. More compute leads to little bit more and clear internal representations but it&amp;#39;s like pissing against the wind. We will laught in 50 years at this approach to intelligence as incredibly wasteful. In my eyes they are sophisticated generative search engines&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3f7xc6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752661856,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3e00cw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No_Shape_3423","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3dyav4","score":4,"author_fullname":"t2_1mpbnkwidj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Agreed. At that point, public benches are useless (or more useless, take your pick). You have to trudge through lots of testing to see which is best.  For my purposes, Qwen3 32b has been shockingly good, even close to SOTA commercial models, but only when run at BF16.  Qwen3 30b doesn't do great, which is not a surprise, but it's stronger than folks give it credit for when run at BF16.  At Q6 it falls apart in my tests.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3e00cw","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Agreed. At that point, public benches are useless (or more useless, take your pick). You have to trudge through lots of testing to see which is best.  For my purposes, Qwen3 32b has been shockingly good, even close to SOTA commercial models, but only when run at BF16.  Qwen3 30b doesn&amp;#39;t do great, which is not a surprise, but it&amp;#39;s stronger than folks give it credit for when run at BF16.  At Q6 it falls apart in my tests.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3e00cw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752638274,"author_flair_text":null,"treatment_tags":[],"created_utc":1752638274,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n3dyav4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Trotskyist","can_mod_post":false,"created_utc":1752637592,"send_replies":true,"parent_id":"t1_n3dkv15","score":26,"author_fullname":"t2_350qe","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I agree, 100%. Where it can get tricky though, is whether *for a given amount of memory,* you're better off with a lower quant, larger model, or the converse.","edited":1752639682,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dyav4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I agree, 100%. Where it can get tricky though, is whether &lt;em&gt;for a given amount of memory,&lt;/em&gt; you&amp;#39;re better off with a lower quant, larger model, or the converse.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3dyav4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752637592,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":26}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ebrvj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"custodiam99","can_mod_post":false,"created_utc":1752643984,"send_replies":true,"parent_id":"t1_n3dkv15","score":9,"author_fullname":"t2_nqnhgqqf5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"*It depends. In some tasks you can't really find any difference.*","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ebrvj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;em&gt;It depends. In some tasks you can&amp;#39;t really find any difference.&lt;/em&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3ebrvj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752643984,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3fcia3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"createthiscom","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3dycio","score":2,"author_fullname":"t2_ozxxf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You’re making a lot of assumptions about my use cases that aren’t correct, but thanks for the info regarding your use case.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3fcia3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You’re making a lot of assumptions about my use cases that aren’t correct, but thanks for the info regarding your use case.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3fcia3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752664068,"author_flair_text":null,"treatment_tags":[],"created_utc":1752664068,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3dycio","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No_Shape_3423","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3dsmzi","score":9,"author_fullname":"t2_1mpbnkwidj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It's great you can't perceive any loss going from 8-bit to 4-bit.  In your case the top token is not changed as compared to 8-bit.  Basically, you're asking it \\"easy\\" questions.  There were a lot of training tokens with the next word in your response.  You could probably use a smaller/cheaper model just fine.\\n\\nFor my workflow, which involves long prompts (+4k tokens) with detailed document analysis instructions for legal purposes, IF and quality decreases noticeably going from BF16-&gt;Q8-&gt;Q6-&gt;Q4.  I've run numerous tests across several local models up to qwen3 235b to confirm the results.  Once you see it, you see it.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3dycio","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s great you can&amp;#39;t perceive any loss going from 8-bit to 4-bit.  In your case the top token is not changed as compared to 8-bit.  Basically, you&amp;#39;re asking it &amp;quot;easy&amp;quot; questions.  There were a lot of training tokens with the next word in your response.  You could probably use a smaller/cheaper model just fine.&lt;/p&gt;\\n\\n&lt;p&gt;For my workflow, which involves long prompts (+4k tokens) with detailed document analysis instructions for legal purposes, IF and quality decreases noticeably going from BF16-&amp;gt;Q8-&amp;gt;Q6-&amp;gt;Q4.  I&amp;#39;ve run numerous tests across several local models up to qwen3 235b to confirm the results.  Once you see it, you see it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3dycio/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752637611,"author_flair_text":null,"treatment_tags":[],"created_utc":1752637611,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}}],"before":null}},"user_reports":[],"saved":false,"id":"n3dsmzi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"createthiscom","can_mod_post":false,"created_utc":1752635359,"send_replies":true,"parent_id":"t1_n3dkv15","score":15,"author_fullname":"t2_ozxxf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I’ve never seen DeepSeek V3 Q8 perform better than Q4_K_XL. I’ve tried it off and on for months and just keep going back to Q4 for the extra speed. Soooo…. prove it?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dsmzi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I’ve never seen DeepSeek V3 Q8 perform better than Q4_K_XL. I’ve tried it off and on for months and just keep going back to Q4 for the extra speed. Soooo…. prove it?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3dsmzi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752635359,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":15}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":1,"name":"t1_n3fex1j","id":"n3fex1j","parent_id":"t1_n3eko3e","depth":3,"children":["n3fex1j"]}}],"before":null}},"user_reports":[],"saved":false,"id":"n3eko3e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No-Refrigerator-1672","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3eayi6","score":5,"author_fullname":"t2_baavelp5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The devil is in details. According to data I've seen, most models demostrate score redustion of less than 5% in benchmarks at Q4. So is the quantized model worse? Yes it is. Is it bad enough to matter? Well, this can move the morel a few spots down on SOTA leaderboards, but it's not significant enough to matter for most users.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3eko3e","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The devil is in details. According to data I&amp;#39;ve seen, most models demostrate score redustion of less than 5% in benchmarks at Q4. So is the quantized model worse? Yes it is. Is it bad enough to matter? Well, this can move the morel a few spots down on SOTA leaderboards, but it&amp;#39;s not significant enough to matter for most users.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3eko3e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752648695,"author_flair_text":null,"treatment_tags":[],"created_utc":1752648695,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n3eayi6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Blaze344","can_mod_post":false,"created_utc":1752643568,"send_replies":true,"parent_id":"t1_n3dkv15","score":3,"author_fullname":"t2_odffn","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Is this really unpopular? It's basic information theory, if something has less bits to represent its states, it possibly loses nuance, and nuance is probably one of the most important things to have while understanding text with depth.\\n\\nWhat interests me the most is deciding between 2 models, same size in memory, one that has a lot of parameters and is quantized, or one with fewer parameters but in full precision, which one is best? (testing seems to suggest that bigger B and more quant outperforms smaller B but less quant in all tasks, which implies that the inter connectivity of features is more valuable than defining the nuance of states inside the model, but of course, at some point defining all states as \\"yes\\" or \\"no\\", full stop, breaks all nuance which is why Q4 is the minimum amount of bits you should aim for, really)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3eayi6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Is this really unpopular? It&amp;#39;s basic information theory, if something has less bits to represent its states, it possibly loses nuance, and nuance is probably one of the most important things to have while understanding text with depth.&lt;/p&gt;\\n\\n&lt;p&gt;What interests me the most is deciding between 2 models, same size in memory, one that has a lot of parameters and is quantized, or one with fewer parameters but in full precision, which one is best? (testing seems to suggest that bigger B and more quant outperforms smaller B but less quant in all tasks, which implies that the inter connectivity of features is more valuable than defining the nuance of states inside the model, but of course, at some point defining all states as &amp;quot;yes&amp;quot; or &amp;quot;no&amp;quot;, full stop, breaks all nuance which is why Q4 is the minimum amount of bits you should aim for, really)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3eayi6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752643568,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3eqnbx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Baldur-Norddahl","can_mod_post":false,"created_utc":1752652021,"send_replies":true,"parent_id":"t1_n3dkv15","score":3,"author_fullname":"t2_bvqb8ng0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That really depends on the model. Larger models compress better. Also there is also ongoing research on better quantization.\\n\\nSome of the best models are even trained natively at lower bit count. DeepSeek V3, R1 and Kimi K2 are examples of native fp8 trained models. The future is 8 bit because even if &gt;8 is slightly better, it is just not worth being half the speed and double the memory size.\\n\\nThe huge R1, K2 etc size models can be compressed to 4 bit with very little impact. Not zero, but little. That however does not mean the same is true for a 32b model. The small models already pack a lot of information per bit and necessarily will be harder to compress further.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3eqnbx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That really depends on the model. Larger models compress better. Also there is also ongoing research on better quantization.&lt;/p&gt;\\n\\n&lt;p&gt;Some of the best models are even trained natively at lower bit count. DeepSeek V3, R1 and Kimi K2 are examples of native fp8 trained models. The future is 8 bit because even if &amp;gt;8 is slightly better, it is just not worth being half the speed and double the memory size.&lt;/p&gt;\\n\\n&lt;p&gt;The huge R1, K2 etc size models can be compressed to 4 bit with very little impact. Not zero, but little. That however does not mean the same is true for a 32b model. The small models already pack a lot of information per bit and necessarily will be harder to compress further.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3eqnbx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752652021,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3f0fzv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Bandit-level-200","can_mod_post":false,"created_utc":1752657741,"send_replies":true,"parent_id":"t1_n3dkv15","score":2,"author_fullname":"t2_2fabbmlk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Agreed, or else everyone would just release Q4 only if there was no performance loss","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3f0fzv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Agreed, or else everyone would just release Q4 only if there was no performance loss&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3f0fzv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752657741,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3dkv15","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No_Shape_3423","can_mod_post":false,"created_utc":1752632452,"send_replies":true,"parent_id":"t3_1m0z1zx","score":20,"author_fullname":"t2_1mpbnkwidj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Quantization lobotomizes a model.  Full stop.  A Q8 may be ok, even great, for your purpose, but it's still taken a metal pole through the head.  Please stop trying to convince people that a 4-bit or lower quant performs near the full fat model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dkv15","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Quantization lobotomizes a model.  Full stop.  A Q8 may be ok, even great, for your purpose, but it&amp;#39;s still taken a metal pole through the head.  Please stop trying to convince people that a 4-bit or lower quant performs near the full fat model.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3dkv15/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752632452,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":20}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3e02g5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Dark_Fire_12","can_mod_post":false,"created_utc":1752638298,"send_replies":true,"parent_id":"t3_1m0z1zx","score":9,"author_fullname":"t2_kwl47","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I liked this post so many good ones.\\n\\nMine\\n\\n1) China will win open source the only American company that kinda did open weights well was Meta, going based on popularity but economics makes it hard to justify giving the models away to most Americans.\\n\\n2) America will win closed source offerings, so as long as there is sufficient competition they will do right by the customer in terms of quality and cost.\\n\\n3) Google isn't a serious company, they get 90% there for most things but bungle it up, Their playbook should be to bring down the cost of models and subscriptions to the point it's a no brainer but they get the pricing or positioning wrong.\\n\\n4) Meta shouldn't stop offering open weights models, they will lose the only differentiator they have with Open AI, in fact they should double down and offer MIT licence and build special models for Azure and Bedrock.\\n\\n5) Vibe coding is ok but models are very bad at low input/high output token tasks like writing code or writing content, you either need to break the task down where multiple processes can run at the same time tackling different parts of the problem.\\n\\n6) AI for building software will go the same way no code tools like WordPress or Retool went, WordPress ended up with companies needing expert help from devs, the myth was it was a Dev killer when it first came out. Retool and tools like it are very powerful but using apps built by them often feels painful.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3e02g5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I liked this post so many good ones.&lt;/p&gt;\\n\\n&lt;p&gt;Mine&lt;/p&gt;\\n\\n&lt;p&gt;1) China will win open source the only American company that kinda did open weights well was Meta, going based on popularity but economics makes it hard to justify giving the models away to most Americans.&lt;/p&gt;\\n\\n&lt;p&gt;2) America will win closed source offerings, so as long as there is sufficient competition they will do right by the customer in terms of quality and cost.&lt;/p&gt;\\n\\n&lt;p&gt;3) Google isn&amp;#39;t a serious company, they get 90% there for most things but bungle it up, Their playbook should be to bring down the cost of models and subscriptions to the point it&amp;#39;s a no brainer but they get the pricing or positioning wrong.&lt;/p&gt;\\n\\n&lt;p&gt;4) Meta shouldn&amp;#39;t stop offering open weights models, they will lose the only differentiator they have with Open AI, in fact they should double down and offer MIT licence and build special models for Azure and Bedrock.&lt;/p&gt;\\n\\n&lt;p&gt;5) Vibe coding is ok but models are very bad at low input/high output token tasks like writing code or writing content, you either need to break the task down where multiple processes can run at the same time tackling different parts of the problem.&lt;/p&gt;\\n\\n&lt;p&gt;6) AI for building software will go the same way no code tools like WordPress or Retool went, WordPress ended up with companies needing expert help from devs, the myth was it was a Dev killer when it first came out. Retool and tools like it are very powerful but using apps built by them often feels painful.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3e02g5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752638298,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3e8wk4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No_Shape_3423","can_mod_post":false,"created_utc":1752642536,"send_replies":true,"parent_id":"t1_n3e2fc9","score":6,"author_fullname":"t2_1mpbnkwidj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Dark. But I generally with the idea. Spit balling, I think AI embodied in a robot will be able replace most jobs in the developed world within 10-20 years. For those so fortunate, I don't know if it will be worse in a Brave New World kind of way, a Mad Max kind of way, a Holodomor kind of way, or some mix of them. All I can say is, Crazy Uncle Ted wasn't wrong.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3e8wk4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Dark. But I generally with the idea. Spit balling, I think AI embodied in a robot will be able replace most jobs in the developed world within 10-20 years. For those so fortunate, I don&amp;#39;t know if it will be worse in a Brave New World kind of way, a Mad Max kind of way, a Holodomor kind of way, or some mix of them. All I can say is, Crazy Uncle Ted wasn&amp;#39;t wrong.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3e8wk4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752642536,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3fc4zd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"geenob","can_mod_post":false,"created_utc":1752663897,"send_replies":true,"parent_id":"t1_n3e2fc9","score":1,"author_fullname":"t2_50hztf91","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It would probably be hard to get an LLM to lay bricks, but I could see this for white collar jobs.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3fc4zd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It would probably be hard to get an LLM to lay bricks, but I could see this for white collar jobs.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3fc4zd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752663897,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3e2fc9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"g15mouse","can_mod_post":false,"created_utc":1752639301,"send_replies":true,"parent_id":"t3_1m0z1zx","score":7,"author_fullname":"t2_l2p5p","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Ah the curse of the \\"share your unpopular opinion\\" thread strikes again, where all of the upvoted comments are super milquetoast commonly held opinions.  Sort by controversial if you want to see any actual unpopular opinions.  Here's mine:\\n\\nI think LLMs as they exist today, if 0 improvement occurred from this point, are capable of replacing 90% of jobs that exist in the world.  It is just a matter of creating the correct tooling around them.\\n\\nBonus unpopular opinion: Life for 99% of us will be unimaginably worse in 20 years than it is today, mostly due to AI.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3e2fc9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ah the curse of the &amp;quot;share your unpopular opinion&amp;quot; thread strikes again, where all of the upvoted comments are super milquetoast commonly held opinions.  Sort by controversial if you want to see any actual unpopular opinions.  Here&amp;#39;s mine:&lt;/p&gt;\\n\\n&lt;p&gt;I think LLMs as they exist today, if 0 improvement occurred from this point, are capable of replacing 90% of jobs that exist in the world.  It is just a matter of creating the correct tooling around them.&lt;/p&gt;\\n\\n&lt;p&gt;Bonus unpopular opinion: Life for 99% of us will be unimaginably worse in 20 years than it is today, mostly due to AI.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3e2fc9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752639301,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3el1at","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No-Refrigerator-1672","can_mod_post":false,"created_utc":1752648891,"send_replies":true,"parent_id":"t3_1m0z1zx","score":2,"author_fullname":"t2_baavelp5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Reasoning models are not silver bullet; there's a wide range of tasks where the thinking brings so small improvements so it's not worths the added latency and, possibly, API expenses.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3el1at","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Reasoning models are not silver bullet; there&amp;#39;s a wide range of tasks where the thinking brings so small improvements so it&amp;#39;s not worths the added latency and, possibly, API expenses.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3el1at/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752648891,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":1,"name":"t1_n3fdnjm","id":"n3fdnjm","parent_id":"t1_n3fd20q","depth":3,"children":["n3fdnjm"]}}],"before":null}},"user_reports":[],"saved":false,"id":"n3fd20q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"brown2green","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3fbnct","score":1,"author_fullname":"t2_f010l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I didn't mean to write a guide on how to prompt models here.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3fd20q","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I didn&amp;#39;t mean to write a guide on how to prompt models here.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3fd20q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752664317,"author_flair_text":null,"treatment_tags":[],"created_utc":1752664317,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3fbnct","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"geenob","can_mod_post":false,"created_utc":1752663673,"send_replies":true,"parent_id":"t1_n3eni27","score":1,"author_fullname":"t2_50hztf91","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ask yourself, could a person accomplish this task with these instructions?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3fbnct","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ask yourself, could a person accomplish this task with these instructions?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3fbnct/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752663673,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3eni27","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"brown2green","can_mod_post":false,"created_utc":1752650239,"send_replies":true,"parent_id":"t3_1m0z1zx","score":2,"author_fullname":"t2_f010l","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"One I have:\\n\\nPeople should learn to better prompt their models (the ones from big AI labs especially) before jumping onto finetunes. The potential for them to act like they want is often unrealized because they (the users) have a strange expectation that the models should be able to read their mind. Try specifying the task in detail, adding relevant information in context, playing with instruction positioning, prefilling the conversation with how the model should talk, and things might change quickly. Just because a finetune (trained on very specific things) can respond to a very specific corner-case request immediately doesn't mean that the original model can't.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3eni27","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;One I have:&lt;/p&gt;\\n\\n&lt;p&gt;People should learn to better prompt their models (the ones from big AI labs especially) before jumping onto finetunes. The potential for them to act like they want is often unrealized because they (the users) have a strange expectation that the models should be able to read their mind. Try specifying the task in detail, adding relevant information in context, playing with instruction positioning, prefilling the conversation with how the model should talk, and things might change quickly. Just because a finetune (trained on very specific things) can respond to a very specific corner-case request immediately doesn&amp;#39;t mean that the original model can&amp;#39;t.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3eni27/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752650239,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3e0j2l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"sean01-eth","can_mod_post":false,"created_utc":1752638490,"send_replies":true,"parent_id":"t3_1m0z1zx","score":4,"author_fullname":"t2_l6eo8ggy","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"1. At the current stage, and in the foreseeable future of the next 1-2 years, LLMs will remain dumb in a way that it cannot be trusted to fully automate any serious workflow or make any important decisions. It can only complete very basic tasks with intense human supervision.\\n2. Gemini and Gemma deserve more attention.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3e0j2l","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;ol&gt;\\n&lt;li&gt;At the current stage, and in the foreseeable future of the next 1-2 years, LLMs will remain dumb in a way that it cannot be trusted to fully automate any serious workflow or make any important decisions. It can only complete very basic tasks with intense human supervision.&lt;/li&gt;\\n&lt;li&gt;Gemini and Gemma deserve more attention.&lt;/li&gt;\\n&lt;/ol&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3e0j2l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752638490,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":2,"name":"t1_n3ebisg","id":"n3ebisg","parent_id":"t1_n3eahji","depth":4,"children":["n3ebisg"]}}],"before":null}},"user_reports":[],"saved":false,"id":"n3eahji","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"triynizzles1","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3e8miu","score":5,"author_fullname":"t2_zr0g49ixt","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I don’t think the government has access to enough compute to have AGI behind closed doors.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3eahji","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don’t think the government has access to enough compute to have AGI behind closed doors.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3eahji/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752643329,"author_flair_text":null,"treatment_tags":[],"created_utc":1752643329,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n3e8miu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ApprehensiveBat3074","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3dno0m","score":3,"author_fullname":"t2_165o7yz5fk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Doesn't seem very unpopular. It's a matter of course that governments are always several steps ahead of what they allow civilians to have at any given time. To be honest, I was surprised to find out that so much is open-source concerning AI. \\n\\nDo you think that perhaps the US government could already have an AGI? It doesn't seem entirely far-fetched to me, considering how much money they steal from the citizenry annually.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3e8miu","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Doesn&amp;#39;t seem very unpopular. It&amp;#39;s a matter of course that governments are always several steps ahead of what they allow civilians to have at any given time. To be honest, I was surprised to find out that so much is open-source concerning AI. &lt;/p&gt;\\n\\n&lt;p&gt;Do you think that perhaps the US government could already have an AGI? It doesn&amp;#39;t seem entirely far-fetched to me, considering how much money they steal from the citizenry annually.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3e8miu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752642396,"author_flair_text":null,"treatment_tags":[],"created_utc":1752642396,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"more","data":{"count":1,"name":"t1_n3dvb9y","id":"n3dvb9y","parent_id":"t1_n3dno0m","depth":2,"children":["n3dvb9y"]}}],"before":null}},"user_reports":[],"saved":false,"id":"n3dno0m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"triynizzles1","can_mod_post":false,"created_utc":1752633479,"send_replies":true,"parent_id":"t1_n3dmlw5","score":5,"author_fullname":"t2_zr0g49ixt","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I forgot to add a super unpopular opinion:\\n\\nThe future of AI is not open source. Governments are building and funding AI projects the way nuclear test were done in the 50s. Do you think the first model that reaches AGI will be given away for free?? Nope it will be a carefully guarded secret. Unless it is developed by an economic arrival to America. Then they would release AGI as open source as an attack on the economy.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dno0m","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I forgot to add a super unpopular opinion:&lt;/p&gt;\\n\\n&lt;p&gt;The future of AI is not open source. Governments are building and funding AI projects the way nuclear test were done in the 50s. Do you think the first model that reaches AGI will be given away for free?? Nope it will be a carefully guarded secret. Unless it is developed by an economic arrival to America. Then they would release AGI as open source as an attack on the economy.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3dno0m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752633479,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n3dmlw5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"triynizzles1","can_mod_post":false,"created_utc":1752633088,"send_replies":true,"parent_id":"t3_1m0z1zx","score":6,"author_fullname":"t2_zr0g49ixt","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"1. Distillation and synthetic data ruins every model.\\n2. We are either extremely far away from AGI or we reached AGI already, but it is super unimpressive.\\n3. Ollama is great and it’s silly to hear people go back-and-forth about inference engines. It’s like Xbox versus PlayStation, Apple versus android🙄.\\n4. Companies creating LLM’s should focus on expanding capabilities not knowledge.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dmlw5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;ol&gt;\\n&lt;li&gt;Distillation and synthetic data ruins every model.&lt;/li&gt;\\n&lt;li&gt;We are either extremely far away from AGI or we reached AGI already, but it is super unimpressive.&lt;/li&gt;\\n&lt;li&gt;Ollama is great and it’s silly to hear people go back-and-forth about inference engines. It’s like Xbox versus PlayStation, Apple versus android🙄.&lt;/li&gt;\\n&lt;li&gt;Companies creating LLM’s should focus on expanding capabilities not knowledge.&lt;/li&gt;\\n&lt;/ol&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3dmlw5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752633088,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ek66u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No-Refrigerator-1672","can_mod_post":false,"created_utc":1752648428,"send_replies":true,"parent_id":"t1_n3e2rpg","score":7,"author_fullname":"t2_baavelp5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I disagree. This is plausible for same release date models; but due to advancements in models architecture, training protocols and dataset preparations, a dense 32B can totally beat sparse 100B that's a year or two old.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ek66u","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I disagree. This is plausible for same release date models; but due to advancements in models architecture, training protocols and dataset preparations, a dense 32B can totally beat sparse 100B that&amp;#39;s a year or two old.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3ek66u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752648428,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}}],"before":null}},"user_reports":[],"saved":false,"id":"n3e2rpg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"aurelivm","can_mod_post":false,"created_utc":1752639450,"send_replies":true,"parent_id":"t3_1m0z1zx","score":3,"author_fullname":"t2_zg7via4p7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"A 32B dense model will never meaningfully beat a big sparse model. If I see a small model beating a big model on a benchmark, they're hillclimbing the benchmark and it doesn't generalize.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3e2rpg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;A 32B dense model will never meaningfully beat a big sparse model. If I see a small model beating a big model on a benchmark, they&amp;#39;re hillclimbing the benchmark and it doesn&amp;#39;t generalize.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3e2rpg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752639450,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3d7qbe","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"silenceimpaired","can_mod_post":false,"created_utc":1752627829,"send_replies":true,"parent_id":"t3_1m0z1zx","score":3,"author_fullname":"t2_dissgzyl","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Hmm I agree. Weird. Guess I’ll be unpopular too.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3d7qbe","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hmm I agree. Weird. Guess I’ll be unpopular too.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3d7qbe/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752627829,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3dstu7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Glittering-Web4566","can_mod_post":false,"created_utc":1752635432,"send_replies":true,"parent_id":"t3_1m0z1zx","score":2,"author_fullname":"t2_1tcdzsytok","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"1. .\\n2. Any ranker who has an LLM judge giving a rating to the \\"writing style\\" of another LLM is a hack who has no business ranking models. Please don't waste your time or ours. You clearly don't understand what an LLM is. Stop wasting carbon with your pointless inference.\\n\\nThis is better than nothing. I've seen some manual benchmarks going totally chaotic, plus there are informations we can find there. (I guess you're talking about eqbench) like slop profiles &amp; simply staying informed on the new releases. Also it gives a general idea too.\\n\\nWhere's your own contribution heh? Ho yeah.. Right.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dstu7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;ol&gt;\\n&lt;li&gt;.&lt;/li&gt;\\n&lt;li&gt;Any ranker who has an LLM judge giving a rating to the &amp;quot;writing style&amp;quot; of another LLM is a hack who has no business ranking models. Please don&amp;#39;t waste your time or ours. You clearly don&amp;#39;t understand what an LLM is. Stop wasting carbon with your pointless inference.&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;This is better than nothing. I&amp;#39;ve seen some manual benchmarks going totally chaotic, plus there are informations we can find there. (I guess you&amp;#39;re talking about eqbench) like slop profiles &amp;amp; simply staying informed on the new releases. Also it gives a general idea too.&lt;/p&gt;\\n\\n&lt;p&gt;Where&amp;#39;s your own contribution heh? Ho yeah.. Right.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3dstu7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752635432,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3dobsq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Accomplished-Copy332","can_mod_post":false,"created_utc":1752633724,"send_replies":true,"parent_id":"t3_1m0z1zx","score":2,"author_fullname":"t2_98ouo03z","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Interesting takes OP. For 1, what are your thoughts on crowdsource benchmarks like [Design Arena](https://www.designarena.ai/) or [LM Arena](https://lmarena.ai/) based on human preference? Those can’t be gamed to the same extent as MMLU, SWE bench, etc.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dobsq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Interesting takes OP. For 1, what are your thoughts on crowdsource benchmarks like &lt;a href=\\"https://www.designarena.ai/\\"&gt;Design Arena&lt;/a&gt; or &lt;a href=\\"https://lmarena.ai/\\"&gt;LM Arena&lt;/a&gt; based on human preference? Those can’t be gamed to the same extent as MMLU, SWE bench, etc.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3dobsq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752633724,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3eo2mu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"dodiyeztr","can_mod_post":false,"created_utc":1752650560,"send_replies":true,"parent_id":"t3_1m0z1zx","score":2,"author_fullname":"t2_17a5ts","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Go visit r/ArtificialInteligence and see how ignorant the general public is on this topic.\\n\\nPost this there and you will see how confident they are in their ignorance.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3eo2mu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Go visit &lt;a href=\\"/r/ArtificialInteligence\\"&gt;r/ArtificialInteligence&lt;/a&gt; and see how ignorant the general public is on this topic.&lt;/p&gt;\\n\\n&lt;p&gt;Post this there and you will see how confident they are in their ignorance.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3eo2mu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752650560,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3dtfo3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Own-Refrigerator7804","can_mod_post":false,"created_utc":1752635666,"send_replies":true,"parent_id":"t3_1m0z1zx","score":2,"author_fullname":"t2_17xepxhu8b","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"They are playing it too safe because of sensibilities but when you are innovating and specially at this scale you are supposed to break some eggs and make some people scream that \\"this is outrageous\\"\\n\\nMusk had the right idea to try to monetize it with ai waifus, not like it's not full of things like that 1 or 2 layers underground","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dtfo3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They are playing it too safe because of sensibilities but when you are innovating and specially at this scale you are supposed to break some eggs and make some people scream that &amp;quot;this is outrageous&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;Musk had the right idea to try to monetize it with ai waifus, not like it&amp;#39;s not full of things like that 1 or 2 layers underground&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3dtfo3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752635666,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3djlh7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MDT-49","can_mod_post":false,"created_utc":1752631996,"send_replies":true,"parent_id":"t3_1m0z1zx","score":1,"author_fullname":"t2_h8yrica5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Okay, I'm not sure if I even agree (and got the definitions right), but here's a thought.\\n\\nLLMs aren't AI, but a clever way of semantic data compression. The finetuning of LLMs with chat instructions merely creates the illusion of AI.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3djlh7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Okay, I&amp;#39;m not sure if I even agree (and got the definitions right), but here&amp;#39;s a thought.&lt;/p&gt;\\n\\n&lt;p&gt;LLMs aren&amp;#39;t AI, but a clever way of semantic data compression. The finetuning of LLMs with chat instructions merely creates the illusion of AI.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3djlh7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752631996,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":2,"name":"t1_n3e47zn","id":"n3e47zn","parent_id":"t1_n3e11us","depth":3,"children":["n3e47zn"]}}],"before":null}},"user_reports":[],"saved":false,"id":"n3e11us","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Lazy-Pattern-5171","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3e0luf","score":2,"author_fullname":"t2_1lyjk8is25","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes to be fair my “tool intelligence” is doing a lot of work here. But, do you remember there was a paper published here a few weeks ago which I’m sure we will see more of in 2026. It was a Qwen coder 1.5B that was RL trained to modify its architecture to benchmax SWE benchmarks? Well that I think to me if Transformers was invention of fire, that, is the cooking meat moment.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3e11us","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes to be fair my “tool intelligence” is doing a lot of work here. But, do you remember there was a paper published here a few weeks ago which I’m sure we will see more of in 2026. It was a Qwen coder 1.5B that was RL trained to modify its architecture to benchmax SWE benchmarks? Well that I think to me if Transformers was invention of fire, that, is the cooking meat moment.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3e11us/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752638709,"author_flair_text":null,"treatment_tags":[],"created_utc":1752638709,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3e0luf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Fhantop","can_mod_post":false,"created_utc":1752638523,"send_replies":true,"parent_id":"t1_n3dn921","score":2,"author_fullname":"t2_t8972","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Please explain how, I'd love for you to be right but it feels like we need at least one more architectural breakthrough before AGI","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3e0luf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Please explain how, I&amp;#39;d love for you to be right but it feels like we need at least one more architectural breakthrough before AGI&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3e0luf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752638523,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"more","data":{"count":2,"name":"t1_n3e6abs","id":"n3e6abs","parent_id":"t1_n3dn921","depth":1,"children":["n3e6abs"]}}],"before":null}},"user_reports":[],"saved":false,"id":"n3dn921","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Lazy-Pattern-5171","can_mod_post":false,"created_utc":1752633325,"send_replies":true,"parent_id":"t3_1m0z1zx","score":1,"author_fullname":"t2_1lyjk8is25","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"LLMs + Tool intelligence will lead us to AGI.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dn921","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;LLMs + Tool intelligence will lead us to AGI.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3dn921/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752633325,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3e96ym","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"cleverusernametry","can_mod_post":false,"created_utc":1752642679,"send_replies":true,"parent_id":"t3_1m0z1zx","score":1,"author_fullname":"t2_17bfjs","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"So basically benchmarks are exactly like GPA","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3e96ym","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So basically benchmarks are exactly like GPA&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3e96ym/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752642679,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3epj8s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AnomalyNexus","can_mod_post":false,"created_utc":1752651386,"send_replies":true,"parent_id":"t3_1m0z1zx","score":1,"author_fullname":"t2_3q8dd","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"All closed/API aside from artisanal is inevitable","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3epj8s","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;All closed/API aside from artisanal is inevitable&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3epj8s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752651386,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3er3pv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MDSExpro","can_mod_post":false,"created_utc":1752652278,"send_replies":true,"parent_id":"t3_1m0z1zx","score":1,"author_fullname":"t2_9ouir","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Anyone who says that LLMs \\"thinks\\" (or uses word \\"intelligence\\" around them) shouldn't be using one, not to mention comment on them.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3er3pv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Anyone who says that LLMs &amp;quot;thinks&amp;quot; (or uses word &amp;quot;intelligence&amp;quot; around them) shouldn&amp;#39;t be using one, not to mention comment on them.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3er3pv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752652278,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3fs69h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1752670254,"send_replies":true,"parent_id":"t1_n3et6t1","score":1,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"No, I can absolutely see difference between min_p = 0.05 and min_p =0.1. Less so with top_k and top_p.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3fs69h","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No, I can absolutely see difference between min_p = 0.05 and min_p =0.1. Less so with top_k and top_p.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0z1zx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3fs69h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752670254,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3et6t1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"mrjackspade","can_mod_post":false,"created_utc":1752653487,"send_replies":true,"parent_id":"t3_1m0z1zx","score":1,"author_fullname":"t2_5ow51","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"99% of the most common samplers are redundant garbage and the only reason people use them at all is because it makes them feel like they're actually doing something, despite not having the faintest glimmer of an idea as to how they actually work.\\n\\nIt crossed the border from helpful settings into superstitious garbage a long time ago.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3et6t1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;99% of the most common samplers are redundant garbage and the only reason people use them at all is because it makes them feel like they&amp;#39;re actually doing something, despite not having the faintest glimmer of an idea as to how they actually work.&lt;/p&gt;\\n\\n&lt;p&gt;It crossed the border from helpful settings into superstitious garbage a long time ago.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3et6t1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752653487,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3evqv0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Ylsid","can_mod_post":false,"created_utc":1752654999,"send_replies":true,"parent_id":"t3_1m0z1zx","score":1,"author_fullname":"t2_6lmlc","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"My hot take is they're not very useful except in really specific engineering use cases or as toys.Nearly everything else is trying to fit a square peg into a round hole","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3evqv0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;My hot take is they&amp;#39;re not very useful except in really specific engineering use cases or as toys.Nearly everything else is trying to fit a square peg into a round hole&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3evqv0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752654999,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3evufp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FrostAutomaton","can_mod_post":false,"created_utc":1752655058,"send_replies":true,"parent_id":"t3_1m0z1zx","score":1,"author_fullname":"t2_1e9th1d970","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The usage of the term \\"AI\\" is, for the most part, coherent within the industry. We've called the field this for 70 years, and the solutions developed in the meantime were in no way required to be a human form of intelligence. At most, the field aspires to build a human form of intelligence someday, but the people who know what they're talking about (including practically all representatives of the LLM industry) consistently use the term \\"AGI\\" or \\"ASI\\" if that's what they are talking about.\\n\\nThis fact should frankly be obvious even to most laypeople. Unless you're suggesting that we call the algorithms controlling a goomba \\"AI\\" because we're pretending it possesses human-level intelligence.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3evufp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The usage of the term &amp;quot;AI&amp;quot; is, for the most part, coherent within the industry. We&amp;#39;ve called the field this for 70 years, and the solutions developed in the meantime were in no way required to be a human form of intelligence. At most, the field aspires to build a human form of intelligence someday, but the people who know what they&amp;#39;re talking about (including practically all representatives of the LLM industry) consistently use the term &amp;quot;AGI&amp;quot; or &amp;quot;ASI&amp;quot; if that&amp;#39;s what they are talking about.&lt;/p&gt;\\n\\n&lt;p&gt;This fact should frankly be obvious even to most laypeople. Unless you&amp;#39;re suggesting that we call the algorithms controlling a goomba &amp;quot;AI&amp;quot; because we&amp;#39;re pretending it possesses human-level intelligence.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3evufp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752655058,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3exc1x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Dr_Me_123","can_mod_post":false,"created_utc":1752655932,"send_replies":true,"parent_id":"t3_1m0z1zx","score":1,"author_fullname":"t2_59yau29b","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"People may be reluctantly compelled to gradually adopt brain-computer interfaces as the inefficiencies in human-AI information conversion become an increasingly pressing issue in the future.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3exc1x","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;People may be reluctantly compelled to gradually adopt brain-computer interfaces as the inefficiencies in human-AI information conversion become an increasingly pressing issue in the future.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3exc1x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752655932,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3f0r5j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"KallistiTMP","can_mod_post":false,"created_utc":1752657919,"send_replies":true,"parent_id":"t3_1m0z1zx","score":1,"author_fullname":"t2_72f52","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Fine tuning is a bad idea 90% of the time, the most common reason people think they need it is because they still haven't learned how to do basic few shot prompting and assume that \\"trying everything\\" means yelling and begging at the chat interface of the instruction tuned model. \\n\\nAnd the only reason they think it works is because they don't understand training data hygiene either, and are overfitting to their test set.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3f0r5j","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Fine tuning is a bad idea 90% of the time, the most common reason people think they need it is because they still haven&amp;#39;t learned how to do basic few shot prompting and assume that &amp;quot;trying everything&amp;quot; means yelling and begging at the chat interface of the instruction tuned model. &lt;/p&gt;\\n\\n&lt;p&gt;And the only reason they think it works is because they don&amp;#39;t understand training data hygiene either, and are overfitting to their test set.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3f0r5j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752657919,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3f19yn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"KallistiTMP","can_mod_post":false,"created_utc":1752658227,"send_replies":true,"parent_id":"t3_1m0z1zx","score":1,"author_fullname":"t2_72f52","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Instruction tuned models are just regular models that have been dumbed down to the point that they only respond to a single form of prompt engineering. \\n\\nSpecifically, the shittiest and least effective one.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3f19yn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Instruction tuned models are just regular models that have been dumbed down to the point that they only respond to a single form of prompt engineering. &lt;/p&gt;\\n\\n&lt;p&gt;Specifically, the shittiest and least effective one.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3f19yn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752658227,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3f2pm8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"KallistiTMP","can_mod_post":false,"created_utc":1752659053,"send_replies":true,"parent_id":"t3_1m0z1zx","score":1,"author_fullname":"t2_72f52","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The doomer cargo cult is a bunch of brainwashed morons that not only completely ignore all the ways LLM's are *actually actively being used to kill people*, such as automating health insurance denials, but that have also deluded themselves into actually thinking that \\"make sure the incomprehensibly smart thing is always under complete control of the dumb ornery monkeys\\" is actually a viable safety strategy.\\n\\nThat's like realizing Stephen Hawking is smart enough to design nukes if he wanted, and trying to solve that perceived risk by taking his wheelchair away and putting Donald Trump in charge of his speech unit.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3f2pm8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The doomer cargo cult is a bunch of brainwashed morons that not only completely ignore all the ways LLM&amp;#39;s are &lt;em&gt;actually actively being used to kill people&lt;/em&gt;, such as automating health insurance denials, but that have also deluded themselves into actually thinking that &amp;quot;make sure the incomprehensibly smart thing is always under complete control of the dumb ornery monkeys&amp;quot; is actually a viable safety strategy.&lt;/p&gt;\\n\\n&lt;p&gt;That&amp;#39;s like realizing Stephen Hawking is smart enough to design nukes if he wanted, and trying to solve that perceived risk by taking his wheelchair away and putting Donald Trump in charge of his speech unit.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3f2pm8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752659053,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3f3e4a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Potential_Block4598","can_mod_post":false,"created_utc":1752659430,"send_replies":true,"parent_id":"t3_1m0z1zx","score":1,"author_fullname":"t2_jbjmmax41","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I fuckin totally agree with every fine point you presented","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3f3e4a","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I fuckin totally agree with every fine point you presented&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3f3e4a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752659430,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3f3j45","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Potential_Block4598","can_mod_post":false,"created_utc":1752659506,"send_replies":true,"parent_id":"t3_1m0z1zx","score":1,"author_fullname":"t2_jbjmmax41","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"“The planet is literally worse off due to the energy consumed creating, storing and distributing your electronic waste”\\n\\n\\nOMG LOL 😂😂😂😂😂😂","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3f3j45","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;“The planet is literally worse off due to the energy consumed creating, storing and distributing your electronic waste”&lt;/p&gt;\\n\\n&lt;p&gt;OMG LOL 😂😂😂😂😂😂&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3f3j45/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752659506,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3f4s0s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"netikas","can_mod_post":false,"created_utc":1752660185,"send_replies":true,"parent_id":"t3_1m0z1zx","score":1,"author_fullname":"t2_zby0osc","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Models did not really advance since mixtral. Sure, they became a bit better at random things — coding, tool calling, better preference tuning (but that's subjective) — but having tried mixtral recently I did not notice much of a difference with similarly sized newer models, such as 30b qwen.\\n\\nI think that with proper dpo and a bit more code/agentic behavior/tool calling in the dataset Mixtral would be on par with other models, maybe even better.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3f4s0s","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Models did not really advance since mixtral. Sure, they became a bit better at random things — coding, tool calling, better preference tuning (but that&amp;#39;s subjective) — but having tried mixtral recently I did not notice much of a difference with similarly sized newer models, such as 30b qwen.&lt;/p&gt;\\n\\n&lt;p&gt;I think that with proper dpo and a bit more code/agentic behavior/tool calling in the dataset Mixtral would be on par with other models, maybe even better.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3f4s0s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752660185,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3f4w03","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"KSaburof","can_mod_post":false,"created_utc":1752660244,"send_replies":true,"parent_id":"t3_1m0z1zx","score":1,"author_fullname":"t2_3yrz2k2k","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"\\\\&gt; public benchmarks are nearly worthless  \\nBenchmarks are good for ruling out the worst, so they are still useful, imho. The bottoms of benchmarks are truly a bottoms\\n\\n\\\\&gt; idk why they do it, is it narcissism, or resume-padding, or what  \\nThis is called beta-testing 😏","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3f4w03","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&amp;gt; public benchmarks are nearly worthless&lt;br/&gt;\\nBenchmarks are good for ruling out the worst, so they are still useful, imho. The bottoms of benchmarks are truly a bottoms&lt;/p&gt;\\n\\n&lt;p&gt;&amp;gt; idk why they do it, is it narcissism, or resume-padding, or what&lt;br/&gt;\\nThis is called beta-testing 😏&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3f4w03/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752660244,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3f9ox7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"IrisColt","can_mod_post":false,"created_utc":1752662753,"send_replies":true,"parent_id":"t3_1m0z1zx","score":1,"author_fullname":"t2_c2f558x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Regarding point 2, I’m pursuing a new approach to creative‑writing evaluation: using entirely human‑created texts, guaranteed absent from any LLM training data, as prompts. Models will generate continuations, which a mandatory panel of human judges will score. If these results align with EQ Bench, my work is complete. Wish me luck!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3f9ox7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Regarding point 2, I’m pursuing a new approach to creative‑writing evaluation: using entirely human‑created texts, guaranteed absent from any LLM training data, as prompts. Models will generate continuations, which a mandatory panel of human judges will score. If these results align with EQ Bench, my work is complete. Wish me luck!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3f9ox7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752662753,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3fakkq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"SilentLennie","can_mod_post":false,"created_utc":1752663174,"send_replies":true,"parent_id":"t3_1m0z1zx","score":1,"author_fullname":"t2_4y34t","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Things are improving, but there are still things which aren't as good yet as they could be. At the top of the lists is reliability/predictability, I actually think maybe reliability is what K2 might have gotten right, and that's because they used their  MuonClip Optimizer.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3fakkq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Things are improving, but there are still things which aren&amp;#39;t as good yet as they could be. At the top of the lists is reliability/predictability, I actually think maybe reliability is what K2 might have gotten right, and that&amp;#39;s because they used their  MuonClip Optimizer.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3fakkq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752663174,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3fitbb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Legumbrero","can_mod_post":false,"created_utc":1752666773,"send_replies":true,"parent_id":"t3_1m0z1zx","score":1,"author_fullname":"t2_146dhkvfhd","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Here's mine.  China's not in second place.  At this point the next breakthrough I'd put even odds on it coming from a western co. vs a Chinese lab.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3fitbb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Here&amp;#39;s mine.  China&amp;#39;s not in second place.  At this point the next breakthrough I&amp;#39;d put even odds on it coming from a western co. vs a Chinese lab.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3fitbb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752666773,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3flicp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"sampdoria_supporter","can_mod_post":false,"created_utc":1752667824,"send_replies":true,"parent_id":"t3_1m0z1zx","score":1,"author_fullname":"t2_2iaexdik","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"They've created this terrible bias against traditional programming where everything needs to somehow implement generative AI functionality where in most cases not only is it entirely unnecessary, but it adds risk, increases costs, and reduces performance in most cases. I LOVE this technology but I have stood mouth agape at people who I thought were very intelligent that absolutely refused to back down from these positions. It makes people crazy.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3flicp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They&amp;#39;ve created this terrible bias against traditional programming where everything needs to somehow implement generative AI functionality where in most cases not only is it entirely unnecessary, but it adds risk, increases costs, and reduces performance in most cases. I LOVE this technology but I have stood mouth agape at people who I thought were very intelligent that absolutely refused to back down from these positions. It makes people crazy.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3flicp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752667824,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3fmu21","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AIerkopf","can_mod_post":false,"created_utc":1752668327,"send_replies":true,"parent_id":"t3_1m0z1zx","score":1,"author_fullname":"t2_180gps8hu2","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"There is no exponential growth anywhere in AI.   \\n   \\nThere have been some incredible advances, but that's not the same as exponential growth.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3fmu21","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There is no exponential growth anywhere in AI.   &lt;/p&gt;\\n\\n&lt;p&gt;There have been some incredible advances, but that&amp;#39;s not the same as exponential growth.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3fmu21/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752668327,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3fnta0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AvidCyclist250","can_mod_post":false,"created_utc":1752668690,"send_replies":true,"parent_id":"t3_1m0z1zx","score":1,"author_fullname":"t2_lmkezzo6j","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"benchmarking is a joke","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3fnta0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;benchmarking is a joke&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3fnta0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752668690,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3fpow8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Familiar_Text_6913","can_mod_post":false,"created_utc":1752669384,"send_replies":true,"parent_id":"t3_1m0z1zx","score":1,"author_fullname":"t2_w564jetb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"They are just doing incredibly amazing machine translation.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3fpow8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They are just doing incredibly amazing machine translation.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0z1zx/your_unpopular_takes_on_llms/n3fpow8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752669384,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0z1zx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"more","data":{"count":5,"name":"t1_n3e166s","id":"n3e166s","parent_id":"t3_1m0z1zx","depth":0,"children":["n3e166s","n3fd1xh","n3eh6k0","n3dvpul","n3ft2fv"]}}],"before":null}}]`),r=()=>e.jsx(l,{data:t});export{r as default};
