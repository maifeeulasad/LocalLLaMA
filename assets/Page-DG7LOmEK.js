import{j as e}from"./index-BgwOAK4-.js";import{R as a}from"./RedditPostRenderer-BOBjDTFu.js";import"./index-BL22wVg5.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"overall:\\n\\nhttps://preview.redd.it/ahbceguvegdf1.png?width=2450&amp;format=png&amp;auto=webp&amp;s=fa83e349894e7d76cf5d4f222fdcf183c322582e\\n\\nhard prompts:\\n\\nhttps://preview.redd.it/7epol170fgdf1.png?width=2458&amp;format=png&amp;auto=webp&amp;s=0002ee1409a3cc4f14458b01bd5a7ba86176f392\\n\\ncoding:\\n\\nhttps://preview.redd.it/gp74ghd8fgdf1.png?width=2442&amp;format=png&amp;auto=webp&amp;s=89da241fe7d8d85c40b41ea8604006581a025d6a\\n\\n  \\n[https://lmarena.ai/leaderboard/text](https://lmarena.ai/leaderboard/text)","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Kimi-k2 on lmarena","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":90,"top_awarded_type":null,"hide_score":false,"media_metadata":{"7epol170fgdf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":69,"x":108,"u":"https://preview.redd.it/7epol170fgdf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bcc585153d1609c34380fa29f53c0896e3246138"},{"y":139,"x":216,"u":"https://preview.redd.it/7epol170fgdf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fc2a3f74c48ebb9648387b4fb3081bc2a0fa432d"},{"y":206,"x":320,"u":"https://preview.redd.it/7epol170fgdf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=186a765d2e80c4758124f6e070051a5636875608"},{"y":413,"x":640,"u":"https://preview.redd.it/7epol170fgdf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=43a00aa4a02b3060ee5610ad3d16594e9c17e953"},{"y":620,"x":960,"u":"https://preview.redd.it/7epol170fgdf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d5b3e78baec90f79c8ad4387d2b048499e27f4a5"},{"y":697,"x":1080,"u":"https://preview.redd.it/7epol170fgdf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=206f59136109a1c2f4125c34e18c168b51f90a57"}],"s":{"y":1588,"x":2458,"u":"https://preview.redd.it/7epol170fgdf1.png?width=2458&amp;format=png&amp;auto=webp&amp;s=0002ee1409a3cc4f14458b01bd5a7ba86176f392"},"id":"7epol170fgdf1"},"gp74ghd8fgdf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":69,"x":108,"u":"https://preview.redd.it/gp74ghd8fgdf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=827f56095fbd7631ef41b32d9fee562b45062da1"},{"y":139,"x":216,"u":"https://preview.redd.it/gp74ghd8fgdf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=103eac8659db6decfe53c17e74df51fbc34244fb"},{"y":207,"x":320,"u":"https://preview.redd.it/gp74ghd8fgdf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ca3d911039dfbe0285437fdc0454e3b55a767efe"},{"y":414,"x":640,"u":"https://preview.redd.it/gp74ghd8fgdf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b4fc5d9cf3cfce60a16f509b598dcc1b6b09112a"},{"y":621,"x":960,"u":"https://preview.redd.it/gp74ghd8fgdf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bfd4523e4619366bb9064f20f2dafa95bac507fd"},{"y":699,"x":1080,"u":"https://preview.redd.it/gp74ghd8fgdf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0a864ca554414ef1a04a8f5584843b547c3f5da1"}],"s":{"y":1582,"x":2442,"u":"https://preview.redd.it/gp74ghd8fgdf1.png?width=2442&amp;format=png&amp;auto=webp&amp;s=89da241fe7d8d85c40b41ea8604006581a025d6a"},"id":"gp74ghd8fgdf1"},"ahbceguvegdf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":69,"x":108,"u":"https://preview.redd.it/ahbceguvegdf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fe4272bcf6b99ffec9ddcd93f51e28d0f47ac470"},{"y":138,"x":216,"u":"https://preview.redd.it/ahbceguvegdf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b75535237f3dd6828b3493a5eda7492f058c7ead"},{"y":205,"x":320,"u":"https://preview.redd.it/ahbceguvegdf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7d98aa305ee4e9f2934f50fab95759833726930e"},{"y":411,"x":640,"u":"https://preview.redd.it/ahbceguvegdf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2a46595762baf0733cac7012b09e7b54913949d4"},{"y":617,"x":960,"u":"https://preview.redd.it/ahbceguvegdf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1ab282615ed89e770c78759c4acaf624d18773de"},{"y":694,"x":1080,"u":"https://preview.redd.it/ahbceguvegdf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b0e2ca6ba61ba42aee89bf43553f76233a856c9e"}],"s":{"y":1576,"x":2450,"u":"https://preview.redd.it/ahbceguvegdf1.png?width=2450&amp;format=png&amp;auto=webp&amp;s=fa83e349894e7d76cf5d4f222fdcf183c322582e"},"id":"ahbceguvegdf1"}},"name":"t3_1m2asou","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.95,"author_flair_background_color":null,"subreddit_type":"public","ups":89,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_xdw24u3am","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":89,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://b.thumbs.redditmedia.com/fy5-o3tc0GF-I3bCGJzPb2bsjXpQ9yAyleERp4yhbOw.jpg","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752766629,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;overall:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/ahbceguvegdf1.png?width=2450&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fa83e349894e7d76cf5d4f222fdcf183c322582e\\"&gt;https://preview.redd.it/ahbceguvegdf1.png?width=2450&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fa83e349894e7d76cf5d4f222fdcf183c322582e&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;hard prompts:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/7epol170fgdf1.png?width=2458&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0002ee1409a3cc4f14458b01bd5a7ba86176f392\\"&gt;https://preview.redd.it/7epol170fgdf1.png?width=2458&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0002ee1409a3cc4f14458b01bd5a7ba86176f392&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;coding:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/gp74ghd8fgdf1.png?width=2442&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=89da241fe7d8d85c40b41ea8604006581a025d6a\\"&gt;https://preview.redd.it/gp74ghd8fgdf1.png?width=2442&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=89da241fe7d8d85c40b41ea8604006581a025d6a&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://lmarena.ai/leaderboard/text\\"&gt;https://lmarena.ai/leaderboard/text&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1m2asou","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"BreakfastFriendly728","discussion_type":null,"num_comments":26,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m2asou/kimik2_on_lmarena/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m2asou/kimik2_on_lmarena/","subreddit_subscribers":501231,"created_utc":1752766629,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3s2er6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"HiddenoO","can_mod_post":false,"created_utc":1752825274,"send_replies":true,"parent_id":"t1_n3p76fo","score":5,"author_fullname":"t2_8127x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Because it's not primarily about how well they perform in practice, it's about how much people messing around on LMArena like the responses, and a huge part of that is style over substance. They've tried addressing that with \\"style control\\", but that still only catches a small part of what's actually the style of a model.\\n\\nE.g., Claude models work the best for coding and tool calls, but practically nobody goes to LMArena to work on a real-world coding project, let alone with tool calls.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3s2er6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Because it&amp;#39;s not primarily about how well they perform in practice, it&amp;#39;s about how much people messing around on LMArena like the responses, and a huge part of that is style over substance. They&amp;#39;ve tried addressing that with &amp;quot;style control&amp;quot;, but that still only catches a small part of what&amp;#39;s actually the style of a model.&lt;/p&gt;\\n\\n&lt;p&gt;E.g., Claude models work the best for coding and tool calls, but practically nobody goes to LMArena to work on a real-world coding project, let alone with tool calls.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2asou","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2asou/kimik2_on_lmarena/n3s2er6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752825274,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3t522m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"pier4r","can_mod_post":false,"created_utc":1752843522,"send_replies":true,"parent_id":"t1_n3p76fo","score":1,"author_fullname":"t2_ci7ay","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"lmarena is \\"what is the best chatbot that people like for answers\\" (unless you pick categories).\\n\\n4o is great at that.\\n\\nHence I am not mad when llama-4 experimental was winning, because it was simply showing that they found the best models for chatbots.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3t522m","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;lmarena is &amp;quot;what is the best chatbot that people like for answers&amp;quot; (unless you pick categories).&lt;/p&gt;\\n\\n&lt;p&gt;4o is great at that.&lt;/p&gt;\\n\\n&lt;p&gt;Hence I am not mad when llama-4 experimental was winning, because it was simply showing that they found the best models for chatbots.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2asou","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2asou/kimik2_on_lmarena/n3t522m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752843522,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3p76fo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"complains_constantly","can_mod_post":false,"created_utc":1752785203,"send_replies":true,"parent_id":"t3_1m2asou","score":10,"author_fullname":"t2_158u6z","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Why does 4o keep staying in the top 5? It's nowhere near that good.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3p76fo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why does 4o keep staying in the top 5? It&amp;#39;s nowhere near that good.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2asou/kimik2_on_lmarena/n3p76fo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752785203,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2asou","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3nuokl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"hapliniste","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3nof0a","score":22,"author_fullname":"t2_fc7rd","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I guess people downvote because\\n\\n1: at home (no, but still open)\\n\\n2: opus 4 level (only on lmarena)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3nuokl","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I guess people downvote because&lt;/p&gt;\\n\\n&lt;p&gt;1: at home (no, but still open)&lt;/p&gt;\\n\\n&lt;p&gt;2: opus 4 level (only on lmarena)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2asou","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2asou/kimik2_on_lmarena/n3nuokl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752771560,"author_flair_text":null,"treatment_tags":[],"created_utc":1752771560,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":22}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ooowg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RYSKZ","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3o3226","score":3,"author_fullname":"t2_ihqq0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes, I am aware, but prompt processing is unbearably slow with CPU-based setups, far from the performance of ChatGPT or any other cloud provider. Generation speed also becomes painfully slow after ingesting some context, making it unusable for coding applications. Furthermore, DDR5 RAM is quite expensive right now, making it unaffordable for many to have that amount. LPDDR5 is cheaper but even far worse in performance. Despite the advantages of a local setup, I believe this compromises doesn't make the cut for many.  \\n  \\nWe will get there eventually, but it will take time.","edited":false,"author_flair_css_class":null,"name":"t1_n3ooowg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, I am aware, but prompt processing is unbearably slow with CPU-based setups, far from the performance of ChatGPT or any other cloud provider. Generation speed also becomes painfully slow after ingesting some context, making it unusable for coding applications. Furthermore, DDR5 RAM is quite expensive right now, making it unaffordable for many to have that amount. LPDDR5 is cheaper but even far worse in performance. Despite the advantages of a local setup, I believe this compromises doesn&amp;#39;t make the cut for many.  &lt;/p&gt;\\n\\n&lt;p&gt;We will get there eventually, but it will take time.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m2asou","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2asou/kimik2_on_lmarena/n3ooowg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752779921,"author_flair_text":null,"collapsed":false,"created_utc":1752779921,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n3o3226","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"vasileer","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3nvh6c","score":7,"author_fullname":"t2_730bgdulm","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I disagree on that: for MoE models like kimi-k2 setups with lpddr5 ram are not that hard to find, and with 512GB RAM (e.g. m3 ultra) you can run quantized versions at decent speed (only 32B active parameters)\\n\\n[https://docs.unsloth.ai/basics/kimi-k2-how-to-run-locally](https://docs.unsloth.ai/basics/kimi-k2-how-to-run-locally)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3o3226","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I disagree on that: for MoE models like kimi-k2 setups with lpddr5 ram are not that hard to find, and with 512GB RAM (e.g. m3 ultra) you can run quantized versions at decent speed (only 32B active parameters)&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://docs.unsloth.ai/basics/kimi-k2-how-to-run-locally\\"&gt;https://docs.unsloth.ai/basics/kimi-k2-how-to-run-locally&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2asou","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2asou/kimik2_on_lmarena/n3o3226/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752773815,"author_flair_text":null,"treatment_tags":[],"created_utc":1752773815,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3rfdfj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"harlekinrains","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3pqgpc","score":1,"author_fullname":"t2_4296b","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Not with this model. Point being, a K2 q4 only needs 14GB or VRAM, but 600GB of DRAM, as it is MoE and not a dense model. So 3000 USD range on old datacenter GPUs currently, and on a used 4090 (speed, easiness of getting ktransformers to run) within a generation of DRAM sizes doubling.\\n\\nIf you buy used xeons and from alibaba/aliexpress.\\n\\nLimiting factor for tps and context length should be your 4090 (vram and GPU speed.)","edited":1752815038,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3rfdfj","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not with this model. Point being, a K2 q4 only needs 14GB or VRAM, but 600GB of DRAM, as it is MoE and not a dense model. So 3000 USD range on old datacenter GPUs currently, and on a used 4090 (speed, easiness of getting ktransformers to run) within a generation of DRAM sizes doubling.&lt;/p&gt;\\n\\n&lt;p&gt;If you buy used xeons and from alibaba/aliexpress.&lt;/p&gt;\\n\\n&lt;p&gt;Limiting factor for tps and context length should be your 4090 (vram and GPU speed.)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m2asou","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2asou/kimik2_on_lmarena/n3rfdfj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752813383,"author_flair_text":null,"treatment_tags":[],"created_utc":1752813383,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3pqgpc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RYSKZ","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3oqih6","score":4,"author_fullname":"t2_ihqq0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The top-tier M3 Ultra that has that 512 GB of unified memory comes in at $14,000. That's simply unaffordable. Bridging the price gap to a point where the average Western enthusiast can reasonably afford it (around $2,000-$3,000) will take years.\\n\\n\\n\\nFurthermore, $14,000 for an absolute sluggish prompt processing is a deal-breaker for me, and believe that is for many of us here. Waiting minutes just for the first prompt is unacceptable, that is what you get with CPU-based builds, including a Mac Studio, and it will get worse with subsequent prompts. With a memory bandwidth four times slower than an H100, the performance gap is still giant, and again, that is spending $14,000. Given that generational improvements typically occur every two years, we're likely looking at almost a decade before we reach GPU level performance and many more years to make that affordable.","edited":false,"author_flair_css_class":null,"name":"t1_n3pqgpc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The top-tier M3 Ultra that has that 512 GB of unified memory comes in at $14,000. That&amp;#39;s simply unaffordable. Bridging the price gap to a point where the average Western enthusiast can reasonably afford it (around $2,000-$3,000) will take years.&lt;/p&gt;\\n\\n&lt;p&gt;Furthermore, $14,000 for an absolute sluggish prompt processing is a deal-breaker for me, and believe that is for many of us here. Waiting minutes just for the first prompt is unacceptable, that is what you get with CPU-based builds, including a Mac Studio, and it will get worse with subsequent prompts. With a memory bandwidth four times slower than an H100, the performance gap is still giant, and again, that is spending $14,000. Given that generational improvements typically occur every two years, we&amp;#39;re likely looking at almost a decade before we reach GPU level performance and many more years to make that affordable.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m2asou","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2asou/kimik2_on_lmarena/n3pqgpc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752791006,"author_flair_text":null,"collapsed":false,"created_utc":1752791006,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ow9kc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"harlekinrains","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3oqih6","score":1,"author_fullname":"t2_4296b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"3800 USD final cost could be approachable. So Apple tax is  roughly 100% as always.. ;) (Buy a used 4090 to reach 5K USD, for peace of mind with ktransformers... :) )\\n\\nhttps://old.reddit.com/r/LocalLLaMA/comments/1lsgtvy/successfully_built_my_first_pc_for_ai_sourcing/\\nhttps://old.reddit.com/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/\\n\\nUntested. ;)\\n\\nedit: Oh, and Q4 it needs 600GB of DRAM not VRAM (because MoE), so.. yeah...","edited":1752812592,"author_flair_css_class":null,"name":"t1_n3ow9kc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;3800 USD final cost could be approachable. So Apple tax is  roughly 100% as always.. ;) (Buy a used 4090 to reach 5K USD, for peace of mind with ktransformers... :) )&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://old.reddit.com/r/LocalLLaMA/comments/1lsgtvy/successfully_built_my_first_pc_for_ai_sourcing/\\"&gt;https://old.reddit.com/r/LocalLLaMA/comments/1lsgtvy/successfully_built_my_first_pc_for_ai_sourcing/&lt;/a&gt;\\n&lt;a href=\\"https://old.reddit.com/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/\\"&gt;https://old.reddit.com/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Untested. ;)&lt;/p&gt;\\n\\n&lt;p&gt;edit: Oh, and Q4 it needs 600GB of DRAM not VRAM (because MoE), so.. yeah...&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m2asou","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2asou/kimik2_on_lmarena/n3ow9kc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752782113,"author_flair_text":null,"collapsed":false,"created_utc":1752782113,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3oqih6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"CommunityTough1","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3nvh6c","score":5,"author_fullname":"t2_1iuzpxw7eg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"\\"which will likely take several years, maybe a decade from now\\" - nah. Yes, Q4 needs about 768GB of VRAM or unified RAM, but the Mac Studio with 512GB of unified RAM is already almost there, with memory bandwidth around 800GB/s. This is about the same throughput that could only be achieved via DDR5-6400 in 16-channel or DDR5-8400 in 12-channel (so high end server setups), and is already enough to run DeepSeek at Q6 with good speeds. It's only enough memory *size* to run Kimi at Q3, though (not amazing, but the point is, we're definitely not a decade away).\\n\\nThe secret isn't that Apple has some kind of magic, it's just a very wide memory bus. This large bus memory system is pretty likely to become normal in the AI age where consumers are demanding hardware that can run LLMs. We'll see this architecture begin to permeate the PC space, and we'll start seeing 768-1TB of RAM come within reach probably within 1-2 years, if that, possibly even reaching terabit speeds. This'll make GPUs obsolete for inference (inference is really only a memory problem, not a compute one. Training is a whole different story where you really need tons and tons of compute power and parallel processing, but for people just wanting to run inference, it's really all about having fast memory).","edited":1752780935,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3oqih6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&amp;quot;which will likely take several years, maybe a decade from now&amp;quot; - nah. Yes, Q4 needs about 768GB of VRAM or unified RAM, but the Mac Studio with 512GB of unified RAM is already almost there, with memory bandwidth around 800GB/s. This is about the same throughput that could only be achieved via DDR5-6400 in 16-channel or DDR5-8400 in 12-channel (so high end server setups), and is already enough to run DeepSeek at Q6 with good speeds. It&amp;#39;s only enough memory &lt;em&gt;size&lt;/em&gt; to run Kimi at Q3, though (not amazing, but the point is, we&amp;#39;re definitely not a decade away).&lt;/p&gt;\\n\\n&lt;p&gt;The secret isn&amp;#39;t that Apple has some kind of magic, it&amp;#39;s just a very wide memory bus. This large bus memory system is pretty likely to become normal in the AI age where consumers are demanding hardware that can run LLMs. We&amp;#39;ll see this architecture begin to permeate the PC space, and we&amp;#39;ll start seeing 768-1TB of RAM come within reach probably within 1-2 years, if that, possibly even reaching terabit speeds. This&amp;#39;ll make GPUs obsolete for inference (inference is really only a memory problem, not a compute one. Training is a whole different story where you really need tons and tons of compute power and parallel processing, but for people just wanting to run inference, it&amp;#39;s really all about having fast memory).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2asou","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2asou/kimik2_on_lmarena/n3oqih6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752780448,"author_flair_text":null,"treatment_tags":[],"created_utc":1752780448,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3osnqm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Afternoon_4260","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3or1d5","score":1,"author_fullname":"t2_cj9kap4bx","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I mean even a couple 3090 can run models that surpass last year's sota, more or less. I feel we are not too far from a philosophical question at this point 😅","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3osnqm","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I mean even a couple 3090 can run models that surpass last year&amp;#39;s sota, more or less. I feel we are not too far from a philosophical question at this point 😅&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m2asou","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2asou/kimik2_on_lmarena/n3osnqm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752781072,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752781072,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3or1d5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RYSKZ","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3ohg12","score":1,"author_fullname":"t2_ihqq0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I agree with you on that, but there is something to keep in mind. The definition of SOTA models and hardware is relative, as they are constantly evolving, making it practically impossible to \\"keep up\\" with the current SOTA indefinitely. However, at some point, consumer hardware will likely be capable of running models that are currently considered SOTA, like Kimi-K2, and that will be more than sufficient for most people, as it is a very solid all-around model.\\n\\nOf course, larger and more powerful models will always be welcomed, but I believe the law of diminishing returns comes into play here: for many users, future improvements will not provide significant benefits, meaning that, at a certain point, we will have essentially \\"caught up.\\" Only very specialized applications will continue to require the most advanced models.\\n\\nAt least, that’s my theory. Personally, a model at the level of the current GPT-4o (such as Kimi-K2) would be sufficient for at least a few years more. I don’t think I would effectively benefit from anything better unless the improvement is very substantial enough to clearly outweigh the potential trade-offs (cost, resource usage, etc.). So if I can ever run Kimi-K2 affordably at home with reasonable performance (ChatGPT-like), I would be set for many years. I believe this will apply for many of us here.","edited":false,"author_flair_css_class":null,"name":"t1_n3or1d5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I agree with you on that, but there is something to keep in mind. The definition of SOTA models and hardware is relative, as they are constantly evolving, making it practically impossible to &amp;quot;keep up&amp;quot; with the current SOTA indefinitely. However, at some point, consumer hardware will likely be capable of running models that are currently considered SOTA, like Kimi-K2, and that will be more than sufficient for most people, as it is a very solid all-around model.&lt;/p&gt;\\n\\n&lt;p&gt;Of course, larger and more powerful models will always be welcomed, but I believe the law of diminishing returns comes into play here: for many users, future improvements will not provide significant benefits, meaning that, at a certain point, we will have essentially &amp;quot;caught up.&amp;quot; Only very specialized applications will continue to require the most advanced models.&lt;/p&gt;\\n\\n&lt;p&gt;At least, that’s my theory. Personally, a model at the level of the current GPT-4o (such as Kimi-K2) would be sufficient for at least a few years more. I don’t think I would effectively benefit from anything better unless the improvement is very substantial enough to clearly outweigh the potential trade-offs (cost, resource usage, etc.). So if I can ever run Kimi-K2 affordably at home with reasonable performance (ChatGPT-like), I would be set for many years. I believe this will apply for many of us here.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m2asou","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2asou/kimik2_on_lmarena/n3or1d5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752780599,"author_flair_text":null,"collapsed":false,"created_utc":1752780599,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"total_awards_received":0,"approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"ups":1,"removal_reason":null,"link_id":"t3_1m2asou","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3qj834","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Afternoon_4260","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3qcqew","score":2,"author_fullname":"t2_cj9kap4bx","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I agree to a certain point. Llm inference as we know today will at some point saturate consumer hardware. But who knows? Today we are using llm agent.  \\nTomorrow might be titans, context engineering might bring what I'd call \\"computational memory\\" and other needs.. world models?  \\nOr simply training? May be tomorrow we'll train specialists slm (or llm) like we write python functions.  \\nOr multimodality, if you want to parse videos that's pretty ressource hungry (back to world model?)\\n\\nI agree but I think you see where i'm going.  \\nToday isn't about prompt engineering anymore, but about the ecosystem you put around your llm, this may bring new resources needs.\\n\\nTo some points labs will have more resources and aim for techs they can run on moderate infrastructure which will keep being 10 folds the consumer hardware","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3qj834","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I agree to a certain point. Llm inference as we know today will at some point saturate consumer hardware. But who knows? Today we are using llm agent.&lt;br/&gt;\\nTomorrow might be titans, context engineering might bring what I&amp;#39;d call &amp;quot;computational memory&amp;quot; and other needs.. world models?&lt;br/&gt;\\nOr simply training? May be tomorrow we&amp;#39;ll train specialists slm (or llm) like we write python functions.&lt;br/&gt;\\nOr multimodality, if you want to parse videos that&amp;#39;s pretty ressource hungry (back to world model?)&lt;/p&gt;\\n\\n&lt;p&gt;I agree but I think you see where i&amp;#39;m going.&lt;br/&gt;\\nToday isn&amp;#39;t about prompt engineering anymore, but about the ecosystem you put around your llm, this may bring new resources needs.&lt;/p&gt;\\n\\n&lt;p&gt;To some points labs will have more resources and aim for techs they can run on moderate infrastructure which will keep being 10 folds the consumer hardware&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m2asou","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2asou/kimik2_on_lmarena/n3qj834/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752800814,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752800814,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3qcqew","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"DELETED","no_follow":true,"author":"[deleted]","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3ohg12","score":1,"approved_by":null,"report_reasons":null,"all_awardings":[],"subreddit_id":"t5_81eyvm","body":"[deleted]","edited":false,"author_flair_css_class":null,"collapsed":true,"downs":0,"is_submitter":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;[deleted]&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"associated_award":null,"stickied":false,"subreddit_type":"public","can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2asou/kimik2_on_lmarena/n3qcqew/","num_reports":null,"locked":false,"name":"t1_n3qcqew","created":1752798520,"subreddit":"LocalLLaMA","author_flair_text":null,"treatment_tags":[],"created_utc":1752798520,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":"","collapsed_because_crowd_control":null,"mod_reports":[],"mod_note":null,"distinguished":null}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ohg12","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Afternoon_4260","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3nvh6c","score":2,"author_fullname":"t2_cj9kap4bx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Consumer hardware will never \\"catch up\\" as sota models will always be sized for professional infrastructure.  \\nNevertheless at some point small models will become more and more relevant and the consumer hardware will be better suited for that.  \\nAs of today I'm really happy with something like devstral that allows me to offload small precise steps.  \\nI feel that makes me faster than having a huge model sending me a train load of slop and having to understand tf it did.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ohg12","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Consumer hardware will never &amp;quot;catch up&amp;quot; as sota models will always be sized for professional infrastructure.&lt;br/&gt;\\nNevertheless at some point small models will become more and more relevant and the consumer hardware will be better suited for that.&lt;br/&gt;\\nAs of today I&amp;#39;m really happy with something like devstral that allows me to offload small precise steps.&lt;br/&gt;\\nI feel that makes me faster than having a huge model sending me a train load of slop and having to understand tf it did.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2asou","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2asou/kimik2_on_lmarena/n3ohg12/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752777859,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752777859,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3nvh6c","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"RYSKZ","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3nof0a","score":4,"author_fullname":"t2_ihqq0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I guess it is not very feasible to have this model running \\"at home,\\" not economically at least. Consumer hardware needs to catch up first, which will likely take several years, maybe a decade from now. Don't get me wrong, it is super nice to have this model weights, and we can finally breathe to have this true ChatGPT experience freely available, but I guess the grand majority of us will have to wait years so we can effectively switch on to it.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3nvh6c","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I guess it is not very feasible to have this model running &amp;quot;at home,&amp;quot; not economically at least. Consumer hardware needs to catch up first, which will likely take several years, maybe a decade from now. Don&amp;#39;t get me wrong, it is super nice to have this model weights, and we can finally breathe to have this true ChatGPT experience freely available, but I guess the grand majority of us will have to wait years so we can effectively switch on to it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2asou","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2asou/kimik2_on_lmarena/n3nvh6c/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752771777,"author_flair_text":null,"treatment_tags":[],"created_utc":1752771777,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n3nof0a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"vasileer","can_mod_post":false,"created_utc":1752769848,"send_replies":true,"parent_id":"t1_n3nimd6","score":26,"author_fullname":"t2_730bgdulm","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"not sure why people are downvoting you, probably they didn't get that you mean kimi-k2 being at opus 4 level and being open weights, and that without being a reasoning model (less tokens to generate=faster)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3nof0a","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;not sure why people are downvoting you, probably they didn&amp;#39;t get that you mean kimi-k2 being at opus 4 level and being open weights, and that without being a reasoning model (less tokens to generate=faster)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2asou","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2asou/kimik2_on_lmarena/n3nof0a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752769848,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":26}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3qt4x0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"OfficialHashPanda","can_mod_post":false,"created_utc":1752804416,"send_replies":true,"parent_id":"t1_n3nimd6","score":1,"author_fullname":"t2_8w6mm4hmo","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I wouldnt say its opus 4 quality yet, but we may well get there later this year","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3qt4x0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I wouldnt say its opus 4 quality yet, but we may well get there later this year&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2asou","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2asou/kimik2_on_lmarena/n3qt4x0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752804416,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3nimd6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"secopsml","can_mod_post":false,"created_utc":1752768229,"send_replies":true,"parent_id":"t3_1m2asou","score":67,"author_fullname":"t2_pmniwf57y","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"So, we have opus 4 at home. Without reasoning wasteful tokens.\\n\\n\\nThe best announcement so far this year","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3nimd6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So, we have opus 4 at home. Without reasoning wasteful tokens.&lt;/p&gt;\\n\\n&lt;p&gt;The best announcement so far this year&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2asou/kimik2_on_lmarena/n3nimd6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752768229,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2asou","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":67}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3nvijy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"createthiscom","can_mod_post":false,"created_utc":1752771787,"send_replies":true,"parent_id":"t3_1m2asou","score":14,"author_fullname":"t2_ozxxf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yeah, my local copy of Kimi-K2 1t Q4\\\\_K\\\\_XL thinks it's claude too. They must have fine tuned it on claude.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3nvijy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, my local copy of Kimi-K2 1t Q4_K_XL thinks it&amp;#39;s claude too. They must have fine tuned it on claude.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2asou/kimik2_on_lmarena/n3nvijy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752771787,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2asou","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":14}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3x3g8n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"adviceguru25","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3u02i3","score":1,"author_fullname":"t2_c3b3edv5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"There’s this other benchmark for UI and frontend dev: https://www.designarena.ai/.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3x3g8n","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There’s this other benchmark for UI and frontend dev: &lt;a href=\\"https://www.designarena.ai/\\"&gt;https://www.designarena.ai/&lt;/a&gt;.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2asou","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2asou/kimik2_on_lmarena/n3x3g8n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752887123,"author_flair_text":null,"treatment_tags":[],"created_utc":1752887123,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3u02i3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"whatstheprobability","can_mod_post":false,"created_utc":1752852724,"send_replies":true,"parent_id":"t1_n3qplgq","score":2,"author_fullname":"t2_a3rta2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I still think it is a good data point, and it's nice that they have many categories.  \\n\\nDo you know which other benchmarks are most trusted right now?  I can't keep up.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3u02i3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I still think it is a good data point, and it&amp;#39;s nice that they have many categories.  &lt;/p&gt;\\n\\n&lt;p&gt;Do you know which other benchmarks are most trusted right now?  I can&amp;#39;t keep up.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2asou","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2asou/kimik2_on_lmarena/n3u02i3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752852724,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3qplgq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cleverusernametry","can_mod_post":false,"created_utc":1752803132,"send_replies":true,"parent_id":"t3_1m2asou","score":3,"author_fullname":"t2_17bfjs","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"LMArena isn't a great or reliable benchmark any more but glad to see Kimi up there","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3qplgq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;LMArena isn&amp;#39;t a great or reliable benchmark any more but glad to see Kimi up there&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2asou/kimik2_on_lmarena/n3qplgq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752803132,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2asou","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3pjt1d","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"QuackMania","can_mod_post":false,"created_utc":1752788913,"send_replies":true,"parent_id":"t3_1m2asou","score":2,"author_fullname":"t2_ux1pavfwr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"it's been here for nearly a day at least. Very happy with how it performs and also very happy that we can test it via lmarena, they're both chads","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3pjt1d","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;it&amp;#39;s been here for nearly a day at least. Very happy with how it performs and also very happy that we can test it via lmarena, they&amp;#39;re both chads&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2asou/kimik2_on_lmarena/n3pjt1d/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752788913,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2asou","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3rto9d","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"foldl-li","can_mod_post":false,"created_utc":1752820479,"send_replies":true,"parent_id":"t1_n3plhep","score":2,"author_fullname":"t2_g644e","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"My initial results are the same, not the same level as DeepSeek R1. \\n\\n(I am using free API from openrouter, and it said it's using fp8)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3rto9d","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;My initial results are the same, not the same level as DeepSeek R1. &lt;/p&gt;\\n\\n&lt;p&gt;(I am using free API from openrouter, and it said it&amp;#39;s using fp8)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2asou","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2asou/kimik2_on_lmarena/n3rto9d/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752820479,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3plhep","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"dubesor86","can_mod_post":false,"created_utc":1752789423,"send_replies":true,"parent_id":"t3_1m2asou","score":0,"author_fullname":"t2_6wbun","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Don't get me wrong, I thoroughly tested and like the model, but it's simply not in the same league as GPT-4.5, Opus, and Gemini 2.5.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3plhep","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Don&amp;#39;t get me wrong, I thoroughly tested and like the model, but it&amp;#39;s simply not in the same league as GPT-4.5, Opus, and Gemini 2.5.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2asou/kimik2_on_lmarena/n3plhep/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752789423,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2asou","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}}]`),n=()=>e.jsx(a,{data:l});export{n as default};
