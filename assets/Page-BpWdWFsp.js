import{j as l}from"./index-DLSqWzaI.js";import{R as e}from"./RedditPostRenderer-CysRo2D_.js";import"./index-COXiL3Lo.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Is there anything I could do with RTX 2070 + 3080 as far as running local models goes? Building a new PC and need to decide whether I should invest in a lager PSU to have both inside, or just stick to the 3080.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Local model on two different GPUs","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m2su9b","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.75,"author_flair_background_color":null,"subreddit_type":"public","ups":2,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_6256r","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":2,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752812649,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Is there anything I could do with RTX 2070 + 3080 as far as running local models goes? Building a new PC and need to decide whether I should invest in a lager PSU to have both inside, or just stick to the 3080.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m2su9b","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"cannabibun","discussion_type":null,"num_comments":15,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m2su9b/local_model_on_two_different_gpus/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m2su9b/local_model_on_two_different_gpus/","subreddit_subscribers":501232,"created_utc":1752812649,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3rh4qs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cannabibun","can_mod_post":false,"created_utc":1752814183,"send_replies":true,"parent_id":"t1_n3rf5aq","score":2,"author_fullname":"t2_6256r","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Sweet, just the answer I needed.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3rh4qs","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sweet, just the answer I needed.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2su9b","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2su9b/local_model_on_two_different_gpus/n3rh4qs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752814183,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3rk6fb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"reacusn","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3rhqat","score":2,"author_fullname":"t2_1ppg6hcqm8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"CPU/RAM shouldn't matter too much unless you can't fit everything in vram.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3rk6fb","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;CPU/RAM shouldn&amp;#39;t matter too much unless you can&amp;#39;t fit everything in vram.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2su9b","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2su9b/local_model_on_two_different_gpus/n3rk6fb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752815629,"author_flair_text":null,"treatment_tags":[],"created_utc":1752815629,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3s782l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Efficiency_1144","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3rhqat","score":1,"author_fullname":"t2_1nkj9l14b0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Makes no difference unless its crazily bad","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3s782l","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Makes no difference unless its crazily bad&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2su9b","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2su9b/local_model_on_two_different_gpus/n3s782l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752828019,"author_flair_text":null,"treatment_tags":[],"created_utc":1752828019,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3rhqat","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cannabibun","can_mod_post":false,"created_utc":1752814460,"send_replies":true,"parent_id":"t1_n3rf5aq","score":1,"author_fullname":"t2_6256r","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Is there anything I should consider when choosing a CPU/RAM? I am thinking of going with AMD CPU, but I know their graphic cards don't do that well with models.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3rhqat","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Is there anything I should consider when choosing a CPU/RAM? I am thinking of going with AMD CPU, but I know their graphic cards don&amp;#39;t do that well with models.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2su9b","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2su9b/local_model_on_two_different_gpus/n3rhqat/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752814460,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3rf5aq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Efficiency_1144","can_mod_post":false,"created_utc":1752813280,"send_replies":true,"parent_id":"t3_1m2su9b","score":2,"author_fullname":"t2_1nkj9l14b0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yeah you can split a model across both cards","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3rf5aq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah you can split a model across both cards&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2su9b/local_model_on_two_different_gpus/n3rf5aq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752813280,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2su9b","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3rm7u6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cannabibun","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3rlry2","score":1,"author_fullname":"t2_6256r","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah I don't care about speed that much either, I am happy with just being able to run a decent model.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3rm7u6","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah I don&amp;#39;t care about speed that much either, I am happy with just being able to run a decent model.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m2su9b","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2su9b/local_model_on_two_different_gpus/n3rm7u6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752816630,"author_flair_text":null,"treatment_tags":[],"created_utc":1752816630,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3rmn53","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GPTrack_ai","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3rlry2","score":1,"author_fullname":"t2_1tpuoj72sa","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Nope, two cards running at x4 IS a waste of resources.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3rmn53","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nope, two cards running at x4 IS a waste of resources.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m2su9b","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2su9b/local_model_on_two_different_gpus/n3rmn53/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752816847,"author_flair_text":null,"treatment_tags":[],"created_utc":1752816847,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3rlry2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"reacusn","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3rkc1m","score":1,"author_fullname":"t2_1ppg6hcqm8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You'll be fine, two 3090s with one running at x4 still generate faster than I can read. Your 2070 does have lower bandwidth ~450gb/s, but with the models you're running, since they're smaller, should still be fast enough. Just don't use tensor parallelism, as that *does* require inter-gpu bandwidth, and will be slower than pipeline parralelism. In my experience with llama.cpp and exllama 2 anyway.","edited":false,"author_flair_css_class":null,"name":"t1_n3rlry2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You&amp;#39;ll be fine, two 3090s with one running at x4 still generate faster than I can read. Your 2070 does have lower bandwidth ~450gb/s, but with the models you&amp;#39;re running, since they&amp;#39;re smaller, should still be fast enough. Just don&amp;#39;t use tensor parallelism, as that &lt;em&gt;does&lt;/em&gt; require inter-gpu bandwidth, and will be slower than pipeline parralelism. In my experience with llama.cpp and exllama 2 anyway.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m2su9b","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2su9b/local_model_on_two_different_gpus/n3rlry2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752816411,"author_flair_text":null,"collapsed":false,"created_utc":1752816411,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3rmgwa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GPTrack_ai","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3rkc1m","score":1,"author_fullname":"t2_1tpuoj72sa","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I suggest selling the whole system and get a RTX Pro 6000 or better.","edited":false,"author_flair_css_class":null,"name":"t1_n3rmgwa","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I suggest selling the whole system and get a RTX Pro 6000 or better.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m2su9b","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2su9b/local_model_on_two_different_gpus/n3rmgwa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752816757,"author_flair_text":null,"collapsed":false,"created_utc":1752816757,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3rkc1m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cannabibun","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3rk5pq","score":1,"author_fullname":"t2_6256r","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I doubt I can go much better than that by trading them for one card, the 2070 ain't worth shit atm.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3rkc1m","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I doubt I can go much better than that by trading them for one card, the 2070 ain&amp;#39;t worth shit atm.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2su9b","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2su9b/local_model_on_two_different_gpus/n3rkc1m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752815704,"author_flair_text":null,"treatment_tags":[],"created_utc":1752815704,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3rk5pq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GPTrack_ai","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3rjb9h","score":1,"author_fullname":"t2_1tpuoj72sa","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"VRAM size is one factor. But memory bandwidth is another. One single card will be much faster than two connected via PCIe.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3rk5pq","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;VRAM size is one factor. But memory bandwidth is another. One single card will be much faster than two connected via PCIe.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2su9b","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2su9b/local_model_on_two_different_gpus/n3rk5pq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752815620,"author_flair_text":null,"treatment_tags":[],"created_utc":1752815620,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3rjb9h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cannabibun","can_mod_post":false,"created_utc":1752815210,"send_replies":true,"parent_id":"t1_n3rhjri","score":1,"author_fullname":"t2_6256r","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I actually got the 3080 as a gift, the 2070 was my old card, so that's not really an option. I was under the impression total VRAM is what matters for running models, tho?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3rjb9h","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I actually got the 3080 as a gift, the 2070 was my old card, so that&amp;#39;s not really an option. I was under the impression total VRAM is what matters for running models, tho?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2su9b","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2su9b/local_model_on_two_different_gpus/n3rjb9h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752815210,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3rhjri","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GPTrack_ai","can_mod_post":false,"created_utc":1752814375,"send_replies":true,"parent_id":"t3_1m2su9b","score":1,"author_fullname":"t2_1tpuoj72sa","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Sell you outdated stuff on ebay and get something real.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3rhjri","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sell you outdated stuff on ebay and get something real.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2su9b/local_model_on_two_different_gpus/n3rhjri/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752814375,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2su9b","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3rzpp7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jacek2023","can_mod_post":false,"created_utc":1752823754,"send_replies":true,"parent_id":"t3_1m2su9b","score":1,"author_fullname":"t2_vqgbql9w","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"My first multi GPU setup was 3090 with 2070. It works with llama.cpp.\\n\\nHowever I recommend using 30x0 cards, because 2070 is older arch.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3rzpp7","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;My first multi GPU setup was 3090 with 2070. It works with llama.cpp.&lt;/p&gt;\\n\\n&lt;p&gt;However I recommend using 30x0 cards, because 2070 is older arch.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2su9b/local_model_on_two_different_gpus/n3rzpp7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752823754,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m2su9b","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3scqre","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MelodicRecognition7","can_mod_post":false,"created_utc":1752831191,"send_replies":true,"parent_id":"t3_1m2su9b","score":1,"author_fullname":"t2_1eex9ug5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"yes, llama.cpp:\\n\\n    -sm,   --split-mode {none,layer,row}    how to split the model across multiple GPUs, one of:\\n                                        - none: use one GPU only\\n                                        - layer (default): split layers and KV across GPUs\\n                                        - row: split rows across GPUs\\n                                        (env: LLAMA_ARG_SPLIT_MODE)\\n    -ts,   --tensor-split N0,N1,N2,...      fraction of the model to offload to each GPU, comma-separated list of                                                                                                     \\n                                        proportions, e.g. 3,1\\n                                        (env: LLAMA_ARG_TENSOR_SPLIT)\\n\\nshould be supported by llama.cpp derivatives such as oLlama but no guarantees.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3scqre","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;yes, llama.cpp:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;-sm,   --split-mode {none,layer,row}    how to split the model across multiple GPUs, one of:\\n                                    - none: use one GPU only\\n                                    - layer (default): split layers and KV across GPUs\\n                                    - row: split rows across GPUs\\n                                    (env: LLAMA_ARG_SPLIT_MODE)\\n-ts,   --tensor-split N0,N1,N2,...      fraction of the model to offload to each GPU, comma-separated list of                                                                                                     \\n                                    proportions, e.g. 3,1\\n                                    (env: LLAMA_ARG_TENSOR_SPLIT)\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;should be supported by llama.cpp derivatives such as oLlama but no guarantees.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2su9b/local_model_on_two_different_gpus/n3scqre/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752831191,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2su9b","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>l.jsx(e,{data:a});export{n as default};
