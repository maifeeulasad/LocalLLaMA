import{j as e}from"./index-BpC9hjVs.js";import{R as l}from"./RedditPostRenderer-BEo6AnSR.js";import"./index-DwkJHX1_.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I mean running o3-like models or better on smartphones/laptop NPUs with only a few watts of power, in an \\"easy way\\" for typical consumers and non technical people. I bet 2 years away","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"How far are we from *convenient* local models supremacy?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m5h2td","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.36,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_smr4fvghc","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753099473,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I mean running o3-like models or better on smartphones/laptop NPUs with only a few watts of power, in an &amp;quot;easy way&amp;quot; for typical consumers and non technical people. I bet 2 years away&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1m5h2td","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Element_H2O","discussion_type":null,"num_comments":34,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m5h2td/how_far_are_we_from_convenient_local_models/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m5h2td/how_far_are_we_from_convenient_local_models/","subreddit_subscribers":502516,"created_utc":1753099473,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4c807a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AppealSame4367","can_mod_post":false,"created_utc":1753105123,"send_replies":true,"parent_id":"t3_1m5h2td","score":5,"author_fullname":"t2_sxud8ccv4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Look at the newest nvidia openreasoning 7B models. I have a rtx 2060 laptop gpu with 6gb ram. started it in ollama with 4bit huggingface gguf and added it in kilocode ask mode: It's the first one in that size class that at least halfway works with roo code / kilocode without spending minutes \\"thinking\\". Instead it works and writes quite fast.\\n\\nAll the people saying \\"Long way\\" don't seem to watch the improvements of the last months. Extrapolate the software improvements for 7B and 3B models since beginning of the year and you have your convenient local model by the end of the year.\\n\\nBecause we see multiple interesting new techniques applied to improve context, speed up inference and reduce memory consumption all while getting better and better scores in all kinds of benchmarks. Companies will mix these approaches and one of them will finally make a small superintelligent model on o3 level.\\n\\nAt least from the benchmarks, nvidias Openreasoning fine tune models are on o3 level already \\\\_today\\\\_","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4c807a","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Look at the newest nvidia openreasoning 7B models. I have a rtx 2060 laptop gpu with 6gb ram. started it in ollama with 4bit huggingface gguf and added it in kilocode ask mode: It&amp;#39;s the first one in that size class that at least halfway works with roo code / kilocode without spending minutes &amp;quot;thinking&amp;quot;. Instead it works and writes quite fast.&lt;/p&gt;\\n\\n&lt;p&gt;All the people saying &amp;quot;Long way&amp;quot; don&amp;#39;t seem to watch the improvements of the last months. Extrapolate the software improvements for 7B and 3B models since beginning of the year and you have your convenient local model by the end of the year.&lt;/p&gt;\\n\\n&lt;p&gt;Because we see multiple interesting new techniques applied to improve context, speed up inference and reduce memory consumption all while getting better and better scores in all kinds of benchmarks. Companies will mix these approaches and one of them will finally make a small superintelligent model on o3 level.&lt;/p&gt;\\n\\n&lt;p&gt;At least from the benchmarks, nvidias Openreasoning fine tune models are on o3 level already _today_&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5h2td/how_far_are_we_from_convenient_local_models/n4c807a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753105123,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5h2td","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4cbop0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Thomas-Lore","can_mod_post":false,"created_utc":1753106290,"send_replies":true,"parent_id":"t1_n4bsfuq","score":4,"author_fullname":"t2_5hobp6m4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"2 years is long, I agree. Comments here will not age well.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4cbop0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;2 years is long, I agree. Comments here will not age well.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5h2td","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5h2td/how_far_are_we_from_convenient_local_models/n4cbop0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753106290,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n4bsfuq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"rorykoehler","can_mod_post":false,"created_utc":1753099588,"send_replies":true,"parent_id":"t3_1m5h2td","score":14,"author_fullname":"t2_ku4i0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Long way","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4bsfuq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Long way&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5h2td/how_far_are_we_from_convenient_local_models/n4bsfuq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753099588,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5h2td","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":14}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4cmwzn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MrPecunius","can_mod_post":false,"created_utc":1753109713,"send_replies":true,"parent_id":"t1_n4cbn98","score":3,"author_fullname":"t2_or0ok9hd7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Progress has been astonishing in the past year and is showing no signs of slowing down. It surprises me to see such lack of imagination in a group like this given the evidence before us.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4cmwzn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Progress has been astonishing in the past year and is showing no signs of slowing down. It surprises me to see such lack of imagination in a group like this given the evidence before us.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5h2td","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5h2td/how_far_are_we_from_convenient_local_models/n4cmwzn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753109713,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n4cbn98","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"bralynn2222","can_mod_post":false,"created_utc":1753106278,"send_replies":true,"parent_id":"t3_1m5h2td","score":3,"author_fullname":"t2_769j0jzd","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I have no clue why the disparity in this comment section seems to be so wide or claiming anything more than a few years as an answer of course none of us know for certain, but based off past history one year ago, local models were practically useless for anything other than role-play today llama 3.2 B consistently outperforms ChatGPT 3.5 turbo on benchmarking and it’s pretty damn close to overall functioning on the same level from personal use most viewed it as impossible last year so assuming current pace I’d say two and a half years maximum","edited":1753110706,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4cbn98","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have no clue why the disparity in this comment section seems to be so wide or claiming anything more than a few years as an answer of course none of us know for certain, but based off past history one year ago, local models were practically useless for anything other than role-play today llama 3.2 B consistently outperforms ChatGPT 3.5 turbo on benchmarking and it’s pretty damn close to overall functioning on the same level from personal use most viewed it as impossible last year so assuming current pace I’d say two and a half years maximum&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5h2td/how_far_are_we_from_convenient_local_models/n4cbn98/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753106278,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5h2td","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4bsz91","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"camracks","can_mod_post":false,"created_utc":1753099802,"send_replies":true,"parent_id":"t3_1m5h2td","score":2,"author_fullname":"t2_8i2tlel4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Typically there’s always gonna be the option to use more power and get something that is more capable, unless we hit a limit on how much power affects a model, I highly doubt that limit would ever be within consumer reach, but who knows, AI is wild and I never would have thought something like this would ever exist","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4bsz91","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Typically there’s always gonna be the option to use more power and get something that is more capable, unless we hit a limit on how much power affects a model, I highly doubt that limit would ever be within consumer reach, but who knows, AI is wild and I never would have thought something like this would ever exist&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5h2td/how_far_are_we_from_convenient_local_models/n4bsz91/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753099802,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5h2td","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4c4hem","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Unable-Finish-514","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4c2x1t","score":3,"author_fullname":"t2_sddn1vvb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ya, I can't argue with your point, as the subscription model has permeated into nearly every aspect of our lives.  That said, I gave the Chinese EV power and Chinese solar panel tech examples, as there was real progress made in both of these.  I'm with you that the big companies will not be the ones giving us this capability.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4c4hem","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ya, I can&amp;#39;t argue with your point, as the subscription model has permeated into nearly every aspect of our lives.  That said, I gave the Chinese EV power and Chinese solar panel tech examples, as there was real progress made in both of these.  I&amp;#39;m with you that the big companies will not be the ones giving us this capability.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5h2td","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5h2td/how_far_are_we_from_convenient_local_models/n4c4hem/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753103948,"author_flair_text":null,"treatment_tags":[],"created_utc":1753103948,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4dhueg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Environmental-Metal9","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4cr8g3","score":2,"author_fullname":"t2_6x9o42az","approved_by":null,"mod_note":null,"all_awardings":[],"body":"He was in the pockets of the billionaire elites from the start. His job was to just normalize the idea to us normies lol","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n4dhueg","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;He was in the pockets of the billionaire elites from the start. His job was to just normalize the idea to us normies lol&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m5h2td","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5h2td/how_far_are_we_from_convenient_local_models/n4dhueg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753118425,"author_flair_text":null,"treatment_tags":[],"created_utc":1753118425,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4cr8g3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MrPecunius","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4cp6nf","score":2,"author_fullname":"t2_or0ok9hd7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah, there's a reason I assigned \\"Neuromancer\\" as a screen time book to both of my kids. As well as the book is holding up, it turns out Gibson was an optimist.","edited":false,"author_flair_css_class":null,"name":"t1_n4cr8g3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, there&amp;#39;s a reason I assigned &amp;quot;Neuromancer&amp;quot; as a screen time book to both of my kids. As well as the book is holding up, it turns out Gibson was an optimist.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m5h2td","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5h2td/how_far_are_we_from_convenient_local_models/n4cr8g3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753110968,"author_flair_text":null,"collapsed":false,"created_utc":1753110968,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4cp6nf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Environmental-Metal9","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4ckgl6","score":2,"author_fullname":"t2_6x9o42az","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Which is even more dystopian than what we already have, and put us squarely in cyberpunk territory… with our neuralinks helping beam the ads straight to our psyche","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4cp6nf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Which is even more dystopian than what we already have, and put us squarely in cyberpunk territory… with our neuralinks helping beam the ads straight to our psyche&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5h2td","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5h2td/how_far_are_we_from_convenient_local_models/n4cp6nf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753110375,"author_flair_text":null,"treatment_tags":[],"created_utc":1753110375,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4ckgl6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MrPecunius","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4c2x1t","score":2,"author_fullname":"t2_or0ok9hd7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"If those \\"free\\" local models are salted with advertising/marketing, then the bigcorps will be happy to push them.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4ckgl6","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If those &amp;quot;free&amp;quot; local models are salted with advertising/marketing, then the bigcorps will be happy to push them.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5h2td","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5h2td/how_far_are_we_from_convenient_local_models/n4ckgl6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753108991,"author_flair_text":null,"treatment_tags":[],"created_utc":1753108991,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4c2x1t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Environmental-Metal9","can_mod_post":false,"created_utc":1753103418,"send_replies":true,"parent_id":"t1_n4c114t","score":0,"author_fullname":"t2_6x9o42az","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I would love to believe this narrative, but I doubt we will ever be allowed to collectively have nice things. There’s no financial incentive in empowering consumers. Every product is becoming a subscription, so your “local model” is only ever going to be a thing for tinkerers like in this community, or models of little to no consequence to main product/service providers. Not to be all doom and gloom, but cyberpunk and the 80s were spot on about how society would evolve with tech","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4c2x1t","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I would love to believe this narrative, but I doubt we will ever be allowed to collectively have nice things. There’s no financial incentive in empowering consumers. Every product is becoming a subscription, so your “local model” is only ever going to be a thing for tinkerers like in this community, or models of little to no consequence to main product/service providers. Not to be all doom and gloom, but cyberpunk and the 80s were spot on about how society would evolve with tech&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5h2td","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5h2td/how_far_are_we_from_convenient_local_models/n4c2x1t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753103418,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n4c114t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Unable-Finish-514","can_mod_post":false,"created_utc":1753102764,"send_replies":true,"parent_id":"t3_1m5h2td","score":2,"author_fullname":"t2_sddn1vvb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think you are spot on.  Being able to run LLMs on phones, in small websites and in apps/video games without significant power consumption will be revolutionary.  I can see why you predict 2 years.  Look the advancements made in China on power for EVs and in solar panels.  I can see a similar effort being made to run LLMs much more efficiently.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4c114t","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think you are spot on.  Being able to run LLMs on phones, in small websites and in apps/video games without significant power consumption will be revolutionary.  I can see why you predict 2 years.  Look the advancements made in China on power for EVs and in solar panels.  I can see a similar effort being made to run LLMs much more efficiently.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5h2td/how_far_are_we_from_convenient_local_models/n4c114t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753102764,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5h2td","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4d1k94","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"pip25hu","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4clv9b","score":1,"author_fullname":"t2_9u8ghp9","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"They are coming since quite a while now. I think I've heard of such plans a year ago, and nothing materialized since.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4d1k94","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They are coming since quite a while now. I think I&amp;#39;ve heard of such plans a year ago, and nothing materialized since.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5h2td","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5h2td/how_far_are_we_from_convenient_local_models/n4d1k94/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753113900,"author_flair_text":null,"treatment_tags":[],"created_utc":1753113900,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4cp2ix","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Cergorach","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4clv9b","score":1,"author_fullname":"t2_cs4w88d2l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Now let's say that running a 70b model costs 70w, and with ASIC it'll still cost 7w, that's besides needing WAY more and faster memory. And that's only a 70b model. We're talking about models that, even if you're able to fix 99% of the required performance, it's still too big for a smartphone...\\n\\nAnd ASIC is 'dangerous' as you're locked into a very particular way of calculation. What I'm hearing in the AI/LLM development circles, locking into a particular ASIC now either hinders development down the road or makes the specific ASIC obsolete.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4cp2ix","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Now let&amp;#39;s say that running a 70b model costs 70w, and with ASIC it&amp;#39;ll still cost 7w, that&amp;#39;s besides needing WAY more and faster memory. And that&amp;#39;s only a 70b model. We&amp;#39;re talking about models that, even if you&amp;#39;re able to fix 99% of the required performance, it&amp;#39;s still too big for a smartphone...&lt;/p&gt;\\n\\n&lt;p&gt;And ASIC is &amp;#39;dangerous&amp;#39; as you&amp;#39;re locked into a very particular way of calculation. What I&amp;#39;m hearing in the AI/LLM development circles, locking into a particular ASIC now either hinders development down the road or makes the specific ASIC obsolete.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5h2td","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5h2td/how_far_are_we_from_convenient_local_models/n4cp2ix/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753110342,"author_flair_text":null,"treatment_tags":[],"created_utc":1753110342,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4clv9b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MrPecunius","can_mod_post":false,"created_utc":1753109404,"send_replies":true,"parent_id":"t1_n4c48uu","score":0,"author_fullname":"t2_or0ok9hd7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"ASICs are coming ...\\n\\n&gt;For example, the power of Snapdragon AR1+ Gen 1 running a 1B vision model is 1 watt. An ASIC implementation of the same model can reduce it to 0.1 watt with design tradeoffs between silicon die area and power consumption by shifting the design from NPU + DDR architecture to ASIC + on-chip memory architecture.\\n\\n[https://www.eetimes.com/why-asic-design-makes-sense-for-llm-on-device/](https://www.eetimes.com/why-asic-design-makes-sense-for-llm-on-device/)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4clv9b","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;ASICs are coming ...&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;For example, the power of Snapdragon AR1+ Gen 1 running a 1B vision model is 1 watt. An ASIC implementation of the same model can reduce it to 0.1 watt with design tradeoffs between silicon die area and power consumption by shifting the design from NPU + DDR architecture to ASIC + on-chip memory architecture.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.eetimes.com/why-asic-design-makes-sense-for-llm-on-device/\\"&gt;https://www.eetimes.com/why-asic-design-makes-sense-for-llm-on-device/&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5h2td","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5h2td/how_far_are_we_from_convenient_local_models/n4clv9b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753109404,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n4c48uu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Cergorach","can_mod_post":false,"created_utc":1753103867,"send_replies":true,"parent_id":"t3_1m5h2td","score":4,"author_fullname":"t2_cs4w88d2l","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you mean size wise: Not in your lifetime. If you mean comparable results, probably not in decades if ever.\\n\\nPeople need to realize that there a limits to things. The largest ship ever build was build 50 years ago, no one is building a bigger ship 'just because'.\\n\\nLook at the performance difference in smartphone development, an iPhone 11 vs. an iPhone 16: [https://www.cpu-monkey.com/en/compare\\\\_cpu-apple\\\\_a18-vs-apple\\\\_a13\\\\_bionic](https://www.cpu-monkey.com/en/compare_cpu-apple_a18-vs-apple_a13_bionic)\\n\\nThe AI benchmark is x6 in 5 years, but memory bandwidth (what we use for inference) has not even doubled. The difference between what you can run on an iPhone 16 and what something like o3 takes in compute power is insane. IF the memory bandwidth were to progress at the same rate, you'll probably get to what the GPUs running the o3 models do in 40 years, but expect far before that for development to slow down significantly. And when you run a model that fits in the iPhone's 8GB of memory, it won't be using a few watts, it's going to get very hot! Not to mention that the TDP of an A18 chip is twice of that an A13 chip...\\n\\nAs an example my M4 pro has about x5 the memory speed of the A18, and x8 times the RAM, with just a keyboard and mouse (no display) connected it uses only 6-7W during normal operation. When I'm using just a 70b model it's using 70W, the fan turns on and it gets hot, and that's pretty insane already. H200 servers cost about half a million each, use about 10Kw each and you need multiple of such servers...","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4c48uu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you mean size wise: Not in your lifetime. If you mean comparable results, probably not in decades if ever.&lt;/p&gt;\\n\\n&lt;p&gt;People need to realize that there a limits to things. The largest ship ever build was build 50 years ago, no one is building a bigger ship &amp;#39;just because&amp;#39;.&lt;/p&gt;\\n\\n&lt;p&gt;Look at the performance difference in smartphone development, an iPhone 11 vs. an iPhone 16: &lt;a href=\\"https://www.cpu-monkey.com/en/compare_cpu-apple_a18-vs-apple_a13_bionic\\"&gt;https://www.cpu-monkey.com/en/compare_cpu-apple_a18-vs-apple_a13_bionic&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;The AI benchmark is x6 in 5 years, but memory bandwidth (what we use for inference) has not even doubled. The difference between what you can run on an iPhone 16 and what something like o3 takes in compute power is insane. IF the memory bandwidth were to progress at the same rate, you&amp;#39;ll probably get to what the GPUs running the o3 models do in 40 years, but expect far before that for development to slow down significantly. And when you run a model that fits in the iPhone&amp;#39;s 8GB of memory, it won&amp;#39;t be using a few watts, it&amp;#39;s going to get very hot! Not to mention that the TDP of an A18 chip is twice of that an A13 chip...&lt;/p&gt;\\n\\n&lt;p&gt;As an example my M4 pro has about x5 the memory speed of the A18, and x8 times the RAM, with just a keyboard and mouse (no display) connected it uses only 6-7W during normal operation. When I&amp;#39;m using just a 70b model it&amp;#39;s using 70W, the fan turns on and it gets hot, and that&amp;#39;s pretty insane already. H200 servers cost about half a million each, use about 10Kw each and you need multiple of such servers...&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5h2td/how_far_are_we_from_convenient_local_models/n4c48uu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753103867,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5h2td","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4bt0it","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"eli_pizza","can_mod_post":false,"created_utc":1753099816,"send_replies":true,"parent_id":"t3_1m5h2td","score":2,"author_fullname":"t2_1pdeyk44rl","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Won’t there be much better models that you can’t run locally in two years?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4bt0it","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Won’t there be much better models that you can’t run locally in two years?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5h2td/how_far_are_we_from_convenient_local_models/n4bt0it/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753099816,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5h2td","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4dotzw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ttkciar","can_mod_post":false,"created_utc":1753120325,"send_replies":true,"parent_id":"t1_n4c7ct5","score":1,"author_fullname":"t2_cpegz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"We have all of the next AI Winter to think about that \\"future tech\\".  Implementing it will trigger the next AI Summer.\\n\\nThe cycle keeps cycling!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4dotzw","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;We have all of the next AI Winter to think about that &amp;quot;future tech&amp;quot;.  Implementing it will trigger the next AI Summer.&lt;/p&gt;\\n\\n&lt;p&gt;The cycle keeps cycling!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5h2td","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5h2td/how_far_are_we_from_convenient_local_models/n4dotzw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753120325,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4c7ct5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"freecodeio","can_mod_post":false,"created_utc":1753104910,"send_replies":true,"parent_id":"t3_1m5h2td","score":2,"author_fullname":"t2_1oeu2j1o","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"we need something better\\n\\njust like llms are to traditional tree traversing chatbots, we need &lt;insert future tech&gt; that is same to llms","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4c7ct5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;we need something better&lt;/p&gt;\\n\\n&lt;p&gt;just like llms are to traditional tree traversing chatbots, we need &amp;lt;insert future tech&amp;gt; that is same to llms&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5h2td/how_far_are_we_from_convenient_local_models/n4c7ct5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753104910,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5h2td","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4bvvm3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Secure_Reflection409","can_mod_post":false,"created_utc":1753100915,"send_replies":true,"parent_id":"t3_1m5h2td","score":1,"author_fullname":"t2_by77ogdhr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Only a few watts of power?\\n\\nNever.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4bvvm3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Only a few watts of power?&lt;/p&gt;\\n\\n&lt;p&gt;Never.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5h2td/how_far_are_we_from_convenient_local_models/n4bvvm3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753100915,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5h2td","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4bzhn9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Thick-Protection-458","can_mod_post":false,"created_utc":1753102228,"send_replies":true,"parent_id":"t3_1m5h2td","score":1,"author_fullname":"t2_abr7phdd","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Quite far if achievable at all.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4bzhn9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Quite far if achievable at all.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5h2td/how_far_are_we_from_convenient_local_models/n4bzhn9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753102228,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5h2td","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4bunfv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Curious-138","can_mod_post":false,"created_utc":1753100448,"send_replies":true,"parent_id":"t3_1m5h2td","score":1,"author_fullname":"t2_a6wxpawj5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I run deepseek3-r1 on my laptop.  Is that what you want to know?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4bunfv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I run deepseek3-r1 on my laptop.  Is that what you want to know?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5h2td/how_far_are_we_from_convenient_local_models/n4bunfv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753100448,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5h2td","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4bvurw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Efficiency_1144","can_mod_post":false,"created_utc":1753100906,"send_replies":true,"parent_id":"t3_1m5h2td","score":1,"author_fullname":"t2_1nkj9l14b0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Those Deepseek distils, or alternatively those 7B-30B LLMs trained on math datasets using Deekseek GRPO technology do work well.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4bvurw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Those Deepseek distils, or alternatively those 7B-30B LLMs trained on math datasets using Deekseek GRPO technology do work well.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5h2td/how_far_are_we_from_convenient_local_models/n4bvurw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753100906,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5h2td","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4c3hl4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Paradigmind","can_mod_post":false,"created_utc":1753103612,"send_replies":true,"parent_id":"t1_n4bswoq","score":5,"author_fullname":"t2_6ste18zta","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It is the other way around.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4c3hl4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It is the other way around.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5h2td","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5h2td/how_far_are_we_from_convenient_local_models/n4c3hl4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753103612,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4dipx4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Environmental-Metal9","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4ct8bo","score":2,"author_fullname":"t2_6x9o42az","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I don’t want to sound like bill gates in the 80s though. I agree with your point about this being a normal progression for tech. I’m not even sure I have a better way for things to be that doesn’t involve massive societal changes. I think I’m more just musing on the definition of local. Being local capable doesn’t mean practical, so maybe it’s a matter of expectations or even just the pain of being left out at times, that makes knowing what local really means. I suppose for me, reading “local” is one of the many signals that I look for in figuring out if I can run it at cost or if I need to pay a service to use it.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n4dipx4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don’t want to sound like bill gates in the 80s though. I agree with your point about this being a normal progression for tech. I’m not even sure I have a better way for things to be that doesn’t involve massive societal changes. I think I’m more just musing on the definition of local. Being local capable doesn’t mean practical, so maybe it’s a matter of expectations or even just the pain of being left out at times, that makes knowing what local really means. I suppose for me, reading “local” is one of the many signals that I look for in figuring out if I can run it at cost or if I need to pay a service to use it.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m5h2td","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5h2td/how_far_are_we_from_convenient_local_models/n4dipx4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753118661,"author_flair_text":null,"treatment_tags":[],"created_utc":1753118661,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4ct8bo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"stan4cb","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4conpv","score":2,"author_fullname":"t2_2n7wncb6","approved_by":null,"mod_note":null,"all_awardings":[],"body":"well yeah, but thats outside of my definition of \\"local\\". My point was I had 8gigs of ram, now I've 64 and that is enough for many things. Tech and many components are moving with enough force that 256 even 1tb at home can happen.\\n\\nI also agree with your point yet I think it is too broad.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n4ct8bo","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;well yeah, but thats outside of my definition of &amp;quot;local&amp;quot;. My point was I had 8gigs of ram, now I&amp;#39;ve 64 and that is enough for many things. Tech and many components are moving with enough force that 256 even 1tb at home can happen.&lt;/p&gt;\\n\\n&lt;p&gt;I also agree with your point yet I think it is too broad.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m5h2td","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5h2td/how_far_are_we_from_convenient_local_models/n4ct8bo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753111541,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1753111541,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4conpv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Environmental-Metal9","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4c36m2","score":2,"author_fullname":"t2_6x9o42az","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Perhaps. I think in this community in specific the definition of local is a moving target. It’s like saying anyone can drive in America. It’s true by some definitions, but when you look at what all goes into that (a car is at least $2k, but can be less if you don’t mind the jank and have the skills to fix everything that breaks, plus insurance, plus gas, plus whatever else) it makes a large portion of the population literally geographically locked due to lack of access. The same thing will happen here, because nobody wants to subside AI for the masses in a non big brother way. Heck, we can’t even give people electricity without finding some moral failing on them when they can’t afford it. “Tough luck, should have had better straps on your boots” we collectively go, hoping we are not the next unlucky ones.","edited":false,"author_flair_css_class":null,"name":"t1_n4conpv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Perhaps. I think in this community in specific the definition of local is a moving target. It’s like saying anyone can drive in America. It’s true by some definitions, but when you look at what all goes into that (a car is at least $2k, but can be less if you don’t mind the jank and have the skills to fix everything that breaks, plus insurance, plus gas, plus whatever else) it makes a large portion of the population literally geographically locked due to lack of access. The same thing will happen here, because nobody wants to subside AI for the masses in a non big brother way. Heck, we can’t even give people electricity without finding some moral failing on them when they can’t afford it. “Tough luck, should have had better straps on your boots” we collectively go, hoping we are not the next unlucky ones.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m5h2td","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5h2td/how_far_are_we_from_convenient_local_models/n4conpv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753110221,"author_flair_text":null,"collapsed":false,"created_utc":1753110221,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4c36m2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"stan4cb","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4c26c6","score":3,"author_fullname":"t2_2n7wncb6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"for me it the ability to run it that makes it local and as relatively new 'niche' it only gets better.\\n\\none or two years ago I couldn't even imagine to run these models.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4c36m2","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;for me it the ability to run it that makes it local and as relatively new &amp;#39;niche&amp;#39; it only gets better.&lt;/p&gt;\\n\\n&lt;p&gt;one or two years ago I couldn&amp;#39;t even imagine to run these models.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5h2td","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5h2td/how_far_are_we_from_convenient_local_models/n4c36m2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753103509,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1753103509,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n4c26c6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Environmental-Metal9","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4btg0j","score":1,"author_fullname":"t2_6x9o42az","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Where easy=money as the model scales in size, therefore making it local in name only for 99% of the world. Still a benefit to everyone as having open source large models means providers can offer cheaper access, but that’s not the same thing as local, that’s just cheaper than buying my own server.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4c26c6","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Where easy=money as the model scales in size, therefore making it local in name only for 99% of the world. Still a benefit to everyone as having open source large models means providers can offer cheaper access, but that’s not the same thing as local, that’s just cheaper than buying my own server.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5h2td","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5h2td/how_far_are_we_from_convenient_local_models/n4c26c6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753103164,"author_flair_text":null,"treatment_tags":[],"created_utc":1753103164,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4btg0j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"stan4cb","can_mod_post":false,"created_utc":1753099984,"send_replies":true,"parent_id":"t1_n4bswoq","score":7,"author_fullname":"t2_2n7wncb6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"how so? we have most capable local modes now and it is getting easier to run them\\n\\njust look at Qwen 3, Deepseek R1 and Kimi 2","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4btg0j","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;how so? we have most capable local modes now and it is getting easier to run them&lt;/p&gt;\\n\\n&lt;p&gt;just look at Qwen 3, Deepseek R1 and Kimi 2&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5h2td","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5h2td/how_far_are_we_from_convenient_local_models/n4btg0j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753099984,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}}],"before":null}},"user_reports":[],"saved":false,"id":"n4bswoq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"redditscraperbot2","can_mod_post":false,"created_utc":1753099774,"send_replies":true,"parent_id":"t3_1m5h2td","score":-3,"author_fullname":"t2_pjna1o39","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Maybe I've been out of the local loop for a while, but it feels like the gap local and private models has been getting wider and wider recently.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4bswoq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Maybe I&amp;#39;ve been out of the local loop for a while, but it feels like the gap local and private models has been getting wider and wider recently.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5h2td/how_far_are_we_from_convenient_local_models/n4bswoq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753099774,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5h2td","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4c2po3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Unlucky-Message8866","can_mod_post":false,"created_utc":1753103348,"send_replies":true,"parent_id":"t3_1m5h2td","score":0,"author_fullname":"t2_de5gm7w40","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"there's only apple/google/huawei/qualcomm interested in that. nvidia dominates the market and it will for at the next years and edge models are in their infancy to be actually useful so I don't this to happen anytime soon.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4c2po3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;there&amp;#39;s only apple/google/huawei/qualcomm interested in that. nvidia dominates the market and it will for at the next years and edge models are in their infancy to be actually useful so I don&amp;#39;t this to happen anytime soon.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5h2td/how_far_are_we_from_convenient_local_models/n4c2po3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753103348,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5h2td","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}}]`),r=()=>e.jsx(l,{data:a});export{r as default};
