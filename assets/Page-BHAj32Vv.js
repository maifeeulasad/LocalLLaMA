import{j as e}from"./index-xfnGEtuL.js";import{R as t}from"./RedditPostRenderer-DAZRIZZK.js";import"./index-BaNn5-RR.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"My suggestion for how to make this profitable is list the hyped model and explain what it is very bad at for you… then list one or two models and the environment you use them in daily that do a better job. \\n\\nI had multiple people gushing over how effective Reka was for creative writing, and so I tried it in a RP conversation in Silly Tavern and also in regular story generation in Oobabooga’s text generation UI. I wasn’t happy with either. \\n\\nI prefer llama 3.3 70b and Gemma 27b over it in those environments … though I love Reka’s license.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Let’s talk about models you believed are more Hyped than Hot","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lyvkhr","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.53,"author_flair_background_color":null,"subreddit_type":"public","ups":1,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_dissgzyl","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":1,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752420473,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;My suggestion for how to make this profitable is list the hyped model and explain what it is very bad at for you… then list one or two models and the environment you use them in daily that do a better job. &lt;/p&gt;\\n\\n&lt;p&gt;I had multiple people gushing over how effective Reka was for creative writing, and so I tried it in a RP conversation in Silly Tavern and also in regular story generation in Oobabooga’s text generation UI. I wasn’t happy with either. &lt;/p&gt;\\n\\n&lt;p&gt;I prefer llama 3.3 70b and Gemma 27b over it in those environments … though I love Reka’s license.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lyvkhr","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"silenceimpaired","discussion_type":null,"num_comments":21,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lyvkhr/lets_talk_about_models_you_believed_are_more/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lyvkhr/lets_talk_about_models_you_believed_are_more/","subreddit_subscribers":498850,"created_utc":1752420473,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n31dtj7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Corporate_Drone31","can_mod_post":false,"send_replies":true,"parent_id":"t1_n300p95","score":1,"author_fullname":"t2_32o8hu91","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yup, I have enough system RAM to load it anyway. Of course the models that do fit in GPU are like 10-20 times faster.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n31dtj7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yup, I have enough system RAM to load it anyway. Of course the models that do fit in GPU are like 10-20 times faster.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyvkhr","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyvkhr/lets_talk_about_models_you_believed_are_more/n31dtj7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752478366,"author_flair_text":null,"treatment_tags":[],"created_utc":1752478366,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n300p95","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Paradigmind","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2x1hqt","score":1,"author_fullname":"t2_6ste18zta","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"But full R1 1.5 bits doesn't fit in 24GB vRam, does it?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n300p95","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;But full R1 1.5 bits doesn&amp;#39;t fit in 24GB vRam, does it?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyvkhr","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyvkhr/lets_talk_about_models_you_believed_are_more/n300p95/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752456317,"author_flair_text":null,"treatment_tags":[],"created_utc":1752456317,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2x1hqt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Corporate_Drone31","can_mod_post":false,"created_utc":1752422619,"send_replies":true,"parent_id":"t1_n2wztig","score":9,"author_fullname":"t2_32o8hu91","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I think Gemma 3 is incomparably better than Gemma 2, at least. I've been exploring it more from the perspective of situational awareness more than RP, and it's surprising that it can do this much. That said, the full R1 beats it by a margin even quantised to 1.5 bits. ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2x1hqt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think Gemma 3 is incomparably better than Gemma 2, at least. I&amp;#39;ve been exploring it more from the perspective of situational awareness more than RP, and it&amp;#39;s surprising that it can do this much. That said, the full R1 beats it by a margin even quantised to 1.5 bits. &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyvkhr","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyvkhr/lets_talk_about_models_you_believed_are_more/n2x1hqt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752422619,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2x2fs0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"stoppableDissolution","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2x21ie","score":1,"author_fullname":"t2_1n0su21k4z","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"No clue tbh, but I dont think so","edited":false,"author_flair_css_class":null,"name":"t1_n2x2fs0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No clue tbh, but I dont think so&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lyvkhr","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyvkhr/lets_talk_about_models_you_believed_are_more/n2x2fs0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752422905,"author_flair_text":null,"collapsed":false,"created_utc":1752422905,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2x21ie","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"silenceimpaired","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2x1ejj","score":1,"author_fullname":"t2_dissgzyl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Neither of those allow commercial use correct?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2x21ie","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Neither of those allow commercial use correct?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyvkhr","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyvkhr/lets_talk_about_models_you_believed_are_more/n2x21ie/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752422785,"author_flair_text":null,"treatment_tags":[],"created_utc":1752422785,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2x1ejj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"stoppableDissolution","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2x0wss","score":0,"author_fullname":"t2_1n0su21k4z","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I eventually settled with nemotron super as a daily driver for 48gb vram, but occasionally go back to monstral. Such a gem of a model.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2x1ejj","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I eventually settled with nemotron super as a daily driver for 48gb vram, but occasionally go back to monstral. Such a gem of a model.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyvkhr","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyvkhr/lets_talk_about_models_you_believed_are_more/n2x1ejj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752422592,"author_flair_text":null,"treatment_tags":[],"created_utc":1752422592,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n2x0wss","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"silenceimpaired","can_mod_post":false,"created_utc":1752422442,"send_replies":true,"parent_id":"t1_n2wztig","score":3,"author_fullname":"t2_dissgzyl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I’m just bummed with the license of mistral large… especially this far out. Wish they updated it to Apache.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2x0wss","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I’m just bummed with the license of mistral large… especially this far out. Wish they updated it to Apache.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyvkhr","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyvkhr/lets_talk_about_models_you_believed_are_more/n2x0wss/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752422442,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2yuhb0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"stoppableDissolution","can_mod_post":false,"created_utc":1752441916,"send_replies":true,"parent_id":"t1_n2yu8vk","score":2,"author_fullname":"t2_1n0su21k4z","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yea, fair.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n2yuhb0","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yea, fair.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyvkhr/lets_talk_about_models_you_believed_are_more/n2yuhb0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752441916,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyvkhr","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":8,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":0,"name":"t1__","id":"_","parent_id":"t1_n30fdbd","depth":10,"children":[]}}],"before":null}},"user_reports":[],"saved":false,"id":"n30fdbd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Lakius_2401","can_mod_post":false,"created_utc":1752461697,"send_replies":true,"parent_id":"t1_n304p7l","score":3,"author_fullname":"t2_9lxhk107","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Hello!\\n\\nLLMs have a rather notable problem with going along with context and they all work off biases. Your system prompt will have a huge impact on output quality, and different models need different system prompts sometime. Without a strong character card and prompt, it will just wing it for reactions, and will try to please you to the detriment of believability. Gemma 3 is particularly bad for being a yes man sycophant by default.\\n\\nSampler settings are more of a stability type thing, and don't really improve the output, they just keep it from spiraling to insanity or exploding. I use Temp between 0.4 and 1, depending on model (low is more predictable and dry with word choice), and Min-P of 0.05. Use DRY of 0.8, 1.8, 4, Rep Range of 4000. If DRY isn't in your system, use Repetition Penalty of 1.07. Every other sampler I disable, they're not really important (and disabled is either 0 or 1, be careful to get this right for each).\\n\\nLooking to RP and play a character interacting with a setting or another character? You'll want to start your adventure by looking into SillyTavern, which is an RP focused front end. It uses character cards, and some are really incredibly complex and well designed. Others encourage your LLM to be very dumb.  \\n(use [https://huggingface.co/MarinaraSpaghetti/SillyTavern-Settings](https://huggingface.co/MarinaraSpaghetti/SillyTavern-Settings) for these ST settings - there's a gemma preset in the \\"customized\\" folder on the Files and Versions tab, and use the master import as described on the main page of that link)\\n\\nLooking to write stories? You'll still want a system prompt, but you can do without if you switch to text completion mode and write a few paragraphs at least to start. I use instruct mode most times, and if you want to mimic instruct mode while in text completion, you can write something like &lt;Author's note: I want X to do Y, I'll pick this up tomorrow when I've had some rest&gt;, which is a very unsubtle push to get X to do Y, and for the AI to \\"reset\\" its thinking, and writing quality. Kobold AI's front end includes Author's Note in the context if you want to automate that.\\n\\n**Critical advice in general:** If you don't like what the AI is outputting, and you liked it before, don't settle. Retry the generation, or, rewrite it yourself in part or in whole. Or provide feedback, if you're in instruct mode or chat. The AI is a trend following machine. If you leave garbage in the mix, it will think garbage is acceptable.\\n\\nRecent models I have liked:   \\n**Cydonia v1.3 Magnum v4.** This is a 22B Mistral mix. An old favorite. Skyfall from TheDrummer is also good, but imho his more recent mixes are... very dark. New models in general have a significant positivity bias, so hitting the sweet spot is tough. Gemma3 in fictional settings was already quite easy to slip into dark, morbid tones (holy shit did it ever make an awful place to live for so many cities), but in chat use it's aggressively sycophantic and preachy.  \\n**QwQ Snowdrop 32B.** I really keep coming back to this. Needs to be able to do CoT to shine (which is a longer wait), but it's one that I keep sighing and opening when new things annoy me in other models.  \\nBase GLM or Gemma3 or Qwen3, for worldbuilding or general purposes. Finetunes rarely improve on the overall intelligence of a model.\\n\\nStuff I haven't liked but want to:  \\nQwen3. Needs some more time for fresh finetunes to get rid of the clinical, benchmaxxing, too smart to write well stink. Slips into chinese like QwQ does. The MoE version is blazing fast but not as good, like the quality of a 12B at 50% faster speeds.  \\nGLM. I really want to like it but the dialogue is atrocious. Everything besides dialogue is **really** nice though.\\n\\nPlease DM me directly if you want to chat quicker or want my system prompts. I'm cool talking here, but, \\\\**gestures at wall of text\\\\** I had to make some assumptions.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n30fdbd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\\n\\n&lt;p&gt;LLMs have a rather notable problem with going along with context and they all work off biases. Your system prompt will have a huge impact on output quality, and different models need different system prompts sometime. Without a strong character card and prompt, it will just wing it for reactions, and will try to please you to the detriment of believability. Gemma 3 is particularly bad for being a yes man sycophant by default.&lt;/p&gt;\\n\\n&lt;p&gt;Sampler settings are more of a stability type thing, and don&amp;#39;t really improve the output, they just keep it from spiraling to insanity or exploding. I use Temp between 0.4 and 1, depending on model (low is more predictable and dry with word choice), and Min-P of 0.05. Use DRY of 0.8, 1.8, 4, Rep Range of 4000. If DRY isn&amp;#39;t in your system, use Repetition Penalty of 1.07. Every other sampler I disable, they&amp;#39;re not really important (and disabled is either 0 or 1, be careful to get this right for each).&lt;/p&gt;\\n\\n&lt;p&gt;Looking to RP and play a character interacting with a setting or another character? You&amp;#39;ll want to start your adventure by looking into SillyTavern, which is an RP focused front end. It uses character cards, and some are really incredibly complex and well designed. Others encourage your LLM to be very dumb.&lt;br/&gt;\\n(use &lt;a href=\\"https://huggingface.co/MarinaraSpaghetti/SillyTavern-Settings\\"&gt;https://huggingface.co/MarinaraSpaghetti/SillyTavern-Settings&lt;/a&gt; for these ST settings - there&amp;#39;s a gemma preset in the &amp;quot;customized&amp;quot; folder on the Files and Versions tab, and use the master import as described on the main page of that link)&lt;/p&gt;\\n\\n&lt;p&gt;Looking to write stories? You&amp;#39;ll still want a system prompt, but you can do without if you switch to text completion mode and write a few paragraphs at least to start. I use instruct mode most times, and if you want to mimic instruct mode while in text completion, you can write something like &amp;lt;Author&amp;#39;s note: I want X to do Y, I&amp;#39;ll pick this up tomorrow when I&amp;#39;ve had some rest&amp;gt;, which is a very unsubtle push to get X to do Y, and for the AI to &amp;quot;reset&amp;quot; its thinking, and writing quality. Kobold AI&amp;#39;s front end includes Author&amp;#39;s Note in the context if you want to automate that.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Critical advice in general:&lt;/strong&gt; If you don&amp;#39;t like what the AI is outputting, and you liked it before, don&amp;#39;t settle. Retry the generation, or, rewrite it yourself in part or in whole. Or provide feedback, if you&amp;#39;re in instruct mode or chat. The AI is a trend following machine. If you leave garbage in the mix, it will think garbage is acceptable.&lt;/p&gt;\\n\\n&lt;p&gt;Recent models I have liked:&lt;br/&gt;\\n&lt;strong&gt;Cydonia v1.3 Magnum v4.&lt;/strong&gt; This is a 22B Mistral mix. An old favorite. Skyfall from TheDrummer is also good, but imho his more recent mixes are... very dark. New models in general have a significant positivity bias, so hitting the sweet spot is tough. Gemma3 in fictional settings was already quite easy to slip into dark, morbid tones (holy shit did it ever make an awful place to live for so many cities), but in chat use it&amp;#39;s aggressively sycophantic and preachy.&lt;br/&gt;\\n&lt;strong&gt;QwQ Snowdrop 32B.&lt;/strong&gt; I really keep coming back to this. Needs to be able to do CoT to shine (which is a longer wait), but it&amp;#39;s one that I keep sighing and opening when new things annoy me in other models.&lt;br/&gt;\\nBase GLM or Gemma3 or Qwen3, for worldbuilding or general purposes. Finetunes rarely improve on the overall intelligence of a model.&lt;/p&gt;\\n\\n&lt;p&gt;Stuff I haven&amp;#39;t liked but want to:&lt;br/&gt;\\nQwen3. Needs some more time for fresh finetunes to get rid of the clinical, benchmaxxing, too smart to write well stink. Slips into chinese like QwQ does. The MoE version is blazing fast but not as good, like the quality of a 12B at 50% faster speeds.&lt;br/&gt;\\nGLM. I really want to like it but the dialogue is atrocious. Everything besides dialogue is &lt;strong&gt;really&lt;/strong&gt; nice though.&lt;/p&gt;\\n\\n&lt;p&gt;Please DM me directly if you want to chat quicker or want my system prompts. I&amp;#39;m cool talking here, but, *&lt;em&gt;gestures at wall of text\\\\&lt;/em&gt;* I had to make some assumptions.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyvkhr/lets_talk_about_models_you_believed_are_more/n30fdbd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752461697,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyvkhr","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":9,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n30hc83","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Lakius_2401","can_mod_post":false,"created_utc":1752462473,"send_replies":true,"parent_id":"t1_n304p7l","score":2,"author_fullname":"t2_9lxhk107","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"See this site for a visual demonstration of how each of the samplers works:\\n\\n[https://artefact2.github.io/llm-sampling/index.xhtml](https://artefact2.github.io/llm-sampling/index.xhtml)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n30hc83","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;See this site for a visual demonstration of how each of the samplers works:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://artefact2.github.io/llm-sampling/index.xhtml\\"&gt;https://artefact2.github.io/llm-sampling/index.xhtml&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyvkhr/lets_talk_about_models_you_believed_are_more/n30hc83/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752462473,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyvkhr","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":9,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n304p7l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Paradigmind","can_mod_post":false,"created_utc":1752457764,"send_replies":true,"parent_id":"t1_n2yu8vk","score":2,"author_fullname":"t2_6ste18zta","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Hey there. I'm relatively new to local llms and downloaded Synthia S1 27B (Q4_K_M) some days ago. I heard a lot of good things about it but I don't know if I'm happy with it. I did some tests in RP and things that happened (in the first 10 messages already): I massively insulted a character and then told it that it was just a joke. It immediately forgave me and continued as nothing happened. Then I insulted it again and it had like a psychic collapse, way too extreme. Or when a character died (okay I confess that I just shot the very same character for behaving so illogical), I talked to it and Synthia let the character live again and let it respond to me.\\nAnd the german is not too good. It is okayish but Synthia will occassionally invent new words or write some existing words or phrases wrong.\\n\\nNow I wrote a lot. I just wanted to ask if you could please share your Synthia presets (numbers and syntax) with me please? Maybe my settings are wrong.\\n\\nAnd do you have another recommendation for me, please? I'm not looking for an uber horny model. Most important for me would be that it keeps being logical and sticks to the roles and doesn't agree with everything I do or say. Unlobotomized uncensoredness is a plus though.\\n\\nThank you very much!","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n304p7l","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey there. I&amp;#39;m relatively new to local llms and downloaded Synthia S1 27B (Q4_K_M) some days ago. I heard a lot of good things about it but I don&amp;#39;t know if I&amp;#39;m happy with it. I did some tests in RP and things that happened (in the first 10 messages already): I massively insulted a character and then told it that it was just a joke. It immediately forgave me and continued as nothing happened. Then I insulted it again and it had like a psychic collapse, way too extreme. Or when a character died (okay I confess that I just shot the very same character for behaving so illogical), I talked to it and Synthia let the character live again and let it respond to me.\\nAnd the german is not too good. It is okayish but Synthia will occassionally invent new words or write some existing words or phrases wrong.&lt;/p&gt;\\n\\n&lt;p&gt;Now I wrote a lot. I just wanted to ask if you could please share your Synthia presets (numbers and syntax) with me please? Maybe my settings are wrong.&lt;/p&gt;\\n\\n&lt;p&gt;And do you have another recommendation for me, please? I&amp;#39;m not looking for an uber horny model. Most important for me would be that it keeps being logical and sticks to the roles and doesn&amp;#39;t agree with everything I do or say. Unlobotomized uncensoredness is a plus though.&lt;/p&gt;\\n\\n&lt;p&gt;Thank you very much!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyvkhr/lets_talk_about_models_you_believed_are_more/n304p7l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752457764,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyvkhr","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":8,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2yu8vk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Lakius_2401","can_mod_post":false,"created_utc":1752441842,"send_replies":true,"parent_id":"t1_n2yq7ml","score":3,"author_fullname":"t2_9lxhk107","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Starting and ending at \\"Gemma 3 is surprisingly inefficient with context VRAM\\" would do it then. I can run Gemma 3 on my system entirely in VRAM, with a quant that does not meaningfully affect its outputs (QAT). I cannot run anything above 32B on my 1x3090, unless the quant renders it effectively braindead, equivalent at best, or runs so slow I'd rather do anything else.\\n\\nConversely, I can run any 3b at fp16. Would I ever? No. VRAM usage means nothing until it means everything. You picked a wild overkill quant to fit your 48GB and were surprised at the context VRAM usage. That's valid. But an incorrect generalization with how you first presented it.\\n\\nI get that you can run bigger models at a smaller quant and potentially get better outputs. But if you're only considering context, and prefill your VRAM with a uselessly large quant, it's going to look bad, because you shifted the starting line by so much, and Gemma 3 is pretty bad there compared to its peers.\\n\\nAnyways, I don't even like Gemma 3 outside of Synthia, or contexts where I need incredibly SFW worldly knowledge and language capability. QWQ is better (and handles larger context intelligence much better). GLM is better. Last year's Mistral finetunes are often better.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n2yu8vk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Starting and ending at &amp;quot;Gemma 3 is surprisingly inefficient with context VRAM&amp;quot; would do it then. I can run Gemma 3 on my system entirely in VRAM, with a quant that does not meaningfully affect its outputs (QAT). I cannot run anything above 32B on my 1x3090, unless the quant renders it effectively braindead, equivalent at best, or runs so slow I&amp;#39;d rather do anything else.&lt;/p&gt;\\n\\n&lt;p&gt;Conversely, I can run any 3b at fp16. Would I ever? No. VRAM usage means nothing until it means everything. You picked a wild overkill quant to fit your 48GB and were surprised at the context VRAM usage. That&amp;#39;s valid. But an incorrect generalization with how you first presented it.&lt;/p&gt;\\n\\n&lt;p&gt;I get that you can run bigger models at a smaller quant and potentially get better outputs. But if you&amp;#39;re only considering context, and prefill your VRAM with a uselessly large quant, it&amp;#39;s going to look bad, because you shifted the starting line by so much, and Gemma 3 is pretty bad there compared to its peers.&lt;/p&gt;\\n\\n&lt;p&gt;Anyways, I don&amp;#39;t even like Gemma 3 outside of Synthia, or contexts where I need incredibly SFW worldly knowledge and language capability. QWQ is better (and handles larger context intelligence much better). GLM is better. Last year&amp;#39;s Mistral finetunes are often better.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lyvkhr","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyvkhr/lets_talk_about_models_you_believed_are_more/n2yu8vk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752441842,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2yq7ml","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"stoppableDissolution","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2yoszn","score":1,"author_fullname":"t2_1n0su21k4z","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Well, I'm comparing in terms of \\"what I can use to fill 48gb of vram\\", not between the same quants or anything, because comparing Q4 of 27b and Q4 of 123b would have been even more disingenuous. And even if we compare more apples to apples with say qwen32 of the same quant, it will allow for waaay more context.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n2yq7ml","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Well, I&amp;#39;m comparing in terms of &amp;quot;what I can use to fill 48gb of vram&amp;quot;, not between the same quants or anything, because comparing Q4 of 27b and Q4 of 123b would have been even more disingenuous. And even if we compare more apples to apples with say qwen32 of the same quant, it will allow for waaay more context.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lyvkhr","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyvkhr/lets_talk_about_models_you_believed_are_more/n2yq7ml/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752440636,"author_flair_text":null,"treatment_tags":[],"created_utc":1752440636,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2yoszn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Lakius_2401","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2ym4rt","score":1,"author_fullname":"t2_9lxhk107","approved_by":null,"mod_note":null,"all_awardings":[],"body":"You're not doing an apples to apples comparison if you're comparing the size taken for a Q8 to anything else at a better quant. I'm not saying Gemma 3's context memory size problems are unknown and worth ignoring, but losing 10GB to a less efficient quant then saying you can fit a different Q2 with less budget for context is disingenuous.\\n\\nSWA and \\"it depends on your use case\\" covers the rest.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n2yoszn","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You&amp;#39;re not doing an apples to apples comparison if you&amp;#39;re comparing the size taken for a Q8 to anything else at a better quant. I&amp;#39;m not saying Gemma 3&amp;#39;s context memory size problems are unknown and worth ignoring, but losing 10GB to a less efficient quant then saying you can fit a different Q2 with less budget for context is disingenuous.&lt;/p&gt;\\n\\n&lt;p&gt;SWA and &amp;quot;it depends on your use case&amp;quot; covers the rest.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lyvkhr","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyvkhr/lets_talk_about_models_you_believed_are_more/n2yoszn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752440218,"author_flair_text":null,"treatment_tags":[],"created_utc":1752440218,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ym4rt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"stoppableDissolution","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2yijke","score":1,"author_fullname":"t2_1n0su21k4z","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Just doublechecked, and ye, I mixed the numbers up. It was Q8, not Q6. It somehow manages to fill 45.5gb, idk on what. Q6 is more reasonable.\\n\\nStill, proper context recycling is non-negotiable for me. I'm mostly using local for RP and stuff, and there is a lot of shuffling of stuff in and out of context going on with ST. And waiting 30-60 seconds after each message is... sad. And if I can run a 49B on the same hardware and even with (almost) the same speed AND have the recycling, it becomes quite a nobrainer.","edited":false,"author_flair_css_class":null,"name":"t1_n2ym4rt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just doublechecked, and ye, I mixed the numbers up. It was Q8, not Q6. It somehow manages to fill 45.5gb, idk on what. Q6 is more reasonable.&lt;/p&gt;\\n\\n&lt;p&gt;Still, proper context recycling is non-negotiable for me. I&amp;#39;m mostly using local for RP and stuff, and there is a lot of shuffling of stuff in and out of context going on with ST. And waiting 30-60 seconds after each message is... sad. And if I can run a 49B on the same hardware and even with (almost) the same speed AND have the recycling, it becomes quite a nobrainer.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lyvkhr","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyvkhr/lets_talk_about_models_you_believed_are_more/n2ym4rt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752439428,"author_flair_text":null,"collapsed":false,"created_utc":1752439428,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2yijke","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Lakius_2401","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2y8gvt","score":1,"author_fullname":"t2_9lxhk107","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"SWA has a very minimal impact compared to running out of VRAM entirely. Requiring 24s to process 32k context every time (which only happens when entirely full), still gets me 24 T/s with unquantized context, using Gemma 3's Q4 QAT version. That's about 600 tokens for an equivalent amount of time, or an effective rate of 12 T/S if we only generate 600 tokens at a time. 15 T/s if we instead get 1000 tokens output at a time.\\n\\nIt depends on your use case though, of course. If you do not operate at the limit, and do not have new context appearing anywhere but the end, SWA is just better.\\n\\nDo I wish koboldCPP had a \\"reserve\\" option for Smart Context besides half? Yeah, setting it to 4k tokens and getting 4-6 full generations between full reprocessing would be nice. Any context above 20k is much less useful, from various benchmarks testing real world performance of said context.\\n\\nAnyways, I don't have a single hope of running a model larger than 19GB (size on disk) at tolerable speeds, even with quantized context. It'd take ages to load the context, and ages to output.\\n\\n(also not sure where you're getting 20k unquantized on 2x3090, even before kcpp supported SWA I had gemma3 27b Q\\\\_4\\\\_K\\\\_M with 12k context on 1x3090... Q6 is only 4gb more? 20K fits in easy with KV8 context with no SWA for me?)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2yijke","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;SWA has a very minimal impact compared to running out of VRAM entirely. Requiring 24s to process 32k context every time (which only happens when entirely full), still gets me 24 T/s with unquantized context, using Gemma 3&amp;#39;s Q4 QAT version. That&amp;#39;s about 600 tokens for an equivalent amount of time, or an effective rate of 12 T/S if we only generate 600 tokens at a time. 15 T/s if we instead get 1000 tokens output at a time.&lt;/p&gt;\\n\\n&lt;p&gt;It depends on your use case though, of course. If you do not operate at the limit, and do not have new context appearing anywhere but the end, SWA is just better.&lt;/p&gt;\\n\\n&lt;p&gt;Do I wish koboldCPP had a &amp;quot;reserve&amp;quot; option for Smart Context besides half? Yeah, setting it to 4k tokens and getting 4-6 full generations between full reprocessing would be nice. Any context above 20k is much less useful, from various benchmarks testing real world performance of said context.&lt;/p&gt;\\n\\n&lt;p&gt;Anyways, I don&amp;#39;t have a single hope of running a model larger than 19GB (size on disk) at tolerable speeds, even with quantized context. It&amp;#39;d take ages to load the context, and ages to output.&lt;/p&gt;\\n\\n&lt;p&gt;(also not sure where you&amp;#39;re getting 20k unquantized on 2x3090, even before kcpp supported SWA I had gemma3 27b Q_4_K_M with 12k context on 1x3090... Q6 is only 4gb more? 20K fits in easy with KV8 context with no SWA for me?)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyvkhr","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyvkhr/lets_talk_about_models_you_believed_are_more/n2yijke/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752438369,"author_flair_text":null,"treatment_tags":[],"created_utc":1752438369,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2y8gvt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"stoppableDissolution","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2xy522","score":1,"author_fullname":"t2_1n0su21k4z","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Without SWA (because it screws context shift, or at least used to) 27b Q6 only fits 20k unquantized context in 2x3090. Same Q6 of nemotron-super, that is 49b model, does fit 16k of unquantized context, same as IQ2\\\\_XS of mistral large.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2y8gvt","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Without SWA (because it screws context shift, or at least used to) 27b Q6 only fits 20k unquantized context in 2x3090. Same Q6 of nemotron-super, that is 49b model, does fit 16k of unquantized context, same as IQ2_XS of mistral large.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyvkhr","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyvkhr/lets_talk_about_models_you_believed_are_more/n2y8gvt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752435394,"author_flair_text":null,"treatment_tags":[],"created_utc":1752435394,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2xy522","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Lakius_2401","can_mod_post":false,"created_utc":1752432280,"send_replies":true,"parent_id":"t1_n2wztig","score":3,"author_fullname":"t2_9lxhk107","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I can run Gemma 3 (Q4\\\\_K\\\\_L at \\\\~18GB) with 32k context on a 3090 with SWA enabled, all on VRAM. Just the gguf for Mistral Large seems to be &gt;40GB even at Q2?\\n\\nHow exactly is that the same footprint???\\n\\nI agree with the earlier sentiment, though, I keep rolling back to earlier generations of models and feel more satisfied with the outputs. QWQ is just good, earlier Mistrals feel less... rigid and clinical. Benchmaxxing does not a good conversationalist make. To say nothing about how they're also making them safety obsessed sycophants too.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2xy522","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I can run Gemma 3 (Q4_K_L at ~18GB) with 32k context on a 3090 with SWA enabled, all on VRAM. Just the gguf for Mistral Large seems to be &amp;gt;40GB even at Q2?&lt;/p&gt;\\n\\n&lt;p&gt;How exactly is that the same footprint???&lt;/p&gt;\\n\\n&lt;p&gt;I agree with the earlier sentiment, though, I keep rolling back to earlier generations of models and feel more satisfied with the outputs. QWQ is just good, earlier Mistrals feel less... rigid and clinical. Benchmaxxing does not a good conversationalist make. To say nothing about how they&amp;#39;re also making them safety obsessed sycophants too.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyvkhr","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyvkhr/lets_talk_about_models_you_believed_are_more/n2xy522/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752432280,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n30y8b5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HiddenoO","can_mod_post":false,"created_utc":1752469961,"send_replies":true,"parent_id":"t1_n2wztig","score":2,"author_fullname":"t2_8127x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Most recent models have shifted their focus away from chatting and towards tool calling, coding, problem solving, etc., partially because that's how you win in benchmarks.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n30y8b5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Most recent models have shifted their focus away from chatting and towards tool calling, coding, problem solving, etc., partially because that&amp;#39;s how you win in benchmarks.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyvkhr","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyvkhr/lets_talk_about_models_you_believed_are_more/n30y8b5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752469961,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2wztig","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"stoppableDissolution","can_mod_post":false,"created_utc":1752422114,"send_replies":true,"parent_id":"t3_1lyvkhr","score":12,"author_fullname":"t2_1n0su21k4z","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Entire last gen of small-ish models. Qwen3, gemma, mistral small... They might be better in doing math or coding or smth, but they are not nice to talk to and are extremely rigid when it comes to more creative/emotional stuff (RP, sudo-therapy, etc). Even 8b llama3/nemo beat them handily.\\n\\nEspecially gemma - its such a memory hog, that you can run q2 mistral large in the same footprint and get infinitely better result.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2wztig","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Entire last gen of small-ish models. Qwen3, gemma, mistral small... They might be better in doing math or coding or smth, but they are not nice to talk to and are extremely rigid when it comes to more creative/emotional stuff (RP, sudo-therapy, etc). Even 8b llama3/nemo beat them handily.&lt;/p&gt;\\n\\n&lt;p&gt;Especially gemma - its such a memory hog, that you can run q2 mistral large in the same footprint and get infinitely better result.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyvkhr/lets_talk_about_models_you_believed_are_more/n2wztig/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752422114,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyvkhr","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}}],"before":null}}]`),r=()=>e.jsx(t,{data:l});export{r as default};
