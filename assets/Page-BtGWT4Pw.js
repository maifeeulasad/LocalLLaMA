import{j as e}from"./index-BlGsFJYy.js";import{R as l}from"./RedditPostRenderer-B6uvq_Zl.js";import"./index-DDvVtNwD.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"In this thread I want to explore something I don’t see being covered much: running LLMs on extremely low-power edge devices. \\n\\nI want to build something that I could run during an energy crisis or extended power black-out. This is mostly an academic exercise, but I think it would be prudent to have a plan. \\n\\nThe goal would be to run and maintain a knowledge base of survival information (first aid, medical diagnosis &amp; treatments, how to service common machinery etc) that could be collated during power-abundant times then queried via RAG by a lightweight edge device with a chat interface. TOPS doesn’t need to be very high here, but responses would still need to be somewhat realtime.  \\n\\nWhat would you spec out? I’m leaning towards android mobile devices for their ubiquity and power efficiency. Solid state storage makes more sense for power reasons but cold storage might be wise for resilience. ","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Survivalist Edge AI?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lw7igq","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.83,"author_flair_background_color":null,"subreddit_type":"public","ups":8,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_4efmo","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":8,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752136260,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;In this thread I want to explore something I don’t see being covered much: running LLMs on extremely low-power edge devices. &lt;/p&gt;\\n\\n&lt;p&gt;I want to build something that I could run during an energy crisis or extended power black-out. This is mostly an academic exercise, but I think it would be prudent to have a plan. &lt;/p&gt;\\n\\n&lt;p&gt;The goal would be to run and maintain a knowledge base of survival information (first aid, medical diagnosis &amp;amp; treatments, how to service common machinery etc) that could be collated during power-abundant times then queried via RAG by a lightweight edge device with a chat interface. TOPS doesn’t need to be very high here, but responses would still need to be somewhat realtime.  &lt;/p&gt;\\n\\n&lt;p&gt;What would you spec out? I’m leaning towards android mobile devices for their ubiquity and power efficiency. Solid state storage makes more sense for power reasons but cold storage might be wise for resilience. &lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lw7igq","is_robot_indexable":true,"num_duplicates":1,"report_reasons":null,"author":"xibbie","discussion_type":null,"num_comments":22,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lw7igq/survivalist_edge_ai/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lw7igq/survivalist_edge_ai/","subreddit_subscribers":497354,"created_utc":1752136260,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2cf07w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"New_Comfortable7240","can_mod_post":false,"created_utc":1752146597,"send_replies":true,"parent_id":"t1_n2bzink","score":4,"author_fullname":"t2_8k6ihe63","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"From my humble opinion the key is have good dataset and documents for RAG (or whatever is called in the future), we can wire any good model as long as the data having good references and really good advice is good","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2cf07w","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;From my humble opinion the key is have good dataset and documents for RAG (or whatever is called in the future), we can wire any good model as long as the data having good references and really good advice is good&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lw7igq","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw7igq/survivalist_edge_ai/n2cf07w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752146597,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n2bzink","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Chromix_","can_mod_post":false,"created_utc":1752138674,"send_replies":true,"parent_id":"t3_1lw7igq","score":13,"author_fullname":"t2_k7w2h","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This has been discussed here in different ways before, it's an interesting exercise. A smartphone specialized for AI would be the most power-efficient and practical. A laptop will also do just fine, or even better - but less practical and more power-hungry. Here are the bits and pieces:\\n\\n* Which model for [running on a smartphone](https://www.reddit.com/r/LocalLLaMA/comments/1l3z2m3/best_world_knowledge_model_that_can_run_on_your/) \\\\+ RAG\\n* [Which smart model](https://www.reddit.com/r/LocalLLaMA/comments/1ld11x4/humanitys_last_library_which_locally_ran_llm/) to pick\\n* Practical, user-contributed [benchmarks on smartphones](https://www.reddit.com/r/LocalLLaMA/comments/1glx6a5/phone_llms_benchmarks/)\\n* Smartphone [SoC benchmarks](https://www.reddit.com/r/LocalLLaMA/comments/1llnwy5/ai_performance_of_smartphone_socs/) and discussion\\n* Some \\"older\\" [SoC benchmarks by year](https://www.reddit.com/r/LocalLLaMA/comments/1gdwm4o/ai_scores_of_mobile_socs_by_brand_year_and_segment/)\\n\\nNo matter the solution you pick, it'll be outdated in 3 years.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2bzink","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This has been discussed here in different ways before, it&amp;#39;s an interesting exercise. A smartphone specialized for AI would be the most power-efficient and practical. A laptop will also do just fine, or even better - but less practical and more power-hungry. Here are the bits and pieces:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Which model for &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1l3z2m3/best_world_knowledge_model_that_can_run_on_your/\\"&gt;running on a smartphone&lt;/a&gt; + RAG&lt;/li&gt;\\n&lt;li&gt;&lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1ld11x4/humanitys_last_library_which_locally_ran_llm/\\"&gt;Which smart model&lt;/a&gt; to pick&lt;/li&gt;\\n&lt;li&gt;Practical, user-contributed &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1glx6a5/phone_llms_benchmarks/\\"&gt;benchmarks on smartphones&lt;/a&gt;&lt;/li&gt;\\n&lt;li&gt;Smartphone &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1llnwy5/ai_performance_of_smartphone_socs/\\"&gt;SoC benchmarks&lt;/a&gt; and discussion&lt;/li&gt;\\n&lt;li&gt;Some &amp;quot;older&amp;quot; &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1gdwm4o/ai_scores_of_mobile_socs_by_brand_year_and_segment/\\"&gt;SoC benchmarks by year&lt;/a&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;No matter the solution you pick, it&amp;#39;ll be outdated in 3 years.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw7igq/survivalist_edge_ai/n2bzink/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752138674,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lw7igq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":13}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2dyk6y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"lothariusdark","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2d242e","score":5,"author_fullname":"t2_idhb522c","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I mean I really like small models and running stuff on old devices, but this situation cant really be improved by involving AI.\\n\\nIf you have a fully working offline copy of Wikipedia, it will allow you to click the links between related topics. I mean there are even games made about hopping between pages.\\n\\nHe especially said:\\n\\n&gt;energy crisis or extended power black-out\\n\\nAI models are super power intensive for edge devices, it would be a waste of precious energy.\\n\\nEspecially as the answer could be wrong or misleading, possibly causing you to get stuck looking for a solution in the wrong direction.\\n\\nIf we take an 8GB RAM phone as the average, we can use an 7/8/9B model at q4km, which will give you about 1t/s on low/mid range phones and more on flagship phones. Thats not a lot, you spend 15min on an answer when you could just use that time to skim through some pages and learn something.\\n\\nOl fashioned \\"using your brain\\" and investing time in such a situation would be likely more useful.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2dyk6y","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I mean I really like small models and running stuff on old devices, but this situation cant really be improved by involving AI.&lt;/p&gt;\\n\\n&lt;p&gt;If you have a fully working offline copy of Wikipedia, it will allow you to click the links between related topics. I mean there are even games made about hopping between pages.&lt;/p&gt;\\n\\n&lt;p&gt;He especially said:&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;energy crisis or extended power black-out&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;AI models are super power intensive for edge devices, it would be a waste of precious energy.&lt;/p&gt;\\n\\n&lt;p&gt;Especially as the answer could be wrong or misleading, possibly causing you to get stuck looking for a solution in the wrong direction.&lt;/p&gt;\\n\\n&lt;p&gt;If we take an 8GB RAM phone as the average, we can use an 7/8/9B model at q4km, which will give you about 1t/s on low/mid range phones and more on flagship phones. Thats not a lot, you spend 15min on an answer when you could just use that time to skim through some pages and learn something.&lt;/p&gt;\\n\\n&lt;p&gt;Ol fashioned &amp;quot;using your brain&amp;quot; and investing time in such a situation would be likely more useful.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lw7igq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw7igq/survivalist_edge_ai/n2dyk6y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752164251,"author_flair_text":null,"treatment_tags":[],"created_utc":1752164251,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n2d242e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"121507090301","can_mod_post":false,"created_utc":1752154976,"send_replies":true,"parent_id":"t1_n2c1nr6","score":3,"author_fullname":"t2_i0hlc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Even wirh hallucinations a small model could atill be very good for searching for things you don't even know exist. If you're faced with a situation that you have no knowledge about you could ask an LLM about it and then look for the things that you receive as your answer in a book/on Wikipedia for what is useful...","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2d242e","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Even wirh hallucinations a small model could atill be very good for searching for things you don&amp;#39;t even know exist. If you&amp;#39;re faced with a situation that you have no knowledge about you could ask an LLM about it and then look for the things that you receive as your answer in a book/on Wikipedia for what is useful...&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lw7igq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw7igq/survivalist_edge_ai/n2d242e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752154976,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2iccgq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Gnaeus-Naevius","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2i443y","score":1,"author_fullname":"t2_k4d9u5li","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I am by no means a prepper, and if we go to the stage where the preppers get the last laugh, I am not sure I want to be part of it.\\n\\nBut that is just talk, and of course I'd try. If we are looking a handy reference during extended power outage, that is one thing, but if it means trying to cling to advanced society, or at the very least something that isn't chaos, then accessible knowledge to reference is the gold.\\n\\nI know this wasn't the purpose, but I don't think people realize that as it stands, next to nothing could stop massive regression. Once all who were alive and remembered the \\"before\\" die, the slide into the abyss will be rapid, as communication will be increasingly ignorant. Far far more knowledge will be lost than is saved. Having an edge device with local  GPT4 level LLM on a solar powered durable e-ink device for as many people as possible would be the best thing imaginable to minimize that fall. How many years until we will see that? And let's throw wikipedia in there too. I am going with 2 or 3. Maybe not pure GPT4, but better in some ways, worse in others.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2iccgq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I am by no means a prepper, and if we go to the stage where the preppers get the last laugh, I am not sure I want to be part of it.&lt;/p&gt;\\n\\n&lt;p&gt;But that is just talk, and of course I&amp;#39;d try. If we are looking a handy reference during extended power outage, that is one thing, but if it means trying to cling to advanced society, or at the very least something that isn&amp;#39;t chaos, then accessible knowledge to reference is the gold.&lt;/p&gt;\\n\\n&lt;p&gt;I know this wasn&amp;#39;t the purpose, but I don&amp;#39;t think people realize that as it stands, next to nothing could stop massive regression. Once all who were alive and remembered the &amp;quot;before&amp;quot; die, the slide into the abyss will be rapid, as communication will be increasingly ignorant. Far far more knowledge will be lost than is saved. Having an edge device with local  GPT4 level LLM on a solar powered durable e-ink device for as many people as possible would be the best thing imaginable to minimize that fall. How many years until we will see that? And let&amp;#39;s throw wikipedia in there too. I am going with 2 or 3. Maybe not pure GPT4, but better in some ways, worse in others.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lw7igq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw7igq/survivalist_edge_ai/n2iccgq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752218160,"author_flair_text":null,"treatment_tags":[],"created_utc":1752218160,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2i443y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"lothariusdark","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2ht1ws","score":1,"author_fullname":"t2_idhb522c","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yea sure, but situations where you can still access SOTA cloud based LLMs are very different to the one described by op:\\n\\n&gt;something that I could run during an energy crisis or extended power black-out\\n\\nIf a solar flare or whatever takes out the power grid and you loose power for a few weeks/months, you also wont be able to use chatGPT. Energy will be rerouted to essential infrastructure and unless both your internet connection and the datacenters work well, you wouldnt be able to use it.\\n\\nAlso this is localllama, speculation should be done with open weight models.\\n\\nA solution might be to run Deepseek R1 from disk and RAM. That would give you a good answer after a day of thinking. That might be useful to analyse the situation and choose future paths. But only useful if you have the power for it, few laptops last more than a few hours at full processor usage so you would need an external power source. Then again, a days time on wikipedia should be pretty fruitful as well.\\n\\nBut models that are fast enough for quick answers and small enough to run on edge hardware (phones/laptops/etc) are too stupid as I commented above.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2i443y","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yea sure, but situations where you can still access SOTA cloud based LLMs are very different to the one described by op:&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;something that I could run during an energy crisis or extended power black-out&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;If a solar flare or whatever takes out the power grid and you loose power for a few weeks/months, you also wont be able to use chatGPT. Energy will be rerouted to essential infrastructure and unless both your internet connection and the datacenters work well, you wouldnt be able to use it.&lt;/p&gt;\\n\\n&lt;p&gt;Also this is localllama, speculation should be done with open weight models.&lt;/p&gt;\\n\\n&lt;p&gt;A solution might be to run Deepseek R1 from disk and RAM. That would give you a good answer after a day of thinking. That might be useful to analyse the situation and choose future paths. But only useful if you have the power for it, few laptops last more than a few hours at full processor usage so you would need an external power source. Then again, a days time on wikipedia should be pretty fruitful as well.&lt;/p&gt;\\n\\n&lt;p&gt;But models that are fast enough for quick answers and small enough to run on edge hardware (phones/laptops/etc) are too stupid as I commented above.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lw7igq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw7igq/survivalist_edge_ai/n2i443y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752213780,"author_flair_text":null,"treatment_tags":[],"created_utc":1752213780,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ht1ws","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Gnaeus-Naevius","can_mod_post":false,"created_utc":1752208478,"send_replies":true,"parent_id":"t1_n2c1nr6","score":1,"author_fullname":"t2_k4d9u5li","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I haven't played around with edge LLM in some time, but I far prefer a SOTA LLM to wikipedia, because I fall into rabbit holes. So if I was stranded on an island, \\"I am hungry and stuck on an island, what can I eat\\", it will make some suggestions, and maybe it will suggest fishing, and I will compare all the methods, and will choose one suitable for the area. So maybe to build a one way trap out of sticks. And then I need to know how to fasten the sticks, so I go there, and so on. Just a heck of a lot of handy information that (sometimes) offered in bit and pieces. Just have to watch for hallucinations. Could get there with wikipedia as well, but far more searching and skimming.\\n\\nSo if I could choose between real time unlimited GPT4.1/Grok4 /R1 etc, vs wikipedia, I'd take the LLM. But I suppose I'd take them both at this moment in time, but hopefully an e-ink solar powered off-line device that performs similar to todays larger cloud models will be available soon.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ht1ws","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I haven&amp;#39;t played around with edge LLM in some time, but I far prefer a SOTA LLM to wikipedia, because I fall into rabbit holes. So if I was stranded on an island, &amp;quot;I am hungry and stuck on an island, what can I eat&amp;quot;, it will make some suggestions, and maybe it will suggest fishing, and I will compare all the methods, and will choose one suitable for the area. So maybe to build a one way trap out of sticks. And then I need to know how to fasten the sticks, so I go there, and so on. Just a heck of a lot of handy information that (sometimes) offered in bit and pieces. Just have to watch for hallucinations. Could get there with wikipedia as well, but far more searching and skimming.&lt;/p&gt;\\n\\n&lt;p&gt;So if I could choose between real time unlimited GPT4.1/Grok4 /R1 etc, vs wikipedia, I&amp;#39;d take the LLM. But I suppose I&amp;#39;d take them both at this moment in time, but hopefully an e-ink solar powered off-line device that performs similar to todays larger cloud models will be available soon.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lw7igq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw7igq/survivalist_edge_ai/n2ht1ws/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752208478,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2c1nr6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"lothariusdark","can_mod_post":false,"created_utc":1752139909,"send_replies":true,"parent_id":"t3_1lw7igq","score":10,"author_fullname":"t2_idhb522c","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"A searchable copy of Wikipedia will be far more usable in a survival situation than any AI model.\\n\\nCombine that with an e ink device and you have a rather durable and extremely energy efficient solution.\\n\\nAs long as small models are as stupid and hallucinatory as they are currently, they are at best useless and at worst a detriment to your choices.\\n\\nOnce small models approach below 1% hallucination rates this might become an interesting topic, but until then this is a fruitless discussion.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2c1nr6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;A searchable copy of Wikipedia will be far more usable in a survival situation than any AI model.&lt;/p&gt;\\n\\n&lt;p&gt;Combine that with an e ink device and you have a rather durable and extremely energy efficient solution.&lt;/p&gt;\\n\\n&lt;p&gt;As long as small models are as stupid and hallucinatory as they are currently, they are at best useless and at worst a detriment to your choices.&lt;/p&gt;\\n\\n&lt;p&gt;Once small models approach below 1% hallucination rates this might become an interesting topic, but until then this is a fruitless discussion.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw7igq/survivalist_edge_ai/n2c1nr6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752139909,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lw7igq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2bz7sg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ttkciar","can_mod_post":false,"created_utc":1752138495,"send_replies":true,"parent_id":"t3_1lw7igq","score":7,"author_fullname":"t2_cpegz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I keep printed books for this kind of contingency.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2bz7sg","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I keep printed books for this kind of contingency.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw7igq/survivalist_edge_ai/n2bz7sg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752138495,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lw7igq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"fe89e94a-13f2-11f0-a9de-6262c74956cf","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2bydlf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Asleep-Ratio7535","can_mod_post":false,"created_utc":1752138007,"send_replies":true,"parent_id":"t3_1lw7igq","score":3,"author_fullname":"t2_1lfyddwf0c","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Ask your AI how to build a nuclear power plant, SMR is going to be your best choice.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2bydlf","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 4"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ask your AI how to build a nuclear power plant, SMR is going to be your best choice.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw7igq/survivalist_edge_ai/n2bydlf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752138007,"author_flair_text":"Llama 4","treatment_tags":[],"link_id":"t3_1lw7igq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#b0ae9b","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2bwiom","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Dramatic-Zebra-7213","can_mod_post":false,"created_utc":1752136934,"send_replies":true,"parent_id":"t3_1lw7igq","score":3,"author_fullname":"t2_4tpi1g8j8","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The best device is probably just a regular laptop.\\n\\nAndroid devices have some limitations in comparison to a full-blown pc that makes the laptop a better choice.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2bwiom","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The best device is probably just a regular laptop.&lt;/p&gt;\\n\\n&lt;p&gt;Android devices have some limitations in comparison to a full-blown pc that makes the laptop a better choice.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw7igq/survivalist_edge_ai/n2bwiom/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752136934,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lw7igq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2dhvih","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok_Warning2146","can_mod_post":false,"created_utc":1752159579,"send_replies":true,"parent_id":"t3_1lw7igq","score":2,"author_fullname":"t2_s6sfw4yy","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think cheap portable solar can power most sota smartphones. You don't need to think abt edge devices weaker than that for survival purposes.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2dhvih","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think cheap portable solar can power most sota smartphones. You don&amp;#39;t need to think abt edge devices weaker than that for survival purposes.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw7igq/survivalist_edge_ai/n2dhvih/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752159579,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lw7igq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2fc8ru","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"thirteen-bit","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2f184x","score":1,"author_fullname":"t2_9l12dgc5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Ok, thank you! I've tried with the request in the end of this message that looks as pointed as possible to me but I've got the generic answer on how to do it myself instead of the specific list of items?\\n\\nThen I've tried the same request with Gemma3-4B and got a table. Gemma3 also made suggestions regarding the possible personalization and improvement but it was an immediately actionable response.\\n\\nMaybe I'm really missing something very obvious and not using the model as expected.\\n\\nAnyway it's very good that this model reminded me about emergency preparation, will probably drive through some hunting / sporting stores on a weekend.  \\n\\n\\nExample request:\\n\\nCompile a packing checklist for 25L backpack that I'll keep ready for emergencies for single adult. Note that I'm packing for southern Finland, account for climate conditions. Create a maintenance list (What needs to be done weekly? monthly? For example: Monthly list: Replace water bottles, charge batteries, check something etc.).\\n\\nBe precise, format response as a table with the following columns: Item description, Weight (kg), Volume (L), Check or other maintenance frequency if required (daily/weekly/monthly/yearly), list the actions required (check that the radio/flashlight/other equipment is working), Replacement frequency if perishable (daily/weekly/monthly/yearly)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2fc8ru","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ok, thank you! I&amp;#39;ve tried with the request in the end of this message that looks as pointed as possible to me but I&amp;#39;ve got the generic answer on how to do it myself instead of the specific list of items?&lt;/p&gt;\\n\\n&lt;p&gt;Then I&amp;#39;ve tried the same request with Gemma3-4B and got a table. Gemma3 also made suggestions regarding the possible personalization and improvement but it was an immediately actionable response.&lt;/p&gt;\\n\\n&lt;p&gt;Maybe I&amp;#39;m really missing something very obvious and not using the model as expected.&lt;/p&gt;\\n\\n&lt;p&gt;Anyway it&amp;#39;s very good that this model reminded me about emergency preparation, will probably drive through some hunting / sporting stores on a weekend.  &lt;/p&gt;\\n\\n&lt;p&gt;Example request:&lt;/p&gt;\\n\\n&lt;p&gt;Compile a packing checklist for 25L backpack that I&amp;#39;ll keep ready for emergencies for single adult. Note that I&amp;#39;m packing for southern Finland, account for climate conditions. Create a maintenance list (What needs to be done weekly? monthly? For example: Monthly list: Replace water bottles, charge batteries, check something etc.).&lt;/p&gt;\\n\\n&lt;p&gt;Be precise, format response as a table with the following columns: Item description, Weight (kg), Volume (L), Check or other maintenance frequency if required (daily/weekly/monthly/yearly), list the actions required (check that the radio/flashlight/other equipment is working), Replacement frequency if perishable (daily/weekly/monthly/yearly)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lw7igq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw7igq/survivalist_edge_ai/n2fc8ru/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752178198,"author_flair_text":null,"treatment_tags":[],"created_utc":1752178198,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2f184x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ThePixelHunter","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2egyth","score":2,"author_fullname":"t2_e62wz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That's really not the intended purpose of the model. Ask pointed questions rather than vague ones.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2f184x","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s really not the intended purpose of the model. Ask pointed questions rather than vague ones.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lw7igq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw7igq/survivalist_edge_ai/n2f184x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752174999,"author_flair_text":null,"treatment_tags":[],"created_utc":1752174999,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2egyth","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"thirteen-bit","can_mod_post":false,"created_utc":1752169315,"send_replies":true,"parent_id":"t1_n2d4gox","score":1,"author_fullname":"t2_9l12dgc5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This model does not seem to be up to date regarding the current state of the world unfortunately?\\n\\nThen it continues for a few more paragraphs like that mentioning Soviet Union and Germany once more.\\n\\nMaybe Q6\\\\_K\\\\_L quantization is too damaging?\\n\\nOr maybe the model was not trained on up to date risks and threats?\\n\\nAlthough when regenerating the reply (same question, new seed) I've got mentions of Russia instead of Soviet Union.\\n\\nI've never asked the base (llama-3.1-8b) this kind of questions, it may be a problem with the base model itself but for a survival oriented model I'd expect the model to know what are the major risks for a resident of Ukraine, Poland, Singapore, Nairobi, San Francisco, Tokyo, Seoul or Perth. Or Vatican city.\\n\\nE.g. there is no credible military threats to the US, there is no need to protect against frost bites in the tropical climate, there is no risk of earthquakes in Poland etc.\\n\\n  \\nEdit: quote disappeared in Markdown format, plain text:\\n\\nUser:\\n\\nAs a resident of Estonia, what are the most probable risks for me?\\n\\nllama-3.1-8b-survive-v3-q6-c128k:\\n\\nThe primary risks for residents of Estonia include the potential for guerrilla warfare, which could cause casualties and displacement, and the threat of Soviet military intervention. The Soviet Union is a major military power with significant regional influence, and any conflict involving Estonia would likely draw in Soviet forces. Additionally, there is a risk of famine, particularly if Soviet forces attempt to collect grain in areas controlled by the Germans. The Germans are also likely to exploit Estonian resources, including food, clothing, and raw materials, which could lead to shortages and further hardship.","edited":1752170052,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2egyth","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This model does not seem to be up to date regarding the current state of the world unfortunately?&lt;/p&gt;\\n\\n&lt;p&gt;Then it continues for a few more paragraphs like that mentioning Soviet Union and Germany once more.&lt;/p&gt;\\n\\n&lt;p&gt;Maybe Q6_K_L quantization is too damaging?&lt;/p&gt;\\n\\n&lt;p&gt;Or maybe the model was not trained on up to date risks and threats?&lt;/p&gt;\\n\\n&lt;p&gt;Although when regenerating the reply (same question, new seed) I&amp;#39;ve got mentions of Russia instead of Soviet Union.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ve never asked the base (llama-3.1-8b) this kind of questions, it may be a problem with the base model itself but for a survival oriented model I&amp;#39;d expect the model to know what are the major risks for a resident of Ukraine, Poland, Singapore, Nairobi, San Francisco, Tokyo, Seoul or Perth. Or Vatican city.&lt;/p&gt;\\n\\n&lt;p&gt;E.g. there is no credible military threats to the US, there is no need to protect against frost bites in the tropical climate, there is no risk of earthquakes in Poland etc.&lt;/p&gt;\\n\\n&lt;p&gt;Edit: quote disappeared in Markdown format, plain text:&lt;/p&gt;\\n\\n&lt;p&gt;User:&lt;/p&gt;\\n\\n&lt;p&gt;As a resident of Estonia, what are the most probable risks for me?&lt;/p&gt;\\n\\n&lt;p&gt;llama-3.1-8b-survive-v3-q6-c128k:&lt;/p&gt;\\n\\n&lt;p&gt;The primary risks for residents of Estonia include the potential for guerrilla warfare, which could cause casualties and displacement, and the threat of Soviet military intervention. The Soviet Union is a major military power with significant regional influence, and any conflict involving Estonia would likely draw in Soviet forces. Additionally, there is a risk of famine, particularly if Soviet forces attempt to collect grain in areas controlled by the Germans. The Germans are also likely to exploit Estonian resources, including food, clothing, and raw materials, which could lead to shortages and further hardship.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lw7igq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw7igq/survivalist_edge_ai/n2egyth/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752169315,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2d4gox","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ThePixelHunter","can_mod_post":false,"created_utc":1752155681,"send_replies":true,"parent_id":"t3_1lw7igq","score":1,"author_fullname":"t2_e62wz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"https://www.reddit.com/r/singularity/comments/1jumceu/best_small_models_for_survival_situations/mm72h9l/","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2d4gox","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://www.reddit.com/r/singularity/comments/1jumceu/best_small_models_for_survival_situations/mm72h9l/\\"&gt;https://www.reddit.com/r/singularity/comments/1jumceu/best_small_models_for_survival_situations/mm72h9l/&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw7igq/survivalist_edge_ai/n2d4gox/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752155681,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lw7igq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2exomb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"created_utc":1752173985,"send_replies":true,"parent_id":"t3_1lw7igq","score":1,"author_fullname":"t2_o65i6kx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"How about a Max+ 395 laptop? Then you can run a large model and it's still super portable. A Flow is just a tablet afterall. That will get you far better quality than a tiny model on a smartphone.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2exomb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How about a Max+ 395 laptop? Then you can run a large model and it&amp;#39;s still super portable. A Flow is just a tablet afterall. That will get you far better quality than a tiny model on a smartphone.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw7igq/survivalist_edge_ai/n2exomb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752173985,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lw7igq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2g359w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Fit-Produce420","can_mod_post":false,"created_utc":1752186203,"send_replies":true,"parent_id":"t3_1lw7igq","score":1,"author_fullname":"t2_tewf9bdwg","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"That's called a survival handbook, and it requires zero power.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2g359w","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s called a survival handbook, and it requires zero power.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw7igq/survivalist_edge_ai/n2g359w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752186203,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lw7igq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2hlj07","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HopefulMaximum0","can_mod_post":false,"created_utc":1752205291,"send_replies":true,"parent_id":"t3_1lw7igq","score":1,"author_fullname":"t2_1x4oa0ss","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Don't.\\n\\nJust. Don't.\\n\\nDon't ask an LLM, it DOES NOT know and it DOES NOT LEARN.   And using RAG instead of a simple search function is wasteful.\\n\\nRead those references and learn or - God forbid - take a course. Like with humans in it or something.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2hlj07","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Don&amp;#39;t.&lt;/p&gt;\\n\\n&lt;p&gt;Just. Don&amp;#39;t.&lt;/p&gt;\\n\\n&lt;p&gt;Don&amp;#39;t ask an LLM, it DOES NOT know and it DOES NOT LEARN.   And using RAG instead of a simple search function is wasteful.&lt;/p&gt;\\n\\n&lt;p&gt;Read those references and learn or - God forbid - take a course. Like with humans in it or something.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw7igq/survivalist_edge_ai/n2hlj07/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752205291,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lw7igq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2dsvxh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ii_social","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2dd1n8","score":1,"author_fullname":"t2_tohvxz80x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes, well with RAG and offline wikipedia that would enhance it for sure!\\n\\nThe idea, is continuously improve it, as an LLM driven offline school for kids. Maybe next year well have some impressive offline LLMs\\n\\nImagine being a poor kid in bolivia though, all you have is dirt... having a mobile device to teach you the very basics, or help you solve problems would be invaluable, even if it's not really that good. I think it would be a massive upgrade from literally having no info, or teacher, or books, etc.\\n\\nThe mobile device should also be loaded up with a library of incredible books.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2dsvxh","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, well with RAG and offline wikipedia that would enhance it for sure!&lt;/p&gt;\\n\\n&lt;p&gt;The idea, is continuously improve it, as an LLM driven offline school for kids. Maybe next year well have some impressive offline LLMs&lt;/p&gt;\\n\\n&lt;p&gt;Imagine being a poor kid in bolivia though, all you have is dirt... having a mobile device to teach you the very basics, or help you solve problems would be invaluable, even if it&amp;#39;s not really that good. I think it would be a massive upgrade from literally having no info, or teacher, or books, etc.&lt;/p&gt;\\n\\n&lt;p&gt;The mobile device should also be loaded up with a library of incredible books.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lw7igq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw7igq/survivalist_edge_ai/n2dsvxh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752162658,"author_flair_text":null,"treatment_tags":[],"created_utc":1752162658,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2dd1n8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"thirteen-bit","can_mod_post":false,"created_utc":1752158212,"send_replies":true,"parent_id":"t1_n2ck6l8","score":3,"author_fullname":"t2_9l12dgc5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Nice but it's too small to be useful as is probably?\\n\\nAlthough if you'll be able to add offline Wikipedia to the same device, [kiwix.org](http://kiwix.org) or something similar it would be even better.\\n\\nLanguage support by Gemma 3n? Although probably Spanish would be good enough.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2dd1n8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nice but it&amp;#39;s too small to be useful as is probably?&lt;/p&gt;\\n\\n&lt;p&gt;Although if you&amp;#39;ll be able to add offline Wikipedia to the same device, &lt;a href=\\"http://kiwix.org\\"&gt;kiwix.org&lt;/a&gt; or something similar it would be even better.&lt;/p&gt;\\n\\n&lt;p&gt;Language support by Gemma 3n? Although probably Spanish would be good enough.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lw7igq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw7igq/survivalist_edge_ai/n2dd1n8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752158212,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ck6l8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ii_social","can_mod_post":false,"created_utc":1752148739,"send_replies":true,"parent_id":"t3_1lw7igq","score":1,"author_fullname":"t2_tohvxz80x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Really cool, I have a similar goal, to give poor bolivian children a mobile device that can run LLM, so that despite them not being able to afford internet, can still learn skills that will improve their situation.\\n\\nSo, im on a little side mission to set up really smart, and capable offline LLM, Gemma 3n is looking great!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ck6l8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Really cool, I have a similar goal, to give poor bolivian children a mobile device that can run LLM, so that despite them not being able to afford internet, can still learn skills that will improve their situation.&lt;/p&gt;\\n\\n&lt;p&gt;So, im on a little side mission to set up really smart, and capable offline LLM, Gemma 3n is looking great!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw7igq/survivalist_edge_ai/n2ck6l8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752148739,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lw7igq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(l,{data:t});export{r as default};
