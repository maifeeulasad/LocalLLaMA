import{j as e}from"./index-Bu7qcPAU.js";import{R as l}from"./RedditPostRenderer-CbHA7O5q.js";import"./index-BKgbfxhf.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I figured I'd post my final setup since many people asked about the P40 and assumed you couldn't do much with it (but you can!).\\n\\n    numactl --cpunodebind=0 -- ./ik_llama.cpp/build/bin/llama-cli \\\\\\n        --numa numactl  \\\\\\n        --model models/unsloth/DeepSeek-R1-0528-GGUF/UD-Q2_K_XL/DeepSeek-R1-0528-UD-Q2_K_XL-00001-of-00006.gguf \\\\\\n        --threads 40 \\\\\\n        --cache-type-k q8_0 \\\\\\n        --cache-type-v q8_0 \\\\\\n        --top-p 0.95 \\\\\\n        --temp 0.6 \\\\\\n        --ctx-size 32768 \\\\\\n        --seed 3407 \\\\\\n        --n-gpu-layers 62 \\\\\\n        -ot \\"exps=CPU\\" \\\\\\n        --mlock \\\\\\n        --no-mmap \\\\\\n        -mla 2 -fa -fmoe \\\\\\n        -ser 5,1 \\\\\\n        -amb 512 \\\\\\n        --prompt \\"&lt;｜User｜&gt;Create a Flappy Bird game in Python.&lt;｜Assistant｜&gt;\\"\\n\\nThe result at the end of the run is around 6.5tk/s. &lt;EDIT: Did another run and added the results. 7tk/s!&gt;\\n\\n    llama_print_timings:        load time =  896376.08 ms\\n    llama_print_timings:      sample time =     594.81 ms /  2549 runs   (    0.23 ms per token,  4285.42 tokens per second)\\n    llama_print_timings: prompt eval time =    1193.93 ms /    12 tokens (   99.49 ms per token,    10.05 tokens per second)\\n    llama_print_timings:        eval time =  363871.92 ms /  2548 runs   (  142.81 ms per token,     7.00 tokens per second)\\n    llama_print_timings:       total time =  366975.53 ms /  2560 tokens\\n\\nI'm open to ideas on how to improve it.\\n\\nHardware:\\n\\n* Fully populated Dell R740 (in performance profile)\\n* Nvidia Tesla P40 (24GB vram)\\n* Xeon Gold 6138\\n* 1.5TB of ram (all ram slots populated)\\n\\nFor other models, like Mistral or QwQ I get around 10tk/s\\n\\nThese are my QwQ settings (I use the regular llama.cpp for this one)\\n\\n    numactl --cpunodebind=0 -- ./llama.cpp/build/bin/llama-cli \\\\\\n        --numa numactl  \\\\\\n        --model models/unsloth/unsloth-QwQ-32B-GGUF/QwQ-32B-Q4_K_M.gguf \\\\\\n        --threads 40 \\\\\\n        --ctx-size 16384 \\\\\\n        --n-gpu-layers 99 \\\\\\n        --seed 3407 \\\\\\n        --temp 0.6 \\\\\\n        --repeat-penalty 1.1 \\\\\\n        --min-p 0.01 \\\\\\n        --top-k 40 \\\\\\n        --top-p 0.95 \\\\\\n        --dry-multiplier 0.5 \\\\\\n        --mlock \\\\\\n        --no-mmap \\\\\\n        --prio 3 \\\\\\n        -no-cnv \\\\\\n        -fa  \\\\\\n        --samplers \\"top_k;top_p;min_p;temperature;dry;typ_p;xtc\\" \\\\\\n        --prompt \\"&lt;|im_start|&gt;user\\\\nCreate a Flappy Bird game in Python. You must include these things:\\\\n1. You must use pygame.\\\\n2. The background color should be randomly chosen and is a light shade. Start with a light blue color.\\\\n3. Pressing SPACE multiple times will accelerate the bird.\\\\n4. The bird's shape should be randomly chosen as a square, circle or triangle. The color should be randomly chosen as a dark color.\\\\n5. Place on the bottom some land colored as dark brown or yellow chosen randomly.\\\\n6. Make a score shown on the top right side. Increment if you pass pipes and don't hit them.\\\\n7. Make randomly spaced pipes with enough space. Color them randomly as dark green or light brown or a dark gray shade.\\\\n8. When you lose, show the best score. Make the text inside the screen. Pressing q or Esc will quit the game. Restarting is pressing SPACE again.\\\\nThe final game should be inside a markdown section in Python. Check your code for errors and fix them before the final markdown section.&lt;|im_end|&gt;\\\\n&lt;|im_start|&gt;assistant\\\\n&lt;think&gt;\\\\n\\"\\n\\nThe details on the selected quants are in the model path. Surprisingly, using ik\\\\_llama.cpp optimized models from *ubergarm* did not speed up Deepseek, but it slowed it down greatly.\\n\\nFeel free to suggest improvements. For models different than deepseek, ik\\\\_llama.cpp was giving me a lot of gibberish output if I enabled fast attention. And some models I couldn't even run on it, so that's why I still use the regular llama.cpp for some of them.\\n\\n  \\n\\\\-----\\n\\nEDIT\\n\\nI left it running in the background while doing other stuff, and with the community suggestions, I'm up to 7.57 tk/s! Thank you all! (notice that I can now use the 80 threads, but the performance is the same as 40 threads, because the bottleneck is in the memory bandwidth)\\n\\n    numactl --interleave=all -- ./ik_llama.cpp/build/bin/llama-cli \\\\\\n        --numa numactl  \\\\\\n        --model models/unsloth/DeepSeek-R1-0528-GGUF/UD-Q2_K_XL/DeepSeek-R1-0528-UD-Q2_K_XL-00001-of-00006.gguf \\\\\\n        --threads 80 \\\\\\n        --cache-type-k q8_0 \\\\\\n        --cache-type-v q8_0 \\\\\\n        --top-p 0.95 \\\\\\n        --temp 0.6 \\\\\\n        --ctx-size 32768 \\\\\\n        --seed 3407 \\\\\\n        --n-gpu-layers 62 \\\\\\n        -ot \\"exps=CPU\\" \\\\\\n        --mlock \\\\\\n        --no-mmap \\\\\\n        -mla 2 -fa -fmoe \\\\\\n        -ser 5,1 \\\\\\n        -amb 512 \\\\\\n        --run-time-repack -b 4096 -ub 4096 \\\\\\n        --prompt \\"&lt;｜User｜&gt;Create a Flappy Bird game in Python.&lt;｜Assistant｜&gt;\\"\\n\\nResults:\\n\\n    llama_print_timings:        load time =  210631.90 ms\\n    llama_print_timings:      sample time =     600.64 ms /  2410 runs   (    0.25 ms per token,  4012.41 tokens per second)\\n    llama_print_timings: prompt eval time =     686.07 ms /    12 tokens (   57.17 ms per token,    17.49 tokens per second)\\n    llama_print_timings:        eval time =  317916.13 ms /  2409 runs   (  131.97 ms per token,     7.58 tokens per second)\\n    llama_print_timings:       total time =  320903.99 ms /  2421 tokens","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Deepseek R1 at 6,5 tk/s on an Nvidia Tesla P40","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lp01c7","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.89,"author_flair_background_color":null,"subreddit_type":"public","ups":46,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_dkwhd0p","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":46,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1751379496,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751371939,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I figured I&amp;#39;d post my final setup since many people asked about the P40 and assumed you couldn&amp;#39;t do much with it (but you can!).&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;numactl --cpunodebind=0 -- ./ik_llama.cpp/build/bin/llama-cli \\\\\\n    --numa numactl  \\\\\\n    --model models/unsloth/DeepSeek-R1-0528-GGUF/UD-Q2_K_XL/DeepSeek-R1-0528-UD-Q2_K_XL-00001-of-00006.gguf \\\\\\n    --threads 40 \\\\\\n    --cache-type-k q8_0 \\\\\\n    --cache-type-v q8_0 \\\\\\n    --top-p 0.95 \\\\\\n    --temp 0.6 \\\\\\n    --ctx-size 32768 \\\\\\n    --seed 3407 \\\\\\n    --n-gpu-layers 62 \\\\\\n    -ot &amp;quot;exps=CPU&amp;quot; \\\\\\n    --mlock \\\\\\n    --no-mmap \\\\\\n    -mla 2 -fa -fmoe \\\\\\n    -ser 5,1 \\\\\\n    -amb 512 \\\\\\n    --prompt &amp;quot;&amp;lt;｜User｜&amp;gt;Create a Flappy Bird game in Python.&amp;lt;｜Assistant｜&amp;gt;&amp;quot;\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;The result at the end of the run is around 6.5tk/s. &amp;lt;EDIT: Did another run and added the results. 7tk/s!&amp;gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;llama_print_timings:        load time =  896376.08 ms\\nllama_print_timings:      sample time =     594.81 ms /  2549 runs   (    0.23 ms per token,  4285.42 tokens per second)\\nllama_print_timings: prompt eval time =    1193.93 ms /    12 tokens (   99.49 ms per token,    10.05 tokens per second)\\nllama_print_timings:        eval time =  363871.92 ms /  2548 runs   (  142.81 ms per token,     7.00 tokens per second)\\nllama_print_timings:       total time =  366975.53 ms /  2560 tokens\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;I&amp;#39;m open to ideas on how to improve it.&lt;/p&gt;\\n\\n&lt;p&gt;Hardware:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Fully populated Dell R740 (in performance profile)&lt;/li&gt;\\n&lt;li&gt;Nvidia Tesla P40 (24GB vram)&lt;/li&gt;\\n&lt;li&gt;Xeon Gold 6138&lt;/li&gt;\\n&lt;li&gt;1.5TB of ram (all ram slots populated)&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;For other models, like Mistral or QwQ I get around 10tk/s&lt;/p&gt;\\n\\n&lt;p&gt;These are my QwQ settings (I use the regular llama.cpp for this one)&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;numactl --cpunodebind=0 -- ./llama.cpp/build/bin/llama-cli \\\\\\n    --numa numactl  \\\\\\n    --model models/unsloth/unsloth-QwQ-32B-GGUF/QwQ-32B-Q4_K_M.gguf \\\\\\n    --threads 40 \\\\\\n    --ctx-size 16384 \\\\\\n    --n-gpu-layers 99 \\\\\\n    --seed 3407 \\\\\\n    --temp 0.6 \\\\\\n    --repeat-penalty 1.1 \\\\\\n    --min-p 0.01 \\\\\\n    --top-k 40 \\\\\\n    --top-p 0.95 \\\\\\n    --dry-multiplier 0.5 \\\\\\n    --mlock \\\\\\n    --no-mmap \\\\\\n    --prio 3 \\\\\\n    -no-cnv \\\\\\n    -fa  \\\\\\n    --samplers &amp;quot;top_k;top_p;min_p;temperature;dry;typ_p;xtc&amp;quot; \\\\\\n    --prompt &amp;quot;&amp;lt;|im_start|&amp;gt;user\\\\nCreate a Flappy Bird game in Python. You must include these things:\\\\n1. You must use pygame.\\\\n2. The background color should be randomly chosen and is a light shade. Start with a light blue color.\\\\n3. Pressing SPACE multiple times will accelerate the bird.\\\\n4. The bird&amp;#39;s shape should be randomly chosen as a square, circle or triangle. The color should be randomly chosen as a dark color.\\\\n5. Place on the bottom some land colored as dark brown or yellow chosen randomly.\\\\n6. Make a score shown on the top right side. Increment if you pass pipes and don&amp;#39;t hit them.\\\\n7. Make randomly spaced pipes with enough space. Color them randomly as dark green or light brown or a dark gray shade.\\\\n8. When you lose, show the best score. Make the text inside the screen. Pressing q or Esc will quit the game. Restarting is pressing SPACE again.\\\\nThe final game should be inside a markdown section in Python. Check your code for errors and fix them before the final markdown section.&amp;lt;|im_end|&amp;gt;\\\\n&amp;lt;|im_start|&amp;gt;assistant\\\\n&amp;lt;think&amp;gt;\\\\n&amp;quot;\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;The details on the selected quants are in the model path. Surprisingly, using ik_llama.cpp optimized models from &lt;em&gt;ubergarm&lt;/em&gt; did not speed up Deepseek, but it slowed it down greatly.&lt;/p&gt;\\n\\n&lt;p&gt;Feel free to suggest improvements. For models different than deepseek, ik_llama.cpp was giving me a lot of gibberish output if I enabled fast attention. And some models I couldn&amp;#39;t even run on it, so that&amp;#39;s why I still use the regular llama.cpp for some of them.&lt;/p&gt;\\n\\n&lt;p&gt;-----&lt;/p&gt;\\n\\n&lt;p&gt;EDIT&lt;/p&gt;\\n\\n&lt;p&gt;I left it running in the background while doing other stuff, and with the community suggestions, I&amp;#39;m up to 7.57 tk/s! Thank you all! (notice that I can now use the 80 threads, but the performance is the same as 40 threads, because the bottleneck is in the memory bandwidth)&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;numactl --interleave=all -- ./ik_llama.cpp/build/bin/llama-cli \\\\\\n    --numa numactl  \\\\\\n    --model models/unsloth/DeepSeek-R1-0528-GGUF/UD-Q2_K_XL/DeepSeek-R1-0528-UD-Q2_K_XL-00001-of-00006.gguf \\\\\\n    --threads 80 \\\\\\n    --cache-type-k q8_0 \\\\\\n    --cache-type-v q8_0 \\\\\\n    --top-p 0.95 \\\\\\n    --temp 0.6 \\\\\\n    --ctx-size 32768 \\\\\\n    --seed 3407 \\\\\\n    --n-gpu-layers 62 \\\\\\n    -ot &amp;quot;exps=CPU&amp;quot; \\\\\\n    --mlock \\\\\\n    --no-mmap \\\\\\n    -mla 2 -fa -fmoe \\\\\\n    -ser 5,1 \\\\\\n    -amb 512 \\\\\\n    --run-time-repack -b 4096 -ub 4096 \\\\\\n    --prompt &amp;quot;&amp;lt;｜User｜&amp;gt;Create a Flappy Bird game in Python.&amp;lt;｜Assistant｜&amp;gt;&amp;quot;\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;Results:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;llama_print_timings:        load time =  210631.90 ms\\nllama_print_timings:      sample time =     600.64 ms /  2410 runs   (    0.25 ms per token,  4012.41 tokens per second)\\nllama_print_timings: prompt eval time =     686.07 ms /    12 tokens (   57.17 ms per token,    17.49 tokens per second)\\nllama_print_timings:        eval time =  317916.13 ms /  2409 runs   (  131.97 ms per token,     7.58 tokens per second)\\nllama_print_timings:       total time =  320903.99 ms /  2421 tokens\\n&lt;/code&gt;&lt;/pre&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lp01c7","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"dc740","discussion_type":null,"num_comments":43,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/","subreddit_subscribers":493458,"created_utc":1751371939,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0u4y58","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"intc3172","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0r9qmq","score":1,"author_fullname":"t2_1ovgeo9d2s","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"yes, it's not about having good pp it's about having concise and short prompt itself","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0u4y58","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;yes, it&amp;#39;s not about having good pp it&amp;#39;s about having concise and short prompt itself&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp01c7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/n0u4y58/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751406767,"author_flair_text":null,"treatment_tags":[],"created_utc":1751406767,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0r9qmq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"101m4n","can_mod_post":false,"created_utc":1751377328,"send_replies":true,"parent_id":"t1_n0qvrgg","score":18,"author_fullname":"t2_p7nc2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It's not the size, it's how you use it","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0r9qmq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s not the size, it&amp;#39;s how you use it&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp01c7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/n0r9qmq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751377328,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":18}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0r81r1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"My_Unbiased_Opinion","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0r3bbt","score":4,"author_fullname":"t2_esiyl0yb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Damn thats slow af. I also have a P40 and M40 in my closet. Been considering pulling those out and putting em to some use","edited":1751377020,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0r81r1","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Damn thats slow af. I also have a P40 and M40 in my closet. Been considering pulling those out and putting em to some use&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp01c7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/n0r81r1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751376770,"author_flair_text":null,"treatment_tags":[],"created_utc":1751376770,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n0r3bbt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"dc740","can_mod_post":false,"created_utc":1751375181,"send_replies":true,"parent_id":"t1_n0qvrgg","score":5,"author_fullname":"t2_dkwhd0p","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"    llama_print_timings:        load time =  896376.08 ms\\n    llama_print_timings:      sample time =     594.81 ms /  2549 runs   (    0.23 ms per token,  4285.42 tokens per second)\\n    llama_print_timings: prompt eval time =    1193.93 ms /    12 tokens (   99.49 ms per token,    10.05 tokens per second)\\n    llama_print_timings:        eval time =  363871.92 ms /  2548 runs   (  142.81 ms per token,     7.00 tokens per second)\\n    llama_print_timings:       total time =  366975.53 ms /  2560 tokens","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0r3bbt","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;pre&gt;&lt;code&gt;llama_print_timings:        load time =  896376.08 ms\\nllama_print_timings:      sample time =     594.81 ms /  2549 runs   (    0.23 ms per token,  4285.42 tokens per second)\\nllama_print_timings: prompt eval time =    1193.93 ms /    12 tokens (   99.49 ms per token,    10.05 tokens per second)\\nllama_print_timings:        eval time =  363871.92 ms /  2548 runs   (  142.81 ms per token,     7.00 tokens per second)\\nllama_print_timings:       total time =  366975.53 ms /  2560 tokens\\n&lt;/code&gt;&lt;/pre&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp01c7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/n0r3bbt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751375181,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0qwe5f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0qvx7d","score":-3,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Fuck off asshole. PP is well established term here. Only a noob bitch like you does not know what that means.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0qwe5f","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Fuck off asshole. PP is well established term here. Only a noob bitch like you does not know what that means.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp01c7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/n0qwe5f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751372669,"author_flair_text":null,"treatment_tags":[],"created_utc":1751372669,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0qvx7d","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"MengerianMango","can_mod_post":false,"created_utc":1751372487,"send_replies":true,"parent_id":"t1_n0qvrgg","score":-9,"author_fullname":"t2_mcvyi","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ask ur mom","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0qvx7d","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ask ur mom&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp01c7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/n0qvx7d/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751372487,"author_flair_text":null,"treatment_tags":[],"collapsed":true,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-9}}],"before":null}},"user_reports":[],"saved":false,"id":"n0qvrgg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1751372426,"send_replies":true,"parent_id":"t3_1lp01c7","score":18,"author_fullname":"t2_uz37qfx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"how big is PP?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0qvrgg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;how big is PP?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/n0qvrgg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751372426,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp01c7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":18}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0r1fs9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ShinyAnkleBalls","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0qxdet","score":7,"author_fullname":"t2_2m3au2xb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I wouldn't say it's disregarded... It costs 25% of a 3090 and has around 33% the throughput. It's still a pretty good deal and in many many cases, better than ram+CPU","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0r1fs9","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I wouldn&amp;#39;t say it&amp;#39;s disregarded... It costs 25% of a 3090 and has around 33% the throughput. It&amp;#39;s still a pretty good deal and in many many cases, better than ram+CPU&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp01c7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/n0r1fs9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751374532,"author_flair_text":null,"treatment_tags":[],"created_utc":1751374532,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0va3c5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"smcnally","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0rhpzw","score":1,"author_fullname":"t2_uoqs","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Your point stands, but Maxwell still works with CUDA 12.  I’ve run it w 12.8 and this says 12.9 supports it. 5.2 works better than 5.0 ime  \\n\\n[https://en.wikipedia.org/wiki/CUDA#GPUs\\\\_supported](https://en.wikipedia.org/wiki/CUDA#GPUs_supported)","edited":false,"author_flair_css_class":null,"name":"t1_n0va3c5","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Your point stands, but Maxwell still works with CUDA 12.  I’ve run it w 12.8 and this says 12.9 supports it. 5.2 works better than 5.0 ime  &lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://en.wikipedia.org/wiki/CUDA#GPUs_supported\\"&gt;https://en.wikipedia.org/wiki/CUDA#GPUs_supported&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lp01c7","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/n0va3c5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751420758,"author_flair_text":"llama.cpp","collapsed":false,"created_utc":1751420758,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0rhpzw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullstackSensei","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0raoj0","score":3,"author_fullname":"t2_17n3nqtj56","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Nvidia removing Pascal support from CUDA 13 doesn't mean the cards will stop working. Maxwell has had support removed in CUDA 12 and llama.cpp still builds against CUDA 11 three years later.\\n\\nIf you're looking at P40 prices now, it doesn't make much sense. But a lot of us got them way back when they were 100 a pop. Even now that the 3090 is down to 500-ish, my P40s are still a better value, especially when I can make them single slot using 1080Ti waterblocks and can fit eight on a single motherboard (ex: Supermicro X10DRX) without risers and still power the entire system with a 1600W PSU.q","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0rhpzw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nvidia removing Pascal support from CUDA 13 doesn&amp;#39;t mean the cards will stop working. Maxwell has had support removed in CUDA 12 and llama.cpp still builds against CUDA 11 three years later.&lt;/p&gt;\\n\\n&lt;p&gt;If you&amp;#39;re looking at P40 prices now, it doesn&amp;#39;t make much sense. But a lot of us got them way back when they were 100 a pop. Even now that the 3090 is down to 500-ish, my P40s are still a better value, especially when I can make them single slot using 1080Ti waterblocks and can fit eight on a single motherboard (ex: Supermicro X10DRX) without risers and still power the entire system with a 1600W PSU.q&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp01c7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/n0rhpzw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751379813,"author_flair_text":null,"treatment_tags":[],"created_utc":1751379813,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0tzdz0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"PDXSonic","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0raoj0","score":2,"author_fullname":"t2_22jcfbsv","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"There are a few people who try and keep vLLM working on Pascal systems, I’ve had okay success on my P100 using it. But unfortunately I think it’s done once the V0 engine is deprecated. Which is a shame since my 4x16GB P100s are solid, but unfortunately haven’t climbed in value like the P40s lol \\n\\nhttps://github.com/sasha0552/pascal-pkgs-ci","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0tzdz0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There are a few people who try and keep vLLM working on Pascal systems, I’ve had okay success on my P100 using it. But unfortunately I think it’s done once the V0 engine is deprecated. Which is a shame since my 4x16GB P100s are solid, but unfortunately haven’t climbed in value like the P40s lol &lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/sasha0552/pascal-pkgs-ci\\"&gt;https://github.com/sasha0552/pascal-pkgs-ci&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp01c7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/n0tzdz0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751405090,"author_flair_text":null,"treatment_tags":[],"created_utc":1751405090,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0v1pah","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Normal-Ad-7114","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0raoj0","score":1,"author_fullname":"t2_8fu8sqhz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt;The P100 does have fp16 support, but with only 12GB\\n\\n\\n16GB","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0v1pah","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;The P100 does have fp16 support, but with only 12GB&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;16GB&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp01c7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/n0v1pah/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751417782,"author_flair_text":null,"treatment_tags":[],"created_utc":1751417782,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0raoj0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"OutlandishnessIll466","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0qxdet","score":2,"author_fullname":"t2_e4ru5ouw","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"They do lack some features like bfloat and native fp16 support which makes them slower then necessary. That is why they are not a viable option for finetuning either. And nvidia will stop the CUDA support soon in newer versions. Also vLLM does not support it which is annoying. \\n\\nThe P100 does have fp16 support, but with only 12GB there are probably better options like a 3060/4060 or something.\\n\\nThe 3090 is roughly 4x faster then a P40 for 2x  the price of a P40.\\n\\nSeeing the 4090 and 5090 are only like 30% - 50% faster then a 3090 but for 3x - 4x the price of a 3090 (6x - 8x the price of a P40), the 3090 is still best value for money imo. \\n\\nBut I guess, like me, not everybody immediately wants to dish out $700 to play around with LLM's which is where the P40 comes in. I bought 4x P40's when they were still $200, but now going to slowly exchange them to 3090's while they are still worth something.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0raoj0","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They do lack some features like bfloat and native fp16 support which makes them slower then necessary. That is why they are not a viable option for finetuning either. And nvidia will stop the CUDA support soon in newer versions. Also vLLM does not support it which is annoying. &lt;/p&gt;\\n\\n&lt;p&gt;The P100 does have fp16 support, but with only 12GB there are probably better options like a 3060/4060 or something.&lt;/p&gt;\\n\\n&lt;p&gt;The 3090 is roughly 4x faster then a P40 for 2x  the price of a P40.&lt;/p&gt;\\n\\n&lt;p&gt;Seeing the 4090 and 5090 are only like 30% - 50% faster then a 3090 but for 3x - 4x the price of a 3090 (6x - 8x the price of a P40), the 3090 is still best value for money imo. &lt;/p&gt;\\n\\n&lt;p&gt;But I guess, like me, not everybody immediately wants to dish out $700 to play around with LLM&amp;#39;s which is where the P40 comes in. I bought 4x P40&amp;#39;s when they were still $200, but now going to slowly exchange them to 3090&amp;#39;s while they are still worth something.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp01c7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/n0raoj0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751377635,"author_flair_text":null,"treatment_tags":[],"created_utc":1751377635,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0rg801","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullstackSensei","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0qxdet","score":1,"author_fullname":"t2_17n3nqtj56","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The P40 has abysmal fp16 performance but llama.cpp and all it's derivatives have custom CUDA kernels that cast fp16 to Fp32. The cast happens in registers so it doesn't affect memory bandwidth and AFAIK takes one clock only.\\n\\nI have a quad P40 rig and performance is very decent on larger models. If you got them before prices went up, they're unbearable for 24GB VRAM.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0rg801","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The P40 has abysmal fp16 performance but llama.cpp and all it&amp;#39;s derivatives have custom CUDA kernels that cast fp16 to Fp32. The cast happens in registers so it doesn&amp;#39;t affect memory bandwidth and AFAIK takes one clock only.&lt;/p&gt;\\n\\n&lt;p&gt;I have a quad P40 rig and performance is very decent on larger models. If you got them before prices went up, they&amp;#39;re unbearable for 24GB VRAM.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp01c7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/n0rg801/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751379369,"author_flair_text":null,"treatment_tags":[],"created_utc":1751379369,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0qxdet","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"dc740","can_mod_post":false,"created_utc":1751373048,"send_replies":true,"parent_id":"t1_n0qvpto","score":6,"author_fullname":"t2_dkwhd0p","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"AFAIK the p40 is known for its slow fp16 performance and it's immediately disregarded when it comes to ai. This is something I see in every post where it gets mentioned. So I figured I'd show how mine runs. There is nothing specific in it other than the fact that it works just fine for my purposes (playing around and experimenting)","edited":1751376956,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0qxdet","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;AFAIK the p40 is known for its slow fp16 performance and it&amp;#39;s immediately disregarded when it comes to ai. This is something I see in every post where it gets mentioned. So I figured I&amp;#39;d show how mine runs. There is nothing specific in it other than the fact that it works just fine for my purposes (playing around and experimenting)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp01c7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/n0qxdet/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751373048,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n0qvpto","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"jacek2023","can_mod_post":false,"created_utc":1751372408,"send_replies":true,"parent_id":"t3_1lp01c7","score":8,"author_fullname":"t2_vqgbql9w","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"could you explan how this is different than 3090? I mean is there anything P40-specific?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0qvpto","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;could you explan how this is different than 3090? I mean is there anything P40-specific?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/n0qvpto/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751372408,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lp01c7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0qyxyv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1751373634,"send_replies":true,"parent_id":"t1_n0qxi2m","score":2,"author_fullname":"t2_h5utwre7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I find RTR only helps at batch size 2048 and ub 1024\\n\\nWhenever I use it, or offline repacking at higher batches, speed goes down. 4096/4096 by itself is faster but obviously takes more vram.\\n\\nI don't see anyone posting before/after just recommending blindly. Also could be related to not having \\"fancy SIMD\\" and being stuck with AVX2. I run lots of benchmarks, but only on my own system.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0qyxyv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I find RTR only helps at batch size 2048 and ub 1024&lt;/p&gt;\\n\\n&lt;p&gt;Whenever I use it, or offline repacking at higher batches, speed goes down. 4096/4096 by itself is faster but obviously takes more vram.&lt;/p&gt;\\n\\n&lt;p&gt;I don&amp;#39;t see anyone posting before/after just recommending blindly. Also could be related to not having &amp;quot;fancy SIMD&amp;quot; and being stuck with AVX2. I run lots of benchmarks, but only on my own system.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp01c7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/n0qyxyv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751373634,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0rkmbi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"dc740","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0resvf","score":1,"author_fullname":"t2_dkwhd0p","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"thanks. I should really change the prompt. but it's something I'm just leaving in the background and I'm not actively changing. I did test CPU only on these settings and only got 4tk/s. Also... my post got removed =(","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0rkmbi","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;thanks. I should really change the prompt. but it&amp;#39;s something I&amp;#39;m just leaving in the background and I&amp;#39;m not actively changing. I did test CPU only on these settings and only got 4tk/s. Also... my post got removed =(&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp01c7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/n0rkmbi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751380667,"author_flair_text":null,"treatment_tags":[],"created_utc":1751380667,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0resvf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CheatCodesOfLife","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0r6md1","score":2,"author_fullname":"t2_32el727b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You're gonna want to put in more than 12 tokens to measure your PP ;)\\n\\nThat 14 t/s won't be accurate because your prompt is only 12. Try at least 60.\\n\\nAlso, have you tested not using the GPU at all? Those numbers are kind of similar to when I don't use any GPUs.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0resvf","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You&amp;#39;re gonna want to put in more than 12 tokens to measure your PP ;)&lt;/p&gt;\\n\\n&lt;p&gt;That 14 t/s won&amp;#39;t be accurate because your prompt is only 12. Try at least 60.&lt;/p&gt;\\n\\n&lt;p&gt;Also, have you tested not using the GPU at all? Those numbers are kind of similar to when I don&amp;#39;t use any GPUs.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp01c7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/n0resvf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751378938,"author_flair_text":null,"treatment_tags":[],"created_utc":1751378938,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0r6md1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"dc740","can_mod_post":false,"created_utc":1751376299,"send_replies":true,"parent_id":"t1_n0qxi2m","score":2,"author_fullname":"t2_dkwhd0p","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"it went from 7 to 6.92, but it improved the prompt eval by 4tk/s (10 -&gt; 14). so that's not bad. Thanks!\\n\\n    llama_print_timings:        load time =  164142.86 ms\\n    llama_print_timings:      sample time =     779.29 ms /  3393 runs   (    0.23 ms per token,  4353.96 tokens per second)\\n    llama_print_timings: prompt eval time =     839.49 ms /    12 tokens (   69.96 ms per token,    14.29 tokens per second)\\n    llama_print_timings:        eval time =  490435.98 ms /  3392 runs   (  144.59 ms per token,     6.92 tokens per second)\\n    llama_print_timings:       total time =  493862.44 ms /  3404 tokens","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0r6md1","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;it went from 7 to 6.92, but it improved the prompt eval by 4tk/s (10 -&amp;gt; 14). so that&amp;#39;s not bad. Thanks!&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;llama_print_timings:        load time =  164142.86 ms\\nllama_print_timings:      sample time =     779.29 ms /  3393 runs   (    0.23 ms per token,  4353.96 tokens per second)\\nllama_print_timings: prompt eval time =     839.49 ms /    12 tokens (   69.96 ms per token,    14.29 tokens per second)\\nllama_print_timings:        eval time =  490435.98 ms /  3392 runs   (  144.59 ms per token,     6.92 tokens per second)\\nllama_print_timings:       total time =  493862.44 ms /  3404 tokens\\n&lt;/code&gt;&lt;/pre&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp01c7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/n0r6md1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751376299,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0qxi2m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"p4s2wd","can_mod_post":false,"created_utc":1751373097,"send_replies":true,"parent_id":"t3_1lp01c7","score":6,"author_fullname":"t2_3tplbw0t","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You may try to add: --run-time-repack -b 4096 -ub 4096 into your command line ;-)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0qxi2m","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You may try to add: --run-time-repack -b 4096 -ub 4096 into your command line ;-)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/n0qxi2m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751373097,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp01c7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0r8aqd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"My_Unbiased_Opinion","can_mod_post":false,"created_utc":1751376852,"send_replies":true,"parent_id":"t3_1lp01c7","score":3,"author_fullname":"t2_esiyl0yb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"P40 and the M40 goes hard especially if you bought them when they hit the price floor.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0r8aqd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;P40 and the M40 goes hard especially if you bought them when they hit the price floor.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/n0r8aqd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751376852,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp01c7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0rerta","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Steuern_Runter","can_mod_post":false,"created_utc":1751378930,"send_replies":true,"parent_id":"t3_1lp01c7","score":2,"author_fullname":"t2_w8pggsa4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Did you try to use a small draft model for DS?\\n\\nlike this one:\\n\\nhttps://huggingface.co/mradermacher/DeepSeek-R1-DRAFT-0.5B-GGUF","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0rerta","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Did you try to use a small draft model for DS?&lt;/p&gt;\\n\\n&lt;p&gt;like this one:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/mradermacher/DeepSeek-R1-DRAFT-0.5B-GGUF\\"&gt;https://huggingface.co/mradermacher/DeepSeek-R1-DRAFT-0.5B-GGUF&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/n0rerta/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751378930,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp01c7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0qy0cs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Deep-Rice9305","can_mod_post":false,"created_utc":1751373289,"send_replies":true,"parent_id":"t3_1lp01c7","score":1,"author_fullname":"t2_fx3y8nb71","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What is the TTFT (time to first token) in average?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0qy0cs","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What is the TTFT (time to first token) in average?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/n0qy0cs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751373289,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp01c7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0vaqry","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"smcnally","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0rg5q5","score":1,"author_fullname":"t2_uoqs","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"how much difference do you see without any ‘--numa’?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0vaqry","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;how much difference do you see without any ‘--numa’?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp01c7","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/n0vaqry/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751420979,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1751420979,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0rg5q5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"dc740","can_mod_post":false,"created_utc":1751379350,"send_replies":true,"parent_id":"t1_n0r06xp","score":2,"author_fullname":"t2_dkwhd0p","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I did test it back on my R730 with Xeon E52699v4, and I kept getting lower numbers, but now that I tried it again, I got even better results. Thank you!\\n\\n    llama_print_timings:        load time =  210631.90 ms\\n    llama_print_timings:      sample time =     600.64 ms /  2410 runs   (    0.25 ms per token,  4012.41 tokens per second)\\n    llama_print_timings: prompt eval time =     686.07 ms /    12 tokens (   57.17 ms per token,    17.49 tokens per second)\\n    llama_print_timings:        eval time =  317916.13 ms /  2409 runs   (  131.97 ms per token,     7.58 tokens per second)\\n    llama_print_timings:       total time =  320903.99 ms /  2421 tokens","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0rg5q5","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I did test it back on my R730 with Xeon E52699v4, and I kept getting lower numbers, but now that I tried it again, I got even better results. Thank you!&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;llama_print_timings:        load time =  210631.90 ms\\nllama_print_timings:      sample time =     600.64 ms /  2410 runs   (    0.25 ms per token,  4012.41 tokens per second)\\nllama_print_timings: prompt eval time =     686.07 ms /    12 tokens (   57.17 ms per token,    17.49 tokens per second)\\nllama_print_timings:        eval time =  317916.13 ms /  2409 runs   (  131.97 ms per token,     7.58 tokens per second)\\nllama_print_timings:       total time =  320903.99 ms /  2421 tokens\\n&lt;/code&gt;&lt;/pre&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp01c7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/n0rg5q5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751379350,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0r06xp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1751374091,"send_replies":true,"parent_id":"t3_1lp01c7","score":1,"author_fullname":"t2_h5utwre7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I somehow get better results using numactl --interleave=all and --numa distribute. My bios is set to only have 2 numa nodes, one for each proc.\\n\\nNeed to test nodebind and numa isolate/numactl and see what happens. At that point I think you also have to adjust your threads to a single processor, right? Just doing isolate lowered performance when I did it initially. Sweep bench is great for this kind of testing.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0r06xp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I somehow get better results using numactl --interleave=all and --numa distribute. My bios is set to only have 2 numa nodes, one for each proc.&lt;/p&gt;\\n\\n&lt;p&gt;Need to test nodebind and numa isolate/numactl and see what happens. At that point I think you also have to adjust your threads to a single processor, right? Just doing isolate lowered performance when I did it initially. Sweep bench is great for this kind of testing.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/n0r06xp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751374091,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp01c7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0rc50j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"dc740","can_mod_post":false,"created_utc":1751378104,"send_replies":true,"parent_id":"t1_n0rbnig","score":2,"author_fullname":"t2_dkwhd0p","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm partially offloading to the cpu. Check the -ot parameter","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0rc50j","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m partially offloading to the cpu. Check the -ot parameter&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp01c7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/n0rc50j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751378104,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0tdzvt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"created_utc":1751398948,"send_replies":true,"parent_id":"t1_n0rbnig","score":1,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It doesn't. It's not running on entirely on the P40. It's mostly running on the server that happens to have a P40 in it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0tdzvt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It doesn&amp;#39;t. It&amp;#39;s not running on entirely on the P40. It&amp;#39;s mostly running on the server that happens to have a P40 in it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp01c7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/n0tdzvt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751398948,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0rbnig","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"NoLeading4922","can_mod_post":false,"created_utc":1751377949,"send_replies":true,"parent_id":"t3_1lp01c7","score":1,"author_fullname":"t2_hsngwhbs","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"how does it fit in your vram?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0rbnig","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;how does it fit in your vram?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/n0rbnig/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751377949,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp01c7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0to85t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"dc740","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0tlvd9","score":3,"author_fullname":"t2_dkwhd0p","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Ah. The title is misleading. I see. I didn't mean that when I posted it. I can't edit it now =(\\n\\nThe post got flagged already because I edited it and added the results from the comments. Hopefully people will see that I didn't mean fully in the p40. There was a comment about running a smaller model but I haven't checked it. I did check qwq fully in the P40 with good results too","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0to85t","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ah. The title is misleading. I see. I didn&amp;#39;t mean that when I posted it. I can&amp;#39;t edit it now =(&lt;/p&gt;\\n\\n&lt;p&gt;The post got flagged already because I edited it and added the results from the comments. Hopefully people will see that I didn&amp;#39;t mean fully in the p40. There was a comment about running a smaller model but I haven&amp;#39;t checked it. I did check qwq fully in the P40 with good results too&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp01c7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/n0to85t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751401871,"author_flair_text":null,"treatment_tags":[],"created_utc":1751401871,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0tlvd9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0tk8sf","score":1,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes. You are partially offloading. But from your title, it says you are running it *entirely* on the P40.\\n\\n\\"Deepseek R1 at 6,5 tk/s **on an Nvidia Tesla P40**\\"","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0tlvd9","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes. You are partially offloading. But from your title, it says you are running it &lt;em&gt;entirely&lt;/em&gt; on the P40.&lt;/p&gt;\\n\\n&lt;p&gt;&amp;quot;Deepseek R1 at 6,5 tk/s &lt;strong&gt;on an Nvidia Tesla P40&lt;/strong&gt;&amp;quot;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp01c7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/n0tlvd9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751401213,"author_flair_text":null,"treatment_tags":[],"created_utc":1751401213,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0tk8sf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"dc740","can_mod_post":false,"created_utc":1751400751,"send_replies":true,"parent_id":"t1_n0tduez","score":1,"author_fullname":"t2_dkwhd0p","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Check my other comments. The server can only run at 4tk/s on the CPU. I'm using partial offloading to get 7.5tk/s after some improvements from other users","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0tk8sf","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Check my other comments. The server can only run at 4tk/s on the CPU. I&amp;#39;m using partial offloading to get 7.5tk/s after some improvements from other users&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp01c7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/n0tk8sf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751400751,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0tduez","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"created_utc":1751398903,"send_replies":true,"parent_id":"t3_1lp01c7","score":1,"author_fullname":"t2_o65i6kx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It's not running on a P40 though. It's running on big server that just happens to have a P40 in it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0tduez","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s not running on a P40 though. It&amp;#39;s running on big server that just happens to have a P40 in it.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/n0tduez/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751398903,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp01c7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ttmcc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"dc740","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0tscfw","score":2,"author_fullname":"t2_dkwhd0p","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yes, it should. This memory came from my Dell r730, and it didn't make any sense to buy faster if I already had this one. The processor does not support faster LRDIMM speeds (at least that's what I found in the Dell datasheet) so I'd also have to upgrade it in order to use faster memory, and it made no economical sense. but it should get better results with faster memory","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ttmcc","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, it should. This memory came from my Dell r730, and it didn&amp;#39;t make any sense to buy faster if I already had this one. The processor does not support faster LRDIMM speeds (at least that&amp;#39;s what I found in the Dell datasheet) so I&amp;#39;d also have to upgrade it in order to use faster memory, and it made no economical sense. but it should get better results with faster memory&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp01c7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/n0ttmcc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751403393,"author_flair_text":null,"treatment_tags":[],"created_utc":1751403393,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0tscfw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"plankalkul-z1","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0toqfn","score":1,"author_fullname":"t2_w73n3yrsx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I see, thank you.\\n\\n\\n&gt; The memory is DDR4 at 2666Mhz\\n\\n\\nSo DDR4 3200 should perform a bit better...","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0tscfw","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I see, thank you.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;The memory is DDR4 at 2666Mhz&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;So DDR4 3200 should perform a bit better...&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp01c7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/n0tscfw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751403031,"author_flair_text":null,"treatment_tags":[],"created_utc":1751403031,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0uhiz4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Caffdy","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0toqfn","score":1,"author_fullname":"t2_ql2vu0wz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"are you using double CPU (Xeon)? [going by Intel specs,](https://www.intel.com/content/www/us/en/products/sku/120476/intel-xeon-gold-6138-processor-27-5m-cache-2-00-ghz/specifications.html) it can only use 768GB of memory. How are you rocking 1.5TB?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0uhiz4","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;are you using double CPU (Xeon)? &lt;a href=\\"https://www.intel.com/content/www/us/en/products/sku/120476/intel-xeon-gold-6138-processor-27-5m-cache-2-00-ghz/specifications.html\\"&gt;going by Intel specs,&lt;/a&gt; it can only use 768GB of memory. How are you rocking 1.5TB?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp01c7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/n0uhiz4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751410827,"author_flair_text":null,"treatment_tags":[],"created_utc":1751410827,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0toqfn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"dc740","can_mod_post":false,"created_utc":1751402014,"send_replies":true,"parent_id":"t1_n0tn6ok","score":1,"author_fullname":"t2_dkwhd0p","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"My last edit was lost. And I'm afraid the post will be flagged and deleted a second time because of too many edits if I try once again. The memory is DDR4 at 2666Mhz","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0toqfn","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;My last edit was lost. And I&amp;#39;m afraid the post will be flagged and deleted a second time because of too many edits if I try once again. The memory is DDR4 at 2666Mhz&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp01c7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/n0toqfn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751402014,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0tn6ok","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"plankalkul-z1","can_mod_post":false,"created_utc":1751401579,"send_replies":true,"parent_id":"t3_1lp01c7","score":1,"author_fullname":"t2_w73n3yrsx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt; because the bottleneck is in the memory bandwidth\\n\\n\\nWhat kind of RAM do you have?\\n\\n\\nI see it's 1.5Tb, but what type/speed?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0tn6ok","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;because the bottleneck is in the memory bandwidth&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;What kind of RAM do you have?&lt;/p&gt;\\n\\n&lt;p&gt;I see it&amp;#39;s 1.5Tb, but what type/speed?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/n0tn6ok/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751401579,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp01c7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"d2642412-d9ce-11ed-ae30-32b11309f5bd","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"d2642412-d9ce-11ed-ae30-32b11309f5bd","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ujcao","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ortegaalfredo","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0uiavw","score":1,"author_fullname":"t2_g177e","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You are correct, I'm using IK\\\\_Q1, its about 150GB","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0ujcao","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"Alpaca"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You are correct, I&amp;#39;m using IK_Q1, its about 150GB&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp01c7","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/n0ujcao/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751411433,"author_flair_text":"Alpaca","treatment_tags":[],"created_utc":1751411433,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bd9e9e","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0uiavw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Caffdy","can_mod_post":false,"created_utc":1751411086,"send_replies":true,"parent_id":"t1_n0rc5fq","score":3,"author_fullname":"t2_ql2vu0wz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"DeepSeek-R1-0528-GGUF at Q2_k_XL dynamic quant like what OP used is 251GB **without context**, I very much doubt you're running the same quant as him","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0uiavw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;DeepSeek-R1-0528-GGUF at Q2_k_XL dynamic quant like what OP used is 251GB &lt;strong&gt;without context&lt;/strong&gt;, I very much doubt you&amp;#39;re running the same quant as him&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp01c7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/n0uiavw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751411086,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0rc5fq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ortegaalfredo","can_mod_post":false,"created_utc":1751378107,"send_replies":true,"parent_id":"t3_1lp01c7","score":-1,"author_fullname":"t2_g177e","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I get 6 tok/s on a 10 year old Xeon with 128GB and a 2x3090. Not that much difference.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0rc5fq","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Alpaca"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I get 6 tok/s on a 10 year old Xeon with 128GB and a 2x3090. Not that much difference.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp01c7/deepseek_r1_at_65_tks_on_an_nvidia_tesla_p40/n0rc5fq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751378107,"author_flair_text":"Alpaca","treatment_tags":[],"link_id":"t3_1lp01c7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bd9e9e","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}}]`),o=()=>e.jsx(l,{data:t});export{o as default};
