import{j as e}from"./index-Bu7qcPAU.js";import{R as t}from"./RedditPostRenderer-CbHA7O5q.js";import"./index-BKgbfxhf.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I find the R1T2 Chimera shorter thinking very suitable for day-to-day code questions. I will give you an example where for the same simple question about modifications to be made to my java spring security cors configuration that allowed all origins, to allow only a specified domain and its subdomains:\\n\\nhttps://preview.redd.it/6aq4kdaff9cf1.png?width=1177&amp;format=png&amp;auto=webp&amp;s=9060f40e214b352e1f20fce21c610abc8077c62b\\n\\nhttps://preview.redd.it/8pf5m0ckf9cf1.png?width=1292&amp;format=png&amp;auto=webp&amp;s=8cd1a405ecccb1b15cbacde6f91955e68c94f81a\\n\\nhttps://preview.redd.it/zbvuc2otk9cf1.png?width=1222&amp;format=png&amp;auto=webp&amp;s=fc2725f46403b0dacf155960877868dae7d17157\\n\\nThe response was good for all three of them, but V3 responded with 500tokens (but only the updated line not all the method, but good enough for me), R1T2 took 1300tokens (2 minutes thinking + response), and R10528 needed 8200tokens (17 minutes thinking + response).  \\nR1-0528 went on and on in the thinking stage wondering about what the developers settings will need to be to be able to test also locally, and allowed also localhost, [127.0.0.1](http://127.0.0.1) and other variations of them, even if I didn't ask for it. R1-0528 is overthinking almost everything, and I will still try it when I will arrive to a more complicated algorithm that maybe the R1T2 can't handle, but i didn't find something like that till now, I think the extra thinking is for difficult math problems or similar complex algorithms.  \\nMy takeaway is that V3, R1T2, R1 all have the same knowledge base. For almost all the questions V3 is enough, for complex architecture ones, that require COT, R1T2 seems good enough without overthinking. For record breaking math solving skills, there is R1-0528. :)\\n\\nThese were the quants I used:  \\n[https://huggingface.co/ubergarm/DeepSeek-TNG-R1T2-Chimera-GGUF/tree/main/IQ3\\\\_KS](https://huggingface.co/ubergarm/DeepSeek-TNG-R1T2-Chimera-GGUF/tree/main/IQ3_KS)  \\n[https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF/tree/main/IQ3\\\\_KS](https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF/tree/main/IQ3_KS)  \\n[https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF-UD/tree/main/UD-Q3\\\\_K\\\\_XL](https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF-UD/tree/main/UD-Q3_K_XL)\\n\\nIs your finding similar?\\n\\nP.S. I hope in the future will have a think/no\\\\_think or even deep\\\\_think option so we can load a unique model that suits all kinds of requests.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"DeepSeek-TNG-R1T2-Chimera vs DeepSeek R1-0528 quick test","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":28,"top_awarded_type":null,"hide_score":false,"media_metadata":{"zbvuc2otk9cf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":61,"x":108,"u":"https://preview.redd.it/zbvuc2otk9cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bd9c2d9b07897afe5021f95d21995f29c7b12efc"},{"y":122,"x":216,"u":"https://preview.redd.it/zbvuc2otk9cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ae8ec16a7742d46f269cf6de105c512e6e7592d1"},{"y":181,"x":320,"u":"https://preview.redd.it/zbvuc2otk9cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=88bfd869b23d0959770adb22ccfa8b082a83f8ef"},{"y":363,"x":640,"u":"https://preview.redd.it/zbvuc2otk9cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9a0441ebd4940513ffe7c7af96f1bcd1ebb3a3f0"},{"y":545,"x":960,"u":"https://preview.redd.it/zbvuc2otk9cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e6ee4e1e658376ed9c0121f628f0c845bc815ba9"},{"y":614,"x":1080,"u":"https://preview.redd.it/zbvuc2otk9cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e0cbef41ad8e3573747152a2fc4f0e22b516db3a"}],"s":{"y":695,"x":1222,"u":"https://preview.redd.it/zbvuc2otk9cf1.png?width=1222&amp;format=png&amp;auto=webp&amp;s=fc2725f46403b0dacf155960877868dae7d17157"},"id":"zbvuc2otk9cf1"},"6aq4kdaff9cf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":21,"x":108,"u":"https://preview.redd.it/6aq4kdaff9cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0f0a8a23a27781fcbc965c6f1717451afac2943f"},{"y":43,"x":216,"u":"https://preview.redd.it/6aq4kdaff9cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a770d2d4d3608954da5c71466dbaacb109388fbb"},{"y":64,"x":320,"u":"https://preview.redd.it/6aq4kdaff9cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=68af46fcc46570195c916ff956ea66af158ae7f2"},{"y":128,"x":640,"u":"https://preview.redd.it/6aq4kdaff9cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1e983b4856c7708b69ba1bdc900f17c52878ac84"},{"y":193,"x":960,"u":"https://preview.redd.it/6aq4kdaff9cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ba80d4ea541510b1b31678c80fad816d57edb3de"},{"y":217,"x":1080,"u":"https://preview.redd.it/6aq4kdaff9cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ebc14eecb6b2bbc7739ffd740d6d1ef7e01b11b7"}],"s":{"y":237,"x":1177,"u":"https://preview.redd.it/6aq4kdaff9cf1.png?width=1177&amp;format=png&amp;auto=webp&amp;s=9060f40e214b352e1f20fce21c610abc8077c62b"},"id":"6aq4kdaff9cf1"},"8pf5m0ckf9cf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":40,"x":108,"u":"https://preview.redd.it/8pf5m0ckf9cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=483c8dd42834a8cd306ee253d944df846b0cd717"},{"y":81,"x":216,"u":"https://preview.redd.it/8pf5m0ckf9cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0c5e52e5cac5ae519225c5d3f7e87ecd8e202008"},{"y":120,"x":320,"u":"https://preview.redd.it/8pf5m0ckf9cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=afaac5d3747dbb429bdd6f87526a662f6ab1fee2"},{"y":241,"x":640,"u":"https://preview.redd.it/8pf5m0ckf9cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0fe64d72dc6365c7b0f97854fee196a68b96ea55"},{"y":361,"x":960,"u":"https://preview.redd.it/8pf5m0ckf9cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1c0c86ead672f2595c519a311f370c05c8cb450b"},{"y":407,"x":1080,"u":"https://preview.redd.it/8pf5m0ckf9cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=66c4e24287a2fc4fd31538ca57dd7c58ff9d1987"}],"s":{"y":487,"x":1292,"u":"https://preview.redd.it/8pf5m0ckf9cf1.png?width=1292&amp;format=png&amp;auto=webp&amp;s=8cd1a405ecccb1b15cbacde6f91955e68c94f81a"},"id":"8pf5m0ckf9cf1"}},"name":"t3_1lxa4hy","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.94,"author_flair_background_color":null,"subreddit_type":"public","ups":12,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_j8fit2p","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":12,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://b.thumbs.redditmedia.com/XvodyXdpc2JgU7KT-qGirrTHxA_APKfHvxxbBimftEw.jpg","edited":1752248667,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752248362,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I find the R1T2 Chimera shorter thinking very suitable for day-to-day code questions. I will give you an example where for the same simple question about modifications to be made to my java spring security cors configuration that allowed all origins, to allow only a specified domain and its subdomains:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/6aq4kdaff9cf1.png?width=1177&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9060f40e214b352e1f20fce21c610abc8077c62b\\"&gt;https://preview.redd.it/6aq4kdaff9cf1.png?width=1177&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9060f40e214b352e1f20fce21c610abc8077c62b&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/8pf5m0ckf9cf1.png?width=1292&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8cd1a405ecccb1b15cbacde6f91955e68c94f81a\\"&gt;https://preview.redd.it/8pf5m0ckf9cf1.png?width=1292&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8cd1a405ecccb1b15cbacde6f91955e68c94f81a&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/zbvuc2otk9cf1.png?width=1222&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fc2725f46403b0dacf155960877868dae7d17157\\"&gt;https://preview.redd.it/zbvuc2otk9cf1.png?width=1222&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fc2725f46403b0dacf155960877868dae7d17157&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;The response was good for all three of them, but V3 responded with 500tokens (but only the updated line not all the method, but good enough for me), R1T2 took 1300tokens (2 minutes thinking + response), and R10528 needed 8200tokens (17 minutes thinking + response).&lt;br/&gt;\\nR1-0528 went on and on in the thinking stage wondering about what the developers settings will need to be to be able to test also locally, and allowed also localhost, &lt;a href=\\"http://127.0.0.1\\"&gt;127.0.0.1&lt;/a&gt; and other variations of them, even if I didn&amp;#39;t ask for it. R1-0528 is overthinking almost everything, and I will still try it when I will arrive to a more complicated algorithm that maybe the R1T2 can&amp;#39;t handle, but i didn&amp;#39;t find something like that till now, I think the extra thinking is for difficult math problems or similar complex algorithms.&lt;br/&gt;\\nMy takeaway is that V3, R1T2, R1 all have the same knowledge base. For almost all the questions V3 is enough, for complex architecture ones, that require COT, R1T2 seems good enough without overthinking. For record breaking math solving skills, there is R1-0528. :)&lt;/p&gt;\\n\\n&lt;p&gt;These were the quants I used:&lt;br/&gt;\\n&lt;a href=\\"https://huggingface.co/ubergarm/DeepSeek-TNG-R1T2-Chimera-GGUF/tree/main/IQ3_KS\\"&gt;https://huggingface.co/ubergarm/DeepSeek-TNG-R1T2-Chimera-GGUF/tree/main/IQ3_KS&lt;/a&gt;&lt;br/&gt;\\n&lt;a href=\\"https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF/tree/main/IQ3_KS\\"&gt;https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF/tree/main/IQ3_KS&lt;/a&gt;&lt;br/&gt;\\n&lt;a href=\\"https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF-UD/tree/main/UD-Q3_K_XL\\"&gt;https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF-UD/tree/main/UD-Q3_K_XL&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Is your finding similar?&lt;/p&gt;\\n\\n&lt;p&gt;P.S. I hope in the future will have a think/no_think or even deep_think option so we can load a unique model that suits all kinds of requests.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lxa4hy","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"ciprianveg","discussion_type":null,"num_comments":9,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lxa4hy/deepseektngr1t2chimera_vs_deepseek_r10528_quick/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lxa4hy/deepseektngr1t2chimera_vs_deepseek_r10528_quick/","subreddit_subscribers":497825,"created_utc":1752248362,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2l5zcr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ciprianveg","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2l47nn","score":1,"author_fullname":"t2_j8fit2p","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"But if you need to reload with different parameter is the same thing for me as starting a different model.. I would like Qwen3 style that can be sent in user message or in prompt..","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2l5zcr","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;But if you need to reload with different parameter is the same thing for me as starting a different model.. I would like Qwen3 style that can be sent in user message or in prompt..&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxa4hy","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxa4hy/deepseektngr1t2chimera_vs_deepseek_r10528_quick/n2l5zcr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752255806,"author_flair_text":null,"treatment_tags":[],"created_utc":1752255806,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2lns6d","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Few-Yam9901","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2l47nn","score":1,"author_fullname":"t2_1rhlf3bcfk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Wow I will try thanks!!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2lns6d","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Wow I will try thanks!!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxa4hy","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxa4hy/deepseektngr1t2chimera_vs_deepseek_r10528_quick/n2lns6d/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752260923,"author_flair_text":null,"treatment_tags":[],"created_utc":1752260923,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2l47nn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"panchovix","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2l3vfn","score":2,"author_fullname":"t2_j1kqr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes, using \\n\\n    --reasoning-budget 0","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2l47nn","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, using &lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;--reasoning-budget 0\\n&lt;/code&gt;&lt;/pre&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxa4hy","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxa4hy/deepseektngr1t2chimera_vs_deepseek_r10528_quick/n2l47nn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752255321,"author_flair_text":"Llama 405B","treatment_tags":[],"created_utc":1752255321,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2l3vfn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Few-Yam9901","can_mod_post":false,"created_utc":1752255226,"send_replies":true,"parent_id":"t1_n2kp519","score":1,"author_fullname":"t2_1rhlf3bcfk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Llama.cpp? Can you turn off R1 0528 thinking?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2l3vfn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Llama.cpp? Can you turn off R1 0528 thinking?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxa4hy","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxa4hy/deepseektngr1t2chimera_vs_deepseek_r10528_quick/n2l3vfn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752255226,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2kp519","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"panchovix","can_mod_post":false,"created_utc":1752251127,"send_replies":true,"parent_id":"t3_1lxa4hy","score":4,"author_fullname":"t2_j1kqr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"On lcpp you can use \\n\\n    --reasoning-budget N                    controls the amount of thinking allowed; currently only one of: -1 for\\n                                           unrestricted thinking budget, or 0 to disable thinking (default: -1)\\n                                           (env: LLAMA_ARG_THINK_BUDGET)\\n\\nIt is not on iklcpp yet but asked the feature here [https://github.com/ikawrakow/ik\\\\_llama.cpp/issues/600](https://github.com/ikawrakow/ik_llama.cpp/issues/600)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2kp519","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;On lcpp you can use &lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;--reasoning-budget N                    controls the amount of thinking allowed; currently only one of: -1 for\\n                                       unrestricted thinking budget, or 0 to disable thinking (default: -1)\\n                                       (env: LLAMA_ARG_THINK_BUDGET)\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;It is not on iklcpp yet but asked the feature here &lt;a href=\\"https://github.com/ikawrakow/ik_llama.cpp/issues/600\\"&gt;https://github.com/ikawrakow/ik_llama.cpp/issues/600&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxa4hy/deepseektngr1t2chimera_vs_deepseek_r10528_quick/n2kp519/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752251127,"author_flair_text":"Llama 405B","treatment_tags":[],"link_id":"t3_1lxa4hy","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2khfyt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SlowFail2433","can_mod_post":false,"created_utc":1752248937,"send_replies":true,"parent_id":"t3_1lxa4hy","score":3,"author_fullname":"t2_131eezppgs","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The thinking models are mostly for things like math problems yes they are often unnecessary","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2khfyt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The thinking models are mostly for things like math problems yes they are often unnecessary&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxa4hy/deepseektngr1t2chimera_vs_deepseek_r10528_quick/n2khfyt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752248937,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxa4hy","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2pz71b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"pharrowking","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2om5fx","score":1,"author_fullname":"t2_73959oe7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"In my case it was roughly the same speed, all it did was add more Dram usage It was still being sped up by the single 16gb vram card im using in ktransformers\\n\\n\\n\\n\\nMy setup involves:\\nSingle cpu epyc 7642 with 256gb 8 channel memory and 1x 16gb 5060 ti\\n\\n\\n\\n\\nThe gpu makes a huge difference i connected mine to my 1U server with a occulink low profile pcie card and the minisforum DEG 1","edited":1752325463,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2pz71b","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;In my case it was roughly the same speed, all it did was add more Dram usage It was still being sped up by the single 16gb vram card im using in ktransformers&lt;/p&gt;\\n\\n&lt;p&gt;My setup involves:\\nSingle cpu epyc 7642 with 256gb 8 channel memory and 1x 16gb 5060 ti&lt;/p&gt;\\n\\n&lt;p&gt;The gpu makes a huge difference i connected mine to my 1U server with a occulink low profile pcie card and the minisforum DEG 1&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxa4hy","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxa4hy/deepseektngr1t2chimera_vs_deepseek_r10528_quick/n2pz71b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752325177,"author_flair_text":null,"treatment_tags":[],"created_utc":1752325177,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2om5fx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ciprianveg","can_mod_post":false,"created_utc":1752299064,"send_replies":true,"parent_id":"t1_n2o9uva","score":1,"author_fullname":"t2_j8fit2p","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Isn't it 7 times slower with 60 experts?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2om5fx","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Isn&amp;#39;t it 7 times slower with 60 experts?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxa4hy","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxa4hy/deepseektngr1t2chimera_vs_deepseek_r10528_quick/n2om5fx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752299064,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2o9uva","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"pharrowking","can_mod_post":false,"created_utc":1752293207,"send_replies":true,"parent_id":"t3_1lxa4hy","score":2,"author_fullname":"t2_73959oe7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"for my own analysis on ktransformers i set experts per token to 60 (up from the default 8) and when i asked it to generate a flappybird game it thought for like 15-20  seconds at 6 tokens a seconds. then generated a fully working flappybird game.\\n\\nfor a simple \\"hi\\" prompt this is what i got, and its fairly short:\\n\\nOkay, the user just said \\"hi\\". I need to respond in a friendly and welcoming way. Maybe say something like \\"Hello! How can I assist you today?\\" to let them know I'm here to help. Keep it simple and open-ended so they feel comfortable asking for whatever they need.\\n\\n&lt;/think&gt;\\n\\n\\n\\nHello! How can I assist you today? 😊😊\\n\\n────────────────────────────────────────\\n\\n→ Received 61 tokens in 9.769s\\n\\n→ Avg. per-token latency: 0.160s\\n\\n→ Throughput: 6.24 tokens/s\\n\\n  \\nTldr; more experts = shorter think tags lol. but i realise thats not practical for everyone.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2o9uva","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;for my own analysis on ktransformers i set experts per token to 60 (up from the default 8) and when i asked it to generate a flappybird game it thought for like 15-20  seconds at 6 tokens a seconds. then generated a fully working flappybird game.&lt;/p&gt;\\n\\n&lt;p&gt;for a simple &amp;quot;hi&amp;quot; prompt this is what i got, and its fairly short:&lt;/p&gt;\\n\\n&lt;p&gt;Okay, the user just said &amp;quot;hi&amp;quot;. I need to respond in a friendly and welcoming way. Maybe say something like &amp;quot;Hello! How can I assist you today?&amp;quot; to let them know I&amp;#39;m here to help. Keep it simple and open-ended so they feel comfortable asking for whatever they need.&lt;/p&gt;\\n\\n&lt;p&gt;&amp;lt;/think&amp;gt;&lt;/p&gt;\\n\\n&lt;p&gt;Hello! How can I assist you today? 😊😊&lt;/p&gt;\\n\\n&lt;p&gt;────────────────────────────────────────&lt;/p&gt;\\n\\n&lt;p&gt;→ Received 61 tokens in 9.769s&lt;/p&gt;\\n\\n&lt;p&gt;→ Avg. per-token latency: 0.160s&lt;/p&gt;\\n\\n&lt;p&gt;→ Throughput: 6.24 tokens/s&lt;/p&gt;\\n\\n&lt;p&gt;Tldr; more experts = shorter think tags lol. but i realise thats not practical for everyone.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxa4hy/deepseektngr1t2chimera_vs_deepseek_r10528_quick/n2o9uva/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752293207,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxa4hy","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),o=()=>e.jsx(t,{data:a});export{o as default};
