import{j as l}from"./index-BOnf-UhU.js";import{R as e}from"./RedditPostRenderer-Ce39qICS.js";import"./index-CDase6VD.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I don't expect Ollama to have every finetuned models on their main library, and I understand that you can import gguf models from hugging face.\\n\\nStill, it seems pretty odd that they're missing Reka Flash-3.2, SmolLM3, GLM-4. I believe other platforms like LMStudio, MLX, unsloth, etc have them.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Ollama, Why No Reka Flash, SmolLM3, GLM-4?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lzsnna","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.64,"author_flair_background_color":null,"subreddit_type":"public","ups":12,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_e9jh97s","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":12,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752514045,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t expect Ollama to have every finetuned models on their main library, and I understand that you can import gguf models from hugging face.&lt;/p&gt;\\n\\n&lt;p&gt;Still, it seems pretty odd that they&amp;#39;re missing Reka Flash-3.2, SmolLM3, GLM-4. I believe other platforms like LMStudio, MLX, unsloth, etc have them.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lzsnna","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"chibop1","discussion_type":null,"num_comments":31,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lzsnna/ollama_why_no_reka_flash_smollm3_glm4/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lzsnna/ollama_why_no_reka_flash_smollm3_glm4/","subreddit_subscribers":499295,"created_utc":1752514045,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n380ply","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AFAIX","can_mod_post":false,"send_replies":true,"parent_id":"t1_n35l2d5","score":1,"author_fullname":"t2_8i8xi","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"llama-swap is not that much of a complete wrapper since you still have to write the same exact llama-server command you would use, but now in a config file.\\n\\nIs a great tool though and really helps keeping config options in one place and switching models on the fly without running into the terminal ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n380ply","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;llama-swap is not that much of a complete wrapper since you still have to write the same exact llama-server command you would use, but now in a config file.&lt;/p&gt;\\n\\n&lt;p&gt;Is a great tool though and really helps keeping config options in one place and switching models on the fly without running into the terminal &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"link_id":"t3_1lzsnna","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzsnna/ollama_why_no_reka_flash_smollm3_glm4/n380ply/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752564543,"author_flair_text":null,"treatment_tags":[],"created_utc":1752564543,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n38mbxz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fish312","can_mod_post":false,"send_replies":true,"parent_id":"t1_n35l2d5","score":1,"author_fullname":"t2_mogjd","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Koboldcpp is best","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n38mbxz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Koboldcpp is best&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzsnna","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzsnna/ollama_why_no_reka_flash_smollm3_glm4/n38mbxz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752576715,"author_flair_text":null,"treatment_tags":[],"created_utc":1752576715,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n36cnt6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"chibop1","can_mod_post":false,"send_replies":true,"parent_id":"t1_n35l2d5","score":-3,"author_fullname":"t2_e9jh97s","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I started with llama.cpp since Llama1 came out. Now I mainly used Ollama. Once in a while I touch llama.cpp just to experiment with extra features.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n36cnt6","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I started with llama.cpp since Llama1 came out. Now I mainly used Ollama. Once in a while I touch llama.cpp just to experiment with extra features.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzsnna","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzsnna/ollama_why_no_reka_flash_smollm3_glm4/n36cnt6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752539171,"author_flair_text":null,"treatment_tags":[],"created_utc":1752539171,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-3}}],"before":null}},"user_reports":[],"saved":false,"id":"n35l2d5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"colin_colout","can_mod_post":false,"send_replies":true,"parent_id":"t1_n34gzw6","score":6,"author_fullname":"t2_14l4ya","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt;If llama.cpp works for you, keep using it shrug.\\n\\nHave you tried other llama.cpp wrappers / forks?  I haven't used them much myself, but I hear good things about llama-swap and koboldai\\n\\nMy experience with ollama lines up with what you're saying, and why I moved to llama.cpp directly, but there are other replacement options I hear (maybe others can chime in with what htey use)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n35l2d5","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;If llama.cpp works for you, keep using it shrug.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Have you tried other llama.cpp wrappers / forks?  I haven&amp;#39;t used them much myself, but I hear good things about llama-swap and koboldai&lt;/p&gt;\\n\\n&lt;p&gt;My experience with ollama lines up with what you&amp;#39;re saying, and why I moved to llama.cpp directly, but there are other replacement options I hear (maybe others can chime in with what htey use)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzsnna","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzsnna/ollama_why_no_reka_flash_smollm3_glm4/n35l2d5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752530202,"author_flair_text":null,"treatment_tags":[],"created_utc":1752530202,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n34k2lg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n34gzw6","score":19,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Then what the point of your diatribe about ollama not having latest models? it is what it is and they do not owe you latest models in their repos. You chose inferior product - dela with it or ask their support forums why they are the way they are.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n34k2lg","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Then what the point of your diatribe about ollama not having latest models? it is what it is and they do not owe you latest models in their repos. You chose inferior product - dela with it or ask their support forums why they are the way they are.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzsnna","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzsnna/ollama_why_no_reka_flash_smollm3_glm4/n34k2lg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752519604,"author_flair_text":null,"treatment_tags":[],"created_utc":1752519604,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":19}}],"before":null}},"user_reports":[],"saved":false,"id":"n34gzw6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"chibop1","can_mod_post":false,"created_utc":1752518704,"send_replies":true,"parent_id":"t1_n349fuu","score":1,"author_fullname":"t2_e9jh97s","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"If llama.cpp works for you, keep using it shrug.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n34gzw6","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If llama.cpp works for you, keep using it shrug.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzsnna","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzsnna/ollama_why_no_reka_flash_smollm3_glm4/n34gzw6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752518704,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n34lg61","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n34l1fw","score":5,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"well in this case continue using it, but OP clearly has problems with ollama. Backlash against ollama is in perceived lack of value over the foundation it is built on.","edited":false,"author_flair_css_class":null,"name":"t1_n34lg61","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;well in this case continue using it, but OP clearly has problems with ollama. Backlash against ollama is in perceived lack of value over the foundation it is built on.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lzsnna","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzsnna/ollama_why_no_reka_flash_smollm3_glm4/n34lg61/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752520002,"author_flair_text":null,"collapsed":false,"created_utc":1752520002,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n34s9d7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FullstackSensei","can_mod_post":false,"send_replies":true,"parent_id":"t1_n34l1fw","score":7,"author_fullname":"t2_17n3nqtj56","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Because ollama is the other software. They wrap llama.cpp while making a lot of the flexibility and power of it obsecure or hard to use. They also do some shady things like giving false names to models.","edited":false,"author_flair_css_class":null,"name":"t1_n34s9d7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Because ollama is the other software. They wrap llama.cpp while making a lot of the flexibility and power of it obsecure or hard to use. They also do some shady things like giving false names to models.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lzsnna","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzsnna/ollama_why_no_reka_flash_smollm3_glm4/n34s9d7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752521965,"author_flair_text":null,"collapsed":false,"created_utc":1752521965,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}}],"before":null}},"user_reports":[],"saved":false,"id":"n34l1fw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FORLLM","can_mod_post":false,"send_replies":true,"parent_id":"t1_n34jryn","score":0,"author_fullname":"t2_1rav5zv0dv","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"So far I have what features I need and I can install any model on huggingface.\\n\\nI'm not opposed to other software, but I've seen a weird backlash against ollama where some people seem upset that others are using it. I'm not trying to convince anyone else to use it, just not sure why people seem to want me not to.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n34l1fw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So far I have what features I need and I can install any model on huggingface.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m not opposed to other software, but I&amp;#39;ve seen a weird backlash against ollama where some people seem upset that others are using it. I&amp;#39;m not trying to convince anyone else to use it, just not sure why people seem to want me not to.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzsnna","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzsnna/ollama_why_no_reka_flash_smollm3_glm4/n34l1fw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752519887,"author_flair_text":null,"treatment_tags":[],"created_utc":1752519887,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n34jryn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n34dhi2","score":6,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"With ollama you are missing features that come with llama.cpp and also at mercy of ollama devs which models you can use.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n34jryn","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;With ollama you are missing features that come with llama.cpp and also at mercy of ollama devs which models you can use.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzsnna","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzsnna/ollama_why_no_reka_flash_smollm3_glm4/n34jryn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752519517,"author_flair_text":null,"treatment_tags":[],"created_utc":1752519517,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n34dhi2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FORLLM","can_mod_post":false,"created_utc":1752517694,"send_replies":true,"parent_id":"t1_n349fuu","score":2,"author_fullname":"t2_1rav5zv0dv","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I first installed it for bolt diy, after that I found it worked nicely with other programs and started building my own frontend around it. It works well and plays well with others, I'm sure it's not the only backend that does, but I've had zero problems with it. Not sure why I'd keep looking for something else until it doesn't meet my needs.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n34dhi2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I first installed it for bolt diy, after that I found it worked nicely with other programs and started building my own frontend around it. It works well and plays well with others, I&amp;#39;m sure it&amp;#39;s not the only backend that does, but I&amp;#39;ve had zero problems with it. Not sure why I&amp;#39;d keep looking for something else until it doesn&amp;#39;t meet my needs.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzsnna","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzsnna/ollama_why_no_reka_flash_smollm3_glm4/n34dhi2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752517694,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n34jg4z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n34d83f","score":7,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; llama.cpp can dynamically figure out how to off load layers?\\n\\nOf course. basic functionality.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n34jg4z","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;llama.cpp can dynamically figure out how to off load layers?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Of course. basic functionality.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzsnna","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzsnna/ollama_why_no_reka_flash_smollm3_glm4/n34jg4z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752519421,"author_flair_text":null,"treatment_tags":[],"created_utc":1752519421,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}}],"before":null}},"user_reports":[],"saved":false,"id":"n34d83f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"throwawayacc201711","can_mod_post":false,"created_utc":1752517620,"send_replies":true,"parent_id":"t1_n349fuu","score":1,"author_fullname":"t2_kwgauoo","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I don’t mind running cli commands and configuring things but my main entry point I use for working with my models is through openwebui. The nice thing I found with ollama is that it will dynamically use my VRAM and system RAM if I’m selecting larger models than my GPU and offers some decent optimizations. Is there a no touch solution that llama.cpp offers for that?\\n\\nExample: go to openwebui and select a model and llama.cpp can dynamically figure out how to off load layers?\\n\\nThis has been my main reason I haven’t made the switch and I’ve been preoccupied with other projects that are r/selfhosted","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n34d83f","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don’t mind running cli commands and configuring things but my main entry point I use for working with my models is through openwebui. The nice thing I found with ollama is that it will dynamically use my VRAM and system RAM if I’m selecting larger models than my GPU and offers some decent optimizations. Is there a no touch solution that llama.cpp offers for that?&lt;/p&gt;\\n\\n&lt;p&gt;Example: go to openwebui and select a model and llama.cpp can dynamically figure out how to off load layers?&lt;/p&gt;\\n\\n&lt;p&gt;This has been my main reason I haven’t made the switch and I’ve been preoccupied with other projects that are &lt;a href=\\"/r/selfhosted\\"&gt;r/selfhosted&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzsnna","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzsnna/ollama_why_no_reka_flash_smollm3_glm4/n34d83f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752517620,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n37wbr1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n35dsfx","score":1,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"never felt this way.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n37wbr1","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;never felt this way.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzsnna","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzsnna/ollama_why_no_reka_flash_smollm3_glm4/n37wbr1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752562105,"author_flair_text":null,"treatment_tags":[],"created_utc":1752562105,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n36rc1z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"hayTGotMhYXkm95q5HW9","can_mod_post":false,"send_replies":true,"parent_id":"t1_n35dsfx","score":1,"author_fullname":"t2_l9dilk1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Less friction is the reason. Its simply the fact on why its more popular. Not sure why we are getting down voted for it.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n36rc1z","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Less friction is the reason. Its simply the fact on why its more popular. Not sure why we are getting down voted for it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzsnna","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzsnna/ollama_why_no_reka_flash_smollm3_glm4/n36rc1z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752544275,"author_flair_text":null,"treatment_tags":[],"created_utc":1752544275,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n35dsfx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"__Maximum__","can_mod_post":false,"created_utc":1752528053,"send_replies":true,"parent_id":"t1_n349fuu","score":1,"author_fullname":"t2_fzqff6k3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"ollama run model_name has much less friction if I want to try out something, then downloading it and running a command line and making sure that all params are correct, plus switching the models back and forth","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n35dsfx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;ollama run model_name has much less friction if I want to try out something, then downloading it and running a command line and making sure that all params are correct, plus switching the models back and forth&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzsnna","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzsnna/ollama_why_no_reka_flash_smollm3_glm4/n35dsfx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752528053,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n358d7q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"hayTGotMhYXkm95q5HW9","can_mod_post":false,"created_utc":1752526504,"send_replies":true,"parent_id":"t1_n349fuu","score":-3,"author_fullname":"t2_l9dilk1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ollama \\"just works\\" especially when using open web ui.\\n\\nEdit: You can down vote me but this is still the most common reason. Its simply a fact.","edited":1752544189,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n358d7q","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ollama &amp;quot;just works&amp;quot; especially when using open web ui.&lt;/p&gt;\\n\\n&lt;p&gt;Edit: You can down vote me but this is still the most common reason. Its simply a fact.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzsnna","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzsnna/ollama_why_no_reka_flash_smollm3_glm4/n358d7q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752526504,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n37w2gp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n35qn3i","score":2,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt;  ollama is just easier to deal with.\\n\\nNot for me. I am, allergic to any wrappers.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n37w2gp","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;ollama is just easier to deal with.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Not for me. I am, allergic to any wrappers.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzsnna","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzsnna/ollama_why_no_reka_flash_smollm3_glm4/n37w2gp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752561967,"author_flair_text":null,"treatment_tags":[],"created_utc":1752561967,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n35qn3i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"klop2031","can_mod_post":false,"created_utc":1752531947,"send_replies":true,"parent_id":"t1_n349fuu","score":-1,"author_fullname":"t2_a1p8p","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Tbh i use ollama because its easy to use. Llama.cpp isnt that much harder but alas, ollama is just easier to deal with.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n35qn3i","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Tbh i use ollama because its easy to use. Llama.cpp isnt that much harder but alas, ollama is just easier to deal with.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzsnna","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzsnna/ollama_why_no_reka_flash_smollm3_glm4/n35qn3i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752531947,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"50c36eba-fdca-11ee-9735-92a88d7e3b87","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n36fnxo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ayylmaonade","can_mod_post":false,"created_utc":1752540218,"send_replies":true,"parent_id":"t1_n349fuu","score":-1,"author_fullname":"t2_oqajf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I find it more convenient to keep models organised compared to llama-server with llama-swap. I know ollama has \\"bad defaults\\" like some in this thread are mentioning, and poorly named models -- both of which I agree with, I get my models from hf.co so it's not an issue for me. \\n\\nSo yeah. Convenience, easy organisation of models, etc.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n36fnxo","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Ollama"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I find it more convenient to keep models organised compared to llama-server with llama-swap. I know ollama has &amp;quot;bad defaults&amp;quot; like some in this thread are mentioning, and poorly named models -- both of which I agree with, I get my models from hf.co so it&amp;#39;s not an issue for me. &lt;/p&gt;\\n\\n&lt;p&gt;So yeah. Convenience, easy organisation of models, etc.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzsnna","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzsnna/ollama_why_no_reka_flash_smollm3_glm4/n36fnxo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752540218,"author_flair_text":"Ollama","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n349fuu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1752516555,"send_replies":true,"parent_id":"t3_1lzsnna","score":33,"author_fullname":"t2_uz37qfx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I still cannot get why would anyone still use ollama if you can run llama.cpp directly, shrug.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n349fuu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I still cannot get why would anyone still use ollama if you can run llama.cpp directly, shrug.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzsnna/ollama_why_no_reka_flash_smollm3_glm4/n349fuu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752516555,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzsnna","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":33}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n36lltw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AaronFeng47","can_mod_post":false,"created_utc":1752542269,"send_replies":true,"parent_id":"t3_1lzsnna","score":4,"author_fullname":"t2_4gc7hf3m","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Llama.cpp + llama swap combination is better than ollama ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n36lltw","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Llama.cpp + llama swap combination is better than ollama &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzsnna/ollama_why_no_reka_flash_smollm3_glm4/n36lltw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752542269,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lzsnna","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n34ojst","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"-Ellary-","can_mod_post":false,"created_utc":1752520882,"send_replies":true,"parent_id":"t1_n34bkif","score":10,"author_fullname":"t2_s4zzntp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Kobold.cpp is our best friend.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n34ojst","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Kobold.cpp is our best friend.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzsnna","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzsnna/ollama_why_no_reka_flash_smollm3_glm4/n34ojst/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752520882,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n354zxp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Federal_Order4324","can_mod_post":false,"send_replies":true,"parent_id":"t1_n34lhx9","score":3,"author_fullname":"t2_rlhobztpn","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I still don't get why they misname models?? It's kind of idiotic imo. It's genuinely just bad for the program as a whole no?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n354zxp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I still don&amp;#39;t get why they misname models?? It&amp;#39;s kind of idiotic imo. It&amp;#39;s genuinely just bad for the program as a whole no?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzsnna","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzsnna/ollama_why_no_reka_flash_smollm3_glm4/n354zxp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752525557,"author_flair_text":null,"treatment_tags":[],"created_utc":1752525557,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n34lhx9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Marksta","can_mod_post":false,"send_replies":true,"parent_id":"t1_n34ghbz","score":15,"author_fullname":"t2_559a1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Convenience looks like bad defaults, confusingly renamed Deepseek distill models, silent quantization, and random models not being available it seems 🤔","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n34lhx9","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Convenience looks like bad defaults, confusingly renamed Deepseek distill models, silent quantization, and random models not being available it seems 🤔&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzsnna","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzsnna/ollama_why_no_reka_flash_smollm3_glm4/n34lhx9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752520015,"author_flair_text":null,"treatment_tags":[],"created_utc":1752520015,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":15}}],"before":null}},"user_reports":[],"saved":false,"id":"n34ghbz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"chibop1","can_mod_post":false,"created_utc":1752518558,"send_replies":true,"parent_id":"t1_n34bkif","score":-2,"author_fullname":"t2_e9jh97s","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"CONVENIENCE!!! Nothing more.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n34ghbz","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;CONVENIENCE!!! Nothing more.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzsnna","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzsnna/ollama_why_no_reka_flash_smollm3_glm4/n34ghbz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752518558,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-2}}],"before":null}},"user_reports":[],"saved":false,"id":"n34bkif","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"jacek2023","can_mod_post":false,"created_utc":1752517154,"send_replies":true,"parent_id":"t3_1lzsnna","score":6,"author_fullname":"t2_vqgbql9w","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What's so awesome in ollama?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n34bkif","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What&amp;#39;s so awesome in ollama?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzsnna/ollama_why_no_reka_flash_smollm3_glm4/n34bkif/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752517154,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lzsnna","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}}]`),r=()=>l.jsx(e,{data:a});export{r as default};
