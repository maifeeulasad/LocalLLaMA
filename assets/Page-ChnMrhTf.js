import{j as e}from"./index-xfnGEtuL.js";import{R as l}from"./RedditPostRenderer-DAZRIZZK.js";import"./index-BaNn5-RR.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I’m trying to understand how models like **Gemini 2.5 Pro** achieve *native* 1 million token context windows.\\n\\nFrom what I’ve seen in models like **Qwen3** or **LLaMA**, they use techniques like **RoPE scaling** (e.g., YaRN, NTK-aware RoPE, Position Interpolation) to extrapolate context beyond what was trained. These methods usually need fine-tuning, and even then, there's often a soft limit beyond which attention weakens significantly.\\n\\nBut Gemini claims *native* 1M context, and benchmarks (like Needle-in-a-Haystack, RULER) suggest it actually performs well across that full range. So my questions are:\\n\\n* Does Gemini use **YaRN** or **RoPE scaling** internally?\\n* Is it trained from scratch with 1M tokens per sequence (i.e., truly native)?\\n* Or is it just doing **clever chunking** or sparse attention under the hood (e.g., blockwise, ring attention)?\\n* Does it use **ALiBi** or some modified positional encoding to stabilize long contexts?\\n\\nIf anyone has insight from papers, leaks, logs, or architecture details, I'd love to learn more.  \\nEven speculation grounded in similar architectures is welcome.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"How does Gemini 2.5 Pro natively support 1M tokens of context? Is it using YaRN, or some kind of disguised chunking?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m6xbru","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.73,"author_flair_background_color":null,"subreddit_type":"public","ups":10,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1qyykcj4","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":10,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753237168,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I’m trying to understand how models like &lt;strong&gt;Gemini 2.5 Pro&lt;/strong&gt; achieve &lt;em&gt;native&lt;/em&gt; 1 million token context windows.&lt;/p&gt;\\n\\n&lt;p&gt;From what I’ve seen in models like &lt;strong&gt;Qwen3&lt;/strong&gt; or &lt;strong&gt;LLaMA&lt;/strong&gt;, they use techniques like &lt;strong&gt;RoPE scaling&lt;/strong&gt; (e.g., YaRN, NTK-aware RoPE, Position Interpolation) to extrapolate context beyond what was trained. These methods usually need fine-tuning, and even then, there&amp;#39;s often a soft limit beyond which attention weakens significantly.&lt;/p&gt;\\n\\n&lt;p&gt;But Gemini claims &lt;em&gt;native&lt;/em&gt; 1M context, and benchmarks (like Needle-in-a-Haystack, RULER) suggest it actually performs well across that full range. So my questions are:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Does Gemini use &lt;strong&gt;YaRN&lt;/strong&gt; or &lt;strong&gt;RoPE scaling&lt;/strong&gt; internally?&lt;/li&gt;\\n&lt;li&gt;Is it trained from scratch with 1M tokens per sequence (i.e., truly native)?&lt;/li&gt;\\n&lt;li&gt;Or is it just doing &lt;strong&gt;clever chunking&lt;/strong&gt; or sparse attention under the hood (e.g., blockwise, ring attention)?&lt;/li&gt;\\n&lt;li&gt;Does it use &lt;strong&gt;ALiBi&lt;/strong&gt; or some modified positional encoding to stabilize long contexts?&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;If anyone has insight from papers, leaks, logs, or architecture details, I&amp;#39;d love to learn more.&lt;br/&gt;\\nEven speculation grounded in similar architectures is welcome.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1m6xbru","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Ranteck","discussion_type":null,"num_comments":19,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m6xbru/how_does_gemini_25_pro_natively_support_1m_tokens/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m6xbru/how_does_gemini_25_pro_natively_support_1m_tokens/","subreddit_subscribers":503519,"created_utc":1753237168,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4n4w8p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"fp4guru","can_mod_post":false,"created_utc":1753237397,"send_replies":true,"parent_id":"t3_1m6xbru","score":24,"author_fullname":"t2_1tp8zldw5g","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"No way to tell.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4n4w8p","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No way to tell.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6xbru/how_does_gemini_25_pro_natively_support_1m_tokens/n4n4w8p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753237397,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6xbru","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":24}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ndvfo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Robos_Basilisk","can_mod_post":false,"created_utc":1753240763,"send_replies":true,"parent_id":"t3_1m6xbru","score":8,"author_fullname":"t2_ta1dmorpo","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think it's Ring Attention which is made possible with Google's in-house custom TPU infrastructure which lets them chain together tons of HBM for massive KV storage and context windows","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ndvfo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think it&amp;#39;s Ring Attention which is made possible with Google&amp;#39;s in-house custom TPU infrastructure which lets them chain together tons of HBM for massive KV storage and context windows&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6xbru/how_does_gemini_25_pro_natively_support_1m_tokens/n4ndvfo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753240763,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6xbru","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4o9mtg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"cantgetthistowork","can_mod_post":false,"created_utc":1753256055,"send_replies":true,"parent_id":"t1_n4n6feg","score":6,"author_fullname":"t2_j1i0o","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Google has the entire stack. All the data in the world + their own TPUs. They can make anything that the rest of the world might never see.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4o9mtg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Google has the entire stack. All the data in the world + their own TPUs. They can make anything that the rest of the world might never see.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6xbru","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6xbru/how_does_gemini_25_pro_natively_support_1m_tokens/n4o9mtg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753256055,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ojqp0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"stoppableDissolution","can_mod_post":false,"created_utc":1753261895,"send_replies":true,"parent_id":"t1_n4n6feg","score":1,"author_fullname":"t2_1n0su21k4z","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Its not like there is a lot of naturally occurring data that is 1M long while still following some kind of instructions at the start tho","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ojqp0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Its not like there is a lot of naturally occurring data that is 1M long while still following some kind of instructions at the start tho&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6xbru","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6xbru/how_does_gemini_25_pro_natively_support_1m_tokens/n4ojqp0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753261895,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"total_awards_received":0,"approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"ups":-3,"removal_reason":null,"link_id":"t3_1m6xbru","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4px2wt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"KrazyKirby99999","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4o4xps","score":1,"author_fullname":"t2_1h5tfxy8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"They don't need to, Google already scrapes the internet","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4px2wt","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They don&amp;#39;t need to, Google already scrapes the internet&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6xbru","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6xbru/how_does_gemini_25_pro_natively_support_1m_tokens/n4px2wt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753281241,"author_flair_text":null,"treatment_tags":[],"created_utc":1753281241,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4o4xps","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"DELETED","no_follow":true,"author":"[deleted]","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4n6feg","score":-3,"approved_by":null,"report_reasons":null,"all_awardings":[],"subreddit_id":"t5_81eyvm","body":"[deleted]","edited":false,"author_flair_css_class":null,"collapsed":true,"downs":0,"is_submitter":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;[deleted]&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"associated_award":null,"stickied":false,"subreddit_type":"public","can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6xbru/how_does_gemini_25_pro_natively_support_1m_tokens/n4o4xps/","num_reports":null,"locked":false,"name":"t1_n4o4xps","created":1753253444,"subreddit":"LocalLLaMA","author_flair_text":null,"treatment_tags":[],"created_utc":1753253444,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"","collapsed_because_crowd_control":null,"mod_reports":[],"mod_note":null,"distinguished":null}}],"before":null}},"user_reports":[],"saved":false,"id":"n4n6feg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"offlinesir","can_mod_post":false,"created_utc":1753237949,"send_replies":true,"parent_id":"t3_1m6xbru","score":18,"author_fullname":"t2_jn5ft2le","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"unless Google DeepMind decides to publish detailed technical reports or open-source the model or logs, we won't know how Gemini 2.5 achieves its 1M context window. There's also no kinda leaks for these things at all.\\n\\nMy assumption: it maybe IS trained from scratch with 1M tokens per sequence! Google is the data king, they own docs, gmail, youtube, etc (however, I don't think they are using gmail or docs data). So they definetly have more resources than most to focus on context. It's also possible they wanted to \\"focus\\" on context and it paid off.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4n6feg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;unless Google DeepMind decides to publish detailed technical reports or open-source the model or logs, we won&amp;#39;t know how Gemini 2.5 achieves its 1M context window. There&amp;#39;s also no kinda leaks for these things at all.&lt;/p&gt;\\n\\n&lt;p&gt;My assumption: it maybe IS trained from scratch with 1M tokens per sequence! Google is the data king, they own docs, gmail, youtube, etc (however, I don&amp;#39;t think they are using gmail or docs data). So they definetly have more resources than most to focus on context. It&amp;#39;s also possible they wanted to &amp;quot;focus&amp;quot; on context and it paid off.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6xbru/how_does_gemini_25_pro_natively_support_1m_tokens/n4n6feg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753237949,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6xbru","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":18}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4nfbel","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DeepWisdomGuy","can_mod_post":false,"created_utc":1753241336,"send_replies":true,"parent_id":"t3_1m6xbru","score":5,"author_fullname":"t2_lznk2wv8h","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"By using this: [https://arxiv.org/pdf/2501.00663v1](https://arxiv.org/pdf/2501.00663v1)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4nfbel","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;By using this: &lt;a href=\\"https://arxiv.org/pdf/2501.00663v1\\"&gt;https://arxiv.org/pdf/2501.00663v1&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6xbru/how_does_gemini_25_pro_natively_support_1m_tokens/n4nfbel/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753241336,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6xbru","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4p37zy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Accomplished_Mode170","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4okbdj","score":1,"author_fullname":"t2_4hfmiefj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Nah, MS’s DSR1 fine tune is coherent up to 80k plus\\n\\nMy heuristic being the refactor of a Python microservice to Rust where the prompt is 50k tokens","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4p37zy","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nah, MS’s DSR1 fine tune is coherent up to 80k plus&lt;/p&gt;\\n\\n&lt;p&gt;My heuristic being the refactor of a Python microservice to Rust where the prompt is 50k tokens&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6xbru","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6xbru/how_does_gemini_25_pro_natively_support_1m_tokens/n4p37zy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753271455,"author_flair_text":null,"treatment_tags":[],"created_utc":1753271455,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4pfwdf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"stoppableDissolution","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4pfa54","score":1,"author_fullname":"t2_1n0su21k4z","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I've not used web version at all, so idk. Claude I only used web with plus account (or whatever its named, cheap one), and it was getting very dumb just around the point when they start timing you out after two or three messages (20-30k?). But yea, it wildly varies with _how_ you talk to them, so idk.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n4pfwdf","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve not used web version at all, so idk. Claude I only used web with plus account (or whatever its named, cheap one), and it was getting very dumb just around the point when they start timing you out after two or three messages (20-30k?). But yea, it wildly varies with &lt;em&gt;how&lt;/em&gt; you talk to them, so idk.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m6xbru","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6xbru/how_does_gemini_25_pro_natively_support_1m_tokens/n4pfwdf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753275980,"author_flair_text":null,"treatment_tags":[],"created_utc":1753275980,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4pfa54","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Jazzlike_Source_5983","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4pdkuu","score":1,"author_fullname":"t2_ap0ra8pe","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"YMMV, I guess. I've been on the max account since it came out and I'm pushing Claude conversations until they end. Gemini starts responding to earlier prompts and losing the script real early. Gemini Deep Research is awesome. In order to get a usable version of the actual raw 2.5 Pro I've had to use the API version and that's how I get to 150k. The [gemini.google.com](http://gemini.google.com) gets dementia super early. It's sad!","edited":false,"author_flair_css_class":null,"name":"t1_n4pfa54","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;YMMV, I guess. I&amp;#39;ve been on the max account since it came out and I&amp;#39;m pushing Claude conversations until they end. Gemini starts responding to earlier prompts and losing the script real early. Gemini Deep Research is awesome. In order to get a usable version of the actual raw 2.5 Pro I&amp;#39;ve had to use the API version and that&amp;#39;s how I get to 150k. The &lt;a href=\\"http://gemini.google.com\\"&gt;gemini.google.com&lt;/a&gt; gets dementia super early. It&amp;#39;s sad!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m6xbru","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6xbru/how_does_gemini_25_pro_natively_support_1m_tokens/n4pfa54/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753275774,"author_flair_text":null,"collapsed":false,"created_utc":1753275774,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4pdkuu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"stoppableDissolution","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4pck2p","score":1,"author_fullname":"t2_1n0su21k4z","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Coding, maybe. Conversation? Hell no.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4pdkuu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Coding, maybe. Conversation? Hell no.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6xbru","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6xbru/how_does_gemini_25_pro_natively_support_1m_tokens/n4pdkuu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753275217,"author_flair_text":null,"treatment_tags":[],"created_utc":1753275217,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4pck2p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Jazzlike_Source_5983","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4okbdj","score":1,"author_fullname":"t2_ap0ra8pe","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"20k? I don't know what models you're working with, but Claude stays coherent almost all the way up to the end of the window - we get 150k good tokens out of him, for sure. DeepSeek around 80-100k for sure. Cohere Command A seems to genuinely make good on its 200+k promise. Gemini bloops out a lot faster than Claude and Command A.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4pck2p","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;20k? I don&amp;#39;t know what models you&amp;#39;re working with, but Claude stays coherent almost all the way up to the end of the window - we get 150k good tokens out of him, for sure. DeepSeek around 80-100k for sure. Cohere Command A seems to genuinely make good on its 200+k promise. Gemini bloops out a lot faster than Claude and Command A.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6xbru","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6xbru/how_does_gemini_25_pro_natively_support_1m_tokens/n4pck2p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753274873,"author_flair_text":null,"treatment_tags":[],"created_utc":1753274873,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4okbdj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"stoppableDissolution","can_mod_post":false,"created_utc":1753262226,"send_replies":true,"parent_id":"t1_n4oa1jp","score":2,"author_fullname":"t2_1n0su21k4z","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Other models lose the plot after 20k at best tho. As much as I dont like gemini, its context coherence is unparalleled","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4okbdj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Other models lose the plot after 20k at best tho. As much as I dont like gemini, its context coherence is unparalleled&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6xbru","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6xbru/how_does_gemini_25_pro_natively_support_1m_tokens/n4okbdj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753262226,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4oa1jp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Jazzlike_Source_5983","can_mod_post":false,"created_utc":1753256294,"send_replies":true,"parent_id":"t3_1m6xbru","score":4,"author_fullname":"t2_ap0ra8pe","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Man I mean... does it? I use Gemini every day and I have not gotten it to regularly stay usefully coherent after 150k-esque tokens. It can push 200k, but man, for all I like about Gemini, its token limit is not one. It might be able to sit there and take a pounding in terms of what you put into it, but boy oh boy does it lose the ability to focus fast.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4oa1jp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Man I mean... does it? I use Gemini every day and I have not gotten it to regularly stay usefully coherent after 150k-esque tokens. It can push 200k, but man, for all I like about Gemini, its token limit is not one. It might be able to sit there and take a pounding in terms of what you put into it, but boy oh boy does it lose the ability to focus fast.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6xbru/how_does_gemini_25_pro_natively_support_1m_tokens/n4oa1jp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753256294,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6xbru","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"2b12e2b8-fdc0-11ee-9a03-6e2f48afd456","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4nhj0e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Former-Ad-5757","can_mod_post":false,"created_utc":1753242238,"send_replies":true,"parent_id":"t3_1m6xbru","score":2,"author_fullname":"t2_ihsdiwk6k","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Basically they have their own hardware that can change the whole game. All the other players are on the nvidia train. Why is groq so much faster than everybody else, they have different hardware…","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4nhj0e","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 3"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Basically they have their own hardware that can change the whole game. All the other players are on the nvidia train. Why is groq so much faster than everybody else, they have different hardware…&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6xbru/how_does_gemini_25_pro_natively_support_1m_tokens/n4nhj0e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753242238,"author_flair_text":"Llama 3","treatment_tags":[],"link_id":"t3_1m6xbru","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#c7b594","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4n8izr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"nullmove","can_mod_post":false,"created_utc":1753238713,"send_replies":true,"parent_id":"t3_1m6xbru","score":1,"author_fullname":"t2_aq4j0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Band attention + NoPE + a fuck ton of compute is what I read somewhere, no way to tell for sure.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4n8izr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Band attention + NoPE + a fuck ton of compute is what I read somewhere, no way to tell for sure.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6xbru/how_does_gemini_25_pro_natively_support_1m_tokens/n4n8izr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753238713,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6xbru","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4t5vgb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"JustinPooDough","can_mod_post":false,"created_utc":1753314795,"send_replies":true,"parent_id":"t3_1m6xbru","score":1,"author_fullname":"t2_4kns99rz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Google invented the Transformer and they build their own chips.\\n\\nThey also know you better than your closest loved one. They will win this race.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4t5vgb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Google invented the Transformer and they build their own chips.&lt;/p&gt;\\n\\n&lt;p&gt;They also know you better than your closest loved one. They will win this race.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6xbru/how_does_gemini_25_pro_natively_support_1m_tokens/n4t5vgb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753314795,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6xbru","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4no042","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Threatening-Silence-","can_mod_post":false,"created_utc":1753244976,"send_replies":true,"parent_id":"t1_n4nafff","score":6,"author_fullname":"t2_15wqsifdjf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"He's asking about Gemini, not Qwen","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4no042","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;He&amp;#39;s asking about Gemini, not Qwen&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6xbru","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6xbru/how_does_gemini_25_pro_natively_support_1m_tokens/n4no042/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753244976,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n4nafff","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"pseudonerv","can_mod_post":false,"created_utc":1753239426,"send_replies":true,"parent_id":"t3_1m6xbru","score":-5,"author_fullname":"t2_eerln","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":true,"body":"See\\n\\nhttps://huggingface.co/Qwen/Qwen3-235B-A22B/blob/main/config.json#L29\\n\\n      \\"rope_theta\\": 1000000.0,\\n\\nhttps://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507/blob/main/config.json#L29\\n\\n      \\"rope_theta\\": 5000000,\\n\\nhttps://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct/blob/main/config.json#L28\\n\\n      \\"rope_theta\\": 10000000,\\n\\nSo, yeah, simple rope scaling and fine-tuning.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4nafff","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;See&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/Qwen/Qwen3-235B-A22B/blob/main/config.json#L29\\"&gt;https://huggingface.co/Qwen/Qwen3-235B-A22B/blob/main/config.json#L29&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;  &amp;quot;rope_theta&amp;quot;: 1000000.0,\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507/blob/main/config.json#L29\\"&gt;https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507/blob/main/config.json#L29&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;  &amp;quot;rope_theta&amp;quot;: 5000000,\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct/blob/main/config.json#L28\\"&gt;https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct/blob/main/config.json#L28&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;  &amp;quot;rope_theta&amp;quot;: 10000000,\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;So, yeah, simple rope scaling and fine-tuning.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6xbru/how_does_gemini_25_pro_natively_support_1m_tokens/n4nafff/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753239426,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6xbru","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-5}}],"before":null}}]`),r=()=>e.jsx(l,{data:t});export{r as default};
