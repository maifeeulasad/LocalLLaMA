import{j as e}from"./index-Dh2YTDbC.js";import{R as l}from"./RedditPostRenderer-BwWe7STC.js";import"./index-D7FMfiLd.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"For those who may be interested, a free-time project that I've now put up on Github: [https://github.com/adriancable/qwen3.c](https://github.com/adriancable/qwen3.c)\\n\\nRun Qwen3-architecture models (like Qwen3-4B, or DeepSeek-R1-0528-Qwen3-8B) locally, no GPU required, using an LLM inference engine you build yourself from just 1 file of C source, with no dependencies. Only requirement is enough RAM to load the models. Think llama.cpp but 100X smaller and simpler, although it's still very functional: multi-language input/output, multi-core CPU support, supports reasoning/thinking models etc.\\n\\nAll you need to build and run is Python3 and a C compiler. The C source is so small, it compiles in around a second. Then, go have fun with the models!\\n\\nAfter you've played around for a bit, if you already understand a bit about how transformers work but want to really learn the detail, the inference engine's C source (unlike llama.cpp) is small enough to dig into without getting a heart attack. Once you've understood how it ticks, you're a transformers expert! ðŸ˜ƒ\\n\\nNot intended to compete with 'heavyweight' engines like llama.cpp, rather, the focus is on being (fun)ctional and educational.\\n\\nMIT license so you can do whatever you want with the source, no restrictions.\\n\\nProject will be a success if at least one person here enjoys it!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Qwen3 inference engine in C: simple, educational, fun","link_flair_richtext":[{"e":"text","t":"Generation"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lpejnj","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.96,"author_flair_background_color":null,"subreddit_type":"public","ups":81,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_fyyk012qp","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Generation","can_mod_post":false,"score":81,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1751406598,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;For those who may be interested, a free-time project that I&amp;#39;ve now put up on Github: &lt;a href=\\"https://github.com/adriancable/qwen3.c\\"&gt;https://github.com/adriancable/qwen3.c&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Run Qwen3-architecture models (like Qwen3-4B, or DeepSeek-R1-0528-Qwen3-8B) locally, no GPU required, using an LLM inference engine you build yourself from just 1 file of C source, with no dependencies. Only requirement is enough RAM to load the models. Think llama.cpp but 100X smaller and simpler, although it&amp;#39;s still very functional: multi-language input/output, multi-core CPU support, supports reasoning/thinking models etc.&lt;/p&gt;\\n\\n&lt;p&gt;All you need to build and run is Python3 and a C compiler. The C source is so small, it compiles in around a second. Then, go have fun with the models!&lt;/p&gt;\\n\\n&lt;p&gt;After you&amp;#39;ve played around for a bit, if you already understand a bit about how transformers work but want to really learn the detail, the inference engine&amp;#39;s C source (unlike llama.cpp) is small enough to dig into without getting a heart attack. Once you&amp;#39;ve understood how it ticks, you&amp;#39;re a transformers expert! ðŸ˜ƒ&lt;/p&gt;\\n\\n&lt;p&gt;Not intended to compete with &amp;#39;heavyweight&amp;#39; engines like llama.cpp, rather, the focus is on being (fun)ctional and educational.&lt;/p&gt;\\n\\n&lt;p&gt;MIT license so you can do whatever you want with the source, no restrictions.&lt;/p&gt;\\n\\n&lt;p&gt;Project will be a success if at least one person here enjoys it!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/LxoqZs4q3Osj78IVaTXSrgUKqNHcOujrOF1Tg6_GYA4.jpeg?auto=webp&amp;s=f64a6eef9fb25bb8dece4a00b49169cb6de85df2","width":640,"height":640},"resolutions":[{"url":"https://external-preview.redd.it/LxoqZs4q3Osj78IVaTXSrgUKqNHcOujrOF1Tg6_GYA4.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b6a4a1ab699ce9984d57b0696bdd1f873de9e614","width":108,"height":108},{"url":"https://external-preview.redd.it/LxoqZs4q3Osj78IVaTXSrgUKqNHcOujrOF1Tg6_GYA4.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=80ccbeb83c907fb5b897374c139c51e76825ec00","width":216,"height":216},{"url":"https://external-preview.redd.it/LxoqZs4q3Osj78IVaTXSrgUKqNHcOujrOF1Tg6_GYA4.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f5ad415a9157f412849b8def8bc5c576f5d41217","width":320,"height":320},{"url":"https://external-preview.redd.it/LxoqZs4q3Osj78IVaTXSrgUKqNHcOujrOF1Tg6_GYA4.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3ac79d8600937790d6301fdd4917b87eabf6336a","width":640,"height":640}],"variants":{},"id":"LxoqZs4q3Osj78IVaTXSrgUKqNHcOujrOF1Tg6_GYA4"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"23bddba8-ff56-11ed-9688-1a11994b71f7","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#b5a3d0","id":"1lpejnj","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"adrian-cable","discussion_type":null,"num_comments":15,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/","subreddit_subscribers":493457,"created_utc":1751406598,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ud3v1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Agreeable-Prompt-666","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ucleq","score":5,"author_fullname":"t2_1l3z4stvkq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"For sure. I just see huge possibilities with this.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0ud3v1","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For sure. I just see huge possibilities with this.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpejnj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n0ud3v1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751409373,"author_flair_text":null,"treatment_tags":[],"created_utc":1751409373,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0uytrx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"adrian-cable","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0uo4in","score":5,"author_fullname":"t2_fyyk012qp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Potentially. The project is only a day old so Iâ€™m really appreciative of any feedback and thoughts on directions I can take it. Thank you!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0uytrx","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Potentially. The project is only a day old so Iâ€™m really appreciative of any feedback and thoughts on directions I can take it. Thank you!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpejnj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n0uytrx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751416759,"author_flair_text":null,"treatment_tags":[],"created_utc":1751416759,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n0uo4in","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Accomplished_Mode170","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ucleq","score":3,"author_fullname":"t2_4hfmiefj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Any interest in supporting â€˜commodity computeâ€™ on something like tenstorrent?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0uo4in","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Any interest in supporting â€˜commodity computeâ€™ on something like tenstorrent?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpejnj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n0uo4in/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751413013,"author_flair_text":null,"treatment_tags":[],"created_utc":1751413013,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ucleq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"adrian-cable","can_mod_post":false,"created_utc":1751409205,"send_replies":true,"parent_id":"t1_n0ub1tk","score":9,"author_fullname":"t2_fyyk012qp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Running the same quantisation (Q8_0) itâ€™s within the same ballpark, generally within a factor of 2. Itâ€™s optimized for simplicity not performance, but it still runs at a very usable speed.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ucleq","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Running the same quantisation (Q8_0) itâ€™s within the same ballpark, generally within a factor of 2. Itâ€™s optimized for simplicity not performance, but it still runs at a very usable speed.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpejnj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n0ucleq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751409205,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ub1tk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Agreeable-Prompt-666","can_mod_post":false,"created_utc":1751408697,"send_replies":true,"parent_id":"t3_1lpejnj","score":14,"author_fullname":"t2_1l3z4stvkq","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Amazing and thank you, looking forward to learning.\\n\\nQuick q , really curious, how's speed relative to llamacpp :D","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ub1tk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Amazing and thank you, looking forward to learning.&lt;/p&gt;\\n\\n&lt;p&gt;Quick q , really curious, how&amp;#39;s speed relative to llamacpp :D&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n0ub1tk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751408697,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpejnj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":14}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0udetj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"adrian-cable","can_mod_post":false,"created_utc":1751409473,"send_replies":true,"parent_id":"t1_n0ucorh","score":3,"author_fullname":"t2_fyyk012qp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Not as fast since it prioritises simplicity over performance, but with everything else equal within 2X.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0udetj","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not as fast since it prioritises simplicity over performance, but with everything else equal within 2X.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpejnj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n0udetj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751409473,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ucorh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"yeah-ok","can_mod_post":false,"created_utc":1751409236,"send_replies":true,"parent_id":"t3_1lpejnj","score":5,"author_fullname":"t2_3xlrs","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Very impressive work, had a browse through runq.c and indeed it is, as c goes, digestible!ðŸ‘\\n\\nHave you done any, however rudimentary, comparison benchmarks in terms of qwen3.c vs llama.cpp?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ucorh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Very impressive work, had a browse through runq.c and indeed it is, as c goes, digestible!ðŸ‘&lt;/p&gt;\\n\\n&lt;p&gt;Have you done any, however rudimentary, comparison benchmarks in terms of qwen3.c vs llama.cpp?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n0ucorh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751409236,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpejnj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ughhw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"althalusian","can_mod_post":false,"created_utc":1751410488,"send_replies":true,"parent_id":"t1_n0ubrl7","score":2,"author_fullname":"t2_1qrqmhyn","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Careers not Carterâ€™s?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ughhw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Careers not Carterâ€™s?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpejnj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n0ughhw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751410488,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ubrl7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"_moria_","can_mod_post":false,"created_utc":1751408932,"send_replies":true,"parent_id":"t3_1lpejnj","score":3,"author_fullname":"t2_10lot4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"My humble opinion is that this is a critical objective. Understanding is a critical aspect of forming new people and ideas. Think about netbsd. The best? No, but surely the most clear code for an operating system, I know a lot of people for which clear simple code has opened high profile Carter's in os development.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ubrl7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;My humble opinion is that this is a critical objective. Understanding is a critical aspect of forming new people and ideas. Think about netbsd. The best? No, but surely the most clear code for an operating system, I know a lot of people for which clear simple code has opened high profile Carter&amp;#39;s in os development.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n0ubrl7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751408932,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpejnj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0uuamk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Languages_Learner","can_mod_post":false,"created_utc":1751415130,"send_replies":true,"parent_id":"t3_1lpejnj","score":2,"author_fullname":"t2_v9x8tm7u","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thanks for great implementation. It reminds me another pure C llm cpu inference engine which supports different models: [pierrel55/llama\\\\_st: Load and run Llama from safetensors files in C](https://github.com/pierrel55/llama_st)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0uuamk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for great implementation. It reminds me another pure C llm cpu inference engine which supports different models: &lt;a href=\\"https://github.com/pierrel55/llama_st\\"&gt;pierrel55/llama_st: Load and run Llama from safetensors files in C&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n0uuamk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751415130,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpejnj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0vyezf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Traditional_Tap1708","can_mod_post":false,"created_utc":1751430023,"send_replies":true,"parent_id":"t3_1lpejnj","score":1,"author_fullname":"t2_aejvth7b","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Really cool","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0vyezf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Really cool&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n0vyezf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751430023,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpejnj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0uvhpi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok_Cow1976","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0uc6xo","score":2,"author_fullname":"t2_3pwbsmdr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks a lot for explanations.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0uvhpi","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks a lot for explanations.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpejnj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n0uvhpi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751415556,"author_flair_text":null,"treatment_tags":[],"created_utc":1751415556,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0uc6xo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"adrian-cable","can_mod_post":false,"created_utc":1751409072,"send_replies":true,"parent_id":"t1_n0u8ovm","score":12,"author_fullname":"t2_fyyk012qp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Everythingâ€™s relative, but llama.cpp is pretty heavy, at around 400,000 lines of code, compared with 1,500 lines of code for this project. (Verify for yourself on codetabs.com)\\n\\nThe idea here is to make an inference engine whose source is small and simple enough so that, if you already understand C/C++, you can quickly understand how inference works in depth. You canâ€™t do that with a 400KLOC project.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0uc6xo","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Everythingâ€™s relative, but llama.cpp is pretty heavy, at around 400,000 lines of code, compared with 1,500 lines of code for this project. (Verify for yourself on codetabs.com)&lt;/p&gt;\\n\\n&lt;p&gt;The idea here is to make an inference engine whose source is small and simple enough so that, if you already understand C/C++, you can quickly understand how inference works in depth. You canâ€™t do that with a 400KLOC project.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpejnj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n0uc6xo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751409072,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}}],"before":null}},"user_reports":[],"saved":false,"id":"n0u8ovm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok_Cow1976","can_mod_post":false,"created_utc":1751407939,"send_replies":true,"parent_id":"t3_1lpejnj","score":1,"author_fullname":"t2_3pwbsmdr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Llama.cpp is not heavy. Vllm is huge and heavy. But nice to see alternatives.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0u8ovm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Llama.cpp is not heavy. Vllm is huge and heavy. But nice to see alternatives.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n0u8ovm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751407939,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpejnj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0uy5k2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"created_utc":1751416520,"send_replies":true,"parent_id":"t3_1lpejnj","score":1,"author_fullname":"t2_1a48h7vf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Masochist.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0uy5k2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Masochist.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n0uy5k2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751416520,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpejnj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),o=()=>e.jsx(l,{data:a});export{o as default};
