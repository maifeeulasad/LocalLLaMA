import{j as e}from"./index-CqAPCjw5.js";import{R as t}from"./RedditPostRenderer-4oBDAtGr.js";import"./index-D3Sdy_Op.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hey everyone,  \\nIâ€™m working on building a search engine for a retail platform with a product catalog that includes things like title, description, size, color, and categories (e.g., â€œmenâ€™s clothing &gt; shirtsâ€ or â€œwomenâ€™s shoesâ€).\\n\\nI'm still new to search, embeddings, and reranking, and Iâ€™ve got a bunch of questions. Would really appreciate any feedback or direction!\\n\\n**1. BM25 preprocessing:**  \\nFor the BM25 part, Iâ€™m wondering whatâ€™s the right preprocessing pipeline. Should I:\\n\\n* Lowercase everything?\\n* Normalize Turkish characters like \\"Ã§\\" to \\"c\\", \\"ÅŸ\\" to \\"s\\"?\\n* Do stemming or lemmatization?\\n* Only keep keywords?\\n\\nAny tips or open-source Turkish tokenizers that actually work well?\\n\\n**2. Embedding inputs:**  \\nWhen embedding products (using models like GPT or other multilingual LLMs), I usually feed them like this:\\n\\n    product title: ...  \\n    product description: ...  \\n    color: ...  \\n    size: ...\\n    \\n\\nI read somewhere (even here) that these key-value labels (\\"product title:\\", etc.) might not help and could even hurt  that LLM-based models can infer structure without them. Is that really true? Is there another sota way to do it?\\n\\nAlso, should I normalize Turkish characters here too, or just leave them as-is?\\n\\n**3. Reranking:**  \\nI tried ColBERT but wasnâ€™t impressed. I had much better results with Qwen-Reranker-4B, but itâ€™s too slow when Iâ€™m comparing query to even 25 products. Are there any smaller/faster rerankers that still perform decently for Turkish/multilingual content and can bu used it production? ColBERT is fast because of it's architecture but Reranker much reliable but slower :/\\n\\nAny advice, practical tips, or general pointers are more than welcome! Especially curious about how people handle multilingual search pipelines (Turkish in my case) and what preprocessing tricks really matter in practice.\\n\\nThanks in advance ðŸ™","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Need advice on search pipeline for retail products (BM25 + embeddings + reranking)","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lz0hk3","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":1,"author_flair_background_color":null,"subreddit_type":"public","ups":1,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_r3c0w369","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":1,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752432488,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey everyone,&lt;br/&gt;\\nIâ€™m working on building a search engine for a retail platform with a product catalog that includes things like title, description, size, color, and categories (e.g., â€œmenâ€™s clothing &amp;gt; shirtsâ€ or â€œwomenâ€™s shoesâ€).&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m still new to search, embeddings, and reranking, and Iâ€™ve got a bunch of questions. Would really appreciate any feedback or direction!&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;1. BM25 preprocessing:&lt;/strong&gt;&lt;br/&gt;\\nFor the BM25 part, Iâ€™m wondering whatâ€™s the right preprocessing pipeline. Should I:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Lowercase everything?&lt;/li&gt;\\n&lt;li&gt;Normalize Turkish characters like &amp;quot;Ã§&amp;quot; to &amp;quot;c&amp;quot;, &amp;quot;ÅŸ&amp;quot; to &amp;quot;s&amp;quot;?&lt;/li&gt;\\n&lt;li&gt;Do stemming or lemmatization?&lt;/li&gt;\\n&lt;li&gt;Only keep keywords?&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Any tips or open-source Turkish tokenizers that actually work well?&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;2. Embedding inputs:&lt;/strong&gt;&lt;br/&gt;\\nWhen embedding products (using models like GPT or other multilingual LLMs), I usually feed them like this:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;product title: ...  \\nproduct description: ...  \\ncolor: ...  \\nsize: ...\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;I read somewhere (even here) that these key-value labels (&amp;quot;product title:&amp;quot;, etc.) might not help and could even hurt  that LLM-based models can infer structure without them. Is that really true? Is there another sota way to do it?&lt;/p&gt;\\n\\n&lt;p&gt;Also, should I normalize Turkish characters here too, or just leave them as-is?&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;3. Reranking:&lt;/strong&gt;&lt;br/&gt;\\nI tried ColBERT but wasnâ€™t impressed. I had much better results with Qwen-Reranker-4B, but itâ€™s too slow when Iâ€™m comparing query to even 25 products. Are there any smaller/faster rerankers that still perform decently for Turkish/multilingual content and can bu used it production? ColBERT is fast because of it&amp;#39;s architecture but Reranker much reliable but slower :/&lt;/p&gt;\\n\\n&lt;p&gt;Any advice, practical tips, or general pointers are more than welcome! Especially curious about how people handle multilingual search pipelines (Turkish in my case) and what preprocessing tricks really matter in practice.&lt;/p&gt;\\n\\n&lt;p&gt;Thanks in advance ðŸ™&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lz0hk3","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"zedeleyici3401","discussion_type":null,"num_comments":2,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lz0hk3/need_advice_on_search_pipeline_for_retail/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lz0hk3/need_advice_on_search_pipeline_for_retail/","subreddit_subscribers":498850,"created_utc":1752432488,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2y3tm0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"vasileer","can_mod_post":false,"created_utc":1752433978,"send_replies":true,"parent_id":"t3_1lz0hk3","score":1,"author_fullname":"t2_730bgdulm","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"sounds like a use case for Typesense hybrid search","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2y3tm0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;sounds like a use case for Typesense hybrid search&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lz0hk3/need_advice_on_search_pipeline_for_retail/n2y3tm0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752433978,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lz0hk3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2y8e9w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Subject-Reach7646","can_mod_post":false,"created_utc":1752435371,"send_replies":true,"parent_id":"t3_1lz0hk3","score":1,"author_fullname":"t2_1socz1cgsc","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"BM25 libraries Iâ€™ve used usually have all of the typical preprocessing integrated. BM25s is a good implementation.\\n\\nYou might also look at sparse embedding like splade. Sentence transformers just integrated it.Â ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2y8e9w","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;BM25 libraries Iâ€™ve used usually have all of the typical preprocessing integrated. BM25s is a good implementation.&lt;/p&gt;\\n\\n&lt;p&gt;You might also look at sparse embedding like splade. Sentence transformers just integrated it.Â &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lz0hk3/need_advice_on_search_pipeline_for_retail/n2y8e9w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752435371,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lz0hk3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),s=()=>e.jsx(t,{data:l});export{s as default};
