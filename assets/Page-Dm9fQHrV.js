import{j as e}from"./index-CqAPCjw5.js";import{R as t}from"./RedditPostRenderer-4oBDAtGr.js";import"./index-D3Sdy_Op.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hey everyone,  \\nI‚Äôm working on building a search engine for a retail platform with a product catalog that includes things like title, description, size, color, and categories (e.g., ‚Äúmen‚Äôs clothing &gt; shirts‚Äù or ‚Äúwomen‚Äôs shoes‚Äù).\\n\\nI'm still new to search, embeddings, and reranking, and I‚Äôve got a bunch of questions. Would really appreciate any feedback or direction!\\n\\n**1. BM25 preprocessing:**  \\nFor the BM25 part, I‚Äôm wondering what‚Äôs the right preprocessing pipeline. Should I:\\n\\n* Lowercase everything?\\n* Normalize Turkish characters like \\"√ß\\" to \\"c\\", \\"≈ü\\" to \\"s\\"?\\n* Do stemming or lemmatization?\\n* Only keep keywords?\\n\\nAny tips or open-source Turkish tokenizers that actually work well?\\n\\n**2. Embedding inputs:**  \\nWhen embedding products (using models like GPT or other multilingual LLMs), I usually feed them like this:\\n\\n    product title: ...  \\n    product description: ...  \\n    color: ...  \\n    size: ...\\n    \\n\\nI read somewhere (even here) that these key-value labels (\\"product title:\\", etc.) might not help and could even hurt  that LLM-based models can infer structure without them. Is that really true? Is there another sota way to do it?\\n\\nAlso, should I normalize Turkish characters here too, or just leave them as-is?\\n\\n**3. Reranking:**  \\nI tried ColBERT but wasn‚Äôt impressed. I had much better results with Qwen-Reranker-4B, but it‚Äôs too slow when I‚Äôm comparing query to even 25 products. Are there any smaller/faster rerankers that still perform decently for Turkish/multilingual content and can bu used it production? ColBERT is fast because of it's architecture but Reranker much reliable but slower :/\\n\\nAny advice, practical tips, or general pointers are more than welcome! Especially curious about how people handle multilingual search pipelines (Turkish in my case) and what preprocessing tricks really matter in practice.\\n\\nThanks in advance üôè","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Need advice on search pipeline for retail products (BM25 + embeddings + reranking)","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lz0hk3","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":1,"author_flair_background_color":null,"subreddit_type":"public","ups":1,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_r3c0w369","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":1,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752432488,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey everyone,&lt;br/&gt;\\nI‚Äôm working on building a search engine for a retail platform with a product catalog that includes things like title, description, size, color, and categories (e.g., ‚Äúmen‚Äôs clothing &amp;gt; shirts‚Äù or ‚Äúwomen‚Äôs shoes‚Äù).&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m still new to search, embeddings, and reranking, and I‚Äôve got a bunch of questions. Would really appreciate any feedback or direction!&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;1. BM25 preprocessing:&lt;/strong&gt;&lt;br/&gt;\\nFor the BM25 part, I‚Äôm wondering what‚Äôs the right preprocessing pipeline. Should I:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Lowercase everything?&lt;/li&gt;\\n&lt;li&gt;Normalize Turkish characters like &amp;quot;√ß&amp;quot; to &amp;quot;c&amp;quot;, &amp;quot;≈ü&amp;quot; to &amp;quot;s&amp;quot;?&lt;/li&gt;\\n&lt;li&gt;Do stemming or lemmatization?&lt;/li&gt;\\n&lt;li&gt;Only keep keywords?&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Any tips or open-source Turkish tokenizers that actually work well?&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;2. Embedding inputs:&lt;/strong&gt;&lt;br/&gt;\\nWhen embedding products (using models like GPT or other multilingual LLMs), I usually feed them like this:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;product title: ...  \\nproduct description: ...  \\ncolor: ...  \\nsize: ...\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;I read somewhere (even here) that these key-value labels (&amp;quot;product title:&amp;quot;, etc.) might not help and could even hurt  that LLM-based models can infer structure without them. Is that really true? Is there another sota way to do it?&lt;/p&gt;\\n\\n&lt;p&gt;Also, should I normalize Turkish characters here too, or just leave them as-is?&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;3. Reranking:&lt;/strong&gt;&lt;br/&gt;\\nI tried ColBERT but wasn‚Äôt impressed. I had much better results with Qwen-Reranker-4B, but it‚Äôs too slow when I‚Äôm comparing query to even 25 products. Are there any smaller/faster rerankers that still perform decently for Turkish/multilingual content and can bu used it production? ColBERT is fast because of it&amp;#39;s architecture but Reranker much reliable but slower :/&lt;/p&gt;\\n\\n&lt;p&gt;Any advice, practical tips, or general pointers are more than welcome! Especially curious about how people handle multilingual search pipelines (Turkish in my case) and what preprocessing tricks really matter in practice.&lt;/p&gt;\\n\\n&lt;p&gt;Thanks in advance üôè&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lz0hk3","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"zedeleyici3401","discussion_type":null,"num_comments":2,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lz0hk3/need_advice_on_search_pipeline_for_retail/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lz0hk3/need_advice_on_search_pipeline_for_retail/","subreddit_subscribers":498850,"created_utc":1752432488,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2y3tm0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"vasileer","can_mod_post":false,"created_utc":1752433978,"send_replies":true,"parent_id":"t3_1lz0hk3","score":1,"author_fullname":"t2_730bgdulm","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"sounds like a use case for Typesense hybrid search","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2y3tm0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;sounds like a use case for Typesense hybrid search&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lz0hk3/need_advice_on_search_pipeline_for_retail/n2y3tm0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752433978,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lz0hk3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2y8e9w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Subject-Reach7646","can_mod_post":false,"created_utc":1752435371,"send_replies":true,"parent_id":"t3_1lz0hk3","score":1,"author_fullname":"t2_1socz1cgsc","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"BM25 libraries I‚Äôve used usually have all of the typical preprocessing integrated. BM25s is a good implementation.\\n\\nYou might also look at sparse embedding like splade. Sentence transformers just integrated it.¬†","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2y8e9w","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;BM25 libraries I‚Äôve used usually have all of the typical preprocessing integrated. BM25s is a good implementation.&lt;/p&gt;\\n\\n&lt;p&gt;You might also look at sparse embedding like splade. Sentence transformers just integrated it.¬†&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lz0hk3/need_advice_on_search_pipeline_for_retail/n2y8e9w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752435371,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lz0hk3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),s=()=>e.jsx(t,{data:l});export{s as default};
