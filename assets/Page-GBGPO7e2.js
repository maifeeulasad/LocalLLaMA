import{j as e}from"./index-CjwP30j7.js";import{R as t}from"./RedditPostRenderer-BbYuEq_V.js";import"./index-C-yxLSPN.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"With context limits being the way there are I wanted to experiment with creating a standalone middleman API server that \\"compresses\\" requests sent to models as a proof of concept. I've seen other methods employed that use a seperate model for compression but, Krunchwrapper completely avoids the need for running a model as an intermediary - which I find particularly in VRAM constrained environments. With KrunchWrapper I wanted to avoid this dependency and instead rely on local processing to identify areas for compression and pass a \\"decoder\\" to the LLM via a system prompt.\\n\\n* **Github Link**: [https://github.com/thad0ctor/KrunchWrapper](https://github.com/thad0ctor/KrunchWrapper)\\n\\nThe server runs on Python 3.12 from its own venv and curently works on both Linux and Windows (mostly tested on linux but I did a few runs on windows). Currently, I have tested it to work on its own embedded WebUI (thank you llama.cpp), SillyTavern and with Cline interfacing with a locally hosted OpenAI compatible server. I also have support for using Cline with the Anthropic API.\\n\\nBetween compression and (optional) comment stripping, **I have been able to acheive &gt;40% compression when passing code files to the LLM that contain lots of repetition.** So far I haven't had any issues with fairly smart models like Qwen3 (14B, 32B, 235B) and Gemma3 understanding and adhering to the compression instructions.\\n\\nAt its core, what KrunchWrapper essentially does is:\\n\\n1. **Receive:** Establishes a proxy server that \\"intercepts\\" prompts going to a LLM server\\n2. **Analyze:** Analyzes those prompts for common patterns of text\\n3. **Assign:** Maps a unicode symbol (known to use fewer tokens) to that pattern of text\\n   1. Analyzes whether savings &gt; system prompt overhead\\n4. **Compress:** Replaces all identified patterns of text with the selected symbol(s)\\n   1.  Preserves JSON, markdown, tool calls\\n5. **Intercept:** Passes a system prompt with the compression decoder to the LLM along with the compressed message\\n6. **Instruct:** Instucts the LLM to use the compressed symbols in any response\\n7. **Decompress:** Decodes any responses received from the LLM that contain the compressed symbols\\n8. **Repeat:** Intilligently adds to and re-uses any compression dictionaries in follow-on messages\\n\\nBeyond the basic functionality there is a wide range of customization and documentation to explain the settings to fine tune compression to your individual needs. For example: users can defer compression to subsequent messages if they intended to provide other files and not \\"waste\\" compression tokens on minimal impact compression opportunities.\\n\\nLooking ahead, I would like to expand this for other popular tools like Roo, Aider, etc. and other APIs. I beleive this could really help save on API costs once expanded.I also did some initial testing with Cursor but given it is proprietary nature and that its requests are encrypted with SSL a lot more work needs to be done to properly intercept its traffic to apply compression for non-local API requests.\\n\\n**Disclaimers:** I am not a programmer by trade. I refuse to use the v-word I so often see on here but let's just say I could have never even attempted this without agentic coding and API invoice payments flying out the door. This is reflected in the code. I have done my best to employ best practices and not have this be some spaghetti code quagmire but to say this tool is production ready would be an insult to every living software engineer - I would like to stress how Beta this is - like Tarkov 2016, not Tarkov 2025.\\n\\nThis type of compression does not come without latency. Be sure to change the thread settings in the configs to maximize throughput. That said, there is a cost to using less context by means of an added processing delay. Lastly, I highly recommend not turning on DEBUG and verbose logging in your terminal output... seriously.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"KrunchWrapper - a LLM compression proxy (beta)","link_flair_richtext":[{"e":"text","t":"Resources"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":116,"top_awarded_type":null,"hide_score":false,"name":"t3_1lotza5","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.95,"author_flair_background_color":null,"ups":65,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_t8zbiflk","secure_media":null,"is_reddit_media_domain":true,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Resources","can_mod_post":false,"score":65,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://a.thumbs.redditmedia.com/OBJHLqO4yd5tQzsoUyNqLxbQOT-2_HHT_VupUGcRmd4.jpg","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"image","content_categories":null,"is_self":false,"subreddit_type":"public","created":1751349085,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"i.redd.it","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;With context limits being the way there are I wanted to experiment with creating a standalone middleman API server that &amp;quot;compresses&amp;quot; requests sent to models as a proof of concept. I&amp;#39;ve seen other methods employed that use a seperate model for compression but, Krunchwrapper completely avoids the need for running a model as an intermediary - which I find particularly in VRAM constrained environments. With KrunchWrapper I wanted to avoid this dependency and instead rely on local processing to identify areas for compression and pass a &amp;quot;decoder&amp;quot; to the LLM via a system prompt.&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;strong&gt;Github Link&lt;/strong&gt;: &lt;a href=\\"https://github.com/thad0ctor/KrunchWrapper\\"&gt;https://github.com/thad0ctor/KrunchWrapper&lt;/a&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;The server runs on Python 3.12 from its own venv and curently works on both Linux and Windows (mostly tested on linux but I did a few runs on windows). Currently, I have tested it to work on its own embedded WebUI (thank you llama.cpp), SillyTavern and with Cline interfacing with a locally hosted OpenAI compatible server. I also have support for using Cline with the Anthropic API.&lt;/p&gt;\\n\\n&lt;p&gt;Between compression and (optional) comment stripping, &lt;strong&gt;I have been able to acheive &amp;gt;40% compression when passing code files to the LLM that contain lots of repetition.&lt;/strong&gt; So far I haven&amp;#39;t had any issues with fairly smart models like Qwen3 (14B, 32B, 235B) and Gemma3 understanding and adhering to the compression instructions.&lt;/p&gt;\\n\\n&lt;p&gt;At its core, what KrunchWrapper essentially does is:&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;&lt;strong&gt;Receive:&lt;/strong&gt; Establishes a proxy server that &amp;quot;intercepts&amp;quot; prompts going to a LLM server&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Analyze:&lt;/strong&gt; Analyzes those prompts for common patterns of text&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Assign:&lt;/strong&gt; Maps a unicode symbol (known to use fewer tokens) to that pattern of text\\n\\n&lt;ol&gt;\\n&lt;li&gt;Analyzes whether savings &amp;gt; system prompt overhead&lt;/li&gt;\\n&lt;/ol&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Compress:&lt;/strong&gt; Replaces all identified patterns of text with the selected symbol(s)\\n\\n&lt;ol&gt;\\n&lt;li&gt; Preserves JSON, markdown, tool calls&lt;/li&gt;\\n&lt;/ol&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Intercept:&lt;/strong&gt; Passes a system prompt with the compression decoder to the LLM along with the compressed message&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Instruct:&lt;/strong&gt; Instucts the LLM to use the compressed symbols in any response&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Decompress:&lt;/strong&gt; Decodes any responses received from the LLM that contain the compressed symbols&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Repeat:&lt;/strong&gt; Intilligently adds to and re-uses any compression dictionaries in follow-on messages&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;Beyond the basic functionality there is a wide range of customization and documentation to explain the settings to fine tune compression to your individual needs. For example: users can defer compression to subsequent messages if they intended to provide other files and not &amp;quot;waste&amp;quot; compression tokens on minimal impact compression opportunities.&lt;/p&gt;\\n\\n&lt;p&gt;Looking ahead, I would like to expand this for other popular tools like Roo, Aider, etc. and other APIs. I beleive this could really help save on API costs once expanded.I also did some initial testing with Cursor but given it is proprietary nature and that its requests are encrypted with SSL a lot more work needs to be done to properly intercept its traffic to apply compression for non-local API requests.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Disclaimers:&lt;/strong&gt; I am not a programmer by trade. I refuse to use the v-word I so often see on here but let&amp;#39;s just say I could have never even attempted this without agentic coding and API invoice payments flying out the door. This is reflected in the code. I have done my best to employ best practices and not have this be some spaghetti code quagmire but to say this tool is production ready would be an insult to every living software engineer - I would like to stress how Beta this is - like Tarkov 2016, not Tarkov 2025.&lt;/p&gt;\\n\\n&lt;p&gt;This type of compression does not come without latency. Be sure to change the thread settings in the configs to maximize throughput. That said, there is a cost to using less context by means of an added processing delay. Lastly, I highly recommend not turning on DEBUG and verbose logging in your terminal output... seriously.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"url_overridden_by_dest":"https://i.redd.it/c4bjroisb7af1.png","view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://preview.redd.it/c4bjroisb7af1.png?auto=webp&amp;s=b88204958dbcdc681dbc1089b559e32adb40c350","width":1172,"height":974},"resolutions":[{"url":"https://preview.redd.it/c4bjroisb7af1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d5ff9190d564962e274b1c6665d855eb4c5d4e4a","width":108,"height":89},{"url":"https://preview.redd.it/c4bjroisb7af1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e4378bd2a4992dfad785d409a006c58317ec7fbd","width":216,"height":179},{"url":"https://preview.redd.it/c4bjroisb7af1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=867a765de6c4ddd2f5d24ea7ab8faf8490ddec22","width":320,"height":265},{"url":"https://preview.redd.it/c4bjroisb7af1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a7b39a5201c024ced3ca9aba3ebe3b3090ade2d9","width":640,"height":531},{"url":"https://preview.redd.it/c4bjroisb7af1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ee523709372bd3657eaf9246f0a874e27807a7ca","width":960,"height":797},{"url":"https://preview.redd.it/c4bjroisb7af1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=21327075786eb9c69b519dbedf62470031d5fb4d","width":1080,"height":897}],"variants":{},"id":"MZzUXdIWeCZllPxSBtOu6BJsl9m2ph14Rq4KIP2wMEs"}],"enabled":true},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"mod_note":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"num_reports":null,"removal_reason":null,"link_flair_background_color":"#ccac2b","id":"1lotza5","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"LA_rent_Aficionado","discussion_type":null,"num_comments":19,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lotza5/krunchwrapper_a_llm_compression_proxy_beta/","stickied":false,"url":"https://i.redd.it/c4bjroisb7af1.png","subreddit_subscribers":493458,"created_utc":1751349085,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"2b12e2b8-fdc0-11ee-9a03-6e2f48afd456","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0rrww3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HiddenoO","can_mod_post":false,"created_utc":1751382792,"send_replies":true,"parent_id":"t1_n0qpgtn","score":3,"author_fullname":"t2_8127x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt;You are basically running two sequences over the text, first a decoding run and then a interpretation run.  \\nDouble chance of hallucinations, errors etc.\\n\\nIsn't it three? They also instruct the model to use the same encoding in its output, so there's another encoding at the end.\\n\\nI'd be highly surprised if this doesn't significantly degrade overall performance of models, especially on tasks they're not already oversized for, to begin with. And if they are, you're saving a lot more by swapping to a smaller model instead.\\n\\nFrankly speaking, I find it a bit irresponsible to post this with zero benchmarking when calling it beta and not experimental.","edited":1751383205,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0rrww3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;You are basically running two sequences over the text, first a decoding run and then a interpretation run.&lt;br/&gt;\\nDouble chance of hallucinations, errors etc.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Isn&amp;#39;t it three? They also instruct the model to use the same encoding in its output, so there&amp;#39;s another encoding at the end.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;d be highly surprised if this doesn&amp;#39;t significantly degrade overall performance of models, especially on tasks they&amp;#39;re not already oversized for, to begin with. And if they are, you&amp;#39;re saving a lot more by swapping to a smaller model instead.&lt;/p&gt;\\n\\n&lt;p&gt;Frankly speaking, I find it a bit irresponsible to post this with zero benchmarking when calling it beta and not experimental.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lotza5","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lotza5/krunchwrapper_a_llm_compression_proxy_beta/n0rrww3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751382792,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0r8hku","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LA_rent_Aficionado","can_mod_post":false,"created_utc":1751376916,"send_replies":true,"parent_id":"t1_n0qpgtn","score":1,"author_fullname":"t2_t8zbiflk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Good point, my original concept would have better supported this approach by instead of using dynamic compression I built dictionaries based on common usage after analyzing code bases.  \\n\\nNot unexpectedly, this limited compression across a wider set of test code since you are essentially bounded by the number of low token symbols available for assignment whose benefit &gt; overhead when combining with system prompt instructions.\\n\\nIn practice it’s really easy to exclude the decompression step with minimal impacts to the overall compression pipeline if asking the LLM questions about code, not any refactoring etc.   That solves one avenue for potential hallucinations but correct - it is a system that would overall benefit from some native token level compression - something I suspect the OpenAIs and Anthropics of the world do within their APIs.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0r8hku","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Good point, my original concept would have better supported this approach by instead of using dynamic compression I built dictionaries based on common usage after analyzing code bases.  &lt;/p&gt;\\n\\n&lt;p&gt;Not unexpectedly, this limited compression across a wider set of test code since you are essentially bounded by the number of low token symbols available for assignment whose benefit &amp;gt; overhead when combining with system prompt instructions.&lt;/p&gt;\\n\\n&lt;p&gt;In practice it’s really easy to exclude the decompression step with minimal impacts to the overall compression pipeline if asking the LLM questions about code, not any refactoring etc.   That solves one avenue for potential hallucinations but correct - it is a system that would overall benefit from some native token level compression - something I suspect the OpenAIs and Anthropics of the world do within their APIs.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lotza5","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lotza5/krunchwrapper_a_llm_compression_proxy_beta/n0r8hku/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751376916,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0qpgtn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Former-Ad-5757","can_mod_post":false,"created_utc":1751369842,"send_replies":true,"parent_id":"t3_1lotza5","score":11,"author_fullname":"t2_ihsdiwk6k","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This is only a good idea if you are also changing the tokenizer of the llm and retrain the llm.\\n\\nYou are basically running two sequences over the text, first a decoding run and then a interpretation run.   \\nDouble chance of hallucinations, errors etc.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0qpgtn","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 3"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is only a good idea if you are also changing the tokenizer of the llm and retrain the llm.&lt;/p&gt;\\n\\n&lt;p&gt;You are basically running two sequences over the text, first a decoding run and then a interpretation run.&lt;br/&gt;\\nDouble chance of hallucinations, errors etc.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lotza5/krunchwrapper_a_llm_compression_proxy_beta/n0qpgtn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751369842,"author_flair_text":"Llama 3","treatment_tags":[],"link_id":"t3_1lotza5","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#c7b594","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0uojs1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LA_rent_Aficionado","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0u74i2","score":1,"author_fullname":"t2_t8zbiflk","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I will look into a means of testing to see how this compares to LLM lingua.  This article seems to imply lingua's method of compression seems to remove information that can break code specifically \\"This suggests that existing compression methods, while removing more information, may also remove semantic information that is critical for the model to generate correct code.\\" My hypothesis with the KrunchWrapper method is that the code syntax never really changes once substitutions are accounted for.  \\n\\n[https://arxiv.org/html/2410.22793v3?utm\\\\_source=chatgpt.com](https://arxiv.org/html/2410.22793v3?utm_source=chatgpt.com)","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0uojs1","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I will look into a means of testing to see how this compares to LLM lingua.  This article seems to imply lingua&amp;#39;s method of compression seems to remove information that can break code specifically &amp;quot;This suggests that existing compression methods, while removing more information, may also remove semantic information that is critical for the model to generate correct code.&amp;quot; My hypothesis with the KrunchWrapper method is that the code syntax never really changes once substitutions are accounted for.  &lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://arxiv.org/html/2410.22793v3?utm_source=chatgpt.com\\"&gt;https://arxiv.org/html/2410.22793v3?utm_source=chatgpt.com&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lotza5","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lotza5/krunchwrapper_a_llm_compression_proxy_beta/n0uojs1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751413155,"author_flair_text":null,"treatment_tags":[],"created_utc":1751413155,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0u74i2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"un_passant","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0r3te5","score":1,"author_fullname":"t2_7rqtc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The coding use case is interesting. I have no idea how LLMLingua performs for coding.\\n\\nAnyway, I think a comparison would be useful.","edited":false,"author_flair_css_class":null,"name":"t1_n0u74i2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The coding use case is interesting. I have no idea how LLMLingua performs for coding.&lt;/p&gt;\\n\\n&lt;p&gt;Anyway, I think a comparison would be useful.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lotza5","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lotza5/krunchwrapper_a_llm_compression_proxy_beta/n0u74i2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751407442,"author_flair_text":null,"collapsed":false,"created_utc":1751407442,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0r3te5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LA_rent_Aficionado","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0qvrhh","score":2,"author_fullname":"t2_t8zbiflk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I haven’t messed with LLM Lingua that much, aside from the speed issue and the need to host another model, what shied my away from LLM lingua is that you are pushing your uncompressed code for instance to the LLM and it is assessing /compressing it at a token level - leaving it more susceptible to break code syntax/variables etc. when working exclusively with code.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0r3te5","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I haven’t messed with LLM Lingua that much, aside from the speed issue and the need to host another model, what shied my away from LLM lingua is that you are pushing your uncompressed code for instance to the LLM and it is assessing /compressing it at a token level - leaving it more susceptible to break code syntax/variables etc. when working exclusively with code.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lotza5","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lotza5/krunchwrapper_a_llm_compression_proxy_beta/n0r3te5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751375353,"author_flair_text":null,"treatment_tags":[],"created_utc":1751375353,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0qvrhh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"un_passant","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0qs988","score":1,"author_fullname":"t2_7rqtc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thx, but I guess my point was \\"Why use this instead of LLMLingua ?\\"\\n\\nFWIW, I don't think that LLMLingua being slower matters that much because it can (should ?) be used offline, storing compressed versions of the context chunks in the vector db for RAG.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0qvrhh","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thx, but I guess my point was &amp;quot;Why use this instead of LLMLingua ?&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;FWIW, I don&amp;#39;t think that LLMLingua being slower matters that much because it can (should ?) be used offline, storing compressed versions of the context chunks in the vector db for RAG.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lotza5","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lotza5/krunchwrapper_a_llm_compression_proxy_beta/n0qvrhh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751372426,"author_flair_text":null,"treatment_tags":[],"created_utc":1751372426,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0qs988","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"phhusson","can_mod_post":false,"created_utc":1751371022,"send_replies":true,"parent_id":"t1_n0q5bty","score":2,"author_fullname":"t2_qwewv","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It's completely different approach. LLMlingua looks at the \\"thoughts\\" of the LLM to find which tokens are the least useful and remove them.\\n\\nKrunchWrapper just has some heuristics of some known tricks to reduce number of tokens. One stupid example would be to replace ==&gt; with → (replacing 2 tokens into one). It is also much faster than LLMLingua.\\n\\nNotably, the output of LLMLingua should be gibberish to a human, while the output of KrunchWrapper should still be meaningful to a human.\\n\\n  \\nPS: Technically you could probably combine the both to reduce even more","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0qs988","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s completely different approach. LLMlingua looks at the &amp;quot;thoughts&amp;quot; of the LLM to find which tokens are the least useful and remove them.&lt;/p&gt;\\n\\n&lt;p&gt;KrunchWrapper just has some heuristics of some known tricks to reduce number of tokens. One stupid example would be to replace ==&amp;gt; with → (replacing 2 tokens into one). It is also much faster than LLMLingua.&lt;/p&gt;\\n\\n&lt;p&gt;Notably, the output of LLMLingua should be gibberish to a human, while the output of KrunchWrapper should still be meaningful to a human.&lt;/p&gt;\\n\\n&lt;p&gt;PS: Technically you could probably combine the both to reduce even more&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lotza5","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lotza5/krunchwrapper_a_llm_compression_proxy_beta/n0qs988/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751371022,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0q5bty","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"un_passant","can_mod_post":false,"created_utc":1751359030,"send_replies":true,"parent_id":"t3_1lotza5","score":3,"author_fullname":"t2_7rqtc","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"How does it compare to [https://github.com/microsoft/LLMLingua?tab=readme-ov-file](https://github.com/microsoft/LLMLingua?tab=readme-ov-file) ?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0q5bty","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How does it compare to &lt;a href=\\"https://github.com/microsoft/LLMLingua?tab=readme-ov-file\\"&gt;https://github.com/microsoft/LLMLingua?tab=readme-ov-file&lt;/a&gt; ?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lotza5/krunchwrapper_a_llm_compression_proxy_beta/n0q5bty/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751359030,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lotza5","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"7d1f04e6-4920-11ef-b2e1-2e580594e1a1","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0r3ky4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"asankhs","can_mod_post":false,"created_utc":1751375272,"send_replies":true,"parent_id":"t3_1lotza5","score":2,"author_fullname":"t2_e0bph","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Great idea, would love to add it to OptiLLM.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0r3ky4","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 3.1"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Great idea, would love to add it to OptiLLM.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lotza5/krunchwrapper_a_llm_compression_proxy_beta/n0r3ky4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751375272,"author_flair_text":"Llama 3.1","treatment_tags":[],"link_id":"t3_1lotza5","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#93b1ba","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ts1f0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LA_rent_Aficionado","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0tqxmw","score":1,"author_fullname":"t2_t8zbiflk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"That example is not a good proxy for gauging efficiency, I noted in the reply that it was just showing the compression mechanism itself vs. the actual full workflow with its token efficiency calculations. \\n\\nThe actual workflow calculates token saving using tiktoken when determining compression &gt; overhead and only compresses when efficiency requirements have been met. \\n\\nWhen I get the opportunity I can post full before and after test utilizing the full pipeline.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ts1f0","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That example is not a good proxy for gauging efficiency, I noted in the reply that it was just showing the compression mechanism itself vs. the actual full workflow with its token efficiency calculations. &lt;/p&gt;\\n\\n&lt;p&gt;The actual workflow calculates token saving using tiktoken when determining compression &amp;gt; overhead and only compresses when efficiency requirements have been met. &lt;/p&gt;\\n\\n&lt;p&gt;When I get the opportunity I can post full before and after test utilizing the full pipeline.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lotza5","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lotza5/krunchwrapper_a_llm_compression_proxy_beta/n0ts1f0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751402945,"author_flair_text":null,"treatment_tags":[],"created_utc":1751402945,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0tqxmw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Leopold_Boom","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0psnac","score":2,"author_fullname":"t2_5weqo","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The problem with this is that most code already gets tokenized nicely by the encoder.\\n\\nI dropped your before and after into openai's tokenizer (https://tiktokenizer.vercel.app/)\\n\\nserver.py: 1623 tokens\\n\\nyour compressed\\\\_20250630\\\\_231952.txt: 1210 tokens\\n\\nyour dictionary: 751 tokens (without the custom prompt)\\n\\nSo you are achiving negative compression in terms of tokens (for code of this length) while significantly degrading your LLM's performance (which will only get worse the longer the code is).\\n\\nStill I do think there is a little juice to be squeezed from thinking deeply about tokenization etc. but you need to get a lot deeper than this.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0tqxmw","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The problem with this is that most code already gets tokenized nicely by the encoder.&lt;/p&gt;\\n\\n&lt;p&gt;I dropped your before and after into openai&amp;#39;s tokenizer (&lt;a href=\\"https://tiktokenizer.vercel.app/\\"&gt;https://tiktokenizer.vercel.app/&lt;/a&gt;)&lt;/p&gt;\\n\\n&lt;p&gt;server.py: 1623 tokens&lt;/p&gt;\\n\\n&lt;p&gt;your compressed_20250630_231952.txt: 1210 tokens&lt;/p&gt;\\n\\n&lt;p&gt;your dictionary: 751 tokens (without the custom prompt)&lt;/p&gt;\\n\\n&lt;p&gt;So you are achiving negative compression in terms of tokens (for code of this length) while significantly degrading your LLM&amp;#39;s performance (which will only get worse the longer the code is).&lt;/p&gt;\\n\\n&lt;p&gt;Still I do think there is a little juice to be squeezed from thinking deeply about tokenization etc. but you need to get a lot deeper than this.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lotza5","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lotza5/krunchwrapper_a_llm_compression_proxy_beta/n0tqxmw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751402636,"author_flair_text":null,"treatment_tags":[],"created_utc":1751402636,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0psnac","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LA_rent_Aficionado","can_mod_post":false,"created_utc":1751351478,"send_replies":true,"parent_id":"t1_n0pqshz","score":2,"author_fullname":"t2_t8zbiflk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I can't say how this would interact with anything else but this is pretty basic so as long as the system prompt and symbols are passed to another too it should work,\\n\\nHere is a test of compressing my [server.py](http://server.py) file in the code with the default settings.  Full results: [https://github.com/thad0ctor/KrunchWrapper/tree/main/compression\\\\_test\\\\_output](https://github.com/thad0ctor/KrunchWrapper/tree/main/compression_test_output)\\n\\nEdit: Note, this test just showed the compression methodolgy and didn't go thorugh the full workflow that accounts for system prompt overhead when making compression decisions, it was just to exemplify how the compression works. \\n\\n**Performance:**\\n\\n\`Original Size: 8,621 characters\`\\n\\n* \`Compressed Size: 5,549 characters\`\\n* \`Compression Ratio: 35.6% reduction\`\\n* \`Dictionary Entries: 60 symbols\`","edited":1751351836,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0psnac","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I can&amp;#39;t say how this would interact with anything else but this is pretty basic so as long as the system prompt and symbols are passed to another too it should work,&lt;/p&gt;\\n\\n&lt;p&gt;Here is a test of compressing my &lt;a href=\\"http://server.py\\"&gt;server.py&lt;/a&gt; file in the code with the default settings.  Full results: &lt;a href=\\"https://github.com/thad0ctor/KrunchWrapper/tree/main/compression_test_output\\"&gt;https://github.com/thad0ctor/KrunchWrapper/tree/main/compression_test_output&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Edit: Note, this test just showed the compression methodolgy and didn&amp;#39;t go thorugh the full workflow that accounts for system prompt overhead when making compression decisions, it was just to exemplify how the compression works. &lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Performance:&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;Original Size: 8,621 characters&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;code&gt;Compressed Size: 5,549 characters&lt;/code&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;code&gt;Compression Ratio: 35.6% reduction&lt;/code&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;code&gt;Dictionary Entries: 60 symbols&lt;/code&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lotza5","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lotza5/krunchwrapper_a_llm_compression_proxy_beta/n0psnac/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751351478,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0pqshz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No-Statement-0001","can_mod_post":false,"created_utc":1751350449,"send_replies":true,"parent_id":"t3_1lotza5","score":2,"author_fullname":"t2_11gh93nhos","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Neat. Can you provide some before and after examples of what the \\\\\`messages: \\\\[...\\\\]\\\\\` array looks like in a request?\\n\\nPrompt/context engineering is already such a black box of optimization that adding this in the middle would really have to be worth it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0pqshz","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Neat. Can you provide some before and after examples of what the \`messages: [...]\` array looks like in a request?&lt;/p&gt;\\n\\n&lt;p&gt;Prompt/context engineering is already such a black box of optimization that adding this in the middle would really have to be worth it.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lotza5/krunchwrapper_a_llm_compression_proxy_beta/n0pqshz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751350449,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lotza5","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0r49c2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LA_rent_Aficionado","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0r2hoc","score":1,"author_fullname":"t2_t8zbiflk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Makes perfect sense, let me look into this","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0r49c2","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Makes perfect sense, let me look into this&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lotza5","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lotza5/krunchwrapper_a_llm_compression_proxy_beta/n0r49c2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751375505,"author_flair_text":null,"treatment_tags":[],"created_utc":1751375505,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0r2hoc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MengerianMango","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0r1wk4","score":5,"author_fullname":"t2_mcvyi","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Forgive me if I'm mistaken, but it sounds like you think I mean computational performance benchmarks (like timing measurements).\\n\\nWhat I mean is how accurate the model is. For example, run MMLU on Qwen3:14b with no compression, then again with compression, and get a quantitative measurement of how much (if any) compression lowers its performance on the benchmark. I.e. a quantitative measure of how much dumber it got. Do the same test with Llama 3:8b and Qwen3:32b. My guess is they'll all get dumber, but which one gets dumber by the least amount? Etc. I feel like this would be the final step you'd need to write it up in an academic paper and publish it.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0r2hoc","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Forgive me if I&amp;#39;m mistaken, but it sounds like you think I mean computational performance benchmarks (like timing measurements).&lt;/p&gt;\\n\\n&lt;p&gt;What I mean is how accurate the model is. For example, run MMLU on Qwen3:14b with no compression, then again with compression, and get a quantitative measurement of how much (if any) compression lowers its performance on the benchmark. I.e. a quantitative measure of how much dumber it got. Do the same test with Llama 3:8b and Qwen3:32b. My guess is they&amp;#39;ll all get dumber, but which one gets dumber by the least amount? Etc. I feel like this would be the final step you&amp;#39;d need to write it up in an academic paper and publish it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lotza5","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lotza5/krunchwrapper_a_llm_compression_proxy_beta/n0r2hoc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751374896,"author_flair_text":null,"treatment_tags":[],"created_utc":1751374896,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n0r1wk4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LA_rent_Aficionado","can_mod_post":false,"created_utc":1751374692,"send_replies":true,"parent_id":"t1_n0qavzj","score":1,"author_fullname":"t2_t8zbiflk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This is mostly model agnostic with the exception that different models use different tokenizers, there are built in performance metrics","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0r1wk4","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is mostly model agnostic with the exception that different models use different tokenizers, there are built in performance metrics&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lotza5","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lotza5/krunchwrapper_a_llm_compression_proxy_beta/n0r1wk4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751374692,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0qavzj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MengerianMango","can_mod_post":false,"created_utc":1751362390,"send_replies":true,"parent_id":"t3_1lotza5","score":1,"author_fullname":"t2_mcvyi","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This is really fuckin cool. Huge respect.\\n\\nYou should conduct some benchmarks. Do a baseline eval and then do it again with compression enabled. Try a few different models to see if there is a trend.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0qavzj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is really fuckin cool. Huge respect.&lt;/p&gt;\\n\\n&lt;p&gt;You should conduct some benchmarks. Do a baseline eval and then do it again with compression enabled. Try a few different models to see if there is a trend.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lotza5/krunchwrapper_a_llm_compression_proxy_beta/n0qavzj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751362390,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lotza5","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ss4ch","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CalangoVelho","can_mod_post":false,"created_utc":1751392723,"send_replies":true,"parent_id":"t3_1lotza5","score":1,"author_fullname":"t2_uxp2xkt","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Tried llmlingua 2?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ss4ch","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Tried llmlingua 2?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lotza5/krunchwrapper_a_llm_compression_proxy_beta/n0ss4ch/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751392723,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lotza5","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(t,{data:l});export{r as default};
