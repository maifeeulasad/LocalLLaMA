import{j as e}from"./index-M4edQi1P.js";import{R as l}from"./RedditPostRenderer-CESBGIIy.js";import"./index-DFpL1mt4.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hi, I need to build a lab AI-Inference/Training/Development machine. Basically something to just get started get experience and burn as less money as possible. Due to availability problems my first choice (cheaper RTX PRO Blackwell cards) are not available. Now my question:\\n\\nWould it be viable to use multiple 5060 Ti (16GB) on a server motherboard (cheap EPYC 9004/8004). In my opinion the card is relatively cheap, supports new versions of CUDA and I can start with one or two and experiment with multiple (other NVIDIA cards). The purpose of the machine would only be getting experience so nothing to worry about meeting some standards for server deployment etc.\\n\\nThe card utilizes only 8 PCIe Lanes, but a 5070 Ti (16GB) utilizes all 16 lanes of the slot and has a way higher memory bandwidth for way more money. What speaks for and against my planned setup?\\n\\nBecause utilizing 8 PCIe 5.0 lanes are about 63.0 GB/s (x16 would be double). But I don't know how much that matters...","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Multiple 5060 Ti's","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lzkcg3","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.67,"author_flair_background_color":null,"subreddit_type":"public","ups":2,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_3ogjqne","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":2,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752493925,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi, I need to build a lab AI-Inference/Training/Development machine. Basically something to just get started get experience and burn as less money as possible. Due to availability problems my first choice (cheaper RTX PRO Blackwell cards) are not available. Now my question:&lt;/p&gt;\\n\\n&lt;p&gt;Would it be viable to use multiple 5060 Ti (16GB) on a server motherboard (cheap EPYC 9004/8004). In my opinion the card is relatively cheap, supports new versions of CUDA and I can start with one or two and experiment with multiple (other NVIDIA cards). The purpose of the machine would only be getting experience so nothing to worry about meeting some standards for server deployment etc.&lt;/p&gt;\\n\\n&lt;p&gt;The card utilizes only 8 PCIe Lanes, but a 5070 Ti (16GB) utilizes all 16 lanes of the slot and has a way higher memory bandwidth for way more money. What speaks for and against my planned setup?&lt;/p&gt;\\n\\n&lt;p&gt;Because utilizing 8 PCIe 5.0 lanes are about 63.0 GB/s (x16 would be double). But I don&amp;#39;t know how much that matters...&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lzkcg3","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"snorixx","discussion_type":null,"num_comments":32,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lzkcg3/multiple_5060_tis/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lzkcg3/multiple_5060_tis/","subreddit_subscribers":499296,"created_utc":1752493925,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n32e02n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cybran3","can_mod_post":false,"send_replies":true,"parent_id":"t1_n32c3va","score":3,"author_fullname":"t2_41gmkw5z","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I ordered Gigabyte B850 AI TOP. It supports 2x PCIe 5.0 8x at maximum speeds. It’s a consumer board tho, I’ll pair this with Ryzen R9 9900x and 128 GB 5600 MT/s of RAM.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n32e02n","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I ordered Gigabyte B850 AI TOP. It supports 2x PCIe 5.0 8x at maximum speeds. It’s a consumer board tho, I’ll pair this with Ryzen R9 9900x and 128 GB 5600 MT/s of RAM.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzkcg3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzkcg3/multiple_5060_tis/n32e02n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752496712,"author_flair_text":null,"treatment_tags":[],"created_utc":1752496712,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n341dln","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FieldProgrammable","can_mod_post":false,"send_replies":true,"parent_id":"t1_n32c3va","score":1,"author_fullname":"t2_moet0t","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Dual PCIE5x8 consumer boards are rarer but they do exist. You can also get a bifurcation riser for a slot wired for x16 to split it into two x8 slots (assuming the MB supports bifurcation which many do). If you are only going to be using two cards, then a server CPU/MB doesn't make much sense cost wise.\\n\\nI have an Asus ProArt X870E, that also does dual PCIE5x8 and has plenty of other high end features if you are looking for those (10Gb ethernet, PCIE5x4 M.2, shitloads of USB 3.2 ports).","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n341dln","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Dual PCIE5x8 consumer boards are rarer but they do exist. You can also get a bifurcation riser for a slot wired for x16 to split it into two x8 slots (assuming the MB supports bifurcation which many do). If you are only going to be using two cards, then a server CPU/MB doesn&amp;#39;t make much sense cost wise.&lt;/p&gt;\\n\\n&lt;p&gt;I have an Asus ProArt X870E, that also does dual PCIE5x8 and has plenty of other high end features if you are looking for those (10Gb ethernet, PCIE5x4 M.2, shitloads of USB 3.2 ports).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzkcg3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzkcg3/multiple_5060_tis/n341dln/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752514378,"author_flair_text":null,"treatment_tags":[],"created_utc":1752514378,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n32c3va","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"snorixx","can_mod_post":false,"created_utc":1752495995,"send_replies":true,"parent_id":"t1_n32aiwu","score":1,"author_fullname":"t2_3ogjqne","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Okay what Mainbord/Platform do you use, Consumer oder Server?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n32c3va","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Okay what Mainbord/Platform do you use, Consumer oder Server?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzkcg3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzkcg3/multiple_5060_tis/n32c3va/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752495995,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n32dvfw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1752496665,"send_replies":true,"parent_id":"t1_n32aiwu","score":1,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I kinda agree. I also thought about used 3090, but I jusged that I already have 3060 and to accomodate 3090 I need to replace PSU. So instead I am buying 5060ti once they get below $500 on our local market.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n32dvfw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I kinda agree. I also thought about used 3090, but I jusged that I already have 3060 and to accomodate 3090 I need to replace PSU. So instead I am buying 5060ti once they get below $500 on our local market.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzkcg3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzkcg3/multiple_5060_tis/n32dvfw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752496665,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n32aiwu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"cybran3","can_mod_post":false,"created_utc":1752495389,"send_replies":true,"parent_id":"t3_1lzkcg3","score":5,"author_fullname":"t2_41gmkw5z","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I ordered 2x 5060 Ti 16GB, they should be arriving any time now. I choose the 5060 instead of 3090 just because it’s gonna last me longer, and used GPUs are hit or miss and I don’t want that kind of trouble.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n32aiwu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I ordered 2x 5060 Ti 16GB, they should be arriving any time now. I choose the 5060 instead of 3090 just because it’s gonna last me longer, and used GPUs are hit or miss and I don’t want that kind of trouble.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzkcg3/multiple_5060_tis/n32aiwu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752495389,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzkcg3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n33y1q7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Direct_Turn_1484","can_mod_post":false,"created_utc":1752513485,"send_replies":true,"parent_id":"t1_n32c0g8","score":3,"author_fullname":"t2_6ywe9a9n5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Sure. But that’s not local.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n33y1q7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sure. But that’s not local.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"link_id":"t3_1lzkcg3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzkcg3/multiple_5060_tis/n33y1q7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752513485,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n32e42a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"snorixx","can_mod_post":false,"created_utc":1752496753,"send_replies":true,"parent_id":"t1_n32c0g8","score":1,"author_fullname":"t2_3ogjqne","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks I will consider that too","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n32e42a","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks I will consider that too&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzkcg3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzkcg3/multiple_5060_tis/n32e42a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752496753,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n32c0g8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"tmvr","can_mod_post":false,"created_utc":1752495959,"send_replies":true,"parent_id":"t3_1lzkcg3","score":3,"author_fullname":"t2_11qlhv","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Two of those cards alone cost USD800 (or in EU land about 860EUR). Check how many hours of 80GB+ GPUs you can rent for that amount (and that without upfront payment).\\n\\nEDIT: an example - on runpod you can get 24GB consumer GPUs for 20-30c/hr or 48GB pro GPUs for 60-70c/hr.  That's \\\\~1500-2200 hours of usage depending on what you go for without the additional expenses for the rest of the system and electricity.","edited":1752496247,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n32c0g8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Two of those cards alone cost USD800 (or in EU land about 860EUR). Check how many hours of 80GB+ GPUs you can rent for that amount (and that without upfront payment).&lt;/p&gt;\\n\\n&lt;p&gt;EDIT: an example - on runpod you can get 24GB consumer GPUs for 20-30c/hr or 48GB pro GPUs for 60-70c/hr.  That&amp;#39;s ~1500-2200 hours of usage depending on what you go for without the additional expenses for the rest of the system and electricity.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzkcg3/multiple_5060_tis/n32c0g8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752495959,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzkcg3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n32fqk0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"EthanMiner","can_mod_post":false,"send_replies":true,"parent_id":"t1_n32c5fn","score":1,"author_fullname":"t2_1vqwb1k7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"AM5 running a skinned Ubuntu 22.04","edited":1752497972,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n32fqk0","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;AM5 running a skinned Ubuntu 22.04&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzkcg3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzkcg3/multiple_5060_tis/n32fqk0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752497340,"author_flair_text":null,"treatment_tags":[],"created_utc":1752497340,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n32c5fn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"snorixx","can_mod_post":false,"created_utc":1752496012,"send_replies":true,"parent_id":"t1_n3280km","score":1,"author_fullname":"t2_3ogjqne","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What platform do you use Epyc/AM5?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n32c5fn","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What platform do you use Epyc/AM5?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzkcg3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzkcg3/multiple_5060_tis/n32c5fn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752496012,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n35nu7k","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"EthanMiner","can_mod_post":false,"send_replies":true,"parent_id":"t1_n33sxr9","score":2,"author_fullname":"t2_1vqwb1k7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I just use founders editions and don’t have those issues. You can water block them too, I fit 3 in a Lian Li A3, no problem, maxes out at 1295w on stress testing for 72gb of vram, normally closer to 700-800w.","edited":1752536716,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n35nu7k","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I just use founders editions and don’t have those issues. You can water block them too, I fit 3 in a Lian Li A3, no problem, maxes out at 1295w on stress testing for 72gb of vram, normally closer to 700-800w.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzkcg3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzkcg3/multiple_5060_tis/n35nu7k/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752531063,"author_flair_text":null,"treatment_tags":[],"created_utc":1752531063,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n33sxr9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HelpfulHand3","can_mod_post":false,"created_utc":1752512081,"send_replies":true,"parent_id":"t1_n3280km","score":1,"author_fullname":"t2_5jzar23i","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"yes, blackwell is still not well supported  \\nonly problem with 3090 is they are massive, and huge power hogs + the OC can require 3x8 PCIe  \\nmy 5070ti is much smaller than my 3090","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n33sxr9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;yes, blackwell is still not well supported&lt;br/&gt;\\nonly problem with 3090 is they are massive, and huge power hogs + the OC can require 3x8 PCIe&lt;br/&gt;\\nmy 5070ti is much smaller than my 3090&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzkcg3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzkcg3/multiple_5060_tis/n33sxr9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752512081,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3280km","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"EthanMiner","can_mod_post":false,"created_utc":1752494386,"send_replies":true,"parent_id":"t3_1lzkcg3","score":1,"author_fullname":"t2_1vqwb1k7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Just get used 3090s. My 5070ti and 5090 are pains to get working with everything training related in linux(inference is fine). It’s like github whack-a-mole figuring what else has to change one you update torch.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3280km","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just get used 3090s. My 5070ti and 5090 are pains to get working with everything training related in linux(inference is fine). It’s like github whack-a-mole figuring what else has to change one you update torch.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzkcg3/multiple_5060_tis/n3280km/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752494386,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzkcg3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n32l910","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AmIDumbOrSmart","can_mod_post":false,"created_utc":1752499243,"send_replies":true,"parent_id":"t3_1lzkcg3","score":1,"author_fullname":"t2_64f36er9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"sup, I have 2 5060 ti's and a 5070 ti. The 5060's are on a pcie 4.0 x4 lanes (probably heavily bottlenecked) and the 5070 ti is on an 5.0 x16. Can run q4km 70b models at 6k context at around 10 tokens a second or so. I dont have much space in my case so having some smaller cards was essential.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n32l910","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;sup, I have 2 5060 ti&amp;#39;s and a 5070 ti. The 5060&amp;#39;s are on a pcie 4.0 x4 lanes (probably heavily bottlenecked) and the 5070 ti is on an 5.0 x16. Can run q4km 70b models at 6k context at around 10 tokens a second or so. I dont have much space in my case so having some smaller cards was essential.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzkcg3/multiple_5060_tis/n32l910/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752499243,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzkcg3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n34iptu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"snorixx","can_mod_post":false,"send_replies":true,"parent_id":"t1_n343d4d","score":1,"author_fullname":"t2_3ogjqne","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thanks that’s interesting my first choice was the RTx 4000 Blackwell but that is not available. The big problem is that you will need a server board if you want to upgrade over time but, that increases initial cost substantially… And AMD cards are not yet an option atm","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n34iptu","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks that’s interesting my first choice was the RTx 4000 Blackwell but that is not available. The big problem is that you will need a server board if you want to upgrade over time but, that increases initial cost substantially… And AMD cards are not yet an option atm&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzkcg3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzkcg3/multiple_5060_tis/n34iptu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752519195,"author_flair_text":null,"treatment_tags":[],"created_utc":1752519195,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n343d4d","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FieldProgrammable","can_mod_post":false,"send_replies":true,"parent_id":"t1_n32v2tr","score":1,"author_fullname":"t2_moet0t","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That video is a single GPU running the entire workload form VRAM. So completely meaningless compared to multi GPU inference let alone training. For training, you need to maximise intercard bandwidth. One reason dual 3090s are so popular is they support NVLINK, which got dropped from consumer Ada onwards.\\n\\nAnother thing to research is PCIE P2P transfers, which Nvidia disable for gaming cards. Without that data has to pass through system memory to get to another card, so way higher latency. I think there was a hack to enable this for 4090s. But this is a feature that would be supported in pro cards out of the box, giving them an edge in training that might not be obvious from just compute and memory bandwidth comparison.","edited":1752515117,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n343d4d","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That video is a single GPU running the entire workload form VRAM. So completely meaningless compared to multi GPU inference let alone training. For training, you need to maximise intercard bandwidth. One reason dual 3090s are so popular is they support NVLINK, which got dropped from consumer Ada onwards.&lt;/p&gt;\\n\\n&lt;p&gt;Another thing to research is PCIE P2P transfers, which Nvidia disable for gaming cards. Without that data has to pass through system memory to get to another card, so way higher latency. I think there was a hack to enable this for 4090s. But this is a feature that would be supported in pro cards out of the box, giving them an edge in training that might not be obvious from just compute and memory bandwidth comparison.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzkcg3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzkcg3/multiple_5060_tis/n343d4d/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752514906,"author_flair_text":null,"treatment_tags":[],"created_utc":1752514906,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n32v2tr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"snorixx","can_mod_post":false,"created_utc":1752502397,"send_replies":true,"parent_id":"t1_n32r78o","score":1,"author_fullname":"t2_3ogjqne","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Nice thanks. I will watch it. I think it will only impact  when running one model on many cards because they have to communicate over PCIe which is way slower than memory","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n32v2tr","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nice thanks. I will watch it. I think it will only impact  when running one model on many cards because they have to communicate over PCIe which is way slower than memory&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzkcg3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzkcg3/multiple_5060_tis/n32v2tr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752502397,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n343sr7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AdamDhahabi","can_mod_post":false,"send_replies":true,"parent_id":"t1_n33y0nz","score":1,"author_fullname":"t2_x5lnbc2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You're right, I overlooked the training part OP mentioned.","edited":false,"author_flair_css_class":null,"name":"t1_n343sr7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You&amp;#39;re right, I overlooked the training part OP mentioned.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lzkcg3","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzkcg3/multiple_5060_tis/n343sr7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752515022,"author_flair_text":null,"collapsed":false,"created_utc":1752515022,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n33y0nz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FieldProgrammable","can_mod_post":false,"send_replies":true,"parent_id":"t1_n33wtwz","score":1,"author_fullname":"t2_moet0t","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"\\"Little\\" is relative, especially compared to a task like training which needs massive intercard bandwidth. Also that quote applies to a classic pipelined mode where data passes serially from one card to the next. In a tensor parallel configuration the cards try to run in parallel requiring far more intercard communication.\\n\\nLike I said I'm not claiming that PCIE5x8 vs PCIE4x4 is going to make or break your speeds, but that's a far cry from trying to claim \\"PCIE3x1 is fine\\" with a completely different memory configuration.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n33y0nz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&amp;quot;Little&amp;quot; is relative, especially compared to a task like training which needs massive intercard bandwidth. Also that quote applies to a classic pipelined mode where data passes serially from one card to the next. In a tensor parallel configuration the cards try to run in parallel requiring far more intercard communication.&lt;/p&gt;\\n\\n&lt;p&gt;Like I said I&amp;#39;m not claiming that PCIE5x8 vs PCIE4x4 is going to make or break your speeds, but that&amp;#39;s a far cry from trying to claim &amp;quot;PCIE3x1 is fine&amp;quot; with a completely different memory configuration.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzkcg3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzkcg3/multiple_5060_tis/n33y0nz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752513477,"author_flair_text":null,"treatment_tags":[],"created_utc":1752513477,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n33wtwz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AdamDhahabi","can_mod_post":false,"send_replies":true,"parent_id":"t1_n33utgs","score":1,"author_fullname":"t2_x5lnbc2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Fair enough. I found a related comment in my notes: \\"For the default modes the pci-e speed really does not matter much for inferencing. You will see a slow down while the model is loading. In the default mode the processing happens on one card, then the next, and so on. Only one card is active at a time. There is little card to card communication.\\" [https://www.reddit.com/r/LocalLLaMA/comments/1gossnd/when\\\\_using\\\\_multi\\\\_gpu\\\\_does\\\\_the\\\\_speed\\\\_between\\\\_the/](https://www.reddit.com/r/LocalLLaMA/comments/1gossnd/when_using_multi_gpu_does_the_speed_between_the/)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n33wtwz","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Fair enough. I found a related comment in my notes: &amp;quot;For the default modes the pci-e speed really does not matter much for inferencing. You will see a slow down while the model is loading. In the default mode the processing happens on one card, then the next, and so on. Only one card is active at a time. There is little card to card communication.&amp;quot; &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1gossnd/when_using_multi_gpu_does_the_speed_between_the/\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1gossnd/when_using_multi_gpu_does_the_speed_between_the/&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzkcg3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzkcg3/multiple_5060_tis/n33wtwz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752513151,"author_flair_text":null,"treatment_tags":[],"created_utc":1752513151,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n33utgs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FieldProgrammable","can_mod_post":false,"created_utc":1752512590,"send_replies":true,"parent_id":"t1_n32r78o","score":1,"author_fullname":"t2_moet0t","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You realise that any test which uses a single GPU with all layers and cache in VRAM is a completely meaningless test for PCIE bandwidth? \\n\\nThis is basically just streaming tokens off the card one at a time. In a dual GPU or CPU offloaded scenario weights are distributed in different memory connected by the PCIE bus, to generate a single token all results have to be passed from one layer to another in a different memory before a new token is generated.\\n\\nOP asked about a dual GPU setup, the amount of data moving over the PCIE bus would be orders of magnitude higher than a single GPU scenario.In a tensor parallel configuration it would be higher again. That's mot to say that you absolutely need the full bandwidth, but that video is absolutely not representative of a multi GPU setup.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n33utgs","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You realise that any test which uses a single GPU with all layers and cache in VRAM is a completely meaningless test for PCIE bandwidth? &lt;/p&gt;\\n\\n&lt;p&gt;This is basically just streaming tokens off the card one at a time. In a dual GPU or CPU offloaded scenario weights are distributed in different memory connected by the PCIE bus, to generate a single token all results have to be passed from one layer to another in a different memory before a new token is generated.&lt;/p&gt;\\n\\n&lt;p&gt;OP asked about a dual GPU setup, the amount of data moving over the PCIE bus would be orders of magnitude higher than a single GPU scenario.In a tensor parallel configuration it would be higher again. That&amp;#39;s mot to say that you absolutely need the full bandwidth, but that video is absolutely not representative of a multi GPU setup.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzkcg3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzkcg3/multiple_5060_tis/n33utgs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752512590,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n32r78o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AdamDhahabi","can_mod_post":false,"created_utc":1752501197,"send_replies":true,"parent_id":"t3_1lzkcg3","score":1,"author_fullname":"t2_x5lnbc2","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Less PCIe lanes won't impact too much, I found a test showing a 5060 Ti on PCIe 3.0x1 vs 5.0x16. [https://www.youtube.com/watch?v=qy0FWfTknFU](https://www.youtube.com/watch?v=qy0FWfTknFU)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n32r78o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Less PCIe lanes won&amp;#39;t impact too much, I found a test showing a 5060 Ti on PCIe 3.0x1 vs 5.0x16. &lt;a href=\\"https://www.youtube.com/watch?v=qy0FWfTknFU\\"&gt;https://www.youtube.com/watch?v=qy0FWfTknFU&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzkcg3/multiple_5060_tis/n32r78o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752501197,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzkcg3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n334wyp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1752505265,"send_replies":true,"parent_id":"t1_n333fdv","score":2,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; combined I was getting 11 tok/s on qwen 32b\\n\\n4060ti is absolute shit for LLMs this is why. It has 288 Gb/s bandwidth which is ass. With 2x5060ti you'll get easy 20t/s esp. if using vllm.\\n\\nBut yes, no point in more than 2x5060ti.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n334wyp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;combined I was getting 11 tok/s on qwen 32b&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;4060ti is absolute shit for LLMs this is why. It has 288 Gb/s bandwidth which is ass. With 2x5060ti you&amp;#39;ll get easy 20t/s esp. if using vllm.&lt;/p&gt;\\n\\n&lt;p&gt;But yes, no point in more than 2x5060ti.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzkcg3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzkcg3/multiple_5060_tis/n334wyp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752505265,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n37qv40","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"snorixx","can_mod_post":false,"send_replies":true,"parent_id":"t1_n35q4td","score":1,"author_fullname":"t2_3ogjqne","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Interesting with two way worse cards I also got very mixed up performance depending on the model (all are big enough to run on two GPU’s)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n37qv40","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Interesting with two way worse cards I also got very mixed up performance depending on the model (all are big enough to run on two GPU’s)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzkcg3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzkcg3/multiple_5060_tis/n37qv40/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752559171,"author_flair_text":null,"treatment_tags":[],"created_utc":1752559171,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n35q4td","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"sixx7","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3348zu","score":1,"author_fullname":"t2_jxjl6u","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Not sure what the above person is doing, but I ran a 3090 + 5060ti together and had way better performance.  Ubuntu + vllm (tensor parallel) and I was seeing over 1000 tok/s prompt processing and generation of 30 tok/s for single prompts and over 100 tok/s for batch/multiple prompts using Qwen3-32b","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n35q4td","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not sure what the above person is doing, but I ran a 3090 + 5060ti together and had way better performance.  Ubuntu + vllm (tensor parallel) and I was seeing over 1000 tok/s prompt processing and generation of 30 tok/s for single prompts and over 100 tok/s for batch/multiple prompts using Qwen3-32b&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzkcg3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzkcg3/multiple_5060_tis/n35q4td/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752531786,"author_flair_text":null,"treatment_tags":[],"created_utc":1752531786,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3348zu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"snorixx","can_mod_post":false,"created_utc":1752505075,"send_replies":true,"parent_id":"t1_n333fdv","score":1,"author_fullname":"t2_3ogjqne","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"My focus will be more on development and gaining experience. But thanks that’s helping. My only test right now is a Tesla P4 in an x4 slot with a RTX 2070 and this runs 16B models fine on both GPUs with Ollama. But maybe I will have to invest try and document…","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3348zu","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;My focus will be more on development and gaining experience. But thanks that’s helping. My only test right now is a Tesla P4 in an x4 slot with a RTX 2070 and this runs 16B models fine on both GPUs with Ollama. But maybe I will have to invest try and document…&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzkcg3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzkcg3/multiple_5060_tis/n3348zu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752505075,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n33wvap","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FieldProgrammable","can_mod_post":false,"created_utc":1752513161,"send_replies":true,"parent_id":"t1_n333fdv","score":1,"author_fullname":"t2_moet0t","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I agree for dense model multi GPU LLM inference, but a third card could be useful for other workloads, e.g. having a third card dedicated to hosting a diffusion model, or in a coding scenario a second smaller lower latency model suitable for FIM tab autocomplete (e.g. the smaller Qwen2.5 coders).","edited":1752513629,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n33wvap","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I agree for dense model multi GPU LLM inference, but a third card could be useful for other workloads, e.g. having a third card dedicated to hosting a diffusion model, or in a coding scenario a second smaller lower latency model suitable for FIM tab autocomplete (e.g. the smaller Qwen2.5 coders).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzkcg3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzkcg3/multiple_5060_tis/n33wvap/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752513161,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n34i84p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"snorixx","can_mod_post":false,"send_replies":true,"parent_id":"t1_n340dzg","score":1,"author_fullname":"t2_3ogjqne","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thanks. I will consider that. I would buy a RTX PRO Card 2000 or 4000 but the Blackwell ones are not available yet to buy mate in 1-3month","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n34i84p","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks. I will consider that. I would buy a RTX PRO Card 2000 or 4000 but the Blackwell ones are not available yet to buy mate in 1-3month&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzkcg3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzkcg3/multiple_5060_tis/n34i84p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752519050,"author_flair_text":null,"treatment_tags":[],"created_utc":1752519050,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n340dzg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Excellent_Produce146","can_mod_post":false,"send_replies":true,"parent_id":"t1_n33zuaf","score":1,"author_fullname":"t2_lrfwmbqcr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"FTR - before buying (now) overpriced RTX 4060 Ti - I would get 2x 5060 Ti instead. Was just curious what is used in the backend.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n340dzg","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;FTR - before buying (now) overpriced RTX 4060 Ti - I would get 2x 5060 Ti instead. Was just curious what is used in the backend.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzkcg3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzkcg3/multiple_5060_tis/n340dzg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752514117,"author_flair_text":null,"treatment_tags":[],"created_utc":1752514117,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n33zuaf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Excellent_Produce146","can_mod_post":false,"created_utc":1752513974,"send_replies":true,"parent_id":"t1_n333fdv","score":1,"author_fullname":"t2_lrfwmbqcr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Which quant/inference server did you use? With vLLM Qwen/Qwen3-32B-AWQ I get   \\n  \\n   Avg generation throughput: 23.4 tokens/s (Cherry Studio says 20 t/s)\\n\\nout of my test system with 2x 4060 Ti. Using v0.9.2 (container version) with \\"--model Qwen/Qwen3-32B-AWQ --tensor-parallel-size 2 --kv-cache-dtype fp8 --max-model-len 24576 --gpu-memory-utilization 0.98\\" and VLLM\\\\_ATTENTION\\\\_BACKEND=FLASHINFER.  \\n  \\nStill in service for tests, because the support for the previous generation is still better than the Blackwell cards at least for vLLM. Blackwell still needs some love:\\n\\n[https://github.com/vllm-project/vllm/issues/20605](https://github.com/vllm-project/vllm/issues/20605)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n33zuaf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Which quant/inference server did you use? With vLLM Qwen/Qwen3-32B-AWQ I get   &lt;/p&gt;\\n\\n&lt;p&gt;Avg generation throughput: 23.4 tokens/s (Cherry Studio says 20 t/s)&lt;/p&gt;\\n\\n&lt;p&gt;out of my test system with 2x 4060 Ti. Using v0.9.2 (container version) with &amp;quot;--model Qwen/Qwen3-32B-AWQ --tensor-parallel-size 2 --kv-cache-dtype fp8 --max-model-len 24576 --gpu-memory-utilization 0.98&amp;quot; and VLLM_ATTENTION_BACKEND=FLASHINFER.  &lt;/p&gt;\\n\\n&lt;p&gt;Still in service for tests, because the support for the previous generation is still better than the Blackwell cards at least for vLLM. Blackwell still needs some love:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/vllm-project/vllm/issues/20605\\"&gt;https://github.com/vllm-project/vllm/issues/20605&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzkcg3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzkcg3/multiple_5060_tis/n33zuaf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752513974,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n333fdv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Deep-Technician-8568","can_mod_post":false,"created_utc":1752504841,"send_replies":true,"parent_id":"t3_1lzkcg3","score":1,"author_fullname":"t2_afjchrab","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you are running dense models, i don't really recommend getting more than 2x 5060 ti. With my testing of 1x 4060 ti and 1x 5060 ti combined I was getting 11 tok/s on qwen 32b. To me i dont really consider anything under 20 tok/s to be usable (especially thinking models). I also dont think 2x 5060 ti will even get to 20 tok/s. So, for dense models, really don't see the point of getting more than 2x 5060 ti.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n333fdv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you are running dense models, i don&amp;#39;t really recommend getting more than 2x 5060 ti. With my testing of 1x 4060 ti and 1x 5060 ti combined I was getting 11 tok/s on qwen 32b. To me i dont really consider anything under 20 tok/s to be usable (especially thinking models). I also dont think 2x 5060 ti will even get to 20 tok/s. So, for dense models, really don&amp;#39;t see the point of getting more than 2x 5060 ti.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzkcg3/multiple_5060_tis/n333fdv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752504841,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzkcg3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(l,{data:t});export{n as default};
