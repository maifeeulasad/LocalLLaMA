import{j as e}from"./index-BlGsFJYy.js";import{R as l}from"./RedditPostRenderer-B6uvq_Zl.js";import"./index-DDvVtNwD.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I've been using [NyxKrage's VRAM Calculator](https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator) for a while, but I find sometimes I want to calculate this stuff without an internet connection or using a webpage. I also needed to calculate how much VRAM was needed for specific quants or for a lot of models. \\n\\nSo, I smacked together a cpp version of the calculator in a few hours. \\n\\nThere are two modes:\\n\\nCall the executable and supply all needed parameters with it as command-line arguments for JSON-formatted data perfect for workflows, or call the executable normally and input each argument manually.\\n\\nI'm planning to add functionality like calculating parameters, letting you use it without a \\\\\`config.json\\\\\`, etc. If you want anything added, add a Github Issue or feel free to fork it.\\n\\n[Link Here](https://github.com/71cj34/llmcalculator)","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Made a local C++ utility to calculate RAM needed to fit a quantized model","link_flair_richtext":[{"e":"text","t":"Resources"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m4djo6","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.9,"author_flair_background_color":null,"subreddit_type":"public","ups":33,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1e392klobf","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Resources","can_mod_post":false,"score":33,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1752977752,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve been using &lt;a href=\\"https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator\\"&gt;NyxKrage&amp;#39;s VRAM Calculator&lt;/a&gt; for a while, but I find sometimes I want to calculate this stuff without an internet connection or using a webpage. I also needed to calculate how much VRAM was needed for specific quants or for a lot of models. &lt;/p&gt;\\n\\n&lt;p&gt;So, I smacked together a cpp version of the calculator in a few hours. &lt;/p&gt;\\n\\n&lt;p&gt;There are two modes:&lt;/p&gt;\\n\\n&lt;p&gt;Call the executable and supply all needed parameters with it as command-line arguments for JSON-formatted data perfect for workflows, or call the executable normally and input each argument manually.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m planning to add functionality like calculating parameters, letting you use it without a \`config.json\`, etc. If you want anything added, add a Github Issue or feel free to fork it.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/71cj34/llmcalculator\\"&gt;Link Here&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/CVC3UCXWBZbCWfBKaMAbb_pHJ3k1bjsiW1It_67lGVY.png?auto=webp&amp;s=3954f306637b48462dfbce5f7db7c79685858088","width":1200,"height":648},"resolutions":[{"url":"https://external-preview.redd.it/CVC3UCXWBZbCWfBKaMAbb_pHJ3k1bjsiW1It_67lGVY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=dab6a28dd0587077a356703d53799748a71b3d4c","width":108,"height":58},{"url":"https://external-preview.redd.it/CVC3UCXWBZbCWfBKaMAbb_pHJ3k1bjsiW1It_67lGVY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0d78410b2ff71ed283d47b779a9b2110015782ee","width":216,"height":116},{"url":"https://external-preview.redd.it/CVC3UCXWBZbCWfBKaMAbb_pHJ3k1bjsiW1It_67lGVY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0ef9b2c4152dc5a850df1c49798c53736aa5db4a","width":320,"height":172},{"url":"https://external-preview.redd.it/CVC3UCXWBZbCWfBKaMAbb_pHJ3k1bjsiW1It_67lGVY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7d0e9c1ae5fa992910c2ee3bd14f2d87ae72e5cd","width":640,"height":345},{"url":"https://external-preview.redd.it/CVC3UCXWBZbCWfBKaMAbb_pHJ3k1bjsiW1It_67lGVY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e15e9744e01b7fabc83b056fd7bef010c2860e32","width":960,"height":518},{"url":"https://external-preview.redd.it/CVC3UCXWBZbCWfBKaMAbb_pHJ3k1bjsiW1It_67lGVY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b4343d18b58d772009825227390e2d2072d4bd48","width":1080,"height":583}],"variants":{},"id":"CVC3UCXWBZbCWfBKaMAbb_pHJ3k1bjsiW1It_67lGVY"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#ccac2b","id":"1m4djo6","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"philetairus_socius","discussion_type":null,"num_comments":10,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/","subreddit_subscribers":501752,"created_utc":1752977752,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n44z3p2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Capable-Ad-7494","can_mod_post":false,"send_replies":true,"parent_id":"t1_n44n1hj","score":2,"author_fullname":"t2_9so78ol2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"That sounds like it wouldn’t scale super well between different model architectures at longer context lengths though. Great for a ballpark though","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44z3p2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That sounds like it wouldn’t scale super well between different model architectures at longer context lengths though. Great for a ballpark though&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4djo6","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/n44z3p2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753002517,"author_flair_text":null,"treatment_tags":[],"created_utc":1753002517,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n44n1hj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"triynizzles1","can_mod_post":false,"send_replies":true,"parent_id":"t1_n44kw3t","score":3,"author_fullname":"t2_zr0g49ixt","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I entered the example typed in to the vram calculator listed in the post and it gave me 19.5gb with 6k context and my napkin math was 19gb. No app needed.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n44n1hj","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I entered the example typed in to the vram calculator listed in the post and it gave me 19.5gb with 6k context and my napkin math was 19gb. No app needed.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4djo6","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/n44n1hj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752995553,"author_flair_text":null,"treatment_tags":[],"created_utc":1752995553,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n44kw3t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MelodicRecognition7","can_mod_post":false,"created_utc":1752994374,"send_replies":true,"parent_id":"t1_n4402q5","score":3,"author_fullname":"t2_1eex9ug5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"ew man, do not undercomplicate things, that's oppression!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44kw3t","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;ew man, do not undercomplicate things, that&amp;#39;s oppression!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4djo6","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/n44kw3t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752994374,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n44yv16","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"tmvr","can_mod_post":false,"created_utc":1753002373,"send_replies":true,"parent_id":"t1_n4402q5","score":1,"author_fullname":"t2_11qlhv","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Roughly this, but the Q4 is a bit more, I go with 5/8 there as it is closer to the real 4.85 bpw of \\"mainstream\\" accepted quality/speed compromise of Q4\\\\_K\\\\_M.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44yv16","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Roughly this, but the Q4 is a bit more, I go with 5/8 there as it is closer to the real 4.85 bpw of &amp;quot;mainstream&amp;quot; accepted quality/speed compromise of Q4_K_M.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4djo6","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/n44yv16/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753002373,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4402q5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"triynizzles1","can_mod_post":false,"created_utc":1752984059,"send_replies":true,"parent_id":"t3_1m4djo6","score":18,"author_fullname":"t2_zr0g49ixt","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Fp16: Parameter size * 2 = GB of memory needed\\nQ8: parameter size * 1 = GB of memory needed.\\nQ4: parameter size * 0.5 = GB of memory needed\\n\\nAnd then plus 20% or so for useable context window.\\n\\nEx: 32b Q4 will take up 16gb and need another 3gb for a few thousand tokens context window.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4402q5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Fp16: Parameter size * 2 = GB of memory needed\\nQ8: parameter size * 1 = GB of memory needed.\\nQ4: parameter size * 0.5 = GB of memory needed&lt;/p&gt;\\n\\n&lt;p&gt;And then plus 20% or so for useable context window.&lt;/p&gt;\\n\\n&lt;p&gt;Ex: 32b Q4 will take up 16gb and need another 3gb for a few thousand tokens context window.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/n4402q5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752984059,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4djo6","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":18}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n43tthy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DeProgrammer99","can_mod_post":false,"created_utc":1752981374,"send_replies":true,"parent_id":"t3_1m4djo6","score":5,"author_fullname":"t2_w4j8t","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Adding to this... I threw this together at some point for viewing GGUF metadata, but I recently also made it calculate KV cache per token. It has both C# and JavaScript versions. Doesn't account for the RAM for storing intermediate values, though.\\nhttps://github.com/dpmm99/GGUFDump\\n\\nZero dependencies other than either a browser or .NET runtime. Just drop a GGUF on the EXE or into the web page.","edited":1752981624,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n43tthy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Adding to this... I threw this together at some point for viewing GGUF metadata, but I recently also made it calculate KV cache per token. It has both C# and JavaScript versions. Doesn&amp;#39;t account for the RAM for storing intermediate values, though.\\n&lt;a href=\\"https://github.com/dpmm99/GGUFDump\\"&gt;https://github.com/dpmm99/GGUFDump&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Zero dependencies other than either a browser or .NET runtime. Just drop a GGUF on the EXE or into the web page.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/n43tthy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752981374,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4djo6","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n44usvq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"admajic","can_mod_post":false,"created_utc":1752999971,"send_replies":true,"parent_id":"t3_1m4djo6","score":2,"author_fullname":"t2_60b9farf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Can you add k and v cache then I can try 128k context. Says a 14gb model need 300gb ram without k cache cache","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44usvq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Can you add k and v cache then I can try 128k context. Says a 14gb model need 300gb ram without k cache cache&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/n44usvq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752999971,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4djo6","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n44fv98","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"NickCanCode","can_mod_post":false,"created_utc":1752991652,"send_replies":true,"parent_id":"t1_n44a47l","score":3,"author_fullname":"t2_srx5k6vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"EXE = An EXEcutable, a compiled application ready to run on WIndows. Unlike webpage hosted on a web server and safe guarded by your browser, you run it directly on your PC and it can contains malicious code, virus, ransomware so it is recommended to only run 3rd party EXE from trusted source. Even if a exe is from an open source project, the compiled exe can still be unsafe and can be totally different from the source. That's why application distributed in Linux is mostly delivered by source code and compiled directly on end user's machine to reduce the risk.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44fv98","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;EXE = An EXEcutable, a compiled application ready to run on WIndows. Unlike webpage hosted on a web server and safe guarded by your browser, you run it directly on your PC and it can contains malicious code, virus, ransomware so it is recommended to only run 3rd party EXE from trusted source. Even if a exe is from an open source project, the compiled exe can still be unsafe and can be totally different from the source. That&amp;#39;s why application distributed in Linux is mostly delivered by source code and compiled directly on end user&amp;#39;s machine to reduce the risk.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4djo6","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/n44fv98/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752991652,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n44a47l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"tronathan","can_mod_post":false,"created_utc":1752988733,"send_replies":true,"parent_id":"t3_1m4djo6","score":-3,"author_fullname":"t2_3aqn7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What’s an EXE?\\n\\nSrsly though, having the equivalent of Mac’s “Get Info…” as a command line utility with llama cpp would be handy.  Maybe even integrated? Kinda scope-creepy.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44a47l","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What’s an EXE?&lt;/p&gt;\\n\\n&lt;p&gt;Srsly though, having the equivalent of Mac’s “Get Info…” as a command line utility with llama cpp would be handy.  Maybe even integrated? Kinda scope-creepy.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/n44a47l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752988733,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4djo6","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-3}}],"before":null}}]`),n=()=>e.jsx(l,{data:a});export{n as default};
