import{j as e}from"./index-M4edQi1P.js";import{R as l}from"./RedditPostRenderer-CESBGIIy.js";import"./index-DFpL1mt4.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hi r/LocalLLaMA does anyone have a good tensor override for hunyuan a13b? I get around 12 t/s on ddr4 3600 and with different offloads to a 3090 I got to 21 t/s. This is the command I'm using just in case it's useful for someone:\\n\\n\`./llama-server -m /mnt/llamas/ggufs/tencent_Hunyuan-A13B-Instruct-Q4_K_M.gguf  -fa -ngl 99 -c 8192 --jinja --temp 0.7 --top-k 20 --top-p 0.8 --repeat-penalty 1.05 -ot \\"blk\\\\.[1-9]\\\\.ffn.*=CPU\\" -ot \\"blk\\\\.1[6-9]\\\\.ffn.*=CPU\\"\`\\n\\nI took it from one of the suggested ot for qwen235, I also tried some ot for llama4-scout but they were slower","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Hunyuan A13B tensor override","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lvirqs","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.9,"author_flair_background_color":null,"subreddit_type":"public","ups":15,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_2xii9ad6","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":15,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752067609,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi &lt;a href=\\"/r/LocalLLaMA\\"&gt;r/LocalLLaMA&lt;/a&gt; does anyone have a good tensor override for hunyuan a13b? I get around 12 t/s on ddr4 3600 and with different offloads to a 3090 I got to 21 t/s. This is the command I&amp;#39;m using just in case it&amp;#39;s useful for someone:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;./llama-server -m /mnt/llamas/ggufs/tencent_Hunyuan-A13B-Instruct-Q4_K_M.gguf  -fa -ngl 99 -c 8192 --jinja --temp 0.7 --top-k 20 --top-p 0.8 --repeat-penalty 1.05 -ot &amp;quot;blk\\\\.[1-9]\\\\.ffn.*=CPU&amp;quot; -ot &amp;quot;blk\\\\.1[6-9]\\\\.ffn.*=CPU&amp;quot;&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;I took it from one of the suggested ot for qwen235, I also tried some ot for llama4-scout but they were slower&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lvirqs","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"marderbot13","discussion_type":null,"num_comments":7,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lvirqs/hunyuan_a13b_tensor_override/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lvirqs/hunyuan_a13b_tensor_override/","subreddit_subscribers":497025,"created_utc":1752067609,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n27dgvd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"GreenPastures2845","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2733co","score":4,"author_fullname":"t2_1eec9087px","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Using \\\\d+ doesn't let you afterwards remove layers until you maximize VRAM usage; parent's workflow is simple and effective.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n27dgvd","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Using \\\\d+ doesn&amp;#39;t let you afterwards remove layers until you maximize VRAM usage; parent&amp;#39;s workflow is simple and effective.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvirqs","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvirqs/hunyuan_a13b_tensor_override/n27dgvd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752079565,"author_flair_text":null,"treatment_tags":[],"created_utc":1752079565,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n2733co","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DeProgrammer99","can_mod_post":false,"created_utc":1752076708,"send_replies":true,"parent_id":"t1_n26optj","score":2,"author_fullname":"t2_w4j8t","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That's fine, but if you're going to list them all anyway, you may as well just use .* instead. Or \\\\d+ also works. I only use a pattern like 1[4-9] to squeeze as many experts as I can into VRAM, myself.\\n\\nOptimally, if we knew the most commonly used experts, we'd put those in VRAM, but that could work even better as a runtime feature instead of a command-line input.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2733co","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s fine, but if you&amp;#39;re going to list them all anyway, you may as well just use .* instead. Or \\\\d+ also works. I only use a pattern like 1[4-9] to squeeze as many experts as I can into VRAM, myself.&lt;/p&gt;\\n\\n&lt;p&gt;Optimally, if we knew the most commonly used experts, we&amp;#39;d put those in VRAM, but that could work even better as a runtime feature instead of a command-line input.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvirqs","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvirqs/hunyuan_a13b_tensor_override/n2733co/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752076708,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n26optj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"YearZero","can_mod_post":false,"created_utc":1752072757,"send_replies":true,"parent_id":"t3_1lvirqs","score":14,"author_fullname":"t2_4kpsn","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"To spare some brain cells I tend to simplify it by listing out all the numbers:\\n\\n\\\\--override-tensor \\"blk\\\\\\\\.(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27|28|29|30|31)\\\\\\\\.ffn\\\\_.\\\\*\\\\_exps.=CPU\\"\\n\\nThis is going to offload all the down/up/gate tensors to the CPU for Hunyuan. For Qwen 30b you gotta go to \\"47\\" to do the same as it has blocks 0 thru 47. If I have VRAM left over, I start removing some numbers to try to keep some of them on the GPU, and I keep removing numbers from the end until I use up as much VRAM as I can.\\n\\nIt's not as short as the more clever regex out there, but it keeps things simple for me. I guess if we start getting models with hundreds of tensors I'll probably start using ranges in the regex.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n26optj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;To spare some brain cells I tend to simplify it by listing out all the numbers:&lt;/p&gt;\\n\\n&lt;p&gt;--override-tensor &amp;quot;blk\\\\.(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27|28|29|30|31)\\\\.ffn_.*_exps.=CPU&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;This is going to offload all the down/up/gate tensors to the CPU for Hunyuan. For Qwen 30b you gotta go to &amp;quot;47&amp;quot; to do the same as it has blocks 0 thru 47. If I have VRAM left over, I start removing some numbers to try to keep some of them on the GPU, and I keep removing numbers from the end until I use up as much VRAM as I can.&lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s not as short as the more clever regex out there, but it keeps things simple for me. I guess if we start getting models with hundreds of tensors I&amp;#39;ll probably start using ranges in the regex.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvirqs/hunyuan_a13b_tensor_override/n26optj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752072757,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvirqs","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":14}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n26mrqc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Agreeable-Prompt-666","can_mod_post":false,"created_utc":1752072212,"send_replies":true,"parent_id":"t3_1lvirqs","score":2,"author_fullname":"t2_1l3z4stvkq","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Awesome thanks for sharing this","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n26mrqc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Awesome thanks for sharing this&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvirqs/hunyuan_a13b_tensor_override/n26mrqc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752072212,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvirqs","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n27zkk0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jacek2023","can_mod_post":false,"created_utc":1752085532,"send_replies":true,"parent_id":"t3_1lvirqs","score":2,"author_fullname":"t2_vqgbql9w","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The point of the regex is to move some tensors to CPU. You can't just copy one regex to new environment. It depends what is the model and how big is your VRAM.\\n\\nHOWTO:\\n\\n1. run your command\\n2. run nvidia-smi\\n3. check VRAM usage, is it close to full?\\n4. if not, modify regex to offload smaller number of layers\\n5. when llama-server refuses to run - increase number of layers\\n6. if you are on Windows, make sure your driver crashes on VRAM overflow, otherwise it will use RAM against your will and it will break your optimization","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n27zkk0","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The point of the regex is to move some tensors to CPU. You can&amp;#39;t just copy one regex to new environment. It depends what is the model and how big is your VRAM.&lt;/p&gt;\\n\\n&lt;p&gt;HOWTO:&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;run your command&lt;/li&gt;\\n&lt;li&gt;run nvidia-smi&lt;/li&gt;\\n&lt;li&gt;check VRAM usage, is it close to full?&lt;/li&gt;\\n&lt;li&gt;if not, modify regex to offload smaller number of layers&lt;/li&gt;\\n&lt;li&gt;when llama-server refuses to run - increase number of layers&lt;/li&gt;\\n&lt;li&gt;if you are on Windows, make sure your driver crashes on VRAM overflow, otherwise it will use RAM against your will and it will break your optimization&lt;/li&gt;\\n&lt;/ol&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvirqs/hunyuan_a13b_tensor_override/n27zkk0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752085532,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lvirqs","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2885nq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"random-tomato","can_mod_post":false,"created_utc":1752087914,"send_replies":true,"parent_id":"t3_1lvirqs","score":1,"author_fullname":"t2_fmd6oq5v6","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I have a 5090 + 60GB of DDR5 ram and I get 21.8 tok/sec\\n\\n    llama-server -m models/Hunyuan-A13B-Instruct-UD-Q4_K_XL.gguf -ngl 99 -c 16384 --override-tensor \\"([0-5]).ffn_.*_exps.=CUDA0\\" --override-tensor \\"([6-9]|[1-9][0-9]+).ffn_.*_exps.=CPU\\" --host 0.0.0.0 --port 8181 -ub 1 --api-key key --jinja --temp 0.7 --top-k 20 --top-p 0.6 --repeat-penalty 1.05","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2885nq","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have a 5090 + 60GB of DDR5 ram and I get 21.8 tok/sec&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;llama-server -m models/Hunyuan-A13B-Instruct-UD-Q4_K_XL.gguf -ngl 99 -c 16384 --override-tensor &amp;quot;([0-5]).ffn_.*_exps.=CUDA0&amp;quot; --override-tensor &amp;quot;([6-9]|[1-9][0-9]+).ffn_.*_exps.=CPU&amp;quot; --host 0.0.0.0 --port 8181 -ub 1 --api-key key --jinja --temp 0.7 --top-k 20 --top-p 0.6 --repeat-penalty 1.05\\n&lt;/code&gt;&lt;/pre&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvirqs/hunyuan_a13b_tensor_override/n2885nq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752087914,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lvirqs","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n28kwa5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mrwang89","can_mod_post":false,"created_utc":1752091442,"send_replies":true,"parent_id":"t3_1lvirqs","score":1,"author_fullname":"t2_sz57zt2","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"how dou get 12t/s on 3090? i only get 5t/s on my 3090 what am i doing wrong?? i have ddr5 btw! how many layers are you offloading?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n28kwa5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;how dou get 12t/s on 3090? i only get 5t/s on my 3090 what am i doing wrong?? i have ddr5 btw! how many layers are you offloading?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvirqs/hunyuan_a13b_tensor_override/n28kwa5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752091442,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvirqs","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(l,{data:t});export{n as default};
