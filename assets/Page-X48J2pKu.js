import{j as e}from"./index-DLSqWzaI.js";import{R as t}from"./RedditPostRenderer-CysRo2D_.js";import"./index-COXiL3Lo.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"**Context of my project idea:**\\n\\nI have been doing some research on self hosting LLMs and, of course, quickly came to the realisation on how complicated it seems to be for a solo developer to pay for the rental costs of an enterprise-grade GPU and run a SOTA open-source model like Kimi K2 32B or Qwen 32B. Renting per hour quickly can rack up insane costs. And trying to pay \\"per request\\" is pretty much unfeasible without factoring in excessive cold startup times.\\n\\nSo it seems that the most commonly chose option is to try and run a much smaller model on ollama; and even then you need a pretty powerful setup to handle it. Otherwise, stick to the usual closed-source commercial models.\\n\\n**An alternative?**\\n\\nAll this got me thinking. Of course, we already have open-source communities like Hugging Face for sharing model weights, transformers etc. What about though a **community-owned live inference server** where the community has a say in what model, infrastructure, stack, data etc we use and share the costs via transparent API pricing?\\n\\nWe, the community, would set up a whole environment, rent the GPU, prepare data for fine-tuning / RL, and even implement some experimental setups like using the new MemOS or other research paths. Of course it would be helpful if the community was also of similar objective, like development / coding focused.\\n\\nI imagine there is a lot to cogitate here but I am open to discussing and brainstorming together the various aspects and obstacles here.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Project Idea: A REAL Community-driven LLM Stack","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lznxy5","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.6,"author_flair_background_color":null,"subreddit_type":"public","ups":1,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_19mrnrt357","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":1,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1752503693,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752503479,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;strong&gt;Context of my project idea:&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;I have been doing some research on self hosting LLMs and, of course, quickly came to the realisation on how complicated it seems to be for a solo developer to pay for the rental costs of an enterprise-grade GPU and run a SOTA open-source model like Kimi K2 32B or Qwen 32B. Renting per hour quickly can rack up insane costs. And trying to pay &amp;quot;per request&amp;quot; is pretty much unfeasible without factoring in excessive cold startup times.&lt;/p&gt;\\n\\n&lt;p&gt;So it seems that the most commonly chose option is to try and run a much smaller model on ollama; and even then you need a pretty powerful setup to handle it. Otherwise, stick to the usual closed-source commercial models.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;An alternative?&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;All this got me thinking. Of course, we already have open-source communities like Hugging Face for sharing model weights, transformers etc. What about though a &lt;strong&gt;community-owned live inference server&lt;/strong&gt; where the community has a say in what model, infrastructure, stack, data etc we use and share the costs via transparent API pricing?&lt;/p&gt;\\n\\n&lt;p&gt;We, the community, would set up a whole environment, rent the GPU, prepare data for fine-tuning / RL, and even implement some experimental setups like using the new MemOS or other research paths. Of course it would be helpful if the community was also of similar objective, like development / coding focused.&lt;/p&gt;\\n\\n&lt;p&gt;I imagine there is a lot to cogitate here but I am open to discussing and brainstorming together the various aspects and obstacles here.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lznxy5","is_robot_indexable":true,"num_duplicates":1,"report_reasons":null,"author":"Budget_Map_3333","discussion_type":null,"num_comments":7,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lznxy5/project_idea_a_real_communitydriven_llm_stack/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lznxy5/project_idea_a_real_communitydriven_llm_stack/","subreddit_subscribers":499295,"created_utc":1752503479,"num_crossposts":1,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"c07aa42e-51fe-11f0-afcc-462aad931709","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"c07aa42e-51fe-11f0-afcc-462aad931709","distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n34u57g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"send_replies":true,"parent_id":"t1_n33671o","score":2,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It sounds fun ngl, maybe start small and evolve it incrementally. I'm going to post some fine-tuning benchmarks here later this week, let's see if we have enough fine-tuners here for a critical mass.","edited":false,"author_flair_css_class":null,"name":"t1_n34u57g","is_submitter":false,"downs":0,"author_flair_richtext":[{"a":":X:","u":"https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X","e":"emoji"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It sounds fun ngl, maybe start small and evolve it incrementally. I&amp;#39;m going to post some fine-tuning benchmarks here later this week, let&amp;#39;s see if we have enough fine-tuners here for a critical mass.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lznxy5","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"dark","treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lznxy5/project_idea_a_real_communitydriven_llm_stack/n34u57g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752522513,"author_flair_text":":X:","collapsed":false,"created_utc":1752522513,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":"transparent","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n33671o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Budget_Map_3333","can_mod_post":false,"send_replies":true,"parent_id":"t1_n334f3l","score":2,"author_fullname":"t2_19mrnrt357","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I totally understand. IMO getting the community to rally around choosing a model fit for purpose (like coding for example) is part of the fun. We could even begin by creating our own benchmarks / tests and selection process for deciding which model is best suited for the specified domain. The idea really is not just to split the GPU cost, but for the stack to evolve as a community-driven AI, which means the community really would need to have a say  across all layers: from data selection to fine-tuning to additional stack like LoRa adapters, memory routers, agentic tools and anything else we decide to try!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n33671o","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I totally understand. IMO getting the community to rally around choosing a model fit for purpose (like coding for example) is part of the fun. We could even begin by creating our own benchmarks / tests and selection process for deciding which model is best suited for the specified domain. The idea really is not just to split the GPU cost, but for the stack to evolve as a community-driven AI, which means the community really would need to have a say  across all layers: from data selection to fine-tuning to additional stack like LoRa adapters, memory routers, agentic tools and anything else we decide to try!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lznxy5","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lznxy5/project_idea_a_real_communitydriven_llm_stack/n33671o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752505636,"author_flair_text":null,"treatment_tags":[],"created_utc":1752505636,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n334f3l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"send_replies":true,"parent_id":"t1_n331zjc","score":2,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"\\\\&gt; what model\\n\\nMinor nitpick but there is no way I'm going to let \\"the community\\" choose my model for me. The use cases vary so wildly. I still profitably use Llama 3.1 8B and 3.2B as my workhorse models. The community will make you believe DeepSeek or Qwen are the way to go, but when I benchmark them on some of my fine-tuning workloads they perform horribly. They're only good for zero-shot.\\n\\nYou already mentioned this, but maybe restrict to a subset of the community with one use case (e.g., coding).\\n\\nI still struggle to see the return on investment over just using Anthropic, Google, or OpenAI. But the idea is very cool in general.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n334f3l","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"a":":X:","u":"https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X","e":"emoji"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&amp;gt; what model&lt;/p&gt;\\n\\n&lt;p&gt;Minor nitpick but there is no way I&amp;#39;m going to let &amp;quot;the community&amp;quot; choose my model for me. The use cases vary so wildly. I still profitably use Llama 3.1 8B and 3.2B as my workhorse models. The community will make you believe DeepSeek or Qwen are the way to go, but when I benchmark them on some of my fine-tuning workloads they perform horribly. They&amp;#39;re only good for zero-shot.&lt;/p&gt;\\n\\n&lt;p&gt;You already mentioned this, but maybe restrict to a subset of the community with one use case (e.g., coding).&lt;/p&gt;\\n\\n&lt;p&gt;I still struggle to see the return on investment over just using Anthropic, Google, or OpenAI. But the idea is very cool in general.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lznxy5","unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lznxy5/project_idea_a_real_communitydriven_llm_stack/n334f3l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752505123,"author_flair_text":":X:","treatment_tags":[],"created_utc":1752505123,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"transparent","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3377za","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Budget_Map_3333","can_mod_post":false,"send_replies":true,"parent_id":"t1_n335fhc","score":1,"author_fullname":"t2_19mrnrt357","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Nice, I checked out the distributed LLM link. Sounds similar in some ways but I think distributed compute introduces its own issues to overcome for such a high RAM-intensive operation. I think renting a decent cloud GPU at least gets us halfway there, then the hard part is like another poster mentioned: getting this configured for the community in a non-profit way that also pays the cloud bills.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3377za","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nice, I checked out the distributed LLM link. Sounds similar in some ways but I think distributed compute introduces its own issues to overcome for such a high RAM-intensive operation. I think renting a decent cloud GPU at least gets us halfway there, then the hard part is like another poster mentioned: getting this configured for the community in a non-profit way that also pays the cloud bills.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lznxy5","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lznxy5/project_idea_a_real_communitydriven_llm_stack/n3377za/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752505933,"author_flair_text":null,"treatment_tags":[],"created_utc":1752505933,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n335fhc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Strange_Test7665","can_mod_post":false,"send_replies":true,"parent_id":"t1_n331zjc","score":2,"author_fullname":"t2_t0zjq9mi","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I deff think you'd find people, myself included, who would join that.  So spin up a basic server to handle users and API keys.  Use something like [jarvislabs](https://jarvislabs.ai/) to rent GPU time and then just bill people per use?  essentially a non-profit LLM api. Also your post made me google distributed llm and there are deff folks working on it like ([this](https://www.cnx-software.com/2025/02/18/exo-software-a-distributed-llm-solution-running-on-a-cluster-of-computers-smartphones-or-sbcs/))","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n335fhc","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I deff think you&amp;#39;d find people, myself included, who would join that.  So spin up a basic server to handle users and API keys.  Use something like &lt;a href=\\"https://jarvislabs.ai/\\"&gt;jarvislabs&lt;/a&gt; to rent GPU time and then just bill people per use?  essentially a non-profit LLM api. Also your post made me google distributed llm and there are deff folks working on it like (&lt;a href=\\"https://www.cnx-software.com/2025/02/18/exo-software-a-distributed-llm-solution-running-on-a-cluster-of-computers-smartphones-or-sbcs/\\"&gt;this&lt;/a&gt;)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lznxy5","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lznxy5/project_idea_a_real_communitydriven_llm_stack/n335fhc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752505413,"author_flair_text":null,"treatment_tags":[],"created_utc":1752505413,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n331zjc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Budget_Map_3333","can_mod_post":false,"created_utc":1752504427,"send_replies":true,"parent_id":"t1_n3314jt","score":1,"author_fullname":"t2_19mrnrt357","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I was actually thinking of a simpler model to start, actually renting GPUs per hour and calculating usage, tokens p/second and doing a regular API billing per tokens like commercial platforms are doing. The **big** difference would be transparent pricing, community access (at least read-permissions) to the whole stack, console, billing, usage etc.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n331zjc","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I was actually thinking of a simpler model to start, actually renting GPUs per hour and calculating usage, tokens p/second and doing a regular API billing per tokens like commercial platforms are doing. The &lt;strong&gt;big&lt;/strong&gt; difference would be transparent pricing, community access (at least read-permissions) to the whole stack, console, billing, usage etc.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lznxy5","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lznxy5/project_idea_a_real_communitydriven_llm_stack/n331zjc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752504427,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3314jt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Strange_Test7665","can_mod_post":false,"created_utc":1752504180,"send_replies":true,"parent_id":"t3_1lznxy5","score":3,"author_fullname":"t2_t0zjq9mi","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Let's assume the community kick starts $100k and buys a bunch of servers and that they just 'run' so only thing needed to do is remote open/community operation.  Load up a SOTA model that is now on the community api.  It's going to use electricity, plus overhead like rent for the space, repair, etc. so there is some base cost, plus the investment.  when you factor everything in my question is are API calls really marked up that much? If they are then I think this is a good idea.  If they are not then I think it would be hard to get legs from an economic argument standpoint.  It would be about control ownership argument not cost.  You'd still need the community to pay access costs everything would just be open.\\n\\nIf there was a way to distribute work across personal machines like a the old [SETI Screen saver](https://en.wikipedia.org/wiki/SETI@home) that would be very awesome but I don't know of anyone doing distributed LLM code.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3314jt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Let&amp;#39;s assume the community kick starts $100k and buys a bunch of servers and that they just &amp;#39;run&amp;#39; so only thing needed to do is remote open/community operation.  Load up a SOTA model that is now on the community api.  It&amp;#39;s going to use electricity, plus overhead like rent for the space, repair, etc. so there is some base cost, plus the investment.  when you factor everything in my question is are API calls really marked up that much? If they are then I think this is a good idea.  If they are not then I think it would be hard to get legs from an economic argument standpoint.  It would be about control ownership argument not cost.  You&amp;#39;d still need the community to pay access costs everything would just be open.&lt;/p&gt;\\n\\n&lt;p&gt;If there was a way to distribute work across personal machines like a the old &lt;a href=\\"https://en.wikipedia.org/wiki/SETI@home\\"&gt;SETI Screen saver&lt;/a&gt; that would be very awesome but I don&amp;#39;t know of anyone doing distributed LLM code.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lznxy5/project_idea_a_real_communitydriven_llm_stack/n3314jt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752504180,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lznxy5","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}}]`),r=()=>e.jsx(t,{data:l});export{r as default};
