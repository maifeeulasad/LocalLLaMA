import{j as e}from"./index-DQXiEb7D.js";import{R as l}from"./RedditPostRenderer-BjndLgq8.js";import"./index-B-ILyjT1.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"**Testing method** \\n\\n* For each question, four instances of the same model were run in parallel (i.e., best-of-4). If any of them successfully solved the question, the most optimized solution among them was selected.\\n* If none of the four produced a solution within the maximum context length, an additional four instances were run, making it a best-of-8 scenario. This second batch was only needed in 2 or 3 cases, where the first four failed but the next four succeeded.\\n* Only one question couldn't be solved by any of the eight instances due to context length limitations. This occurred with Qwen-235B, as noted in the results table.\\n* Note that quantizations are not same. It's just me, trying to find the best reasoning &amp; coding model for my setup. \\n\\n**Coloring strategy:**\\n\\n* Mark the solution green if it's accepted.\\n* Use red if it fails in the pre-test cases.\\n* Use red if it fails in the test cases (due to wrong answer or time limit) and passes less than 90% of them.\\n* Use orange if it fails in the test cases but still manages to pass over 90%.\\n\\n**A few observations:**\\n\\n* Occasionally, the generated code contains minor typos, such as a missing comma. I corrected these manually and didn’t treat them as failures, since they were limited to single character issues that clearly qualify as typos.\\n* Hunyuan fell short of my expectations.\\n* Qwen-32B and OpenCodeReasoning model both performed better than expected.\\n* The NVIDIA model tends to be overly verbose ( A LOT ), which likely explains its higher context limit of 65k tokens, compared to 32k in the other models.\\n\\n**Hardware: 2x H100**\\n\\n**Backend: vLLM (for hunyuan, use 0.9.2 and for others 0.9.1)**\\n\\nFeel free to recommend another reasoning model for me to test but it must have a vLLM compatible quantized version that fits within 160 GB.\\n\\n**Keep in mind that strong performance on LeetCode doesn't automatically reflect real world coding skills**, since everyday programming tasks faced by typical users are usually far less complex.\\n\\nAll questions are recent, with no data leakage involved. So don’t come back saying “LeetCode problems are easy for models, this test isn’t meaningful”. It's just your test questions have been seen by the model before.\\n\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Comparison of latest reasoning models on the most recent LeetCode questions (Qwen-32B vs Qwen-235B vs nvidia-OpenCodeReasoning-32B vs Hunyuan-A13B)","link_flair_richtext":[{"e":"text","t":"Resources"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":36,"top_awarded_type":null,"hide_score":false,"name":"t3_1lzhns3","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.99,"author_flair_background_color":null,"ups":129,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_slwqrxz3","secure_media":null,"is_reddit_media_domain":true,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Resources","can_mod_post":false,"score":129,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://b.thumbs.redditmedia.com/WppDZCrZ0xycGtKVlAsBndunRlo8Km7IfOuHDADfvik.jpg","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"image","content_categories":null,"is_self":false,"subreddit_type":"public","created":1752484340,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"i.redd.it","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;strong&gt;Testing method&lt;/strong&gt; &lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;For each question, four instances of the same model were run in parallel (i.e., best-of-4). If any of them successfully solved the question, the most optimized solution among them was selected.&lt;/li&gt;\\n&lt;li&gt;If none of the four produced a solution within the maximum context length, an additional four instances were run, making it a best-of-8 scenario. This second batch was only needed in 2 or 3 cases, where the first four failed but the next four succeeded.&lt;/li&gt;\\n&lt;li&gt;Only one question couldn&amp;#39;t be solved by any of the eight instances due to context length limitations. This occurred with Qwen-235B, as noted in the results table.&lt;/li&gt;\\n&lt;li&gt;Note that quantizations are not same. It&amp;#39;s just me, trying to find the best reasoning &amp;amp; coding model for my setup. &lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Coloring strategy:&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Mark the solution green if it&amp;#39;s accepted.&lt;/li&gt;\\n&lt;li&gt;Use red if it fails in the pre-test cases.&lt;/li&gt;\\n&lt;li&gt;Use red if it fails in the test cases (due to wrong answer or time limit) and passes less than 90% of them.&lt;/li&gt;\\n&lt;li&gt;Use orange if it fails in the test cases but still manages to pass over 90%.&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;&lt;strong&gt;A few observations:&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Occasionally, the generated code contains minor typos, such as a missing comma. I corrected these manually and didn’t treat them as failures, since they were limited to single character issues that clearly qualify as typos.&lt;/li&gt;\\n&lt;li&gt;Hunyuan fell short of my expectations.&lt;/li&gt;\\n&lt;li&gt;Qwen-32B and OpenCodeReasoning model both performed better than expected.&lt;/li&gt;\\n&lt;li&gt;The NVIDIA model tends to be overly verbose ( A LOT ), which likely explains its higher context limit of 65k tokens, compared to 32k in the other models.&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Hardware: 2x H100&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Backend: vLLM (for hunyuan, use 0.9.2 and for others 0.9.1)&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Feel free to recommend another reasoning model for me to test but it must have a vLLM compatible quantized version that fits within 160 GB.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Keep in mind that strong performance on LeetCode doesn&amp;#39;t automatically reflect real world coding skills&lt;/strong&gt;, since everyday programming tasks faced by typical users are usually far less complex.&lt;/p&gt;\\n\\n&lt;p&gt;All questions are recent, with no data leakage involved. So don’t come back saying “LeetCode problems are easy for models, this test isn’t meaningful”. It&amp;#39;s just your test questions have been seen by the model before.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"url_overridden_by_dest":"https://i.redd.it/nyu5vpzx2tcf1.png","view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://preview.redd.it/nyu5vpzx2tcf1.png?auto=webp&amp;s=e45702995060cc687a645562d2df2d39d92ccdf8","width":1565,"height":408},"resolutions":[{"url":"https://preview.redd.it/nyu5vpzx2tcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4ba57d01c877fa0d1bec3f4aef3af9baaad55463","width":108,"height":28},{"url":"https://preview.redd.it/nyu5vpzx2tcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=baf11fe9849c7a2c0833889c03bd68754c8b4e45","width":216,"height":56},{"url":"https://preview.redd.it/nyu5vpzx2tcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1d5e29a1cd3ba5283fb529c5d5457c3cce7a36a0","width":320,"height":83},{"url":"https://preview.redd.it/nyu5vpzx2tcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=47f48f8bee49ad1c403134b86ad6d3fc3d3c55b4","width":640,"height":166},{"url":"https://preview.redd.it/nyu5vpzx2tcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=44e9301ed958519990d12a001176c79624b7a9fe","width":960,"height":250},{"url":"https://preview.redd.it/nyu5vpzx2tcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7efed2bc4442573f344c5203ab1b05379a7429be","width":1080,"height":281}],"variants":{},"id":"Yt8sdbd4WSl3QWw399ju3ntGhqCOHF8RdVFnkafe5Hs"}],"enabled":true},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"mod_note":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"num_reports":null,"removal_reason":null,"link_flair_background_color":"#ccac2b","id":"1lzhns3","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"kyazoglu","discussion_type":null,"num_comments":30,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lzhns3/comparison_of_latest_reasoning_models_on_the_most/","stickied":false,"url":"https://i.redd.it/nyu5vpzx2tcf1.png","subreddit_subscribers":499296,"created_utc":1752484340,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n32nf3x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"tomz17","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3268ez","score":4,"author_fullname":"t2_1mhx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt; Maybe you've tried both under conditions not represented in those benchmarks.\\n\\nThis is the correct answer.  A lot of these coding models suffer from quantization (including KV quantization), often utilized by hobbyists to fit a model into VRAM (e.g. in this case OP had to go down to INT4).  I would take the official numbers over benchmarks like this any day.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n32nf3x","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Maybe you&amp;#39;ve tried both under conditions not represented in those benchmarks.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;This is the correct answer.  A lot of these coding models suffer from quantization (including KV quantization), often utilized by hobbyists to fit a model into VRAM (e.g. in this case OP had to go down to INT4).  I would take the official numbers over benchmarks like this any day.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzhns3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhns3/comparison_of_latest_reasoning_models_on_the_most/n32nf3x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752499969,"author_flair_text":null,"treatment_tags":[],"created_utc":1752499969,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n37env2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"PurpleUpbeat2820","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3268ez","score":1,"author_fullname":"t2_7xnuxw8f","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt; The 235B MoE beats the 32B dense in all benchmarks published by the Qwen team, except for instruction following. It also beats it on the Aider coding leaderboard by quite some margin. Maybe you've tried both under conditions not represented in those benchmarks.\\n\\nAbsolutely. I tried them on real-world problems. As [many others have pointed out](https://www.reddit.com/r/LocalLLaMA/comments/1m04a20/exaone_40_32b/n36ruzt/), today's benchmarks are a poor indicator of practical utility.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n37env2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;The 235B MoE beats the 32B dense in all benchmarks published by the Qwen team, except for instruction following. It also beats it on the Aider coding leaderboard by quite some margin. Maybe you&amp;#39;ve tried both under conditions not represented in those benchmarks.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Absolutely. I tried them on real-world problems. As &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1m04a20/exaone_40_32b/n36ruzt/\\"&gt;many others have pointed out&lt;/a&gt;, today&amp;#39;s benchmarks are a poor indicator of practical utility.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzhns3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhns3/comparison_of_latest_reasoning_models_on_the_most/n37env2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752553273,"author_flair_text":null,"treatment_tags":[],"created_utc":1752553273,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3268ez","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Chromix_","can_mod_post":false,"send_replies":true,"parent_id":"t1_n323im6","score":13,"author_fullname":"t2_k7w2h","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt;IME, the 32b is clearly better than the 235b in practice\\n\\nThe 235B MoE beats the 32B dense in all [benchmarks](https://qwenlm.github.io/blog/qwen3/#introduction) published by the Qwen team, except for instruction following. It also beats it on the [Aider coding leaderboard](https://aider.chat/docs/leaderboards/) by quite some margin. Maybe you've tried both under conditions not represented in those benchmarks.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3268ez","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;IME, the 32b is clearly better than the 235b in practice&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;The 235B MoE beats the 32B dense in all &lt;a href=\\"https://qwenlm.github.io/blog/qwen3/#introduction\\"&gt;benchmarks&lt;/a&gt; published by the Qwen team, except for instruction following. It also beats it on the &lt;a href=\\"https://aider.chat/docs/leaderboards/\\"&gt;Aider coding leaderboard&lt;/a&gt; by quite some margin. Maybe you&amp;#39;ve tried both under conditions not represented in those benchmarks.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzhns3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhns3/comparison_of_latest_reasoning_models_on_the_most/n3268ez/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752493652,"author_flair_text":null,"treatment_tags":[],"created_utc":1752493652,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":13}}],"before":null}},"user_reports":[],"saved":false,"id":"n323im6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"PurpleUpbeat2820","can_mod_post":false,"created_utc":1752492479,"send_replies":true,"parent_id":"t1_n31p6ns","score":-5,"author_fullname":"t2_7xnuxw8f","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; Interesting, the Qwen3 235B model should beat the Qwen3 32B in general, despite a slightly lower number of active parameters.\\n\\nI don't understand why people keep stating this. IME, the 32b is clearly better than the 235b in practice and the reason seems obvious to me: the number of active parameters is too small. I've seen this many times with models from mixtral to llama4. Just look at how bad the 30b is compared to the 32b.\\n\\n&gt; In any case, a Qwen3-Coder 32B model will probably be a great thing to have.\\n\\nOMG, for sure. I cannot wait. Qwen2.5-coder:32b has been hands-down the best coding model for me (including frontier models).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n323im6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Interesting, the Qwen3 235B model should beat the Qwen3 32B in general, despite a slightly lower number of active parameters.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I don&amp;#39;t understand why people keep stating this. IME, the 32b is clearly better than the 235b in practice and the reason seems obvious to me: the number of active parameters is too small. I&amp;#39;ve seen this many times with models from mixtral to llama4. Just look at how bad the 30b is compared to the 32b.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;In any case, a Qwen3-Coder 32B model will probably be a great thing to have.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;OMG, for sure. I cannot wait. Qwen2.5-coder:32b has been hands-down the best coding model for me (including frontier models).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzhns3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhns3/comparison_of_latest_reasoning_models_on_the_most/n323im6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752492479,"author_flair_text":null,"treatment_tags":[],"collapsed":true,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-5}}],"before":null}},"user_reports":[],"saved":false,"id":"n31p6ns","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Chromix_","can_mod_post":false,"created_utc":1752485057,"send_replies":true,"parent_id":"t3_1lzhns3","score":31,"author_fullname":"t2_k7w2h","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Interesting, the Qwen3 235B model should beat the Qwen3 32B in general, despite a slightly lower number of active parameters. It was a INT4 to FP8 comparison though. So maybe that's the reason why it performed worse in 3 cases and never better. Yet the number of tests doesn't seem that large, maybe running 500 will paint a different picture. Especially as running 4 to 8 generations means that the generated code could still be subject to a bad dice roll.\\n\\nIn any case, a Qwen3-Coder 32B model will probably be a great thing to have.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n31p6ns","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Interesting, the Qwen3 235B model should beat the Qwen3 32B in general, despite a slightly lower number of active parameters. It was a INT4 to FP8 comparison though. So maybe that&amp;#39;s the reason why it performed worse in 3 cases and never better. Yet the number of tests doesn&amp;#39;t seem that large, maybe running 500 will paint a different picture. Especially as running 4 to 8 generations means that the generated code could still be subject to a bad dice roll.&lt;/p&gt;\\n\\n&lt;p&gt;In any case, a Qwen3-Coder 32B model will probably be a great thing to have.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhns3/comparison_of_latest_reasoning_models_on_the_most/n31p6ns/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752485057,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzhns3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":31}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n31r10l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ffpeanut15","can_mod_post":false,"created_utc":1752486126,"send_replies":true,"parent_id":"t3_1lzhns3","score":17,"author_fullname":"t2_10fuzp","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Impressive results from Qwen3 32B","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n31r10l","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Impressive results from Qwen3 32B&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhns3/comparison_of_latest_reasoning_models_on_the_most/n31r10l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752486126,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzhns3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":17}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n31znvf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"skyline159","can_mod_post":false,"created_utc":1752490707,"send_replies":true,"parent_id":"t3_1lzhns3","score":13,"author_fullname":"t2_dzdr0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Tl;dr Qwen3-32B is the best size/performance","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n31znvf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Tl;dr Qwen3-32B is the best size/performance&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhns3/comparison_of_latest_reasoning_models_on_the_most/n31znvf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752490707,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzhns3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":13}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n320xqy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"a_slay_nub","can_mod_post":false,"created_utc":1752491308,"send_replies":true,"parent_id":"t3_1lzhns3","score":14,"author_fullname":"t2_u8o4d","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt; Keep in mind that strong performance on LeetCode doesn't automatically reflect real world coding skills\\n\\nWe know, it's the recruiters who don't","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n320xqy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Keep in mind that strong performance on LeetCode doesn&amp;#39;t automatically reflect real world coding skills&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;We know, it&amp;#39;s the recruiters who don&amp;#39;t&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhns3/comparison_of_latest_reasoning_models_on_the_most/n320xqy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752491308,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzhns3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":14}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n31vwqz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AdamDhahabi","can_mod_post":false,"created_utc":1752488828,"send_replies":true,"parent_id":"t3_1lzhns3","score":8,"author_fullname":"t2_x5lnbc2","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Waiting for Qwen3 coder, they are building it as mentioned here: [https://www.youtube.com/watch?v=b0xlsQ\\\\_6wUQ&amp;t=985s](https://www.youtube.com/watch?v=b0xlsQ_6wUQ&amp;t=985s)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n31vwqz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Waiting for Qwen3 coder, they are building it as mentioned here: &lt;a href=\\"https://www.youtube.com/watch?v=b0xlsQ_6wUQ&amp;amp;t=985s\\"&gt;https://www.youtube.com/watch?v=b0xlsQ_6wUQ&amp;amp;t=985s&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhns3/comparison_of_latest_reasoning_models_on_the_most/n31vwqz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752488828,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzhns3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n31q7ja","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kyazoglu","can_mod_post":false,"created_utc":1752485656,"send_replies":true,"parent_id":"t1_n31pc5h","score":1,"author_fullname":"t2_slwqrxz3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"take a look at my observations","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n31q7ja","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;take a look at my observations&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzhns3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhns3/comparison_of_latest_reasoning_models_on_the_most/n31q7ja/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752485656,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n31pc5h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Secure_Reflection409","can_mod_post":false,"created_utc":1752485145,"send_replies":true,"parent_id":"t3_1lzhns3","score":5,"author_fullname":"t2_by77ogdhr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What's the tldr here for people who can't see the picture properly?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n31pc5h","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What&amp;#39;s the tldr here for people who can&amp;#39;t see the picture properly?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhns3/comparison_of_latest_reasoning_models_on_the_most/n31pc5h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752485145,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzhns3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n32z9dv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"kyazoglu","can_mod_post":false,"created_utc":1752503636,"send_replies":true,"parent_id":"t1_n32cjju","score":5,"author_fullname":"t2_slwqrxz3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"https://preview.redd.it/cgyp0rd2pucf1.png?width=1761&amp;format=png&amp;auto=webp&amp;s=881bbce8b293d6a04b2e4c7075520aecf8a2a6e9\\n\\nlooks like there is not enough time for it today. I'll post it on Thursday. So far:","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n32z9dv","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://preview.redd.it/cgyp0rd2pucf1.png?width=1761&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=881bbce8b293d6a04b2e4c7075520aecf8a2a6e9\\"&gt;https://preview.redd.it/cgyp0rd2pucf1.png?width=1761&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=881bbce8b293d6a04b2e4c7075520aecf8a2a6e9&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;looks like there is not enough time for it today. I&amp;#39;ll post it on Thursday. So far:&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzhns3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhns3/comparison_of_latest_reasoning_models_on_the_most/n32z9dv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752503636,"media_metadata":{"cgyp0rd2pucf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":25,"x":108,"u":"https://preview.redd.it/cgyp0rd2pucf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ccb7f8dc0a0974cb633b98fe68d13d833b01fca0"},{"y":50,"x":216,"u":"https://preview.redd.it/cgyp0rd2pucf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b6b6039285d832d3f498f9ad1db1f46c9c0c672a"},{"y":75,"x":320,"u":"https://preview.redd.it/cgyp0rd2pucf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f2ac126aa960be538beddaedc8fbbf8e609b69b7"},{"y":150,"x":640,"u":"https://preview.redd.it/cgyp0rd2pucf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=84954c13b7091d7842a676f6578a3c622e82b1d0"},{"y":225,"x":960,"u":"https://preview.redd.it/cgyp0rd2pucf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ec4567530cc0b00a3adb5317e6bdf7ba201cd56e"},{"y":253,"x":1080,"u":"https://preview.redd.it/cgyp0rd2pucf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c529268fd4a87a6b489421dfbc09dd5b83cbaf77"}],"s":{"y":413,"x":1761,"u":"https://preview.redd.it/cgyp0rd2pucf1.png?width=1761&amp;format=png&amp;auto=webp&amp;s=881bbce8b293d6a04b2e4c7075520aecf8a2a6e9"},"id":"cgyp0rd2pucf1"}},"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n32cjju","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"kyazoglu","can_mod_post":false,"created_utc":1752496162,"send_replies":true,"parent_id":"t3_1lzhns3","score":4,"author_fullname":"t2_slwqrxz3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I've just seen the MetaStone-S1-32B model which looks promising. I started benchmarking it. It'll be here couple of hours later.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n32cjju","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve just seen the MetaStone-S1-32B model which looks promising. I started benchmarking it. It&amp;#39;ll be here couple of hours later.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhns3/comparison_of_latest_reasoning_models_on_the_most/n32cjju/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752496162,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzhns3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n32f52s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FalseMap1582","can_mod_post":false,"created_utc":1752497127,"send_replies":true,"parent_id":"t3_1lzhns3","score":5,"author_fullname":"t2_14u3g9s5kx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I would be really interesting to see how much worse Qwen 3 235b INT4 is compared to Qwen 3 235b FP8/FP16","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n32f52s","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I would be really interesting to see how much worse Qwen 3 235b INT4 is compared to Qwen 3 235b FP8/FP16&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhns3/comparison_of_latest_reasoning_models_on_the_most/n32f52s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752497127,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzhns3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n35os9r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"kmouratidis","can_mod_post":false,"created_utc":1752531362,"send_replies":true,"parent_id":"t1_n34nvei","score":5,"author_fullname":"t2_k6u7rfxb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"^ This. You can run it unquantized too, it would be interesting to see if it is any better than FP8 / AWQ. I think AWQ MoE fixes were recently merged in vLLM too (same version as the Hunyuan fixes).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n35os9r","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;^ This. You can run it unquantized too, it would be interesting to see if it is any better than FP8 / AWQ. I think AWQ MoE fixes were recently merged in vLLM too (same version as the Hunyuan fixes).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzhns3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhns3/comparison_of_latest_reasoning_models_on_the_most/n35os9r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752531362,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n34nvei","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MKU64","can_mod_post":false,"created_utc":1752520685,"send_replies":true,"parent_id":"t3_1lzhns3","score":5,"author_fullname":"t2_wn7it","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Can you by chance try Qwen 3 30B-A3B? It’s a really good model and I think it would be good to see how well it does compared to these bigger models!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n34nvei","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Can you by chance try Qwen 3 30B-A3B? It’s a really good model and I think it would be good to see how well it does compared to these bigger models!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhns3/comparison_of_latest_reasoning_models_on_the_most/n34nvei/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752520685,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzhns3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n33z0a5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"daank","can_mod_post":false,"created_utc":1752513749,"send_replies":true,"parent_id":"t1_n32a4xi","score":5,"author_fullname":"t2_6mryd","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I just tried it after seeing it here, but it ain't working well for me. The unsloth quantizations seem to get stuck in a thinking loop. It spent 2000+ tokens thinking about writing a sorting algorithm in python before I cut it off.\\n\\nThe difference might be quantization. Would be interesting to see which models react most graciously to quantization and which suffer the most.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n33z0a5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I just tried it after seeing it here, but it ain&amp;#39;t working well for me. The unsloth quantizations seem to get stuck in a thinking loop. It spent 2000+ tokens thinking about writing a sorting algorithm in python before I cut it off.&lt;/p&gt;\\n\\n&lt;p&gt;The difference might be quantization. Would be interesting to see which models react most graciously to quantization and which suffer the most.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzhns3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhns3/comparison_of_latest_reasoning_models_on_the_most/n33z0a5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752513749,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n32ce5c","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kyazoglu","can_mod_post":false,"created_utc":1752496105,"send_replies":true,"parent_id":"t1_n32a4xi","score":2,"author_fullname":"t2_slwqrxz3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"certainly very strong. beats qwen3-32b? arguable","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n32ce5c","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;certainly very strong. beats qwen3-32b? arguable&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzhns3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhns3/comparison_of_latest_reasoning_models_on_the_most/n32ce5c/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752496105,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n32a4xi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"EternalOptimister","can_mod_post":false,"created_utc":1752495236,"send_replies":true,"parent_id":"t3_1lzhns3","score":3,"author_fullname":"t2_1cfh8hg6","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Why is everyone ignoring the nemotron? Looks to me like it beats all of the rest?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n32a4xi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why is everyone ignoring the nemotron? Looks to me like it beats all of the rest?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhns3/comparison_of_latest_reasoning_models_on_the_most/n32a4xi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752495236,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzhns3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n338qcv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Chromix_","can_mod_post":false,"created_utc":1752506364,"send_replies":true,"parent_id":"t1_n329ud1","score":2,"author_fullname":"t2_k7w2h","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes, faster than 32b, but roughly on par with 14B in terms of capability.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n338qcv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, faster than 32b, but roughly on par with 14B in terms of capability.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzhns3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhns3/comparison_of_latest_reasoning_models_on_the_most/n338qcv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752506364,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n329ud1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Single-Persimmon9439","can_mod_post":false,"created_utc":1752495120,"send_replies":true,"parent_id":"t3_1lzhns3","score":2,"author_fullname":"t2_k8oo9hc0x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"qwen3 30b a3b  \\nfast moe model. should be much faster qwen3 32b","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n329ud1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;qwen3 30b a3b&lt;br/&gt;\\nfast moe model. should be much faster qwen3 32b&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhns3/comparison_of_latest_reasoning_models_on_the_most/n329ud1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752495120,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzhns3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n32js73","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kyazoglu","can_mod_post":false,"created_utc":1752498748,"send_replies":true,"parent_id":"t1_n32j1g0","score":2,"author_fullname":"t2_slwqrxz3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"well, I have to automate everything to keep track of these kind of details. For now, I'm doing it manually but if I find enough time, I'll automate everything and repeat this test again in the future with different models","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n32js73","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;well, I have to automate everything to keep track of these kind of details. For now, I&amp;#39;m doing it manually but if I find enough time, I&amp;#39;ll automate everything and repeat this test again in the future with different models&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzhns3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhns3/comparison_of_latest_reasoning_models_on_the_most/n32js73/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752498748,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n32j1g0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"choose_a_guest","can_mod_post":false,"created_utc":1752498494,"send_replies":true,"parent_id":"t3_1lzhns3","score":2,"author_fullname":"t2_cbnw4l4g","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt;For each question, four instances of the same model were run in parallel (i.e., best-of-4). If any of them successfully solved the question, the most optimized solution among them was selected.\\n\\n&gt;If none of the four produced a solution within the maximum context length, an additional four instances were run, making it a best-of-8 scenario. This second batch was only needed in 2 or 3 cases, where the first four failed but the next four succeeded.\\n\\nCan you provide the success rate for each model in each question (success count/number of attempts)?\\n\\nEven for this small number of samples, knowing that a model succeeded 4/4 and the alternatives only succeeded 1/8 would paint a very different picture in this comparison.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n32j1g0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;For each question, four instances of the same model were run in parallel (i.e., best-of-4). If any of them successfully solved the question, the most optimized solution among them was selected.&lt;/p&gt;\\n\\n&lt;p&gt;If none of the four produced a solution within the maximum context length, an additional four instances were run, making it a best-of-8 scenario. This second batch was only needed in 2 or 3 cases, where the first four failed but the next four succeeded.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Can you provide the success rate for each model in each question (success count/number of attempts)?&lt;/p&gt;\\n\\n&lt;p&gt;Even for this small number of samples, knowing that a model succeeded 4/4 and the alternatives only succeeded 1/8 would paint a very different picture in this comparison.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhns3/comparison_of_latest_reasoning_models_on_the_most/n32j1g0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752498494,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzhns3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n32l9ua","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kyazoglu","can_mod_post":false,"created_utc":1752499251,"send_replies":true,"parent_id":"t1_n32kne8","score":3,"author_fullname":"t2_slwqrxz3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I've used 2.5 Coder for a long time before it was bested by the others. It's a great model for speed and constructing the backbone of the code but fails miserably in complex coding tasks. I have never used Devstral but it is advertised as agentic model so I'd assume not a great fit","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n32l9ua","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve used 2.5 Coder for a long time before it was bested by the others. It&amp;#39;s a great model for speed and constructing the backbone of the code but fails miserably in complex coding tasks. I have never used Devstral but it is advertised as agentic model so I&amp;#39;d assume not a great fit&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzhns3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhns3/comparison_of_latest_reasoning_models_on_the_most/n32l9ua/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752499251,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n32kne8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"henfiber","can_mod_post":false,"created_utc":1752499043,"send_replies":true,"parent_id":"t3_1lzhns3","score":2,"author_fullname":"t2_lw9me25","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Although not a reasoning model, you could also include Qwen2.5-coder-32b in your tests as a baseline.\\n\\nDevstral would also be an interesting one.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n32kne8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Although not a reasoning model, you could also include Qwen2.5-coder-32b in your tests as a baseline.&lt;/p&gt;\\n\\n&lt;p&gt;Devstral would also be an interesting one.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhns3/comparison_of_latest_reasoning_models_on_the_most/n32kne8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752499043,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzhns3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n35adqq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"maxpayne07","can_mod_post":false,"created_utc":1752527074,"send_replies":true,"parent_id":"t3_1lzhns3","score":2,"author_fullname":"t2_2wfw2zhf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Can you do qwen3 30b a3b please?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n35adqq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Can you do qwen3 30b a3b please?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhns3/comparison_of_latest_reasoning_models_on_the_most/n35adqq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752527074,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzhns3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n32j1sf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"gamblingapocalypse","can_mod_post":false,"created_utc":1752498497,"send_replies":true,"parent_id":"t3_1lzhns3","score":1,"author_fullname":"t2_fz3utn30","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Tiny (ish) and mighty (ish)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n32j1sf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Tiny (ish) and mighty (ish)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhns3/comparison_of_latest_reasoning_models_on_the_most/n32j1sf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752498497,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzhns3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n35qhmp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"JacopoBandoni","can_mod_post":false,"created_utc":1752531899,"send_replies":true,"parent_id":"t3_1lzhns3","score":1,"author_fullname":"t2_6a3kzsa2","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Are this totally new invented leetcode questions?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n35qhmp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Are this totally new invented leetcode questions?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhns3/comparison_of_latest_reasoning_models_on_the_most/n35qhmp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752531899,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzhns3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n35ts8l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Shape_3423","can_mod_post":false,"created_utc":1752532951,"send_replies":true,"parent_id":"t3_1lzhns3","score":1,"author_fullname":"t2_1mpbnkwidj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"In my personal tests involving long prompts with a series of instructions, quantization impacted performance and specifically instruction following (IF).  235b Q3KL performed worse for me than Qwen 2.5 70b Q8 and was even beaten by Qwen 3 32b BF16/Q8.  There was a measurable drop-off from BF16-&gt;Q8 for Qwen3 32b and 30b, although Q8 usually scored well. Taking 70b down to Q4KM?  Forget about it. I bet 235b Q8 would crush it here.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n35ts8l","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;In my personal tests involving long prompts with a series of instructions, quantization impacted performance and specifically instruction following (IF).  235b Q3KL performed worse for me than Qwen 2.5 70b Q8 and was even beaten by Qwen 3 32b BF16/Q8.  There was a measurable drop-off from BF16-&amp;gt;Q8 for Qwen3 32b and 30b, although Q8 usually scored well. Taking 70b down to Q4KM?  Forget about it. I bet 235b Q8 would crush it here.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhns3/comparison_of_latest_reasoning_models_on_the_most/n35ts8l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752532951,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzhns3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n36smyn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"bennmann","can_mod_post":false,"created_utc":1752544729,"send_replies":true,"parent_id":"t3_1lzhns3","score":1,"author_fullname":"t2_5lqoi","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Best part about this is OpenCodeReasoning dataset is \\"only\\" about 7B tokens.\\n\\n\\nCould be used on 235B for pretty cheap","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n36smyn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Best part about this is OpenCodeReasoning dataset is &amp;quot;only&amp;quot; about 7B tokens.&lt;/p&gt;\\n\\n&lt;p&gt;Could be used on 235B for pretty cheap&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhns3/comparison_of_latest_reasoning_models_on_the_most/n36smyn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752544729,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzhns3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(l,{data:t});export{r as default};
