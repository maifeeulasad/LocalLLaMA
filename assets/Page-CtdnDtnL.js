import{j as e}from"./index-CeRg6Q3f.js";import{R as t}from"./RedditPostRenderer-D7n1g-D8.js";import"./index-DPToWe3n.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"\\\\[As with all my posts, the code and text are organic with no LLM involved. Note that I myself have not confirmed that this works in all cases--I personally have no interest in voice cloning--but in my head the theory is strong and I am confident it should work. Plus, there is historical precedent in soft prompting and control vectors.\\\\]\\n\\nLet's say you have a local TTS model that takes a speaker embedding \`spk_emb\`, but the model to produce the speaker embedding is unavailable. You can simply apply gradient descent on the speaker embedding and freeze everything else.\\n\\nHere is the pseudocode. You will need to change the code depending on the model you are using, and there are plenty of knobs to tune.\\n\\n    import torch\\n    # 1. Initialize the embedding, either randomly or nearest neighbor\\n    spk_emb = torch.randn(1, 512) # if batch size 1, dim 512\\n    spk_emb.requires_grad = True\\n    # 2. Initialize the model and freeze its parameters\\n    model = YourModelClass.from_pretrained('TODO')\\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\\n    model.to(device).eval()\\n    for p in model.parameters():\\n        p.requires_grad = False\\n    # 3. Optimizer and dataset, LR is up to you\\n    optimizer = torch.optim.Adam([spk_emb], lr=0.001)\\n    TODO_your_dataset_of_text_audio_pairs = [\\n    ('This is some text.', 'corresponding_audio.wav'),\\n    # ...\\n    ]\\n    # 4. Barebones training loop. You can add a learning rate scheduler, etc.\\n    for epoch in range(10): # how many epochs is up to you\\n        for text, audio in TODO_your_dataset_of_text_audio_pairs:\\n            loss = model.forward_with_loss(text, audio, spk_emb)\\n            loss.backward()\\n            optimizer.step()\\n            optimizer.zero_grad()\\n\\nThe big caveat here is that you cannot get blood out of a stone; if a speaker is firmly out-of-distribution for the model, no amount of gradient descent will get you to where you want to go.\\n\\nAnd that's it. If you have any questions you can post them below.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Dark Arts: Speaker embedding gradient descent for local TTS models","link_flair_richtext":[{"e":"text","t":"Tutorial | Guide"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lymlgp","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.82,"author_flair_background_color":null,"subreddit_type":"public","ups":11,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1e2jjp1mqg","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Tutorial | Guide","can_mod_post":false,"score":11,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1752392740,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752390519,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;[As with all my posts, the code and text are organic with no LLM involved. Note that I myself have not confirmed that this works in all cases--I personally have no interest in voice cloning--but in my head the theory is strong and I am confident it should work. Plus, there is historical precedent in soft prompting and control vectors.]&lt;/p&gt;\\n\\n&lt;p&gt;Let&amp;#39;s say you have a local TTS model that takes a speaker embedding &lt;code&gt;spk_emb&lt;/code&gt;, but the model to produce the speaker embedding is unavailable. You can simply apply gradient descent on the speaker embedding and freeze everything else.&lt;/p&gt;\\n\\n&lt;p&gt;Here is the pseudocode. You will need to change the code depending on the model you are using, and there are plenty of knobs to tune.&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;import torch\\n# 1. Initialize the embedding, either randomly or nearest neighbor\\nspk_emb = torch.randn(1, 512) # if batch size 1, dim 512\\nspk_emb.requires_grad = True\\n# 2. Initialize the model and freeze its parameters\\nmodel = YourModelClass.from_pretrained(&amp;#39;TODO&amp;#39;)\\ndevice = &amp;#39;cuda&amp;#39; if torch.cuda.is_available() else &amp;#39;cpu&amp;#39;\\nmodel.to(device).eval()\\nfor p in model.parameters():\\n    p.requires_grad = False\\n# 3. Optimizer and dataset, LR is up to you\\noptimizer = torch.optim.Adam([spk_emb], lr=0.001)\\nTODO_your_dataset_of_text_audio_pairs = [\\n(&amp;#39;This is some text.&amp;#39;, &amp;#39;corresponding_audio.wav&amp;#39;),\\n# ...\\n]\\n# 4. Barebones training loop. You can add a learning rate scheduler, etc.\\nfor epoch in range(10): # how many epochs is up to you\\n    for text, audio in TODO_your_dataset_of_text_audio_pairs:\\n        loss = model.forward_with_loss(text, audio, spk_emb)\\n        loss.backward()\\n        optimizer.step()\\n        optimizer.zero_grad()\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;The big caveat here is that you cannot get blood out of a stone; if a speaker is firmly out-of-distribution for the model, no amount of gradient descent will get you to where you want to go.&lt;/p&gt;\\n\\n&lt;p&gt;And that&amp;#39;s it. If you have any questions you can post them below.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"449b05a6-bf8e-11ed-b4bd-66961e47bd50","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#0079d3","id":"1lymlgp","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"rzvzn","discussion_type":null,"num_comments":1,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lymlgp/dark_arts_speaker_embedding_gradient_descent_for/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lymlgp/dark_arts_speaker_embedding_gradient_descent_for/","subreddit_subscribers":498344,"created_utc":1752390519,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2vahwv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Chromix_","can_mod_post":false,"created_utc":1752396893,"send_replies":true,"parent_id":"t3_1lymlgp","score":2,"author_fullname":"t2_k7w2h","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"A working example for a popular TTS model would be nice - a comparison between the original voice, trained voice and the result.\\n\\nHere's something that [works for Kokoro](https://www.reddit.com/r/LocalLLaMA/comments/1ks0arl/voice_cloning_for_kokoro_tts_using_random_walk/), yet uses a completely different approach.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2vahwv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;A working example for a popular TTS model would be nice - a comparison between the original voice, trained voice and the result.&lt;/p&gt;\\n\\n&lt;p&gt;Here&amp;#39;s something that &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1ks0arl/voice_cloning_for_kokoro_tts_using_random_walk/\\"&gt;works for Kokoro&lt;/a&gt;, yet uses a completely different approach.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lymlgp/dark_arts_speaker_embedding_gradient_descent_for/n2vahwv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752396893,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lymlgp","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),r=()=>e.jsx(t,{data:a});export{r as default};
