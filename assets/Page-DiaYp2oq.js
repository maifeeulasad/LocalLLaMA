import{j as e}from"./index-CqAPCjw5.js";import{R as a}from"./RedditPostRenderer-4oBDAtGr.js";import"./index-D3Sdy_Op.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Finally found this leaderboard that explains my experiences with fine-tuning jobs. My workloads are pretty much 100% fine-tuning, and I found that zero-shot performance does *not* correlate with fine-tuning performance (Qwen3 vs. Llama 3.1 was my big revelation). None of the big leaderboards report fine-tunability. There's something to leaving the model less-trained like a blank canvas.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Fine-tuning Leaderboard!","link_flair_richtext":[{"e":"text","t":"Resources"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":73,"top_awarded_type":null,"hide_score":false,"name":"t3_1m0y3a6","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.91,"author_flair_background_color":"transparent","ups":82,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":"c07aa42e-51fe-11f0-afcc-462aad931709","is_original_content":false,"author_fullname":"t2_1a48h7vf","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Resources","can_mod_post":false,"score":82,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://external-preview.redd.it/1DLouaRlS6TuRxGt-c0X9cSEuiFu-W3XyC_nxmG1k4s.png?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=90cfa9c9d985465ff2f212339aca919c2f6b97a1","edited":false,"author_flair_css_class":null,"author_flair_richtext":[{"a":":X:","u":"https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X","e":"emoji"}],"gildings":{},"post_hint":"link","content_categories":null,"is_self":false,"subreddit_type":"public","created":1752624433,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"richtext","domain":"predibase.com","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Finally found this leaderboard that explains my experiences with fine-tuning jobs. My workloads are pretty much 100% fine-tuning, and I found that zero-shot performance does &lt;em&gt;not&lt;/em&gt; correlate with fine-tuning performance (Qwen3 vs. Llama 3.1 was my big revelation). None of the big leaderboards report fine-tunability. There&amp;#39;s something to leaving the model less-trained like a blank canvas.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"url_overridden_by_dest":"https://predibase.com/fine-tuning-index","view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/1DLouaRlS6TuRxGt-c0X9cSEuiFu-W3XyC_nxmG1k4s.png?auto=webp&amp;s=b8ed76b6fd82574f7e8beb209667ea35d3d7e266","width":1200,"height":630},"resolutions":[{"url":"https://external-preview.redd.it/1DLouaRlS6TuRxGt-c0X9cSEuiFu-W3XyC_nxmG1k4s.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=656ccf5ab3a777913e7a625c02f477e2e3ebeef2","width":108,"height":56},{"url":"https://external-preview.redd.it/1DLouaRlS6TuRxGt-c0X9cSEuiFu-W3XyC_nxmG1k4s.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3ede7cd94792d8596f867480eaf6c764db411cd7","width":216,"height":113},{"url":"https://external-preview.redd.it/1DLouaRlS6TuRxGt-c0X9cSEuiFu-W3XyC_nxmG1k4s.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2e63ec9d5c8376d9021d2ecde41c8443bb9947f6","width":320,"height":168},{"url":"https://external-preview.redd.it/1DLouaRlS6TuRxGt-c0X9cSEuiFu-W3XyC_nxmG1k4s.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=477a0620594fc1ec25f6cb09693ff29925108ee4","width":640,"height":336},{"url":"https://external-preview.redd.it/1DLouaRlS6TuRxGt-c0X9cSEuiFu-W3XyC_nxmG1k4s.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=34c7bf1b2db21970ebec567270e5ce4704a3231f","width":960,"height":504},{"url":"https://external-preview.redd.it/1DLouaRlS6TuRxGt-c0X9cSEuiFu-W3XyC_nxmG1k4s.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fd640f7cf95a3545374d45ec62945c392b4da5ee","width":1080,"height":567}],"variants":{},"id":"1DLouaRlS6TuRxGt-c0X9cSEuiFu-W3XyC_nxmG1k4s"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":":X:","treatment_tags":[],"visited":false,"removed_by":null,"mod_note":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"num_reports":null,"removal_reason":null,"link_flair_background_color":"#ccac2b","id":"1m0y3a6","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"entsnack","discussion_type":null,"num_comments":23,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":"dark","permalink":"/r/LocalLLaMA/comments/1m0y3a6/finetuning_leaderboard/","stickied":false,"url":"https://predibase.com/fine-tuning-index","subreddit_subscribers":499774,"created_utc":1752624433,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"c07aa42e-51fe-11f0-afcc-462aad931709","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"c07aa42e-51fe-11f0-afcc-462aad931709","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3dv9np","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3djrdk","score":1,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'm not the first one, the old OpenAI OGs have been fine-tuning the now-deprecated babbage and ada models since 2023 (pre-ChatGPT days). I picked up on it after GPT-3.5 launched and eventually moved to Llama 2 after having a lot of success (it killed *all* my previous pipelines and I needed to pivot to survive).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dv9np","is_submitter":true,"downs":0,"author_flair_richtext":[{"a":":X:","u":"https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X","e":"emoji"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m not the first one, the old OpenAI OGs have been fine-tuning the now-deprecated babbage and ada models since 2023 (pre-ChatGPT days). I picked up on it after GPT-3.5 launched and eventually moved to Llama 2 after having a lot of success (it killed &lt;em&gt;all&lt;/em&gt; my previous pipelines and I needed to pivot to survive).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0y3a6","unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0y3a6/finetuning_leaderboard/n3dv9np/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752636386,"author_flair_text":":X:","treatment_tags":[],"created_utc":1752636386,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"transparent","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3djrdk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"TorontoBiker","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3dh5w4","score":2,"author_fullname":"t2_wepcy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Fine tuning for predictive analytics? That’s really interesting - I never thought that would work well. Hunh.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3djrdk","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Fine tuning for predictive analytics? That’s really interesting - I never thought that would work well. Hunh.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0y3a6","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0y3a6/finetuning_leaderboard/n3djrdk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752632054,"author_flair_text":null,"treatment_tags":[],"created_utc":1752632054,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"c07aa42e-51fe-11f0-afcc-462aad931709","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3femlu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3f3rny","score":2,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Just put the structured data into the prompt. As long as what you're forecasting is the future of a discrete sequence, LLMs often work well.\\n\\nThey destroyed all my previous \\"hand-crafted\\" models built over the past decade with basically no hyperparameter tuning. It's because they've been pretrained on a LOT of text, it's hard to beat that pretraining knowledge.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3femlu","is_submitter":true,"downs":0,"author_flair_richtext":[{"a":":X:","u":"https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X","e":"emoji"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just put the structured data into the prompt. As long as what you&amp;#39;re forecasting is the future of a discrete sequence, LLMs often work well.&lt;/p&gt;\\n\\n&lt;p&gt;They destroyed all my previous &amp;quot;hand-crafted&amp;quot; models built over the past decade with basically no hyperparameter tuning. It&amp;#39;s because they&amp;#39;ve been pretrained on a LOT of text, it&amp;#39;s hard to beat that pretraining knowledge.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0y3a6","unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0y3a6/finetuning_leaderboard/n3femlu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752665014,"author_flair_text":":X:","treatment_tags":[],"created_utc":1752665014,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"transparent","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3f3rny","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HiddenoO","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3dh5w4","score":3,"author_fullname":"t2_8127x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Does \\"historical return and chargeback data\\" include textual data or why are you using LLMs for this task?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3f3rny","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Does &amp;quot;historical return and chargeback data&amp;quot; include textual data or why are you using LLMs for this task?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0y3a6","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0y3a6/finetuning_leaderboard/n3f3rny/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752659635,"author_flair_text":null,"treatment_tags":[],"created_utc":1752659635,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"c07aa42e-51fe-11f0-afcc-462aad931709","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ffswr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3f9pib","score":1,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Trust me I want to believe this as much as you do, I have published papers on my hand-crafted models. They're obsolete now.\\n\\nI think if your data is not a sequence, and heavily structured, a classical classifier would still work.\\n\\nBut Transformers are turning out to be general purpose computers for any kind of sequential learning task, not just language.\\n\\nCheck out the work on LLMs for robotics: https://palm-e.github.io\\n\\nYou could ask: why use an LLM to control a robot? Why not classical optimal control?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ffswr","is_submitter":true,"downs":0,"author_flair_richtext":[{"a":":X:","u":"https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X","e":"emoji"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Trust me I want to believe this as much as you do, I have published papers on my hand-crafted models. They&amp;#39;re obsolete now.&lt;/p&gt;\\n\\n&lt;p&gt;I think if your data is not a sequence, and heavily structured, a classical classifier would still work.&lt;/p&gt;\\n\\n&lt;p&gt;But Transformers are turning out to be general purpose computers for any kind of sequential learning task, not just language.&lt;/p&gt;\\n\\n&lt;p&gt;Check out the work on LLMs for robotics: &lt;a href=\\"https://palm-e.github.io\\"&gt;https://palm-e.github.io&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;You could ask: why use an LLM to control a robot? Why not classical optimal control?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0y3a6","unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0y3a6/finetuning_leaderboard/n3ffswr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752665527,"author_flair_text":":X:","treatment_tags":[],"created_utc":1752665527,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"transparent","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3f9pib","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"YellowTree11","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3dh5w4","score":2,"author_fullname":"t2_11qeqtr9z5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I think a machine learning model would be sufficient, using a language model for classification seems a bit extra, doesn’t it?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3f9pib","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think a machine learning model would be sufficient, using a language model for classification seems a bit extra, doesn’t it?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0y3a6","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0y3a6/finetuning_leaderboard/n3f9pib/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752662761,"author_flair_text":null,"treatment_tags":[],"created_utc":1752662761,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"c07aa42e-51fe-11f0-afcc-462aad931709","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3fdvhl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3em2d3","score":2,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I just use Transformers and TRL from Huggingface, nothing fancy. I also use OpenAI but their models don't fine tune well. I have an  H100 server (96GB VRAM, 512GB RAM) that I prototype on, and then switch to a cluster on Runpod for final runs.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3fdvhl","is_submitter":true,"downs":0,"author_flair_richtext":[{"a":":X:","u":"https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X","e":"emoji"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I just use Transformers and TRL from Huggingface, nothing fancy. I also use OpenAI but their models don&amp;#39;t fine tune well. I have an  H100 server (96GB VRAM, 512GB RAM) that I prototype on, and then switch to a cluster on Runpod for final runs.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0y3a6","unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0y3a6/finetuning_leaderboard/n3fdvhl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752664683,"author_flair_text":":X:","treatment_tags":[],"created_utc":1752664683,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"transparent","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3em2d3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MammayKaiseHain","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3dh5w4","score":1,"author_fullname":"t2_17owgu9uf5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What is your current setup for fine-tuning (libraries, machine/instances) ?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3em2d3","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What is your current setup for fine-tuning (libraries, machine/instances) ?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0y3a6","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0y3a6/finetuning_leaderboard/n3em2d3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752649445,"author_flair_text":null,"treatment_tags":[],"created_utc":1752649445,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"c07aa42e-51fe-11f0-afcc-462aad931709","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3fdwyy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3eql5y","score":1,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I don't use LORA.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3fdwyy","is_submitter":true,"downs":0,"author_flair_richtext":[{"a":":X:","u":"https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X","e":"emoji"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t use LORA.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0y3a6","unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0y3a6/finetuning_leaderboard/n3fdwyy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752664701,"author_flair_text":":X:","treatment_tags":[],"created_utc":1752664701,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"transparent","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3eql5y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Babouche_Le_Singe","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3dh5w4","score":1,"author_fullname":"t2_1mlog17z2z","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"So, based on your experiments, Lora is sufficient to achieve good results for this task? I wouldn't have guessed so.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3eql5y","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So, based on your experiments, Lora is sufficient to achieve good results for this task? I wouldn&amp;#39;t have guessed so.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0y3a6","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0y3a6/finetuning_leaderboard/n3eql5y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752651987,"author_flair_text":null,"treatment_tags":[],"created_utc":1752651987,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3dh5w4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"entsnack","can_mod_post":false,"created_utc":1752631136,"send_replies":true,"parent_id":"t1_n3d5kpr","score":4,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"My side gig is just using LLMs to forecast things and using that to deliver value in some way for clients.\\n\\nSimple example is forecasting whether a customer is going to return a product that they purchased, or do a chargeback. I have historical return and chargeback data from the client, dump everything into prompt-completion pairs, fine-tune a bunch of LLMs and deliver the best one if it works well enough.\\n\\nI'm literally fine-tuning-as-a-service but I do the hyperparameter tuning by hand.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dh5w4","is_submitter":true,"downs":0,"author_flair_richtext":[{"a":":X:","u":"https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X","e":"emoji"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;My side gig is just using LLMs to forecast things and using that to deliver value in some way for clients.&lt;/p&gt;\\n\\n&lt;p&gt;Simple example is forecasting whether a customer is going to return a product that they purchased, or do a chargeback. I have historical return and chargeback data from the client, dump everything into prompt-completion pairs, fine-tune a bunch of LLMs and deliver the best one if it works well enough.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m literally fine-tuning-as-a-service but I do the hyperparameter tuning by hand.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0y3a6","unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0y3a6/finetuning_leaderboard/n3dh5w4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752631136,"author_flair_text":":X:","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"transparent","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n3d5kpr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"TheLocalDrummer","can_mod_post":false,"created_utc":1752627081,"send_replies":true,"parent_id":"t3_1m0y3a6","score":9,"author_fullname":"t2_w6l58p741","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Love this! There are definitely models out there that are difficult to finetune properly.\\n\\n&gt;My workloads are pretty much 100% fine-tuning\\n\\nWhat do you do for work? Lol","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3d5kpr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Love this! There are definitely models out there that are difficult to finetune properly.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;My workloads are pretty much 100% fine-tuning&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;What do you do for work? Lol&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0y3a6/finetuning_leaderboard/n3d5kpr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752627081,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0y3a6","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"c07aa42e-51fe-11f0-afcc-462aad931709","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3e8vj3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cleverusernametry","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3e0jpb","score":2,"author_fullname":"t2_17bfjs","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Still out of date, but by a lesser extent. \\n\\nIt doesn't have Gemma3, Qwen notably","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3e8vj3","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Still out of date, but by a lesser extent. &lt;/p&gt;\\n\\n&lt;p&gt;It doesn&amp;#39;t have Gemma3, Qwen notably&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0y3a6","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0y3a6/finetuning_leaderboard/n3e8vj3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752642521,"author_flair_text":null,"treatment_tags":[],"created_utc":1752642521,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3e0jpb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"created_utc":1752638497,"send_replies":true,"parent_id":"t1_n3dzz7m","score":-1,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"[https://predibase.com/fine-tuning-leaderboard](https://predibase.com/fine-tuning-leaderboard)\\n\\nSeems like the link above has been updated recently.\\n\\nI can confirm that Llama fine-tunes really well but does poorly at zero-shot. I was surprised at Phi's fine-tuning performance, need to try that.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3e0jpb","is_submitter":true,"downs":0,"author_flair_richtext":[{"a":":X:","u":"https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X","e":"emoji"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://predibase.com/fine-tuning-leaderboard\\"&gt;https://predibase.com/fine-tuning-leaderboard&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Seems like the link above has been updated recently.&lt;/p&gt;\\n\\n&lt;p&gt;I can confirm that Llama fine-tunes really well but does poorly at zero-shot. I was surprised at Phi&amp;#39;s fine-tuning performance, need to try that.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0y3a6","unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0y3a6/finetuning_leaderboard/n3e0jpb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752638497,"author_flair_text":":X:","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"transparent","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3dzz7m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Mybrandnewaccount95","can_mod_post":false,"created_utc":1752638261,"send_replies":true,"parent_id":"t3_1m0y3a6","score":3,"author_fullname":"t2_2r6yfku9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It's unfortunate that this is a year old and won't be updated. How does it line up with your personal experience of fine-tuning models?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dzz7m","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s unfortunate that this is a year old and won&amp;#39;t be updated. How does it line up with your personal experience of fine-tuning models?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0y3a6/finetuning_leaderboard/n3dzz7m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752638261,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0y3a6","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"c07aa42e-51fe-11f0-afcc-462aad931709","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"c07aa42e-51fe-11f0-afcc-462aad931709","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3fdh30","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3f6y87","score":1,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Cool, let me know when you find a more up to date fine tuning benchmark then.\\n\\nEdit: Smaller Qwens are good but don't fine tune as well as the Llamas.","edited":1752665061,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3fdh30","is_submitter":true,"downs":0,"author_flair_richtext":[{"a":":X:","u":"https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X","e":"emoji"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Cool, let me know when you find a more up to date fine tuning benchmark then.&lt;/p&gt;\\n\\n&lt;p&gt;Edit: Smaller Qwens are good but don&amp;#39;t fine tune as well as the Llamas.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0y3a6","unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0y3a6/finetuning_leaderboard/n3fdh30/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752664505,"author_flair_text":":X:","treatment_tags":[],"created_utc":1752664505,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"transparent","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3f6y87","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HiddenoO","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3d10lc","score":1,"author_fullname":"t2_8127x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The smaller Qwen 2/2.5/3 models are some of the most finetuned models out there, and they're regularly used in research for that purpose. Meanwhile, they're completely missing from that list even though the company behind that site supports 13 different Qwen models themselves.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3f6y87","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The smaller Qwen 2/2.5/3 models are some of the most finetuned models out there, and they&amp;#39;re regularly used in research for that purpose. Meanwhile, they&amp;#39;re completely missing from that list even though the company behind that site supports 13 different Qwen models themselves.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0y3a6","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0y3a6/finetuning_leaderboard/n3f6y87/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752661345,"author_flair_text":null,"treatment_tags":[],"created_utc":1752661345,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3d10lc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"entsnack","can_mod_post":false,"created_utc":1752625491,"send_replies":true,"parent_id":"t1_n3d0lsk","score":5,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"wtf does \\"skill issue\\" mean?\\n\\nAnd the benchmarks still hold up, I've tried the newer models and they're too benchmaxxd to fine-tune. No one makes fine-tunable models anymore because they look bad on leaderboards.\\n\\nWhat's your workload?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3d10lc","is_submitter":true,"downs":0,"author_flair_richtext":[{"a":":X:","u":"https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X","e":"emoji"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;wtf does &amp;quot;skill issue&amp;quot; mean?&lt;/p&gt;\\n\\n&lt;p&gt;And the benchmarks still hold up, I&amp;#39;ve tried the newer models and they&amp;#39;re too benchmaxxd to fine-tune. No one makes fine-tunable models anymore because they look bad on leaderboards.&lt;/p&gt;\\n\\n&lt;p&gt;What&amp;#39;s your workload?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0y3a6","unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0y3a6/finetuning_leaderboard/n3d10lc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752625491,"author_flair_text":":X:","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"transparent","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n3d0lsk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Much-Contract-1397","can_mod_post":false,"created_utc":1752625346,"send_replies":true,"parent_id":"t3_1m0y3a6","score":3,"author_fullname":"t2_1iqvb5w490","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This is over a year old and they clearly state they will not be updating models so not really too relevant. Fine tuning is more a skill issue than a model issue too.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3d0lsk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is over a year old and they clearly state they will not be updating models so not really too relevant. Fine tuning is more a skill issue than a model issue too.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0y3a6/finetuning_leaderboard/n3d0lsk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752625346,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0y3a6","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"c07aa42e-51fe-11f0-afcc-462aad931709","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3fdz0u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"created_utc":1752664726,"send_replies":true,"parent_id":"t1_n3ez8b3","score":1,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah I don't use LORA for this reason.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3fdz0u","is_submitter":true,"downs":0,"author_flair_richtext":[{"a":":X:","u":"https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X","e":"emoji"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah I don&amp;#39;t use LORA for this reason.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0y3a6","unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0y3a6/finetuning_leaderboard/n3fdz0u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752664726,"author_flair_text":":X:","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"transparent","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ez8b3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Logical_Divide_3595","can_mod_post":false,"created_utc":1752657033,"send_replies":true,"parent_id":"t3_1m0y3a6","score":1,"author_fullname":"t2_18riberpl8","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The performance of LoRA is much worse than full parameter fine-tune in my tasks","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ez8b3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The performance of LoRA is much worse than full parameter fine-tune in my tasks&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0y3a6/finetuning_leaderboard/n3ez8b3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752657033,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0y3a6","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"c07aa42e-51fe-11f0-afcc-462aad931709","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3fe3tg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"created_utc":1752664784,"send_replies":true,"parent_id":"t1_n3ezu0q","score":1,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The leaderboard doesn't say but the paper says LORA, good question. I think I'll put together my own simple benchmark and post here.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3fe3tg","is_submitter":true,"downs":0,"author_flair_richtext":[{"a":":X:","u":"https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X","e":"emoji"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The leaderboard doesn&amp;#39;t say but the paper says LORA, good question. I think I&amp;#39;ll put together my own simple benchmark and post here.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0y3a6","unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0y3a6/finetuning_leaderboard/n3fe3tg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752664784,"author_flair_text":":X:","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"transparent","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ezu0q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"generaluser123","can_mod_post":false,"created_utc":1752657384,"send_replies":true,"parent_id":"t3_1m0y3a6","score":1,"author_fullname":"t2_127304","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Is this for full parameters fine-tuning or lora?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ezu0q","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Is this for full parameters fine-tuning or lora?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0y3a6/finetuning_leaderboard/n3ezu0q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752657384,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0y3a6","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),o=()=>e.jsx(a,{data:l});export{o as default};
