import{j as l}from"./index-DLSqWzaI.js";import{R as e}from"./RedditPostRenderer-CysRo2D_.js";import"./index-COXiL3Lo.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I have an intel arc graphics card and ai - npu , powered with intel core ultra 7-155H processor, with 16gb ram (though that this would be useful for doing ai work but i am regretting my deicision , i could have easily bought a gaming laptop with this money). Pls pls pls it would be so much better if anyone could help  \\nBut when running an ai model locally using ollama, it neither uses gpu nor npu , can someone else suggest any other service platform like ollama, where we can locally download and run ai model efficiently, as i want to train small 1b model with a .csv file .  \\nOr can anyone also suggest any other ways where i can use gpu, (i am an undergrad student).","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Locally Running AI model with Intel GPU","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m2furm","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.75,"author_flair_background_color":null,"subreddit_type":"public","ups":2,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1s30c7qod5","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":2,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752778183,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I have an intel arc graphics card and ai - npu , powered with intel core ultra 7-155H processor, with 16gb ram (though that this would be useful for doing ai work but i am regretting my deicision , i could have easily bought a gaming laptop with this money). Pls pls pls it would be so much better if anyone could help&lt;br/&gt;\\nBut when running an ai model locally using ollama, it neither uses gpu nor npu , can someone else suggest any other service platform like ollama, where we can locally download and run ai model efficiently, as i want to train small 1b model with a .csv file .&lt;br/&gt;\\nOr can anyone also suggest any other ways where i can use gpu, (i am an undergrad student).&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m2furm","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"dragonknight-18","discussion_type":null,"num_comments":7,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m2furm/locally_running_ai_model_with_intel_gpu/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m2furm/locally_running_ai_model_with_intel_gpu/","subreddit_subscribers":500897,"created_utc":1752778183,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3oo5sm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"clazifer","can_mod_post":false,"created_utc":1752779768,"send_replies":true,"parent_id":"t3_1m2furm","score":2,"author_fullname":"t2_63p8634f","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Try koboldCpp","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3oo5sm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Try koboldCpp&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2furm/locally_running_ai_model_with_intel_gpu/n3oo5sm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752779768,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2furm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3qhmam","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"EugenePopcorn","can_mod_post":false,"created_utc":1752800245,"send_replies":true,"parent_id":"t3_1m2furm","score":2,"author_fullname":"t2_g6hpxxgss","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you're on windows, you can try their experimental llama.cpp portable zip with NPU support:\\n\\n[https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/npu\\\\_quickstart.md](https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/npu_quickstart.md)\\n\\nOtherwise, the best way to use intel hardware is with their docker images. \\n\\n[https://github.com/intel/ipex-llm/blob/main/docker/llm/inference-cpp/README.md](https://github.com/intel/ipex-llm/blob/main/docker/llm/inference-cpp/README.md)\\n\\nAnd if all else fails, there's always koboldcpp with Vulkan support.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3qhmam","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you&amp;#39;re on windows, you can try their experimental llama.cpp portable zip with NPU support:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/npu_quickstart.md\\"&gt;https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/npu_quickstart.md&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Otherwise, the best way to use intel hardware is with their docker images. &lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/intel/ipex-llm/blob/main/docker/llm/inference-cpp/README.md\\"&gt;https://github.com/intel/ipex-llm/blob/main/docker/llm/inference-cpp/README.md&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;And if all else fails, there&amp;#39;s always koboldcpp with Vulkan support.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2furm/locally_running_ai_model_with_intel_gpu/n3qhmam/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752800245,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2furm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3pow62","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Thellton","can_mod_post":false,"created_utc":1752790501,"send_replies":true,"parent_id":"t3_1m2furm","score":1,"author_fullname":"t2_gzzli","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Use llamacpp (with either the SYCL or Vulkan backends EDIT: or the latest [IPEX](https://github.com/ipex-llm/ipex-llm/releases) build which is from before Qwen 3 and Qwen 3 MoE were integrated into Llamacpp) or koboldcpp (only Vulkan). If you need an ollama type end point specifically, use koboldcpp.","edited":1752810369,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3pow62","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Use llamacpp (with either the SYCL or Vulkan backends EDIT: or the latest &lt;a href=\\"https://github.com/ipex-llm/ipex-llm/releases\\"&gt;IPEX&lt;/a&gt; build which is from before Qwen 3 and Qwen 3 MoE were integrated into Llamacpp) or koboldcpp (only Vulkan). If you need an ollama type end point specifically, use koboldcpp.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2furm/locally_running_ai_model_with_intel_gpu/n3pow62/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752790501,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2furm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3r4amz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SkyFeistyLlama8","can_mod_post":false,"created_utc":1752808679,"send_replies":true,"parent_id":"t3_1m2furm","score":1,"author_fullname":"t2_1hgbaqgbnq","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think llama.cpp has limited OpenCL support for some Intel integrated GPUs. The NPU isn't used much or at all. I think only the Snapdragon X chips allow running of LLMs on their NPUs but you're limited to Microsoft-provided models.\\n\\nAs for training (or more likely finetuning), I have no idea if it's possible on a laptop integrated GPU. You might look at renting cloud GPUs for that.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3r4amz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think llama.cpp has limited OpenCL support for some Intel integrated GPUs. The NPU isn&amp;#39;t used much or at all. I think only the Snapdragon X chips allow running of LLMs on their NPUs but you&amp;#39;re limited to Microsoft-provided models.&lt;/p&gt;\\n\\n&lt;p&gt;As for training (or more likely finetuning), I have no idea if it&amp;#39;s possible on a laptop integrated GPU. You might look at renting cloud GPUs for that.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2furm/locally_running_ai_model_with_intel_gpu/n3r4amz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752808679,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2furm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3rh3yv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GPTshop_ai","can_mod_post":false,"created_utc":1752814172,"send_replies":true,"parent_id":"t3_1m2furm","score":0,"author_fullname":"t2_rkmud0isr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You can use a an axe to fell a tree. But smart people use a chainsaw.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3rh3yv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can use a an axe to fell a tree. But smart people use a chainsaw.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2furm/locally_running_ai_model_with_intel_gpu/n3rh3yv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752814172,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2furm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3rh4lq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"bumblebeargrey","can_mod_post":false,"created_utc":1752814181,"send_replies":true,"parent_id":"t3_1m2furm","score":1,"author_fullname":"t2_clm04xtf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"How much is your arc graphics card ?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3rh4lq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How much is your arc graphics card ?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2furm/locally_running_ai_model_with_intel_gpu/n3rh4lq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752814181,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2furm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3scax3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Fussy-Fur3608","can_mod_post":false,"created_utc":1752830939,"send_replies":true,"parent_id":"t3_1m2furm","score":1,"author_fullname":"t2_8383arktn","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"check out intel ai playground. it's pretty useful.   \\nas others have mentioned, IPEX-LLM is your way to run ollama. when you get it running, it might tell you it's in CPU mode and that's ok, it offloads to GPU as soon as you give it a job. that little nugget of info was hard earned.  \\nMy B580 does pretty good when you get the software right.  \\nrunning it on a linux box though is tough.  \\nI found intel's website is really hard to navigate, and at times, conflicting information.  \\nthe ipex-llm protable zip was the easiest way to make it work.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3scax3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;check out intel ai playground. it&amp;#39;s pretty useful.&lt;br/&gt;\\nas others have mentioned, IPEX-LLM is your way to run ollama. when you get it running, it might tell you it&amp;#39;s in CPU mode and that&amp;#39;s ok, it offloads to GPU as soon as you give it a job. that little nugget of info was hard earned.&lt;br/&gt;\\nMy B580 does pretty good when you get the software right.&lt;br/&gt;\\nrunning it on a linux box though is tough.&lt;br/&gt;\\nI found intel&amp;#39;s website is really hard to navigate, and at times, conflicting information.&lt;br/&gt;\\nthe ipex-llm protable zip was the easiest way to make it work.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2furm/locally_running_ai_model_with_intel_gpu/n3scax3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752830939,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2furm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>l.jsx(e,{data:a});export{n as default};
