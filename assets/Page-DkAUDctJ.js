import{j as e}from"./index-BpC9hjVs.js";import{R as l}from"./RedditPostRenderer-BEo6AnSR.js";import"./index-DwkJHX1_.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Dots\\n\\nMinimax\\n\\nHunyuan\\n\\nErnie\\n\\n\\nI’m not seeing much enthusiasm in the community for these models like there was for Qwen and Deepseek.\\n\\nSorry, just wanted to put this out here.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"No love for these new models?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lqh55j","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.92,"author_flair_background_color":null,"subreddit_type":"public","ups":190,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_jqxb4pte","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":190,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751519001,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Dots&lt;/p&gt;\\n\\n&lt;p&gt;Minimax&lt;/p&gt;\\n\\n&lt;p&gt;Hunyuan&lt;/p&gt;\\n\\n&lt;p&gt;Ernie&lt;/p&gt;\\n\\n&lt;p&gt;I’m not seeing much enthusiasm in the community for these models like there was for Qwen and Deepseek.&lt;/p&gt;\\n\\n&lt;p&gt;Sorry, just wanted to put this out here.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lqh55j","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"No_Conversation9561","discussion_type":null,"num_comments":63,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/","subreddit_subscribers":494198,"created_utc":1751519001,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n12uudn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"custodiam99","can_mod_post":false,"created_utc":1751521181,"send_replies":true,"parent_id":"t1_n12tqy6","score":24,"author_fullname":"t2_nqnhgqqf5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Dots q4 is good, but not as good as Qwen3 235b even at q3\\\\_k\\\\_m.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n12uudn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Dots q4 is good, but not as good as Qwen3 235b even at q3_k_m.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqh55j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n12uudn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751521181,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":24}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n17xkpe","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Hufflegguf","can_mod_post":false,"send_replies":true,"parent_id":"t1_n14lh9v","score":1,"author_fullname":"t2_1fzg0l5rrg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Look at ExLlamaV2 models. They’ll take advantage of your dual 3090s if you keep the entire model in VRAM. ExLlamaV3 is new and experimental","edited":false,"author_flair_css_class":null,"name":"t1_n17xkpe","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Look at ExLlamaV2 models. They’ll take advantage of your dual 3090s if you keep the entire model in VRAM. ExLlamaV3 is new and experimental&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lqh55j","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n17xkpe/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751586912,"author_flair_text":null,"collapsed":false,"created_utc":1751586912,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n14lh9v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Bpthewise","can_mod_post":false,"send_replies":true,"parent_id":"t1_n13fnxb","score":1,"author_fullname":"t2_7cnvv","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thanks this helped. I’m running two 3090’s and feel like I should be getting better performance with models that don’t get fully offloaded to a single GPU. What should I be using to run inference?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n14lh9v","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks this helped. I’m running two 3090’s and feel like I should be getting better performance with models that don’t get fully offloaded to a single GPU. What should I be using to run inference?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqh55j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n14lh9v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751550904,"author_flair_text":null,"treatment_tags":[],"created_utc":1751550904,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n13fnxb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Marksta","can_mod_post":false,"send_replies":true,"parent_id":"t1_n13147t","score":22,"author_fullname":"t2_559a1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"LM Studio is a GUI wrapper for llama.cpp. If it doesn't run in llama.cpp, it probably doesn't elsewhere. So no, if the goal is latest models then no. If the goal is concurrency or tensor parralel then the answer is yes you want to look at other options.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n13fnxb","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;LM Studio is a GUI wrapper for llama.cpp. If it doesn&amp;#39;t run in llama.cpp, it probably doesn&amp;#39;t elsewhere. So no, if the goal is latest models then no. If the goal is concurrency or tensor parralel then the answer is yes you want to look at other options.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqh55j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n13fnxb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751532832,"author_flair_text":null,"treatment_tags":[],"created_utc":1751532832,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":22}}],"before":null}},"user_reports":[],"saved":false,"id":"n13147t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Bpthewise","can_mod_post":false,"created_utc":1751524484,"send_replies":true,"parent_id":"t1_n12tqy6","score":7,"author_fullname":"t2_7cnvv","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This has been making me wonder if I need to branch out of LM Studio (I’m still new to this). I’m trying to learn more about running in parallel LM studio just makes it easy. Do I need to look to vLLM or others?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n13147t","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This has been making me wonder if I need to branch out of LM Studio (I’m still new to this). I’m trying to learn more about running in parallel LM studio just makes it easy. Do I need to look to vLLM or others?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqh55j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n13147t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751524484,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"d2642412-d9ce-11ed-ae30-32b11309f5bd","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"d2642412-d9ce-11ed-ae30-32b11309f5bd","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n15omfy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Klutzy-Snow8016","can_mod_post":false,"send_replies":true,"parent_id":"t1_n15mjb3","score":2,"author_fullname":"t2_1d5l610jz3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The last two times I tried, the model I wanted to use supported only tensor parallel but not pipeline parallel. Hmm, I'll have to try it again some time.","edited":false,"author_flair_css_class":null,"name":"t1_n15omfy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The last two times I tried, the model I wanted to use supported only tensor parallel but not pipeline parallel. Hmm, I&amp;#39;ll have to try it again some time.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lqh55j","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n15omfy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751562132,"author_flair_text":null,"collapsed":false,"created_utc":1751562132,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n15mjb3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ortegaalfredo","can_mod_post":false,"send_replies":true,"parent_id":"t1_n15hogg","score":2,"author_fullname":"t2_g177e","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"\\\\&gt; VLLM doesn't work well when you have multi GPU and weak PCIe connectivity\\n\\nI'm runing vllm/sglang/aphrodite engine on multi GPU ex-mining 1X PCIE 3.0 hardware, it works perfectly. Yes you can use odd numbers of GPUs with pipeline-parallel.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n15mjb3","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Alpaca"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&amp;gt; VLLM doesn&amp;#39;t work well when you have multi GPU and weak PCIe connectivity&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m runing vllm/sglang/aphrodite engine on multi GPU ex-mining 1X PCIE 3.0 hardware, it works perfectly. Yes you can use odd numbers of GPUs with pipeline-parallel.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqh55j","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n15mjb3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751561556,"author_flair_text":"Alpaca","treatment_tags":[],"created_utc":1751561556,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#bd9e9e","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n15hogg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Klutzy-Snow8016","can_mod_post":false,"send_replies":true,"parent_id":"t1_n15cvp9","score":5,"author_fullname":"t2_1d5l610jz3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Requiring full GPU offload is kind of a big deal, though, especially with these recent giant MoEs. And in my experience, VLLM doesn't work well when you have multi GPU and weak PCIe connectivity, like in a consumer box where some of the lanes are coming off the chipset. And you can't use odd numbers of GPUs above 1. I doubt mismatched GPUs would work, either, but haven't tried it. To be fair, I'm not familiar with sglang and don't know if it's better.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n15hogg","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Requiring full GPU offload is kind of a big deal, though, especially with these recent giant MoEs. And in my experience, VLLM doesn&amp;#39;t work well when you have multi GPU and weak PCIe connectivity, like in a consumer box where some of the lanes are coming off the chipset. And you can&amp;#39;t use odd numbers of GPUs above 1. I doubt mismatched GPUs would work, either, but haven&amp;#39;t tried it. To be fair, I&amp;#39;m not familiar with sglang and don&amp;#39;t know if it&amp;#39;s better.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqh55j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n15hogg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751560175,"author_flair_text":null,"treatment_tags":[],"created_utc":1751560175,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n15gyzy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Sudden-Lingonberry-8","can_mod_post":false,"send_replies":true,"parent_id":"t1_n15cvp9","score":4,"author_fullname":"t2_7j2k5hlp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"no llama.cpp no like","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n15gyzy","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;no llama.cpp no like&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqh55j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n15gyzy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751559971,"author_flair_text":null,"treatment_tags":[],"created_utc":1751559971,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n15cvp9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ortegaalfredo","can_mod_post":false,"created_utc":1751558801,"send_replies":true,"parent_id":"t1_n12tqy6","score":1,"author_fullname":"t2_g177e","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"\\\\&gt;  The other inference engines seem tailored for enterprise systems with serious GPUs and fast interconnects,\\n\\nNot really. VLLM and sglang are no harder to run than llama.cpp, however they require a GPU, but any GPU would do.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n15cvp9","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Alpaca"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&amp;gt;  The other inference engines seem tailored for enterprise systems with serious GPUs and fast interconnects,&lt;/p&gt;\\n\\n&lt;p&gt;Not really. VLLM and sglang are no harder to run than llama.cpp, however they require a GPU, but any GPU would do.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqh55j","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n15cvp9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751558801,"author_flair_text":"Alpaca","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bd9e9e","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n12tqy6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Klutzy-Snow8016","can_mod_post":false,"created_utc":1751520631,"send_replies":true,"parent_id":"t3_1lqh55j","score":142,"author_fullname":"t2_1d5l610jz3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It's hard to run them, since they're not supported in llama.cpp. The other inference engines seem tailored for enterprise systems with serious GPUs and fast interconnects, not gaming rigs with used GeForces wedged in.\\n\\nI did at least get Ernie 0.3B and 21B-A3B to run using the instructions on their github (using FastDeploy).\\n\\nAh, I just saw that Dots has GGUFs on Unsloth's huggingface page. Has anyone tried them yet?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n12tqy6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s hard to run them, since they&amp;#39;re not supported in llama.cpp. The other inference engines seem tailored for enterprise systems with serious GPUs and fast interconnects, not gaming rigs with used GeForces wedged in.&lt;/p&gt;\\n\\n&lt;p&gt;I did at least get Ernie 0.3B and 21B-A3B to run using the instructions on their github (using FastDeploy).&lt;/p&gt;\\n\\n&lt;p&gt;Ah, I just saw that Dots has GGUFs on Unsloth&amp;#39;s huggingface page. Has anyone tried them yet?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n12tqy6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751520631,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqh55j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":142}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"d2642412-d9ce-11ed-ae30-32b11309f5bd","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"2b12e2b8-fdc0-11ee-9a03-6e2f48afd456","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n12uxe6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"xXWarMachineRoXx","can_mod_post":false,"send_replies":true,"parent_id":"t1_n12sbqs","score":23,"author_fullname":"t2_6zhl6n94","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yea\\n\\nTha mamba jamba just… went by","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n12uxe6","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"Llama 3"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yea&lt;/p&gt;\\n\\n&lt;p&gt;Tha mamba jamba just… went by&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqh55j","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n12uxe6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751521221,"author_flair_text":"Llama 3","treatment_tags":[],"created_utc":1751521221,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#c7b594","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":23}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n14sdsf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FullOf_Bad_Ideas","can_mod_post":false,"send_replies":true,"parent_id":"t1_n142qqn","score":4,"author_fullname":"t2_9s7pmakgx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It's not the standard. Qwen2.5 and Qwen3 were the most well thought out releases in recent memory.\\n\\nSupposedly OpenAI's model is close, I wonder how well they'll do on that end.","edited":false,"author_flair_css_class":null,"name":"t1_n14sdsf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s not the standard. Qwen2.5 and Qwen3 were the most well thought out releases in recent memory.&lt;/p&gt;\\n\\n&lt;p&gt;Supposedly OpenAI&amp;#39;s model is close, I wonder how well they&amp;#39;ll do on that end.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lqh55j","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n14sdsf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751552987,"author_flair_text":null,"collapsed":false,"created_utc":1751552987,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n142qqn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ionizing","can_mod_post":false,"send_replies":true,"parent_id":"t1_n13tdwd","score":9,"author_fullname":"t2_6am0r","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I started getting into this stuff only a week before Qwen3 released and as a noob was extremely impressed with the ease with which I was able to immediately start learning and using Qwen3, thanks to their seemingly impressive documentation and commitment to the community. I am naive though, perhaps that is standard? Either way, I enjoy the qwen3 models a lot and just found out they have an 8b embedding version which I intend to us. I don't know what my point is, I need coffee.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n142qqn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I started getting into this stuff only a week before Qwen3 released and as a noob was extremely impressed with the ease with which I was able to immediately start learning and using Qwen3, thanks to their seemingly impressive documentation and commitment to the community. I am naive though, perhaps that is standard? Either way, I enjoy the qwen3 models a lot and just found out they have an 8b embedding version which I intend to us. I don&amp;#39;t know what my point is, I need coffee.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqh55j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n142qqn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751544386,"author_flair_text":null,"treatment_tags":[],"created_utc":1751544386,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}}],"before":null}},"user_reports":[],"saved":false,"id":"n13tdwd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"SkyFeistyLlama8","can_mod_post":false,"send_replies":true,"parent_id":"t1_n12sbqs","score":15,"author_fullname":"t2_1hgbaqgbnq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The Qwen and Google teams contributed code to get their models running on llama.cpp. Without that, forget it.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n13tdwd","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The Qwen and Google teams contributed code to get their models running on llama.cpp. Without that, forget it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqh55j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n13tdwd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751540339,"author_flair_text":null,"treatment_tags":[],"created_utc":1751540339,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":15}}],"before":null}},"user_reports":[],"saved":false,"id":"n12sbqs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"SomeOddCodeGuy","can_mod_post":false,"created_utc":1751519928,"send_replies":true,"parent_id":"t1_n12qp3f","score":73,"author_fullname":"t2_kle75fbd6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This happened before with Jamba and a few others. The release came and went, and by the time support was added to anything most folks could run, they had been surpassed by other models and left behind.\\n\\nLots of us are excited for the models... just can't do a lot with that excitement atm lol","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n12sbqs","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This happened before with Jamba and a few others. The release came and went, and by the time support was added to anything most folks could run, they had been surpassed by other models and left behind.&lt;/p&gt;\\n\\n&lt;p&gt;Lots of us are excited for the models... just can&amp;#39;t do a lot with that excitement atm lol&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqh55j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n12sbqs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751519928,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":73}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n142mgg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"getfitdotus","can_mod_post":false,"created_utc":1751544338,"send_replies":true,"parent_id":"t1_n12qp3f","score":2,"author_fullname":"t2_dst51dcb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Vllm PRs were merged","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n142mgg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Vllm PRs were merged&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqh55j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n142mgg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751544338,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n12qp3f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ortegaalfredo","can_mod_post":false,"created_utc":1751519140,"send_replies":true,"parent_id":"t3_1lqh55j","score":122,"author_fullname":"t2_g177e","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"There is a reason the Qwen team waited until the patches were merged to release qwen3.  \\n  \\nCurrently it's really hard to run them. I'm wating until VLLM/llama.cpp have support.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n12qp3f","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Alpaca"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There is a reason the Qwen team waited until the patches were merged to release qwen3.  &lt;/p&gt;\\n\\n&lt;p&gt;Currently it&amp;#39;s really hard to run them. I&amp;#39;m wating until VLLM/llama.cpp have support.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n12qp3f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751519140,"author_flair_text":"Alpaca","treatment_tags":[],"link_id":"t3_1lqh55j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bd9e9e","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":122}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n16qyvb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Qual_","can_mod_post":false,"send_replies":true,"parent_id":"t1_n14sg6h","score":4,"author_fullname":"t2_c3ca7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Gemma 3 27b is my champ. But I admit it's because Qwen sounds kinda weird in French. It seems coherent but sounds like... some stranger who learned the language, not like a native speaker.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n16qyvb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Gemma 3 27b is my champ. But I admit it&amp;#39;s because Qwen sounds kinda weird in French. It seems coherent but sounds like... some stranger who learned the language, not like a native speaker.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqh55j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n16qyvb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751573218,"author_flair_text":null,"treatment_tags":[],"created_utc":1751573218,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n14sg6h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FullOf_Bad_Ideas","can_mod_post":false,"send_replies":true,"parent_id":"t1_n14kp5w","score":6,"author_fullname":"t2_9s7pmakgx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"what worked for you then?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n14sg6h","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;what worked for you then?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqh55j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n14sg6h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751553006,"author_flair_text":null,"treatment_tags":[],"created_utc":1751553006,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n15np9b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ambitious-Most4485","can_mod_post":false,"send_replies":true,"parent_id":"t1_n14kp5w","score":2,"author_fullname":"t2_y8jr8mmod","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Im.curious as well","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n15np9b","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Im.curious as well&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqh55j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n15np9b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751561877,"author_flair_text":null,"treatment_tags":[],"created_utc":1751561877,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n14kp5w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Qual_","can_mod_post":false,"created_utc":1751550665,"send_replies":true,"parent_id":"t1_n13iwkf","score":6,"author_fullname":"t2_c3ca7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"that's how I deleted Qwen. :'(","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n14kp5w","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;that&amp;#39;s how I deleted Qwen. :&amp;#39;(&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqh55j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n14kp5w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751550665,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n13iwkf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Secure_Reflection409","can_mod_post":false,"created_utc":1751534742,"send_replies":true,"parent_id":"t3_1lqh55j","score":73,"author_fullname":"t2_by77ogdhr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Generally speaking, this is what happens:\\n\\n\\n1. Get excited about model.\\n2. Download model.\\n3. Throw an old prompt from history at model.\\n4. Compare output to what Qwen generated.\\n5. ...\\n6. Delete model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n13iwkf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Generally speaking, this is what happens:&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;Get excited about model.&lt;/li&gt;\\n&lt;li&gt;Download model.&lt;/li&gt;\\n&lt;li&gt;Throw an old prompt from history at model.&lt;/li&gt;\\n&lt;li&gt;Compare output to what Qwen generated.&lt;/li&gt;\\n&lt;li&gt;...&lt;/li&gt;\\n&lt;li&gt;Delete model.&lt;/li&gt;\\n&lt;/ol&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n13iwkf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751534742,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqh55j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":73}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n12qty8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"sunshinecheung","can_mod_post":false,"created_utc":1751519205,"send_replies":true,"parent_id":"t3_1lqh55j","score":54,"author_fullname":"t2_u398xzta","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"they dont support llama.cpp and no gguf","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n12qty8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;they dont support llama.cpp and no gguf&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n12qty8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751519205,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqh55j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":54}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n12r47y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"robberviet","can_mod_post":false,"created_utc":1751519344,"send_replies":true,"parent_id":"t3_1lqh55j","score":29,"author_fullname":"t2_jxc5a","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Not sure about others but just tried Ernie 4.5 yesterday, only 0.3B is supported on llama.cpp, not MoE yet. Most of the time, it's just that people cannot run them, or it's a weak model, not worth it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n12r47y","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not sure about others but just tried Ernie 4.5 yesterday, only 0.3B is supported on llama.cpp, not MoE yet. Most of the time, it&amp;#39;s just that people cannot run them, or it&amp;#39;s a weak model, not worth it.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n12r47y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751519344,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqh55j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":29}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n12zm5x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"TheRealMasonMac","can_mod_post":false,"created_utc":1751523673,"send_replies":true,"parent_id":"t1_n12sljl","score":7,"author_fullname":"t2_101haj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Minimax should've trained off a better base model IMO. The one they have is weak compared to what's out there now, probably because it was trained with less quality data than what's been developed since then.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n12zm5x","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Minimax should&amp;#39;ve trained off a better base model IMO. The one they have is weak compared to what&amp;#39;s out there now, probably because it was trained with less quality data than what&amp;#39;s been developed since then.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqh55j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n12zm5x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751523673,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n131lqm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1751524749,"send_replies":true,"parent_id":"t1_n12sljl","score":2,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"21b Ernie feels better than Qwen 3 30b but alas suffers from much worse instruction following in my tests.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n131lqm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;21b Ernie feels better than Qwen 3 30b but alas suffers from much worse instruction following in my tests.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqh55j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n131lqm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751524749,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n12y1fi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"palyer69","can_mod_post":false,"created_utc":1751522831,"send_replies":true,"parent_id":"t1_n12sljl","score":0,"author_fullname":"t2_11iqrlp64q","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"so erin is not better thn dsv3 can u tell more about ds vs erin 300b like ur comparision.. thax ","edited":1751523972,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n12y1fi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;so erin is not better thn dsv3 can u tell more about ds vs erin 300b like ur comparision.. thax &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqh55j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n12y1fi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751522831,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n12sljl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"True_Requirement_891","can_mod_post":false,"created_utc":1751520062,"send_replies":true,"parent_id":"t3_1lqh55j","score":35,"author_fullname":"t2_yfi9sqrzf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Ernie 300b-47b is better than Maverick. Worse than DeepSeek-V3-0324.\\n\\nMinimax is like 2.0 flash with reasoning slapped on top. And it's pretty meh... lacks deep Reasoning and comprehension even with 80k limit. The reasoning is very shallow. I'm surprised it's ranked higher than qwen3-235b reasoning. \\n\\nDidn't try hunyan or dots yet.\\n\\nTbh, nearly everything feels pointless except qwen or deepseek models.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n12sljl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ernie 300b-47b is better than Maverick. Worse than DeepSeek-V3-0324.&lt;/p&gt;\\n\\n&lt;p&gt;Minimax is like 2.0 flash with reasoning slapped on top. And it&amp;#39;s pretty meh... lacks deep Reasoning and comprehension even with 80k limit. The reasoning is very shallow. I&amp;#39;m surprised it&amp;#39;s ranked higher than qwen3-235b reasoning. &lt;/p&gt;\\n\\n&lt;p&gt;Didn&amp;#39;t try hunyan or dots yet.&lt;/p&gt;\\n\\n&lt;p&gt;Tbh, nearly everything feels pointless except qwen or deepseek models.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n12sljl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751520062,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqh55j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":35}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"2b12e2b8-fdc0-11ee-9a03-6e2f48afd456","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n13oqyc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Arkonias","can_mod_post":false,"created_utc":1751538002,"send_replies":true,"parent_id":"t3_1lqh55j","score":11,"author_fullname":"t2_6jyyd","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"No or poor llama.cpp support = no LM Studio/ollama support = no general adoption by the wider community.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n13oqyc","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 3"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No or poor llama.cpp support = no LM Studio/ollama support = no general adoption by the wider community.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n13oqyc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751538002,"author_flair_text":"Llama 3","treatment_tags":[],"link_id":"t3_1lqh55j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#c7b594","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n13rinx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"silenceimpaired","can_mod_post":false,"send_replies":true,"parent_id":"t1_n13qllv","score":2,"author_fullname":"t2_dissgzyl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Apparently I need to… though maybe I’m not handling the gguf splits correctly","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n13rinx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Apparently I need to… though maybe I’m not handling the gguf splits correctly&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqh55j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n13rinx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751539440,"author_flair_text":null,"treatment_tags":[],"created_utc":1751539440,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n13qllv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jacek2023","can_mod_post":false,"send_replies":true,"parent_id":"t1_n13pmbj","score":2,"author_fullname":"t2_vqgbql9w","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You don't use llama.cpp?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n13qllv","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You don&amp;#39;t use llama.cpp?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqh55j","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n13qllv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751538972,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1751538972,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n13pmbj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"silenceimpaired","can_mod_post":false,"created_utc":1751538459,"send_replies":true,"parent_id":"t1_n13ccjk","score":1,"author_fullname":"t2_dissgzyl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Hmm couldn’t get Dots working in Oobabooga","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n13pmbj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hmm couldn’t get Dots working in Oobabooga&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqh55j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n13pmbj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751538459,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n13ccjk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"jacek2023","can_mod_post":false,"created_utc":1751530908,"send_replies":true,"parent_id":"t3_1lqh55j","score":9,"author_fullname":"t2_vqgbql9w","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Dots is great.\\n\\nHunyuan is work in progress in llama.cpp.\\n\\nThere is no support for Ernie in llama.cpp yet.\\n\\nMinimax is too big to use.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n13ccjk","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Dots is great.&lt;/p&gt;\\n\\n&lt;p&gt;Hunyuan is work in progress in llama.cpp.&lt;/p&gt;\\n\\n&lt;p&gt;There is no support for Ernie in llama.cpp yet.&lt;/p&gt;\\n\\n&lt;p&gt;Minimax is too big to use.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n13ccjk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751530908,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lqh55j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n13wwxo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AltruisticList6000","can_mod_post":false,"created_utc":1751541958,"send_replies":true,"parent_id":"t1_n12yewr","score":4,"author_fullname":"t2_hnjq9xn4a","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes that's why Qwen team is doing it right, they always have a wide selection of model sizes and they themselves upload official ggufs aswell, that not many (if any) other developers do that. Same with Ace-step when they provided an official webui for it and there is also comfyui support.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n13wwxo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes that&amp;#39;s why Qwen team is doing it right, they always have a wide selection of model sizes and they themselves upload official ggufs aswell, that not many (if any) other developers do that. Same with Ace-step when they provided an official webui for it and there is also comfyui support.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqh55j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n13wwxo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751541958,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n12yewr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"kironlau","can_mod_post":false,"created_utc":1751523031,"send_replies":true,"parent_id":"t3_1lqh55j","score":37,"author_fullname":"t2_tb0dz2ds","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If an LLM company wants the open-source community to champion their models, they need to make things easy for that community. This means offering a variety of model sizes (including distilled versions) and providing early support for formats like GGUF—ideally by sharing structural details with projects like *llama.cpp* at least a week before launch.\\n\\nOn the flip side, if a company open-sources a model primarily for SMEs or enterprise users, they may only release it in formats like *safetensors*, assuming the community won’t need broader compatibility. But this approach often results in low traction among open-source users, meaning the models don’t build the momentum needed to be seen as truly competitive.\\n\\nThere’s no such thing as a free lunch. LLM companies aren’t doing this out of charity—they open-source their models to cultivate community support, grow their ecosystem, gather feedback from free users, and boost their brand reputation.\\n\\nAs open-source users, we get access to free (license-dependent) models. In return, developers benefit from real-world usage and exposure. It’s a mutually beneficial strategy—when done right.\\n\\nAt the end of the day, most of us are just end users, not engineers—whether we paid for the model or not. If an LLM isn’t easy to set up, it’s like an app with a bad user experience. The better companies aren’t just showing off benchmark scores and research papers—they’re thinking strategically about how to make a real impact on the community and the market.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n12yewr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If an LLM company wants the open-source community to champion their models, they need to make things easy for that community. This means offering a variety of model sizes (including distilled versions) and providing early support for formats like GGUF—ideally by sharing structural details with projects like &lt;em&gt;llama.cpp&lt;/em&gt; at least a week before launch.&lt;/p&gt;\\n\\n&lt;p&gt;On the flip side, if a company open-sources a model primarily for SMEs or enterprise users, they may only release it in formats like &lt;em&gt;safetensors&lt;/em&gt;, assuming the community won’t need broader compatibility. But this approach often results in low traction among open-source users, meaning the models don’t build the momentum needed to be seen as truly competitive.&lt;/p&gt;\\n\\n&lt;p&gt;There’s no such thing as a free lunch. LLM companies aren’t doing this out of charity—they open-source their models to cultivate community support, grow their ecosystem, gather feedback from free users, and boost their brand reputation.&lt;/p&gt;\\n\\n&lt;p&gt;As open-source users, we get access to free (license-dependent) models. In return, developers benefit from real-world usage and exposure. It’s a mutually beneficial strategy—when done right.&lt;/p&gt;\\n\\n&lt;p&gt;At the end of the day, most of us are just end users, not engineers—whether we paid for the model or not. If an LLM isn’t easy to set up, it’s like an app with a bad user experience. The better companies aren’t just showing off benchmark scores and research papers—they’re thinking strategically about how to make a real impact on the community and the market.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n12yewr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751523031,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqh55j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":37}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n134mcl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kironlau","can_mod_post":false,"created_utc":1751526421,"send_replies":true,"parent_id":"t1_n12v9wz","score":3,"author_fullname":"t2_tb0dz2ds","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"baidu，the company of Ernie models，is  bad reputated in China.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n134mcl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;baidu，the company of Ernie models，is  bad reputated in China.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqh55j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n134mcl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751526421,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n12v9wz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Conscious_Cut_6144","can_mod_post":false,"created_utc":1751521399,"send_replies":true,"parent_id":"t3_1lqh55j","score":6,"author_fullname":"t2_9hl4ymvj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Looking forward to trying Ernie,\\nSo far options for big open multimodal models has been very limited.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n12v9wz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Looking forward to trying Ernie,\\nSo far options for big open multimodal models has been very limited.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n12v9wz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751521399,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqh55j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n13a6bb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"nmkd","can_mod_post":false,"created_utc":1751529632,"send_replies":true,"parent_id":"t3_1lqh55j","score":8,"author_fullname":"t2_rg6rx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"No GGUF","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n13a6bb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No GGUF&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n13a6bb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751529632,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqh55j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n131r90","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1751524834,"send_replies":true,"parent_id":"t1_n130qpd","score":5,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah among the latest only GLM-4 is a pleasant surprise, still has too many quirks.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n131r90","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah among the latest only GLM-4 is a pleasant surprise, still has too many quirks.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqh55j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n131r90/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751524834,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n13pl60","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Zc5Gwu","can_mod_post":false,"created_utc":1751538443,"send_replies":true,"parent_id":"t1_n130qpd","score":3,"author_fullname":"t2_67qrvlir","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah, and often the “improvements” are only on paper.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n13pl60","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, and often the “improvements” are only on paper.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqh55j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n13pl60/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751538443,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n130qpd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"IngwiePhoenix","can_mod_post":false,"created_utc":1751524277,"send_replies":true,"parent_id":"t3_1lqh55j","score":11,"author_fullname":"t2_2c8xodlx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Fatigue. There's been constant model drops for so long, constant \\"best in class\\" and \\"beating all the others\\" and also \\"benchmarked top in X\\".\\n\\nIt's tiring, and in most cases, the improvements are minimal. Sure, there are shining stars, but, honestly, I have just settled with DeepSeek R1 and Gemma... and honestly don't see a big point in why I should \\"care\\" (pay attention, spend time) on \\"just another finetine\\".\\n\\nI don't mean that in a bad or negative way - just in a...saturated way. o.o I just wanna do stuff, not sit down and read _yet another_ announcement post with the most bold marketing claims of all times... x) I'd rather read the menu of a new burger restaurant and place an order instead.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n130qpd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Fatigue. There&amp;#39;s been constant model drops for so long, constant &amp;quot;best in class&amp;quot; and &amp;quot;beating all the others&amp;quot; and also &amp;quot;benchmarked top in X&amp;quot;.&lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s tiring, and in most cases, the improvements are minimal. Sure, there are shining stars, but, honestly, I have just settled with DeepSeek R1 and Gemma... and honestly don&amp;#39;t see a big point in why I should &amp;quot;care&amp;quot; (pay attention, spend time) on &amp;quot;just another finetine&amp;quot;.&lt;/p&gt;\\n\\n&lt;p&gt;I don&amp;#39;t mean that in a bad or negative way - just in a...saturated way. o.o I just wanna do stuff, not sit down and read &lt;em&gt;yet another&lt;/em&gt; announcement post with the most bold marketing claims of all times... x) I&amp;#39;d rather read the menu of a new burger restaurant and place an order instead.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n130qpd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751524277,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqh55j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n13cnzx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Admirable-Star7088","can_mod_post":false,"created_utc":1751531094,"send_replies":true,"parent_id":"t3_1lqh55j","score":5,"author_fullname":"t2_qhlcbiy3k","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I've been playing around with Dots at Q4\\\\_K\\\\_XL a bit, and it's one of those models that gives me mixed feelings. It's super-impressive at times, one of the best performing models I've ever used locally, but unimpressive other times, worse than much smaller models at 20b-30b.\\n\\nBecause Dots is pretty large at a total of 142b parameters, I get the impression that it \\"brute force\\" intelligence with its vast knowledge base. I find Mistral Small 3.2 (24b) to be actually more intelligent with prompts that require more logic and less knowledge.","edited":1751531606,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n13cnzx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve been playing around with Dots at Q4_K_XL a bit, and it&amp;#39;s one of those models that gives me mixed feelings. It&amp;#39;s super-impressive at times, one of the best performing models I&amp;#39;ve ever used locally, but unimpressive other times, worse than much smaller models at 20b-30b.&lt;/p&gt;\\n\\n&lt;p&gt;Because Dots is pretty large at a total of 142b parameters, I get the impression that it &amp;quot;brute force&amp;quot; intelligence with its vast knowledge base. I find Mistral Small 3.2 (24b) to be actually more intelligent with prompts that require more logic and less knowledge.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n13cnzx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751531094,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqh55j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n13emd5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ttkciar","can_mod_post":false,"created_utc":1751532228,"send_replies":true,"parent_id":"t3_1lqh55j","score":6,"author_fullname":"t2_cpegz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'm still plumbing the potential of Tulu3, OLMo2, Phi4, and Qwen3.\\n\\nWhen there are GGUFs and I can evaluate these new models with llama.cpp, maybe I'll play with them, too.  Until then my hands are full.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n13emd5","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m still plumbing the potential of Tulu3, OLMo2, Phi4, and Qwen3.&lt;/p&gt;\\n\\n&lt;p&gt;When there are GGUFs and I can evaluate these new models with llama.cpp, maybe I&amp;#39;ll play with them, too.  Until then my hands are full.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n13emd5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751532228,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lqh55j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n13229g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"dobomex761604","can_mod_post":false,"created_utc":1751525000,"send_replies":true,"parent_id":"t3_1lqh55j","score":3,"author_fullname":"t2_o0bqaw8dp","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Dots is very interesting, at least from their demo on Huggingface (not-so-generic writing style, responses felt original), but it's too large for most users. Waiting for Ernie to be added to llama.cpp, plus Mamba models are finally available there.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n13229g","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Dots is very interesting, at least from their demo on Huggingface (not-so-generic writing style, responses felt original), but it&amp;#39;s too large for most users. Waiting for Ernie to be added to llama.cpp, plus Mamba models are finally available there.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n13229g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751525000,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqh55j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n13rhaw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1751539421,"send_replies":true,"parent_id":"t3_1lqh55j","score":4,"author_fullname":"t2_h5utwre7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"1. Dots - Not better than 235b which I already have. Where benefit?\\n\\n2. Minmax - No support in ik_llama. No free api sans \\"sign in with google\\"\\n\\n3. Hunyuan - Maybe I'll try it. Support still spotty. Samples from people who did say it's extra censored unlike the video model. Kills enthusiasm. Active 13(!)B...\\n\\n4. Ernie - Waiting for this because it's a \\"lighter\\" deepseek and has a vision version. Probably the best out of the bunch. There's smaller versions too. All excitement rests here.\\n\\n\\nMany of these are MoE and larger than my vram so they'll require hybrid inference. Hunyuan and dots have low active parameter counts. Tell me again why I should use them over a larger dense model or existing solutions. \\n\\nSupposedly some of them are stem and safetymaxxed. Yay.. more of *those* kinds of models. Absolutely look forward to chinese characters in my replies and fighting off refusals. Bonus points for no cultural knowledge. In this case \\"just rag it\\", even if it worked perfectly, would require reprocessing the context.\\n\\nIf some of these were free on say openrouter, even for a limited time, more people could try them and push for engines to support them. There would be some hype among those with larger rigs. As it stands, they're going to go out with a whimper.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n13rhaw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;ol&gt;\\n&lt;li&gt;&lt;p&gt;Dots - Not better than 235b which I already have. Where benefit?&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Minmax - No support in ik_llama. No free api sans &amp;quot;sign in with google&amp;quot;&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Hunyuan - Maybe I&amp;#39;ll try it. Support still spotty. Samples from people who did say it&amp;#39;s extra censored unlike the video model. Kills enthusiasm. Active 13(!)B...&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Ernie - Waiting for this because it&amp;#39;s a &amp;quot;lighter&amp;quot; deepseek and has a vision version. Probably the best out of the bunch. There&amp;#39;s smaller versions too. All excitement rests here.&lt;/p&gt;&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;Many of these are MoE and larger than my vram so they&amp;#39;ll require hybrid inference. Hunyuan and dots have low active parameter counts. Tell me again why I should use them over a larger dense model or existing solutions. &lt;/p&gt;\\n\\n&lt;p&gt;Supposedly some of them are stem and safetymaxxed. Yay.. more of &lt;em&gt;those&lt;/em&gt; kinds of models. Absolutely look forward to chinese characters in my replies and fighting off refusals. Bonus points for no cultural knowledge. In this case &amp;quot;just rag it&amp;quot;, even if it worked perfectly, would require reprocessing the context.&lt;/p&gt;\\n\\n&lt;p&gt;If some of these were free on say openrouter, even for a limited time, more people could try them and push for engines to support them. There would be some hype among those with larger rigs. As it stands, they&amp;#39;re going to go out with a whimper.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n13rhaw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751539421,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqh55j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n14y17o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FunnyAsparagus1253","can_mod_post":false,"send_replies":true,"parent_id":"t1_n14tfw7","score":2,"author_fullname":"t2_i6c8tay3w","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah, thanks! I got this btw:\\n\\nhttps://preview.redd.it/ghpt8p54boaf1.jpeg?width=1170&amp;format=pjpg&amp;auto=webp&amp;s=10e964a104ad9d00c6eccb94581bd38e8d622ab2\\n\\nThat is *not* your average RP model 😅👀👍","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n14y17o","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, thanks! I got this btw:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/ghpt8p54boaf1.jpeg?width=1170&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=10e964a104ad9d00c6eccb94581bd38e8d622ab2\\"&gt;https://preview.redd.it/ghpt8p54boaf1.jpeg?width=1170&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=10e964a104ad9d00c6eccb94581bd38e8d622ab2&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;That is &lt;em&gt;not&lt;/em&gt; your average RP model 😅👀👍&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqh55j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n14y17o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751554624,"media_metadata":{"ghpt8p54boaf1":{"status":"valid","e":"Image","m":"image/jpeg","p":[{"y":216,"x":108,"u":"https://preview.redd.it/ghpt8p54boaf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d95fb66def2259fe1276daa4096782ad503827d3"},{"y":432,"x":216,"u":"https://preview.redd.it/ghpt8p54boaf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=74fdf1c35823bd20607522cec37c8408e695e662"},{"y":640,"x":320,"u":"https://preview.redd.it/ghpt8p54boaf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4cdb949ac884ed092f79b6afecffe6346425bd3a"},{"y":1280,"x":640,"u":"https://preview.redd.it/ghpt8p54boaf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a7a22118bfdfd54b25fda657e871c02a94c2d9ca"},{"y":1920,"x":960,"u":"https://preview.redd.it/ghpt8p54boaf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=48fb6d7eadd42da6eccc97e0d63a1d8f01f5bb8b"},{"y":2160,"x":1080,"u":"https://preview.redd.it/ghpt8p54boaf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=14db16c29b01a82b5ce11681b8ff0aa5e5af8210"}],"s":{"y":29832,"x":1170,"u":"https://preview.redd.it/ghpt8p54boaf1.jpeg?width=1170&amp;format=pjpg&amp;auto=webp&amp;s=10e964a104ad9d00c6eccb94581bd38e8d622ab2"},"id":"ghpt8p54boaf1"}},"author_flair_text":null,"treatment_tags":[],"created_utc":1751554624,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n14tfw7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullOf_Bad_Ideas","can_mod_post":false,"created_utc":1751553294,"send_replies":true,"parent_id":"t1_n13m5g1","score":2,"author_fullname":"t2_9s7pmakgx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Minimax is hosted on OpenRouter and there's no 25 usd minimum spend there, I was able to start with $5 top up. I hope this helps!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n14tfw7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Minimax is hosted on OpenRouter and there&amp;#39;s no 25 usd minimum spend there, I was able to start with $5 top up. I hope this helps!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqh55j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n14tfw7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751553294,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n13m5g1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FunnyAsparagus1253","can_mod_post":false,"created_utc":1751536600,"send_replies":true,"parent_id":"t3_1lqh55j","score":2,"author_fullname":"t2_i6c8tay3w","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Minimax way too big for me to self host, but I’m enthusiastic about it because its history is *interesting*. Afaik the company started off as a character.ai type app called Talkie, and it’s a model available on the app, though they don’t say the name. I figure it’s surely trained on a lot of that proprietary roleplay data, and it’s *their flagship model for that app*, so for people interested in *social* AI, and not just one scoring highest in MMLU, it is surely worth checking out. I would have bought some API credit already if it wasn’t a $25 minimum spend…","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n13m5g1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Minimax way too big for me to self host, but I’m enthusiastic about it because its history is &lt;em&gt;interesting&lt;/em&gt;. Afaik the company started off as a character.ai type app called Talkie, and it’s a model available on the app, though they don’t say the name. I figure it’s surely trained on a lot of that proprietary roleplay data, and it’s &lt;em&gt;their flagship model for that app&lt;/em&gt;, so for people interested in &lt;em&gt;social&lt;/em&gt; AI, and not just one scoring highest in MMLU, it is surely worth checking out. I would have bought some API credit already if it wasn’t a $25 minimum spend…&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n13m5g1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751536600,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqh55j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n13qmyz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AtomicProgramming","can_mod_post":false,"created_utc":1751538992,"send_replies":true,"parent_id":"t3_1lqh55j","score":2,"author_fullname":"t2_ymwtrfoeo","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I finally got the dots base model at I think Q4\\\\_K\\\\_M running with partial offloading and I'm happy to have it, a little hard to direct sometimes (maybe in its nature, maybe something about how I'm running it) but gets pretty interesting sometimes when investigating weird things. There was some bug with trying to put the embedding layer on the GPU and I had to leave that on the CPU, and I had to quantize the KV cache to get anything resembling decent speeds.\\n\\nEdit: 128GB RAM / 24GB vRAM with about 10 layers fully offloaded, and all the shared ones except the embedding layer IIRC, if you're trying to run either dots model on a similar setup. Possible I could have gotten Q5-something running, also, but I stuck with the one I got working.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n13qmyz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I finally got the dots base model at I think Q4_K_M running with partial offloading and I&amp;#39;m happy to have it, a little hard to direct sometimes (maybe in its nature, maybe something about how I&amp;#39;m running it) but gets pretty interesting sometimes when investigating weird things. There was some bug with trying to put the embedding layer on the GPU and I had to leave that on the CPU, and I had to quantize the KV cache to get anything resembling decent speeds.&lt;/p&gt;\\n\\n&lt;p&gt;Edit: 128GB RAM / 24GB vRAM with about 10 layers fully offloaded, and all the shared ones except the embedding layer IIRC, if you&amp;#39;re trying to run either dots model on a similar setup. Possible I could have gotten Q5-something running, also, but I stuck with the one I got working.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n13qmyz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751538992,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqh55j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n142d98","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"OGScottingham","can_mod_post":false,"created_utc":1751544236,"send_replies":true,"parent_id":"t3_1lqh55j","score":2,"author_fullname":"t2_5p0rvz5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'm personally looking forward to IBM's Granite 4.0 release. They said it'd be out this summer. 🤞","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n142d98","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m personally looking forward to IBM&amp;#39;s Granite 4.0 release. They said it&amp;#39;d be out this summer. 🤞&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n142d98/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751544236,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqh55j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n14pnll","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullOf_Bad_Ideas","can_mod_post":false,"created_utc":1751552179,"send_replies":true,"parent_id":"t3_1lqh55j","score":2,"author_fullname":"t2_9s7pmakgx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt;Dots\\n\\nToo big to run locally.\\n\\n&gt;Minimax\\n\\nEven bigger, it wasn't anything impressive when quickly testing on OpenRouter.\\n\\n&gt;Hunyuan\\n\\nI can't get 4-bit GPTQ quant to work in vLLM/SGLang, but it's interesting. From quick testing on rented H100s, it's noticeably worse than Qwen3 32B unfortunately at coding tasks.\\n\\n&gt;Ernie\\n\\nSo far I think it's only supported in their FastDeploy inference stack. Interesting architecture design and plenty of choices size-wise, definitely could be a competition to Qwen3 30B A3B.\\n\\nI'll add Pangu Pro, I made a post about it a few days ago, and it's similar to Hunyuan. For now, inference works only on Ascend NPUs. I don't have one on me, so I can't run it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n14pnll","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Dots&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Too big to run locally.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Minimax&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Even bigger, it wasn&amp;#39;t anything impressive when quickly testing on OpenRouter.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Hunyuan&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I can&amp;#39;t get 4-bit GPTQ quant to work in vLLM/SGLang, but it&amp;#39;s interesting. From quick testing on rented H100s, it&amp;#39;s noticeably worse than Qwen3 32B unfortunately at coding tasks.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Ernie&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;So far I think it&amp;#39;s only supported in their FastDeploy inference stack. Interesting architecture design and plenty of choices size-wise, definitely could be a competition to Qwen3 30B A3B.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ll add Pangu Pro, I made a post about it a few days ago, and it&amp;#39;s similar to Hunyuan. For now, inference works only on Ascend NPUs. I don&amp;#39;t have one on me, so I can&amp;#39;t run it.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n14pnll/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751552179,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqh55j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n16jxkt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"IngenuityNo1411","can_mod_post":false,"created_utc":1751571120,"send_replies":true,"parent_id":"t1_n16j61m","score":1,"author_fullname":"t2_a7vt64a9","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Dare to say, for open source models now they better either go big (be API only for most people, but valuable if can compete in top tiers with those top close models, like what DeepSeek does) or go home (tiny, local friendly models with specific strengths like Jan-nano and qwen3-30b). Everything on the middle ground looks not that appealing...","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n16jxkt","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Dare to say, for open source models now they better either go big (be API only for most people, but valuable if can compete in top tiers with those top close models, like what DeepSeek does) or go home (tiny, local friendly models with specific strengths like Jan-nano and qwen3-30b). Everything on the middle ground looks not that appealing...&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqh55j","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n16jxkt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751571120,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n16j61m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"IngenuityNo1411","can_mod_post":false,"created_utc":1751570888,"send_replies":true,"parent_id":"t3_1lqh55j","score":2,"author_fullname":"t2_a7vt64a9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I tested Minimax and Ernie with my creative writing cases then found out they are super bad at following instructions, tend to write something very \\"safe for public and commercial scenario\\" with slops... Maybe not wrong of them, just newer top tier models raised the baseline too high (new r1, gemini 2.5 pro, claude 4 opus,...) and most models won't catch them in this case. But I'm afraid they won't be great at other use cases either...","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n16j61m","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I tested Minimax and Ernie with my creative writing cases then found out they are super bad at following instructions, tend to write something very &amp;quot;safe for public and commercial scenario&amp;quot; with slops... Maybe not wrong of them, just newer top tier models raised the baseline too high (new r1, gemini 2.5 pro, claude 4 opus,...) and most models won&amp;#39;t catch them in this case. But I&amp;#39;m afraid they won&amp;#39;t be great at other use cases either...&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n16j61m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751570888,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lqh55j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n13bkgt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Marksta","can_mod_post":false,"created_utc":1751530453,"send_replies":true,"parent_id":"t3_1lqh55j","score":2,"author_fullname":"t2_559a1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"/u/No_Conversation9561 did you try them yourself? How's your love going for them?\\n\\nThe community is very excited for things they can run, or even just things they can quantize to hell and back to make it fit and run. Most of the models you listed don't run.\\n\\nI left [my review of Hunyuan](https://www.reddit.com/r/LocalLLaMA/comments/1lpl3mv/hosting_your_local_huanyuan_a13b_moe/n0watm0/), very interested in it but the guys have been working hard at trying to get it going for a week now and it's not there yet. Didn't try Dots yet myself, Ernie and Minimax don't run. Dots is sandwiched in Qwen3-A235B and Deepseek model sizes, I haven't seen much talk about it but if it doesn't perform competitively with them then it's definitely not going to be much talk on it. Also, doesn't help this community got locked down during its release, tough catch on that part.\\n\\nGLM4, Devstral, and Magistral were definitely received with some excitement. You can just click on [Unsloth's recently updated models](https://huggingface.co/unsloth/models) list to see what's going on and can be run. Speaking of, that DeepSeek-TNG-R1T2-Chimera is looking tasty.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n13bkgt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"/u/No_Conversation9561\\"&gt;/u/No_Conversation9561&lt;/a&gt; did you try them yourself? How&amp;#39;s your love going for them?&lt;/p&gt;\\n\\n&lt;p&gt;The community is very excited for things they can run, or even just things they can quantize to hell and back to make it fit and run. Most of the models you listed don&amp;#39;t run.&lt;/p&gt;\\n\\n&lt;p&gt;I left &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1lpl3mv/hosting_your_local_huanyuan_a13b_moe/n0watm0/\\"&gt;my review of Hunyuan&lt;/a&gt;, very interested in it but the guys have been working hard at trying to get it going for a week now and it&amp;#39;s not there yet. Didn&amp;#39;t try Dots yet myself, Ernie and Minimax don&amp;#39;t run. Dots is sandwiched in Qwen3-A235B and Deepseek model sizes, I haven&amp;#39;t seen much talk about it but if it doesn&amp;#39;t perform competitively with them then it&amp;#39;s definitely not going to be much talk on it. Also, doesn&amp;#39;t help this community got locked down during its release, tough catch on that part.&lt;/p&gt;\\n\\n&lt;p&gt;GLM4, Devstral, and Magistral were definitely received with some excitement. You can just click on &lt;a href=\\"https://huggingface.co/unsloth/models\\"&gt;Unsloth&amp;#39;s recently updated models&lt;/a&gt; list to see what&amp;#39;s going on and can be run. Speaking of, that DeepSeek-TNG-R1T2-Chimera is looking tasty.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n13bkgt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751530453,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqh55j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n14acv7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Zestyclose_Yak_3174","can_mod_post":false,"created_utc":1751547213,"send_replies":true,"parent_id":"t3_1lqh55j","score":1,"author_fullname":"t2_o0jgdhlij","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Both the lack of inference software support (and by extension the lack of the original developers to make code to run them) and the fact that many reported increased levels of censorship for these new Chinese models make it a non-ideal combination.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n14acv7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Both the lack of inference software support (and by extension the lack of the original developers to make code to run them) and the fact that many reported increased levels of censorship for these new Chinese models make it a non-ideal combination.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n14acv7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751547213,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqh55j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n150rv1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ulterior-Motive_","can_mod_post":false,"created_utc":1751555401,"send_replies":true,"parent_id":"t3_1lqh55j","score":1,"author_fullname":"t2_127atw4awd","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I haven't gotten around to testing Dots even though I have it downloaded, that's on me. Everything else falls victim to a corollary of \\"no local no care\\"; \\"no gguf no care\\". They sound awesome! But I don't have an easy way to run them that I'm used to.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n150rv1","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I haven&amp;#39;t gotten around to testing Dots even though I have it downloaded, that&amp;#39;s on me. Everything else falls victim to a corollary of &amp;quot;no local no care&amp;quot;; &amp;quot;no gguf no care&amp;quot;. They sound awesome! But I don&amp;#39;t have an easy way to run them that I&amp;#39;m used to.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n150rv1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751555401,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lqh55j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n15zqpx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kevin_1994","can_mod_post":false,"created_utc":1751565207,"send_replies":true,"parent_id":"t3_1lqh55j","score":1,"author_fullname":"t2_o015g","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The only one easily runnable is dots. I tried it and was unimpressed compared to qwen3 32b q8xl. It passes the vibe check but its not very good at reasoning.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n15zqpx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The only one easily runnable is dots. I tried it and was unimpressed compared to qwen3 32b q8xl. It passes the vibe check but its not very good at reasoning.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n15zqpx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751565207,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqh55j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n13aszn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"nmkd","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1318y2","score":6,"author_fullname":"t2_rg6rx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"EU is the reason Apple finally uses USB-C on their phones\\n\\nEU is the reason I don't need a passport to travel\\n\\nEU is the reason even the biggest tech companies are forced to provide you with all data collected from you\\n\\nAnd if you mean the countries themselves, uh, at least we're not busy bombing others","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n13aszn","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;EU is the reason Apple finally uses USB-C on their phones&lt;/p&gt;\\n\\n&lt;p&gt;EU is the reason I don&amp;#39;t need a passport to travel&lt;/p&gt;\\n\\n&lt;p&gt;EU is the reason even the biggest tech companies are forced to provide you with all data collected from you&lt;/p&gt;\\n\\n&lt;p&gt;And if you mean the countries themselves, uh, at least we&amp;#39;re not busy bombing others&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqh55j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n13aszn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751530005,"author_flair_text":null,"treatment_tags":[],"created_utc":1751530005,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n132ikg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"thirteen-bit","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1318y2","score":-1,"author_fullname":"t2_9l12dgc5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm not saying that EU is all roses and a beacon of productivity, lol.\\n\\nThere are specific provisions in the AI act for models meant for research and non-professional purposes and if these provisions are not used and instead there is just a blanket ban this probably means that some sketchy data was used in training?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n132ikg","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m not saying that EU is all roses and a beacon of productivity, lol.&lt;/p&gt;\\n\\n&lt;p&gt;There are specific provisions in the AI act for models meant for research and non-professional purposes and if these provisions are not used and instead there is just a blanket ban this probably means that some sketchy data was used in training?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqh55j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n132ikg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751525247,"author_flair_text":null,"treatment_tags":[],"created_utc":1751525247,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1318y2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"beijinghouse","can_mod_post":false,"created_utc":1751524556,"send_replies":true,"parent_id":"t1_n12ylqy","score":-17,"author_fullname":"t2_1osh4bmmz9","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"\\"Anecdotal evidence shows that models forbidding use in EU are immediately becoming trash\\" &lt;&lt;--- lol wut?\\n\\nEU = backwater of retired 70+ year old + unemployable Somali peasants + 55 year old nitwit legislators who are computers illiterate.\\n\\nWhy bend over backwards to support unproductive, useless people who aren't even ambitious enough to leave the EU?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1318y2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&amp;quot;Anecdotal evidence shows that models forbidding use in EU are immediately becoming trash&amp;quot; &amp;lt;&amp;lt;--- lol wut?&lt;/p&gt;\\n\\n&lt;p&gt;EU = backwater of retired 70+ year old + unemployable Somali peasants + 55 year old nitwit legislators who are computers illiterate.&lt;/p&gt;\\n\\n&lt;p&gt;Why bend over backwards to support unproductive, useless people who aren&amp;#39;t even ambitious enough to leave the EU?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqh55j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n1318y2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751524556,"author_flair_text":null,"treatment_tags":[],"collapsed":true,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-17}}],"before":null}},"user_reports":[],"saved":false,"id":"n12ylqy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"thirteen-bit","can_mod_post":false,"created_utc":1751523132,"send_replies":true,"parent_id":"t3_1lqh55j","score":-7,"author_fullname":"t2_9l12dgc5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":true,"body":"\\n\\n\\\\- Dots: ~~found nothing in web search. It'd be even better to rename the model to \\"a\\", \\"or\\" or \\"the\\" to make searching more interesting.~~ Is it \\"dots.llm1\\"? 142B. Cannot run on 24Gb.\\n\\n\\\\- Minimax: 456B. Cannot run on 24Gb.\\n\\n\\\\- Hunyuan: forbids use in EU so will not even try. Anecdotal evidence shows that models forbidding use in EU are immediately becoming trash: Llama 3.1 was ok, Llama 4 with EU excluded?\\n\\n\\\\- Ernie: 0.3B works in llama.cpp, downloaded and tried it. Nothing to be excited about from this size directly, it's probably only meant for speculative decoding. 21B and 28B would be interesting to try if it'll be possible sometime. Larger ones: nothing that can run locally on 24Gb.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n12ylqy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;- Dots: &lt;del&gt;found nothing in web search. It&amp;#39;d be even better to rename the model to &amp;quot;a&amp;quot;, &amp;quot;or&amp;quot; or &amp;quot;the&amp;quot; to make searching more interesting.&lt;/del&gt; Is it &amp;quot;dots.llm1&amp;quot;? 142B. Cannot run on 24Gb.&lt;/p&gt;\\n\\n&lt;p&gt;- Minimax: 456B. Cannot run on 24Gb.&lt;/p&gt;\\n\\n&lt;p&gt;- Hunyuan: forbids use in EU so will not even try. Anecdotal evidence shows that models forbidding use in EU are immediately becoming trash: Llama 3.1 was ok, Llama 4 with EU excluded?&lt;/p&gt;\\n\\n&lt;p&gt;- Ernie: 0.3B works in llama.cpp, downloaded and tried it. Nothing to be excited about from this size directly, it&amp;#39;s probably only meant for speculative decoding. 21B and 28B would be interesting to try if it&amp;#39;ll be possible sometime. Larger ones: nothing that can run locally on 24Gb.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/n12ylqy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751523132,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqh55j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-7}}],"before":null}}]`),r=()=>e.jsx(l,{data:t});export{r as default};
