import{j as e}from"./index-BpC9hjVs.js";import{R as t}from"./RedditPostRenderer-BEo6AnSR.js";import"./index-DwkJHX1_.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Looking for feedback on a mixed-use AI workstation build. Work is pushing me to get serious about local AI/model training or I'm basically toast career-wise, so trying to build something capable but not break the bank.\\n\\n\\n\\nPlanned specs:\\n\\nCPU: Ryzen 9 9950X3D\\n\\nMobo: X870E (eyeing ASUS ROG Crosshair Hero for expansion)\\n\\nRAM: 256GB DDR5-6000\\n\\nGPUs: 1x RTX 3090 + 2x MI50 32GB\\n\\nUse case split: RTX 3090 for Stable Diffusion, dual MI50s for LLM inference\\n\\n\\n\\nMain questions:\\n\\nMI50 real-world performance? I've got zero hands-on experience with them but the 32GB VRAM each for \\\\~$250 on eBay seems insane value. How's ROCm compatibility these days for inference?\\n\\nCan this actually run 70B models? With 64GB across the MI50s, should handle Llama 70B + smaller models simultaneously right?\\n\\nCoding/creative writing performance? Main LLM use will be code assistance and creative writing (scripts, etc). Are the MI50s fast enough or will I be frustrated coming from API services?\\n\\n\\n\\nGoals:\\n\\nKeep under $5k initially but want expansion path\\n\\nHandle Stable Diffusion without compromise (hence the 3090)\\n\\nRun multiple LLM models for different users/tasks\\n\\nLearn fine-tuning and custom models for work requirements\\n\\n\\n\\nAlternatives I'm considering:\\n\\nJust go dual RTX 3090s and call it a day, but the MI50 value proposition is tempting if they actually work well\\n\\nMac Studio M3 Ultra 256GB - saw one on eBay for $5k. Unified memory seems appealing but worried about AI ecosystem limitations vs CUDA\\n\\n\\n\\nMac Studio vs custom build thoughts? The 256GB unified memory on the Mac seems compelling for large models, but I'm concerned about software compatibility for training/fine-tuning. Most tutorials assume CUDA/PyTorch setup. Would I be limiting myself with Apple Silicon for serious AI development work?\\n\\nAnyone running MI50s for LLM work? Is ROCm mature enough or am I setting myself up for driver hell? The job pressure is real so I need something that works reliably, not a weekend project that maybe runs sometimes.\\n\\n\\n\\nBudget flexibility exists if there's a compelling reason to spend more, but I'm trying to be smart about price/performance. ","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Build advice: Consumer AI workstation with RTX 3090 + dual MI50s for LLM inference and Stable Diffusion (~$5k budget)","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m42gid","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.84,"author_flair_background_color":null,"subreddit_type":"public","ups":4,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_4fmbw86j","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":4,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752947519,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Looking for feedback on a mixed-use AI workstation build. Work is pushing me to get serious about local AI/model training or I&amp;#39;m basically toast career-wise, so trying to build something capable but not break the bank.&lt;/p&gt;\\n\\n&lt;p&gt;Planned specs:&lt;/p&gt;\\n\\n&lt;p&gt;CPU: Ryzen 9 9950X3D&lt;/p&gt;\\n\\n&lt;p&gt;Mobo: X870E (eyeing ASUS ROG Crosshair Hero for expansion)&lt;/p&gt;\\n\\n&lt;p&gt;RAM: 256GB DDR5-6000&lt;/p&gt;\\n\\n&lt;p&gt;GPUs: 1x RTX 3090 + 2x MI50 32GB&lt;/p&gt;\\n\\n&lt;p&gt;Use case split: RTX 3090 for Stable Diffusion, dual MI50s for LLM inference&lt;/p&gt;\\n\\n&lt;p&gt;Main questions:&lt;/p&gt;\\n\\n&lt;p&gt;MI50 real-world performance? I&amp;#39;ve got zero hands-on experience with them but the 32GB VRAM each for ~$250 on eBay seems insane value. How&amp;#39;s ROCm compatibility these days for inference?&lt;/p&gt;\\n\\n&lt;p&gt;Can this actually run 70B models? With 64GB across the MI50s, should handle Llama 70B + smaller models simultaneously right?&lt;/p&gt;\\n\\n&lt;p&gt;Coding/creative writing performance? Main LLM use will be code assistance and creative writing (scripts, etc). Are the MI50s fast enough or will I be frustrated coming from API services?&lt;/p&gt;\\n\\n&lt;p&gt;Goals:&lt;/p&gt;\\n\\n&lt;p&gt;Keep under $5k initially but want expansion path&lt;/p&gt;\\n\\n&lt;p&gt;Handle Stable Diffusion without compromise (hence the 3090)&lt;/p&gt;\\n\\n&lt;p&gt;Run multiple LLM models for different users/tasks&lt;/p&gt;\\n\\n&lt;p&gt;Learn fine-tuning and custom models for work requirements&lt;/p&gt;\\n\\n&lt;p&gt;Alternatives I&amp;#39;m considering:&lt;/p&gt;\\n\\n&lt;p&gt;Just go dual RTX 3090s and call it a day, but the MI50 value proposition is tempting if they actually work well&lt;/p&gt;\\n\\n&lt;p&gt;Mac Studio M3 Ultra 256GB - saw one on eBay for $5k. Unified memory seems appealing but worried about AI ecosystem limitations vs CUDA&lt;/p&gt;\\n\\n&lt;p&gt;Mac Studio vs custom build thoughts? The 256GB unified memory on the Mac seems compelling for large models, but I&amp;#39;m concerned about software compatibility for training/fine-tuning. Most tutorials assume CUDA/PyTorch setup. Would I be limiting myself with Apple Silicon for serious AI development work?&lt;/p&gt;\\n\\n&lt;p&gt;Anyone running MI50s for LLM work? Is ROCm mature enough or am I setting myself up for driver hell? The job pressure is real so I need something that works reliably, not a weekend project that maybe runs sometimes.&lt;/p&gt;\\n\\n&lt;p&gt;Budget flexibility exists if there&amp;#39;s a compelling reason to spend more, but I&amp;#39;m trying to be smart about price/performance. &lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m42gid","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"neighbornugs","discussion_type":null,"num_comments":10,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m42gid/build_advice_consumer_ai_workstation_with_rtx/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m42gid/build_advice_consumer_ai_workstation_with_rtx/","subreddit_subscribers":502030,"created_utc":1752947519,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n41ocxn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Threatening-Silence-","can_mod_post":false,"send_replies":true,"parent_id":"t1_n41hhdh","score":1,"author_fullname":"t2_15wqsifdjf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I have all the parts except for the mining frame, that arrives on Monday. Then I'll put it together and test it out. My previous build was a whole bunch of 3090 eGPUs on a consumer board so this will be a learning process for me as well. Was thinking I'd at least make a medium article about it and maybe a YT video\\n\\nI have pms disabled but ping me in a few days here and I'll have more answers for ya","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n41ocxn","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have all the parts except for the mining frame, that arrives on Monday. Then I&amp;#39;ll put it together and test it out. My previous build was a whole bunch of 3090 eGPUs on a consumer board so this will be a learning process for me as well. Was thinking I&amp;#39;d at least make a medium article about it and maybe a YT video&lt;/p&gt;\\n\\n&lt;p&gt;I have pms disabled but ping me in a few days here and I&amp;#39;ll have more answers for ya&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m42gid","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m42gid/build_advice_consumer_ai_workstation_with_rtx/n41ocxn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752953712,"author_flair_text":null,"treatment_tags":[],"created_utc":1752953712,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n41hhdh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"neighbornugs","can_mod_post":false,"created_utc":1752951487,"send_replies":true,"parent_id":"t1_n419e2f","score":2,"author_fullname":"t2_4fmbw86j","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Oh man, that's a beast of build. When do you plan on having it finished? I will follow closely and honestly might replicate yours for that price, not sure that I could beat it. Are you concerned at all with rocm support and the mi50s? I'm so glad I made this post - I was about to dump 5k into something that doesn't come close to yours. That's insane. I would love to see the performance you get out of it! Mind if I send you a message so I can ask questions after you get done with it and follow along with your journey?!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n41hhdh","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Oh man, that&amp;#39;s a beast of build. When do you plan on having it finished? I will follow closely and honestly might replicate yours for that price, not sure that I could beat it. Are you concerned at all with rocm support and the mi50s? I&amp;#39;m so glad I made this post - I was about to dump 5k into something that doesn&amp;#39;t come close to yours. That&amp;#39;s insane. I would love to see the performance you get out of it! Mind if I send you a message so I can ask questions after you get done with it and follow along with your journey?!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m42gid","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m42gid/build_advice_consumer_ai_workstation_with_rtx/n41hhdh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752951487,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n419e2f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Threatening-Silence-","can_mod_post":false,"created_utc":1752948885,"send_replies":true,"parent_id":"t3_1m42gid","score":3,"author_fullname":"t2_15wqsifdjf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'm about to start putting together a dual Xeon with 768GB, 11 MI50s and one 3090 for pp.\\n\\nTotal spend under £5k.\\n\\nSpecs:\\n\\nhttps://www.reddit.com/r/LocalLLaMA/s/LOa48hFusC","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n419e2f","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m about to start putting together a dual Xeon with 768GB, 11 MI50s and one 3090 for pp.&lt;/p&gt;\\n\\n&lt;p&gt;Total spend under £5k.&lt;/p&gt;\\n\\n&lt;p&gt;Specs:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/s/LOa48hFusC\\"&gt;https://www.reddit.com/r/LocalLLaMA/s/LOa48hFusC&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m42gid/build_advice_consumer_ai_workstation_with_rtx/n419e2f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752948885,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m42gid","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n41rz37","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n41qfko","score":1,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; MI25 is like, 5 minutes for an SDXL image and a free intense cring thinking of the power usage.\\n\\nThat's not good. Even my slow Max+ only takes like 15 seconds.\\n\\nBut in OP's case, that's what the 3090 will be for. SD is not a multi-gpu thing.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n41rz37","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;MI25 is like, 5 minutes for an SDXL image and a free intense cring thinking of the power usage.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;That&amp;#39;s not good. Even my slow Max+ only takes like 15 seconds.&lt;/p&gt;\\n\\n&lt;p&gt;But in OP&amp;#39;s case, that&amp;#39;s what the 3090 will be for. SD is not a multi-gpu thing.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m42gid","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m42gid/build_advice_consumer_ai_workstation_with_rtx/n41rz37/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752954885,"author_flair_text":null,"treatment_tags":[],"created_utc":1752954885,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n41qfko","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Marksta","can_mod_post":false,"created_utc":1752954387,"send_replies":true,"parent_id":"t1_n41lwhq","score":1,"author_fullname":"t2_559a1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah SwarmUI multi ROCm is a bitch, I spent the hour figuring that one out too. To immidately shake my head at the performance and bin it LMAO. \\n\\nSD is so compute focused the old cards are awful at it. MI25 is like, 5 minutes for an SDXL image and a free intense cring thinking of the power usage.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n41qfko","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah SwarmUI multi ROCm is a bitch, I spent the hour figuring that one out too. To immidately shake my head at the performance and bin it LMAO. &lt;/p&gt;\\n\\n&lt;p&gt;SD is so compute focused the old cards are awful at it. MI25 is like, 5 minutes for an SDXL image and a free intense cring thinking of the power usage.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m42gid","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m42gid/build_advice_consumer_ai_workstation_with_rtx/n41qfko/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752954387,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n42ynku","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1752969340,"send_replies":true,"parent_id":"t1_n41lwhq","score":2,"author_fullname":"t2_h5utwre7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Without xformers SD is gonna be fucky. Maybe you can use older sageattn if triton works, better than nothing. The whole stack is probably tweak/patch/compile/pray.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n42ynku","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Without xformers SD is gonna be fucky. Maybe you can use older sageattn if triton works, better than nothing. The whole stack is probably tweak/patch/compile/pray.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m42gid","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m42gid/build_advice_consumer_ai_workstation_with_rtx/n42ynku/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752969340,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n41lwhq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"juss-i","can_mod_post":false,"created_utc":1752952922,"send_replies":true,"parent_id":"t3_1m42gid","score":2,"author_fullname":"t2_j0lnb4w3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"As an owner of a couple of MI50s, I have to say they kind of suck for SD. I didn't properly benchmark since I don't do it a lot, but feels about the same as the P40. Also a bit of a fight getting multi-gpu ROCm to work with SwarmUI. No one documented how to do it, had to read some code to figure it out.\\n\\n\\nFor LLM inference MI50 32GB is good value. A bit slow on prompt processing though. And if you have multiple, they're really picky on what kind of hardware they can do pcie p2p on, which means stuff like tensor parallel might just not work for you because of your other hardware.\\n\\n\\n\\nAnd then there's cooling server GPUs. You need server-like airflow, or some big blowers that may or may not fit in your case.\\n\\n\\nSummary:\\nYou like the feeling you get after getting something crazy to kind of work for cheap, after you put a lot pf hours into it? Go for MI50.\\n\\n\\nIf you like that stuff just works and you got the cash, get more 3090s or a Mac.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n41lwhq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;As an owner of a couple of MI50s, I have to say they kind of suck for SD. I didn&amp;#39;t properly benchmark since I don&amp;#39;t do it a lot, but feels about the same as the P40. Also a bit of a fight getting multi-gpu ROCm to work with SwarmUI. No one documented how to do it, had to read some code to figure it out.&lt;/p&gt;\\n\\n&lt;p&gt;For LLM inference MI50 32GB is good value. A bit slow on prompt processing though. And if you have multiple, they&amp;#39;re really picky on what kind of hardware they can do pcie p2p on, which means stuff like tensor parallel might just not work for you because of your other hardware.&lt;/p&gt;\\n\\n&lt;p&gt;And then there&amp;#39;s cooling server GPUs. You need server-like airflow, or some big blowers that may or may not fit in your case.&lt;/p&gt;\\n\\n&lt;p&gt;Summary:\\nYou like the feeling you get after getting something crazy to kind of work for cheap, after you put a lot pf hours into it? Go for MI50.&lt;/p&gt;\\n\\n&lt;p&gt;If you like that stuff just works and you got the cash, get more 3090s or a Mac.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m42gid/build_advice_consumer_ai_workstation_with_rtx/n41lwhq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752952922,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m42gid","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n44586p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ArsNeph","can_mod_post":false,"created_utc":1752986380,"send_replies":true,"parent_id":"t3_1m42gid","score":2,"author_fullname":"t2_vt0xkv60d","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Your build is not a bad idea, but you should note that MI50s only work on Linux unless you install a custom driver. They have about 1 TB/s memory bandwidth, so they are about as fast as a 3090, but they don't have CUDA support, and their compute is a little weaker, meaning that they will be a bit slower. Using a Vulkan backend and tensor parallelism can help mitigate this, but it will slow down your 3090 if you're using them concurrently.\\n\\nMI50s are not a good option for training, so you would only be able to train small models using your 3090. I would recommend to only using your 3090 for diffusion models, as they're compute bound, not memory bandwidth bound. You would be able to load large models, with a total of 88GB VRAM, enough for 70B 8 bit with 32k context, or a medium quant of 110B. You could load multiple models, but I don't see a point in loading multiple models at the same time unless you plan to do speculative decoding. Llama-swap would be a much better solution to that.\\n\\nThe machine should work most of the time as long as you don't mess with the MI50 drivers after setting them up, but if you need a truly reliable daily driver, MI50s are generally defunct niche tech with a limited lifetime. They do their job well, but there's no guarantee they won't crap out eventually. PC will be making is a cobbled together homelab Franken creation, not an extremely reliable enterprise grade daily driver. It shouldn't malfunction, but it's not ever possible to guarantee that it won't.","edited":1752986960,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44586p","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Your build is not a bad idea, but you should note that MI50s only work on Linux unless you install a custom driver. They have about 1 TB/s memory bandwidth, so they are about as fast as a 3090, but they don&amp;#39;t have CUDA support, and their compute is a little weaker, meaning that they will be a bit slower. Using a Vulkan backend and tensor parallelism can help mitigate this, but it will slow down your 3090 if you&amp;#39;re using them concurrently.&lt;/p&gt;\\n\\n&lt;p&gt;MI50s are not a good option for training, so you would only be able to train small models using your 3090. I would recommend to only using your 3090 for diffusion models, as they&amp;#39;re compute bound, not memory bandwidth bound. You would be able to load large models, with a total of 88GB VRAM, enough for 70B 8 bit with 32k context, or a medium quant of 110B. You could load multiple models, but I don&amp;#39;t see a point in loading multiple models at the same time unless you plan to do speculative decoding. Llama-swap would be a much better solution to that.&lt;/p&gt;\\n\\n&lt;p&gt;The machine should work most of the time as long as you don&amp;#39;t mess with the MI50 drivers after setting them up, but if you need a truly reliable daily driver, MI50s are generally defunct niche tech with a limited lifetime. They do their job well, but there&amp;#39;s no guarantee they won&amp;#39;t crap out eventually. PC will be making is a cobbled together homelab Franken creation, not an extremely reliable enterprise grade daily driver. It shouldn&amp;#39;t malfunction, but it&amp;#39;s not ever possible to guarantee that it won&amp;#39;t.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m42gid/build_advice_consumer_ai_workstation_with_rtx/n44586p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752986380,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m42gid","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n42l07x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"coolestmage","can_mod_post":false,"created_utc":1752964503,"send_replies":true,"parent_id":"t3_1m42gid","score":1,"author_fullname":"t2_6dtdz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'm running mi50s. They handle 70B models just fine. Even without using tensor parallelism they manage 10tk/s","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n42l07x","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m running mi50s. They handle 70B models just fine. Even without using tensor parallelism they manage 10tk/s&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m42gid/build_advice_consumer_ai_workstation_with_rtx/n42l07x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752964503,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m42gid","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n45gpra","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GPTrack_ai","can_mod_post":false,"created_utc":1753012218,"send_replies":true,"parent_id":"t3_1m42gid","score":1,"author_fullname":"t2_1tpuoj72sa","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If I may, buy new stuff, newer is always much better.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n45gpra","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If I may, buy new stuff, newer is always much better.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m42gid/build_advice_consumer_ai_workstation_with_rtx/n45gpra/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753012218,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m42gid","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(t,{data:l});export{r as default};
