import{j as e}from"./index-M4edQi1P.js";import{R as l}from"./RedditPostRenderer-CESBGIIy.js";import"./index-DFpL1mt4.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I've just managed to cobble together a machine with 3x24GB GPUs, looking to see of the models currently available, what are the best ones I should be looking at now.\\n\\nI know \\"best model\\" isn't entirely a thing, some are better than others at certain things. Like so far of the 70b and 110b models I've tried on my previous 48gb of VRAM, none came even close to Gemma3 27b for creative writing and instruction following. But I'm wondering if there are some bigger ones that might beat it.\\n\\nAlso coding, would anything I can run now beat Qwen2.5-coder 32b?\\n\\nSo far I haven't yet found anything in the ~70b range that can beat these smaller models, but maybe something bigger can?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Best current models for 72GB VRAM","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lr1ypr","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.95,"author_flair_background_color":null,"subreddit_type":"public","ups":16,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_g0qor","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":16,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751579524,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve just managed to cobble together a machine with 3x24GB GPUs, looking to see of the models currently available, what are the best ones I should be looking at now.&lt;/p&gt;\\n\\n&lt;p&gt;I know &amp;quot;best model&amp;quot; isn&amp;#39;t entirely a thing, some are better than others at certain things. Like so far of the 70b and 110b models I&amp;#39;ve tried on my previous 48gb of VRAM, none came even close to Gemma3 27b for creative writing and instruction following. But I&amp;#39;m wondering if there are some bigger ones that might beat it.&lt;/p&gt;\\n\\n&lt;p&gt;Also coding, would anything I can run now beat Qwen2.5-coder 32b?&lt;/p&gt;\\n\\n&lt;p&gt;So far I haven&amp;#39;t yet found anything in the ~70b range that can beat these smaller models, but maybe something bigger can?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lr1ypr","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"GregoryfromtheHood","discussion_type":null,"num_comments":10,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lr1ypr/best_current_models_for_72gb_vram/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lr1ypr/best_current_models_for_72gb_vram/","subreddit_subscribers":494198,"created_utc":1751579524,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n17s8pq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"coolestmage","can_mod_post":false,"created_utc":1751585056,"send_replies":true,"parent_id":"t3_1lr1ypr","score":4,"author_fullname":"t2_6dtdz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you have free ram you can probably run a smaller quant of Qwen3-235B-A22B. It is fantastic at both creative writing and coding, in my experience.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n17s8pq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you have free ram you can probably run a smaller quant of Qwen3-235B-A22B. It is fantastic at both creative writing and coding, in my experience.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lr1ypr/best_current_models_for_72gb_vram/n17s8pq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751585056,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lr1ypr","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n197zpv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DinoAmino","can_mod_post":false,"send_replies":true,"parent_id":"t1_n197d5a","score":2,"author_fullname":"t2_j1v7f","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes. 3bit models give 3 bit responses. Go FP8 on vLLM with this one for sure.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n197zpv","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes. 3bit models give 3 bit responses. Go FP8 on vLLM with this one for sure.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lr1ypr","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lr1ypr/best_current_models_for_72gb_vram/n197zpv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751605301,"author_flair_text":null,"treatment_tags":[],"created_utc":1751605301,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n197d5a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ShengrenR","can_mod_post":false,"created_utc":1751605010,"send_replies":true,"parent_id":"t1_n17u8lr","score":2,"author_fullname":"t2_ji4n4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"psh.. can run that on a single 24GB card with exl3 at 3bpw and it works well. Go bigger!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n197d5a","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;psh.. can run that on a single 24GB card with exl3 at 3bpw and it works well. Go bigger!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lr1ypr","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lr1ypr/best_current_models_for_72gb_vram/n197d5a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751605010,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n17u8lr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DinoAmino","can_mod_post":false,"created_utc":1751585747,"send_replies":true,"parent_id":"t3_1lr1ypr","score":5,"author_fullname":"t2_j1v7f","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Nemotron super is 49B and you'd still have room for 32k context.\\n\\nhttps://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n17u8lr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nemotron super is 49B and you&amp;#39;d still have room for 32k context.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1\\"&gt;https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lr1ypr/best_current_models_for_72gb_vram/n17u8lr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751585747,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lr1ypr","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n18qgqp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"dubesor86","can_mod_post":false,"created_utc":1751597763,"send_replies":true,"parent_id":"t3_1lr1ypr","score":2,"author_fullname":"t2_6wbun","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For coding in particular command-a-03-2025 (if it can fit), and for general purpose and total intelligence Llama 3.3 70B is still the most solid pick.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n18qgqp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For coding in particular command-a-03-2025 (if it can fit), and for general purpose and total intelligence Llama 3.3 70B is still the most solid pick.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lr1ypr/best_current_models_for_72gb_vram/n18qgqp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751597763,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lr1ypr","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n17mk2o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LagOps91","can_mod_post":false,"created_utc":1751583128,"send_replies":true,"parent_id":"t3_1lr1ypr","score":2,"author_fullname":"t2_3wi6j7vwh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"might be able to fit dot.llm1? would be a tight fit i think, but i did hear a lot of good about the model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n17mk2o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;might be able to fit dot.llm1? would be a tight fit i think, but i did hear a lot of good about the model.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lr1ypr/best_current_models_for_72gb_vram/n17mk2o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751583128,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lr1ypr","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n197p1n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ShengrenR","can_mod_post":false,"created_utc":1751605163,"send_replies":true,"parent_id":"t3_1lr1ypr","score":1,"author_fullname":"t2_ji4n4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The kimi folks [https://huggingface.co/moonshotai/Kimi-Dev-72B](https://huggingface.co/moonshotai/Kimi-Dev-72B) were sure talking their model up, and the benchmarks make it look like it might be worth a spin for code.. though I've not tried it yet to say y/n.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n197p1n","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The kimi folks &lt;a href=\\"https://huggingface.co/moonshotai/Kimi-Dev-72B\\"&gt;https://huggingface.co/moonshotai/Kimi-Dev-72B&lt;/a&gt; were sure talking their model up, and the benchmarks make it look like it might be worth a spin for code.. though I&amp;#39;ve not tried it yet to say y/n.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lr1ypr/best_current_models_for_72gb_vram/n197p1n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751605163,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lr1ypr","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n19b2hz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Klutzy-Snow8016","can_mod_post":false,"created_utc":1751606774,"send_replies":true,"parent_id":"t3_1lr1ypr","score":1,"author_fullname":"t2_1d5l610jz3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Dots.llm.1 and Command-A are worth trying. You can also run higher quants of smaller models, which does seem to make a difference.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n19b2hz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Dots.llm.1 and Command-A are worth trying. You can also run higher quants of smaller models, which does seem to make a difference.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lr1ypr/best_current_models_for_72gb_vram/n19b2hz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751606774,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lr1ypr","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n19bmvr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"serige","can_mod_post":false,"created_utc":1751607052,"send_replies":true,"parent_id":"t3_1lr1ypr","score":1,"author_fullname":"t2_kxwnu","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Same vram as op, also waiting for people to comment if they found anything better than Qwen2.5 coder.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n19bmvr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Same vram as op, also waiting for people to comment if they found anything better than Qwen2.5 coder.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lr1ypr/best_current_models_for_72gb_vram/n19bmvr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751607052,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lr1ypr","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(l,{data:a});export{n as default};
