import{j as e}from"./index-BlGsFJYy.js";import{R as l}from"./RedditPostRenderer-B6uvq_Zl.js";import"./index-DDvVtNwD.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I feel so dirty using cloud models. They even admit to storing your queries forever and manually inspecting them if you trigger flags.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Can home sized LLMs (32b, etc.) or home GPUs ever improve to the point where they can compete with cloud models?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lrpjpc","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.48,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1bl579qtd6","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751652917,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I feel so dirty using cloud models. They even admit to storing your queries forever and manually inspecting them if you trigger flags.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lrpjpc","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"TumbleweedDeep825","discussion_type":null,"num_comments":32,"send_replies":false,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lrpjpc/can_home_sized_llms_32b_etc_or_home_gpus_ever/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lrpjpc/can_home_sized_llms_32b_etc_or_home_gpus_ever/","subreddit_subscribers":494898,"created_utc":1751652917,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1cu4vq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"sourceholder","can_mod_post":false,"created_utc":1751656977,"send_replies":true,"parent_id":"t1_n1ciykr","score":20,"author_fullname":"t2_35jjv","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Cloud models will always have the advantage running on $200k+ inference nodes.\\n\\nI would reframe the question and consider that todays 32b models already outperform original cloud GPT 3.5 in all meaningful benchmarks.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1cu4vq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Cloud models will always have the advantage running on $200k+ inference nodes.&lt;/p&gt;\\n\\n&lt;p&gt;I would reframe the question and consider that todays 32b models already outperform original cloud GPT 3.5 in all meaningful benchmarks.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrpjpc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrpjpc/can_home_sized_llms_32b_etc_or_home_gpus_ever/n1cu4vq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751656977,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":20}}],"before":null}},"user_reports":[],"saved":false,"id":"n1ciykr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Waste-Spare1417","can_mod_post":false,"created_utc":1751653358,"send_replies":true,"parent_id":"t3_1lrpjpc","score":22,"author_fullname":"t2_a45sfon6","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Home LLMs keep improving, but cloud models are also getting better, so it is difficult to catch up.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ciykr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Home LLMs keep improving, but cloud models are also getting better, so it is difficult to catch up.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrpjpc/can_home_sized_llms_32b_etc_or_home_gpus_ever/n1ciykr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751653358,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrpjpc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":22}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1d5hpb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ArsNeph","can_mod_post":false,"created_utc":1751660690,"send_replies":true,"parent_id":"t3_1lrpjpc","score":5,"author_fullname":"t2_vt0xkv60d","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I mean, today's home sized LLMs, Gemma 3 27B and Qwen 3 32B are pretty damn competitive with cloud models considering they're only about 1/10 of the size. And honestly, there haven't been any new 70B class models, (we're still waiting on support for Hunyuan) but I would assume a good one would be even more competitive. I think one of the biggest issues with home inference though is that we can fit quanted models, but we can't fit nearly the same amount of context as cloud providers, and whatever long context models we have, they are relatively weak in overall fidelity compared to something like Gemini.\\n\\nThe limitations on the models we can run at home are entirely artificial, propped up by Nvidia's Monopoly on CUDA and high bandwidth VRAM. We've seen people hack together 48GB 4090s. While the RTX 6000 Pro 96GB is a massive improvement in pricing over A100s, I can sure as hell guarantee that it doesn't cost $8,000 to make. When the day comes that someone makes a 1000GB/s 48 GB card for a reasonable price, Nvidia will be in very hot water. Let Nvidia rest on their laurels,  there are billions of dollars going into breaking their Monopoly.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1d5hpb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I mean, today&amp;#39;s home sized LLMs, Gemma 3 27B and Qwen 3 32B are pretty damn competitive with cloud models considering they&amp;#39;re only about 1/10 of the size. And honestly, there haven&amp;#39;t been any new 70B class models, (we&amp;#39;re still waiting on support for Hunyuan) but I would assume a good one would be even more competitive. I think one of the biggest issues with home inference though is that we can fit quanted models, but we can&amp;#39;t fit nearly the same amount of context as cloud providers, and whatever long context models we have, they are relatively weak in overall fidelity compared to something like Gemini.&lt;/p&gt;\\n\\n&lt;p&gt;The limitations on the models we can run at home are entirely artificial, propped up by Nvidia&amp;#39;s Monopoly on CUDA and high bandwidth VRAM. We&amp;#39;ve seen people hack together 48GB 4090s. While the RTX 6000 Pro 96GB is a massive improvement in pricing over A100s, I can sure as hell guarantee that it doesn&amp;#39;t cost $8,000 to make. When the day comes that someone makes a 1000GB/s 48 GB card for a reasonable price, Nvidia will be in very hot water. Let Nvidia rest on their laurels,  there are billions of dollars going into breaking their Monopoly.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrpjpc/can_home_sized_llms_32b_etc_or_home_gpus_ever/n1d5hpb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751660690,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrpjpc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1em6jv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ttkciar","can_mod_post":false,"created_utc":1751680683,"send_replies":true,"parent_id":"t3_1lrpjpc","score":8,"author_fullname":"t2_cpegz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yes and no.\\n\\nToday's \\"home sized\\" models have comparable skills (but not competence in those skills) to the commercial models of a couple of years ago.\\n\\nI expect that in a couple more years, the \\"home sized\\" models will be comparable in skillsets to the commercial models we have today.\\n\\nIf commercial models hit a major speedbump which local models do not, then local models might catch up, but barring that I expect there will always be a skill gap of about two years.\\n\\nThe question of competence is more interesting, I think, or at least more complex.\\n\\nWith a few exceptions (like JetBrains and other codegen models) commercial models are pretty much stuck being general-purpose, and we have seen that sufficiently well retrained/finetuned \\"home models\\" can close the competency gap for specific skills.  MedGemma-27B, for example, shows competence in the narrow topic of medicine comparable to modern GPT4.\\n\\nAs more sophisticated users, we in this community have the option of choosing different specialized models for different tasks, so depending on our use-cases we might see skill competence in our models similar to the commercial services.\\n\\nIn the realm of general-purpose models, competence comes from a mixture of factors:  Parameter count, training data quality, transfer learning, and inference-time augmentation (RAG, \\"thinking\\", Guided Generation, self-mixing, self-critique, etc).\\n\\nThe commercial services' more expensive models will always have an edge on parameter count, which allows them a larger mix of memorized world knowledge, generalized knowledge, and heuristics.  However, competence does not seem to scale linearly with parameter count.  Competence seems to increase logarithmically with parameter count, so even a much larger model will only be somewhat more competent than our more humble local models, all other factors being equal (which they aren't).\\n\\nCommercial inference services frequently also have smaller models, like OpenAI's \\"Turbo\\" GPT models.  These seem to have much lower parameter counts, but benefit from \\"transfer learning\\" which gives them some of the knowledge, skills, and competence of the larger models from which learning is transferred.\\n\\nWe can play that game, too, though.  Qwen2 and Qwen2.5 had smaller models which benefitted from transferring learning from their larger models.  We also have massive models based on Llama3-405B (like Tulu3-405B, which is an excellent STEM model, comparable in this niche to GPT4o) from which we could transfer learning to \\"home sized\\" models.\\n\\nInference-time augmentation is where the open weight community really shines, because by default the commercial vendors don't use any of these methods to improve inference quality, whereas we can set up our inference stacks to always use them.\\n\\nFor example, it's easy to set up our home rigs to always use RAG from a database of high-quality, highly-factual content (like a Wikipedia dump).  Using a commercial service, you usually have to provide it with your documents (which is very limited) or use the *entire internet* as the RAG database (eg, Google Web Search AI Overview, or OpenAI Deep Research) which is not great because the internet's average content quality is low, and it contains loads of counterfactual content too.  These factors limit the competence of commercial inference services.\\n\\nRegarding \\"Guided Generation\\", llama.cpp had \\"grammars\\" about a year before OpenAI came out with their equivalent \\"schemas\\" feature, which is a lot harder to use than grammars.  This means we have a much easier time guiding inference in the directions we want, like forcing it to use a specific language or structured output format (JSON, XML, etc).  We will probably get other Guided Generation features long before the commercial services do (though they will catch up of course).\\n\\nIt's also worth pointing out that we \\"GPU Poor\\" are getting less and less poor all the time.  We will never have the GPU power of the commercial services, but they are bumping up against the limits of hardware scaling already, while we are not.  As our hardware continues to get better, the effective gap should get narrower.\\n\\nFinally, I want to bring up something people here hate to hear:  [AI Winter](https://wikipedia.org/wiki/AI_winter)\\n\\nThe AI industry has *always* exhibited boom/bust cycles, with AI Summers being followed by AI Winters.  I'm too young to have witnessed the first AI Winter, but was active in the field for the second one (in the 1990s), and can tell you that the same factors which caused that AI Winter are in full swing today -- hype and overpromising, leading to inflated expectations, which make disillusionment inevitable.\\n\\nAs capable and genuinely useful as LLM inference is, it can still be overhyped, and it is being overhyped, which causes people to expect things of it which it will never actually deliver (like AGI).\\n\\nWhen the promised capabilities fail to materialize, it causes a popular backlash, which in turn causes investment to dry up, and academics to focus on other topics.\\n\\n***That does not mean LLM inference will go away!!*** It only means it will no longer carry the same buzz that it does today.  It will remain a useful tool, and will continue to be used and developed (albeit at a slower rate), just like the technologies which emerged from AI Summers of the past (compilers, regular expressions, robotics, OCR, databases, search engines, etc).\\n\\nWhy is that relevant?\\n\\nIt is relevant to us because in an AI Winter, businesses will invest less in AI R&amp;D, but the open source community doesn't have to.  While they stagnate, we will have the opportunity to not only catch up, but surpass them.\\n\\nWe may also see a big sell-off of used datacenter GPUs, as commercial demand for LLM inference and training diminishes.  We might be able to buy those up to fuel open source model R&amp;D, or even home inference.\\n\\nThis is of course speculation.  Time will tell.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1em6jv","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes and no.&lt;/p&gt;\\n\\n&lt;p&gt;Today&amp;#39;s &amp;quot;home sized&amp;quot; models have comparable skills (but not competence in those skills) to the commercial models of a couple of years ago.&lt;/p&gt;\\n\\n&lt;p&gt;I expect that in a couple more years, the &amp;quot;home sized&amp;quot; models will be comparable in skillsets to the commercial models we have today.&lt;/p&gt;\\n\\n&lt;p&gt;If commercial models hit a major speedbump which local models do not, then local models might catch up, but barring that I expect there will always be a skill gap of about two years.&lt;/p&gt;\\n\\n&lt;p&gt;The question of competence is more interesting, I think, or at least more complex.&lt;/p&gt;\\n\\n&lt;p&gt;With a few exceptions (like JetBrains and other codegen models) commercial models are pretty much stuck being general-purpose, and we have seen that sufficiently well retrained/finetuned &amp;quot;home models&amp;quot; can close the competency gap for specific skills.  MedGemma-27B, for example, shows competence in the narrow topic of medicine comparable to modern GPT4.&lt;/p&gt;\\n\\n&lt;p&gt;As more sophisticated users, we in this community have the option of choosing different specialized models for different tasks, so depending on our use-cases we might see skill competence in our models similar to the commercial services.&lt;/p&gt;\\n\\n&lt;p&gt;In the realm of general-purpose models, competence comes from a mixture of factors:  Parameter count, training data quality, transfer learning, and inference-time augmentation (RAG, &amp;quot;thinking&amp;quot;, Guided Generation, self-mixing, self-critique, etc).&lt;/p&gt;\\n\\n&lt;p&gt;The commercial services&amp;#39; more expensive models will always have an edge on parameter count, which allows them a larger mix of memorized world knowledge, generalized knowledge, and heuristics.  However, competence does not seem to scale linearly with parameter count.  Competence seems to increase logarithmically with parameter count, so even a much larger model will only be somewhat more competent than our more humble local models, all other factors being equal (which they aren&amp;#39;t).&lt;/p&gt;\\n\\n&lt;p&gt;Commercial inference services frequently also have smaller models, like OpenAI&amp;#39;s &amp;quot;Turbo&amp;quot; GPT models.  These seem to have much lower parameter counts, but benefit from &amp;quot;transfer learning&amp;quot; which gives them some of the knowledge, skills, and competence of the larger models from which learning is transferred.&lt;/p&gt;\\n\\n&lt;p&gt;We can play that game, too, though.  Qwen2 and Qwen2.5 had smaller models which benefitted from transferring learning from their larger models.  We also have massive models based on Llama3-405B (like Tulu3-405B, which is an excellent STEM model, comparable in this niche to GPT4o) from which we could transfer learning to &amp;quot;home sized&amp;quot; models.&lt;/p&gt;\\n\\n&lt;p&gt;Inference-time augmentation is where the open weight community really shines, because by default the commercial vendors don&amp;#39;t use any of these methods to improve inference quality, whereas we can set up our inference stacks to always use them.&lt;/p&gt;\\n\\n&lt;p&gt;For example, it&amp;#39;s easy to set up our home rigs to always use RAG from a database of high-quality, highly-factual content (like a Wikipedia dump).  Using a commercial service, you usually have to provide it with your documents (which is very limited) or use the &lt;em&gt;entire internet&lt;/em&gt; as the RAG database (eg, Google Web Search AI Overview, or OpenAI Deep Research) which is not great because the internet&amp;#39;s average content quality is low, and it contains loads of counterfactual content too.  These factors limit the competence of commercial inference services.&lt;/p&gt;\\n\\n&lt;p&gt;Regarding &amp;quot;Guided Generation&amp;quot;, llama.cpp had &amp;quot;grammars&amp;quot; about a year before OpenAI came out with their equivalent &amp;quot;schemas&amp;quot; feature, which is a lot harder to use than grammars.  This means we have a much easier time guiding inference in the directions we want, like forcing it to use a specific language or structured output format (JSON, XML, etc).  We will probably get other Guided Generation features long before the commercial services do (though they will catch up of course).&lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s also worth pointing out that we &amp;quot;GPU Poor&amp;quot; are getting less and less poor all the time.  We will never have the GPU power of the commercial services, but they are bumping up against the limits of hardware scaling already, while we are not.  As our hardware continues to get better, the effective gap should get narrower.&lt;/p&gt;\\n\\n&lt;p&gt;Finally, I want to bring up something people here hate to hear:  &lt;a href=\\"https://wikipedia.org/wiki/AI_winter\\"&gt;AI Winter&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;The AI industry has &lt;em&gt;always&lt;/em&gt; exhibited boom/bust cycles, with AI Summers being followed by AI Winters.  I&amp;#39;m too young to have witnessed the first AI Winter, but was active in the field for the second one (in the 1990s), and can tell you that the same factors which caused that AI Winter are in full swing today -- hype and overpromising, leading to inflated expectations, which make disillusionment inevitable.&lt;/p&gt;\\n\\n&lt;p&gt;As capable and genuinely useful as LLM inference is, it can still be overhyped, and it is being overhyped, which causes people to expect things of it which it will never actually deliver (like AGI).&lt;/p&gt;\\n\\n&lt;p&gt;When the promised capabilities fail to materialize, it causes a popular backlash, which in turn causes investment to dry up, and academics to focus on other topics.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;That does not mean LLM inference will go away!!&lt;/em&gt;&lt;/strong&gt; It only means it will no longer carry the same buzz that it does today.  It will remain a useful tool, and will continue to be used and developed (albeit at a slower rate), just like the technologies which emerged from AI Summers of the past (compilers, regular expressions, robotics, OCR, databases, search engines, etc).&lt;/p&gt;\\n\\n&lt;p&gt;Why is that relevant?&lt;/p&gt;\\n\\n&lt;p&gt;It is relevant to us because in an AI Winter, businesses will invest less in AI R&amp;amp;D, but the open source community doesn&amp;#39;t have to.  While they stagnate, we will have the opportunity to not only catch up, but surpass them.&lt;/p&gt;\\n\\n&lt;p&gt;We may also see a big sell-off of used datacenter GPUs, as commercial demand for LLM inference and training diminishes.  We might be able to buy those up to fuel open source model R&amp;amp;D, or even home inference.&lt;/p&gt;\\n\\n&lt;p&gt;This is of course speculation.  Time will tell.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrpjpc/can_home_sized_llms_32b_etc_or_home_gpus_ever/n1em6jv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751680683,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lrpjpc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1flzo3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"--Tintin","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1fbbok","score":2,"author_fullname":"t2_3rsks8xy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"üôè","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1flzo3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;üôè&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrpjpc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrpjpc/can_home_sized_llms_32b_etc_or_home_gpus_ever/n1flzo3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751698467,"author_flair_text":null,"treatment_tags":[],"created_utc":1751698467,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1fbbok","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DunklerErpel","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1fadc8","score":2,"author_fullname":"t2_alho5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"https://preview.redd.it/vmcifhf5pzaf1.png?width=1254&amp;format=png&amp;auto=webp&amp;s=a5a0ff7349ea3893034c820e6a179885f717cfc0\\n\\nIt says right here","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1fbbok","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://preview.redd.it/vmcifhf5pzaf1.png?width=1254&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a5a0ff7349ea3893034c820e6a179885f717cfc0\\"&gt;https://preview.redd.it/vmcifhf5pzaf1.png?width=1254&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a5a0ff7349ea3893034c820e6a179885f717cfc0&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;It says right here&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrpjpc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrpjpc/can_home_sized_llms_32b_etc_or_home_gpus_ever/n1fbbok/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751692510,"media_metadata":{"vmcifhf5pzaf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":33,"x":108,"u":"https://preview.redd.it/vmcifhf5pzaf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d95c19fa51dc78a3e930c21fd5cb66b581508b47"},{"y":67,"x":216,"u":"https://preview.redd.it/vmcifhf5pzaf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=cdfaebddc7214e3f3dba3d64655362ef7b9a9d8a"},{"y":100,"x":320,"u":"https://preview.redd.it/vmcifhf5pzaf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3f9b7ae58e2fd61b23ad68995077b7eb70a46609"},{"y":201,"x":640,"u":"https://preview.redd.it/vmcifhf5pzaf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cd8604ca49a372ac4307e23f5b3d433463953688"},{"y":301,"x":960,"u":"https://preview.redd.it/vmcifhf5pzaf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8c5065cac9c828adb9d988d3f2ec3c8eca202338"},{"y":339,"x":1080,"u":"https://preview.redd.it/vmcifhf5pzaf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4ceb3bbab804cd316825f9e33e2d3b8225b0a2a9"}],"s":{"y":394,"x":1254,"u":"https://preview.redd.it/vmcifhf5pzaf1.png?width=1254&amp;format=png&amp;auto=webp&amp;s=a5a0ff7349ea3893034c820e6a179885f717cfc0"},"id":"vmcifhf5pzaf1"}},"author_flair_text":null,"treatment_tags":[],"created_utc":1751692510,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1fadc8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"--Tintin","can_mod_post":false,"created_utc":1751692001,"send_replies":true,"parent_id":"t1_n1cjdor","score":2,"author_fullname":"t2_3rsks8xy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"In which way is Jan-Nano a specialized model?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1fadc8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;In which way is Jan-Nano a specialized model?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrpjpc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrpjpc/can_home_sized_llms_32b_etc_or_home_gpus_ever/n1fadc8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751692001,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1cjdor","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DunklerErpel","can_mod_post":false,"created_utc":1751653491,"send_replies":true,"parent_id":"t3_1lrpjpc","score":7,"author_fullname":"t2_alho5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The only possibility I could think of is to have specialised models. Cloud models are HUGE, but that's also due to them providing (and computing) A LOT of things outside of what you might desire. You might not need French haikus at home, possibly no South-African cooking recipes. \\n\\nSo, that brings me back to having specialised models with custom knowledge. Good examples would be Jan-nano, or Zeta.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1cjdor","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The only possibility I could think of is to have specialised models. Cloud models are HUGE, but that&amp;#39;s also due to them providing (and computing) A LOT of things outside of what you might desire. You might not need French haikus at home, possibly no South-African cooking recipes. &lt;/p&gt;\\n\\n&lt;p&gt;So, that brings me back to having specialised models with custom knowledge. Good examples would be Jan-nano, or Zeta.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrpjpc/can_home_sized_llms_32b_etc_or_home_gpus_ever/n1cjdor/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751653491,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrpjpc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1cjaub","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Conversation9561","can_mod_post":false,"created_utc":1751653466,"send_replies":true,"parent_id":"t3_1lrpjpc","score":3,"author_fullname":"t2_jqxb4pte","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Probably not as there‚Äôs only so many things you can fit in 32B. But I believe we‚Äôll be able to run bigger models as price to performance keeps improving for consumer hardware.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1cjaub","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Probably not as there‚Äôs only so many things you can fit in 32B. But I believe we‚Äôll be able to run bigger models as price to performance keeps improving for consumer hardware.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrpjpc/can_home_sized_llms_32b_etc_or_home_gpus_ever/n1cjaub/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751653466,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrpjpc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1csp8b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"colin_colout","can_mod_post":false,"created_utc":1751656501,"send_replies":true,"parent_id":"t3_1lrpjpc","score":3,"author_fullname":"t2_14l4ya","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you asked that a few years ago when ChatGPT 3.5 turbo was hot shit, then we're already there and then some (maybe not general knowledge because smaller models have a physical limitation, but we have native tool calling now, so I still call it a win). \\n\\nI wouldn't be surprised if we can run \\"Claude sonnet 4 at home\\" in 2 years, but by then it might be brain-dead compared to what these giant clusters can do (unless we're about to bump into a scaling ceiling)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1csp8b","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you asked that a few years ago when ChatGPT 3.5 turbo was hot shit, then we&amp;#39;re already there and then some (maybe not general knowledge because smaller models have a physical limitation, but we have native tool calling now, so I still call it a win). &lt;/p&gt;\\n\\n&lt;p&gt;I wouldn&amp;#39;t be surprised if we can run &amp;quot;Claude sonnet 4 at home&amp;quot; in 2 years, but by then it might be brain-dead compared to what these giant clusters can do (unless we&amp;#39;re about to bump into a scaling ceiling)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrpjpc/can_home_sized_llms_32b_etc_or_home_gpus_ever/n1csp8b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751656501,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrpjpc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1czspy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BidWestern1056","can_mod_post":false,"created_utc":1751658855,"send_replies":true,"parent_id":"t3_1lrpjpc","score":3,"author_fullname":"t2_uzxql7po","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"the key way we will catch up is through powerful agentic frameworks and tools that give smaller local models the context they need to produce results reliably and intelligently.\\n\\ntry out npcpy/npcsh if you can\\n\\n[https://github.com/npc-worldwide/npcpy](https://github.com/npc-worldwide/npcpy)\\n\\nand npc studio\\n\\n[https://github.com/npc-worldwide/npc-studio](https://github.com/npc-worldwide/npc-studio)\\n\\nwhich gives a more user-friendly experience for local models with varying agents. \\n\\nI'm finishing up a paper tn on one of the npcpy modes then will be focusing on integrating automated context compaction and semantic memory evolution into the core npcsh/npc studio flows.\\n\\nwe can win","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1czspy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;the key way we will catch up is through powerful agentic frameworks and tools that give smaller local models the context they need to produce results reliably and intelligently.&lt;/p&gt;\\n\\n&lt;p&gt;try out npcpy/npcsh if you can&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/npc-worldwide/npcpy\\"&gt;https://github.com/npc-worldwide/npcpy&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;and npc studio&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/npc-worldwide/npc-studio\\"&gt;https://github.com/npc-worldwide/npc-studio&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;which gives a more user-friendly experience for local models with varying agents. &lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m finishing up a paper tn on one of the npcpy modes then will be focusing on integrating automated context compaction and semantic memory evolution into the core npcsh/npc studio flows.&lt;/p&gt;\\n\\n&lt;p&gt;we can win&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrpjpc/can_home_sized_llms_32b_etc_or_home_gpus_ever/n1czspy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751658855,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrpjpc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"body":"Yes, if you mean can future locals models compete with current cloud models. \\n\\nProbably not, if you mean can future local models overtake future cloud models, since advances that make the local ones better will likely make the cloud ones better too. That said, it is also possible there is an upper bound to what LLMs can do, which could make the differences insignificant.","subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1ckn77","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"sxales","can_mod_post":false,"created_utc":1751653896,"send_replies":true,"parent_id":"t3_1lrpjpc","score":2,"author_fullname":"t2_5h1ye","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"author_cakeday":true,"edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ckn77","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, if you mean can future locals models compete with current cloud models. &lt;/p&gt;\\n\\n&lt;p&gt;Probably not, if you mean can future local models overtake future cloud models, since advances that make the local ones better will likely make the cloud ones better too. That said, it is also possible there is an upper bound to what LLMs can do, which could make the differences insignificant.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrpjpc/can_home_sized_llms_32b_etc_or_home_gpus_ever/n1ckn77/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751653896,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lrpjpc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1cphk1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Fun-Wolf-2007","can_mod_post":false,"created_utc":1751655453,"send_replies":true,"parent_id":"t3_1lrpjpc","score":2,"author_fullname":"t2_lsixf36sr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You can use specific local LLMs for specific needs, and you have the ability to control the model settings and behavior which is something that cloud based models don't provide.\\n\\nI only use local LLM models for private data, or business related information as that data must be kept private and you own it.\\n\\nI use cloud based inferences for public data or just browse the Web\\n\\nI don't think it is about comparing, it is about applying the technology to resolve specific use cases and solve problems","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1cphk1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can use specific local LLMs for specific needs, and you have the ability to control the model settings and behavior which is something that cloud based models don&amp;#39;t provide.&lt;/p&gt;\\n\\n&lt;p&gt;I only use local LLM models for private data, or business related information as that data must be kept private and you own it.&lt;/p&gt;\\n\\n&lt;p&gt;I use cloud based inferences for public data or just browse the Web&lt;/p&gt;\\n\\n&lt;p&gt;I don&amp;#39;t think it is about comparing, it is about applying the technology to resolve specific use cases and solve problems&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrpjpc/can_home_sized_llms_32b_etc_or_home_gpus_ever/n1cphk1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751655453,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrpjpc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"50c36eba-fdca-11ee-9735-92a88d7e3b87","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1cskj9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Zyj","can_mod_post":false,"created_utc":1751656459,"send_replies":true,"parent_id":"t3_1lrpjpc","score":2,"author_fullname":"t2_9dnfa","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You can already run models at home that are better than the first GPT4. They lag behind the big models, but they keep improving.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1cskj9","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Ollama"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can already run models at home that are better than the first GPT4. They lag behind the big models, but they keep improving.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrpjpc/can_home_sized_llms_32b_etc_or_home_gpus_ever/n1cskj9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751656459,"author_flair_text":"Ollama","treatment_tags":[],"link_id":"t3_1lrpjpc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1cji0l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"createthiscom","can_mod_post":false,"created_utc":1751653529,"send_replies":true,"parent_id":"t3_1lrpjpc","score":3,"author_fullname":"t2_ozxxf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I mean, o4-mini-high still solves things R1-0528 can't solve, but it's pretty infrequent. I'm literally looking at the server I run R1 on as I type this. It's in my livingroom.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1cji0l","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I mean, o4-mini-high still solves things R1-0528 can&amp;#39;t solve, but it&amp;#39;s pretty infrequent. I&amp;#39;m literally looking at the server I run R1 on as I type this. It&amp;#39;s in my livingroom.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrpjpc/can_home_sized_llms_32b_etc_or_home_gpus_ever/n1cji0l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751653529,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrpjpc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1clt6i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Entubulated","can_mod_post":false,"created_utc":1751654270,"send_replies":true,"parent_id":"t3_1lrpjpc","score":2,"author_fullname":"t2_1opxde6hyq","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Points to consider,\\n\\n1. Model scaling (parameter count, training tokens) is seeing diminishing returns, and with current architectures, there must be some kind of logical size cap after which the incremental improvements don't much matter anymore.\\n2. Bus speeds and CPU memory bandwidth keeps creeping upwards.\\n\\nSo, over the long term, unless there's a radical departure from trends, specialized equipment may not be required for 'good enough' performance for home users, or even small orgs.  If this is to be the case, odds are very much against it happening all that quickly.  Check back every 'x' years ; - )","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1clt6i","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Points to consider,&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;Model scaling (parameter count, training tokens) is seeing diminishing returns, and with current architectures, there must be some kind of logical size cap after which the incremental improvements don&amp;#39;t much matter anymore.&lt;/li&gt;\\n&lt;li&gt;Bus speeds and CPU memory bandwidth keeps creeping upwards.&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;So, over the long term, unless there&amp;#39;s a radical departure from trends, specialized equipment may not be required for &amp;#39;good enough&amp;#39; performance for home users, or even small orgs.  If this is to be the case, odds are very much against it happening all that quickly.  Check back every &amp;#39;x&amp;#39; years ; - )&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrpjpc/can_home_sized_llms_32b_etc_or_home_gpus_ever/n1clt6i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751654270,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrpjpc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1cmrhs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Terminator857","can_mod_post":false,"created_utc":1751654576,"send_replies":true,"parent_id":"t3_1lrpjpc","score":1,"author_fullname":"t2_m40tjcn","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Will likely always be behind cloud models, but the best home models are only a year behind last year's sota models.  Memory for hardware is an easy problem to solve in my opinion.\\n\\nIt is only a matter of time, maybe next year?, that we get a huge boost in memory with integrated memory hardware.  I saw a rumor that next years AMD AI Max will have double the memory capacity and double the memory bandwidth, 256GB max memory.  Perhaps twice the cost though.\\n\\nIf LLM diffusion models become popular that will also lower the cost of home LLMs substantially.  Speed will likely not be an issue, but memory will still remain an important spec with diffusion models.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1cmrhs","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Will likely always be behind cloud models, but the best home models are only a year behind last year&amp;#39;s sota models.  Memory for hardware is an easy problem to solve in my opinion.&lt;/p&gt;\\n\\n&lt;p&gt;It is only a matter of time, maybe next year?, that we get a huge boost in memory with integrated memory hardware.  I saw a rumor that next years AMD AI Max will have double the memory capacity and double the memory bandwidth, 256GB max memory.  Perhaps twice the cost though.&lt;/p&gt;\\n\\n&lt;p&gt;If LLM diffusion models become popular that will also lower the cost of home LLMs substantially.  Speed will likely not be an issue, but memory will still remain an important spec with diffusion models.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrpjpc/can_home_sized_llms_32b_etc_or_home_gpus_ever/n1cmrhs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751654576,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrpjpc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1cmwdy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ggone20","can_mod_post":false,"created_utc":1751654619,"send_replies":true,"parent_id":"t3_1lrpjpc","score":1,"author_fullname":"t2_nxled","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Short answer: yes. Context management is the name of the game. Difficult, but possible. Just letting a small model loop a bunch (over simplifying here) is the same as a ‚Äòthinking‚Äô model (test time compute) and can raise performance significantly. Even more so with advanced scaffolding/prompting.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1cmwdy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Short answer: yes. Context management is the name of the game. Difficult, but possible. Just letting a small model loop a bunch (over simplifying here) is the same as a ‚Äòthinking‚Äô model (test time compute) and can raise performance significantly. Even more so with advanced scaffolding/prompting.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrpjpc/can_home_sized_llms_32b_etc_or_home_gpus_ever/n1cmwdy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751654619,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrpjpc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"2b12e2b8-fdc0-11ee-9a03-6e2f48afd456","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1cnugl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Former-Ad-5757","can_mod_post":false,"created_utc":1751654925,"send_replies":true,"parent_id":"t3_1lrpjpc","score":1,"author_fullname":"t2_ihsdiwk6k","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Simple : no.\\nBut the no is mostly because of you naming it home and mentioning 32b etc. Which makes it a money question and a cloud provider can always spend more than an average home.\\n\\nIf you look at it from an OS perspective ( you can run r1 from an OS perspective just not from a home perspective) then it becomes difficult to answer because currently there are competitive powers at work which are OSing models which cost millions to billions of dollars just to frustrate the competition.\\n\\nAnd imho also llms are not the endpoint, just the startpoint of a toolbox which will lead to better ai. Nobody is going to retrain an llm every month on current knowledge, they will need to be fed current data ( for example rag etc), they will need to be fed special data ( for example context7), they will need tools to edit images etc etc llms are just the logic, not the complete ai.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1cnugl","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 3"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Simple : no.\\nBut the no is mostly because of you naming it home and mentioning 32b etc. Which makes it a money question and a cloud provider can always spend more than an average home.&lt;/p&gt;\\n\\n&lt;p&gt;If you look at it from an OS perspective ( you can run r1 from an OS perspective just not from a home perspective) then it becomes difficult to answer because currently there are competitive powers at work which are OSing models which cost millions to billions of dollars just to frustrate the competition.&lt;/p&gt;\\n\\n&lt;p&gt;And imho also llms are not the endpoint, just the startpoint of a toolbox which will lead to better ai. Nobody is going to retrain an llm every month on current knowledge, they will need to be fed current data ( for example rag etc), they will need to be fed special data ( for example context7), they will need tools to edit images etc etc llms are just the logic, not the complete ai.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrpjpc/can_home_sized_llms_32b_etc_or_home_gpus_ever/n1cnugl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751654925,"author_flair_text":"Llama 3","treatment_tags":[],"link_id":"t3_1lrpjpc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#c7b594","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1coxm1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"yaosio","can_mod_post":false,"created_utc":1751655274,"send_replies":true,"parent_id":"t3_1lrpjpc","score":1,"author_fullname":"t2_3z3zm","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"No. Even if local models were identical to cloud models the cloud models would have a massive advantage in speed.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1coxm1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No. Even if local models were identical to cloud models the cloud models would have a massive advantage in speed.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrpjpc/can_home_sized_llms_32b_etc_or_home_gpus_ever/n1coxm1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751655274,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrpjpc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1cp8j3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"simon_zzz","can_mod_post":false,"created_utc":1751655373,"send_replies":true,"parent_id":"t3_1lrpjpc","score":1,"author_fullname":"t2_13auqi","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Currently, I think the biggest shortcomings are small context windows and inability to follow instructions (for instance, tool calling), which really restrict against more complex applications.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1cp8j3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Currently, I think the biggest shortcomings are small context windows and inability to follow instructions (for instance, tool calling), which really restrict against more complex applications.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrpjpc/can_home_sized_llms_32b_etc_or_home_gpus_ever/n1cp8j3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751655373,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrpjpc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1cpth1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"InterstellarReddit","can_mod_post":false,"created_utc":1751655560,"send_replies":true,"parent_id":"t3_1lrpjpc","score":1,"author_fullname":"t2_3aooiye4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Chicken and the egg scenario.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1cpth1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Chicken and the egg scenario.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrpjpc/can_home_sized_llms_32b_etc_or_home_gpus_ever/n1cpth1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751655560,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrpjpc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1dyutj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"curtish77","can_mod_post":false,"created_utc":1751671047,"send_replies":true,"parent_id":"t3_1lrpjpc","score":1,"author_fullname":"t2_91ltgdch","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Due to hardware constraints, cloud will probably always be better.  \\n\\nA better question, IMO, is when will we hit the intersection of good enough local models and good enough local hardware?  Neither exists today, but in several years you‚Äôll probably be thinking why should I bother being tied to the cloud when local can do everything I need, and I have my privacy.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1dyutj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Due to hardware constraints, cloud will probably always be better.  &lt;/p&gt;\\n\\n&lt;p&gt;A better question, IMO, is when will we hit the intersection of good enough local models and good enough local hardware?  Neither exists today, but in several years you‚Äôll probably be thinking why should I bother being tied to the cloud when local can do everything I need, and I have my privacy.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrpjpc/can_home_sized_llms_32b_etc_or_home_gpus_ever/n1dyutj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751671047,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrpjpc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1i0t7j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Pale_Reputation_511","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1emzvb","score":1,"author_fullname":"t2_1s2in94irg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"fair, I guess eventually it will emerge more llms optimized devices than GPUs, since GPUs with more than 24GB are really expensive.  \\nI've opted for a M4 Max instead of a GPU it will not be fast as a GPU but can run larger models in local.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1i0t7j","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;fair, I guess eventually it will emerge more llms optimized devices than GPUs, since GPUs with more than 24GB are really expensive.&lt;br/&gt;\\nI&amp;#39;ve opted for a M4 Max instead of a GPU it will not be fast as a GPU but can run larger models in local.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrpjpc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrpjpc/can_home_sized_llms_32b_etc_or_home_gpus_ever/n1i0t7j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751736236,"author_flair_text":null,"treatment_tags":[],"created_utc":1751736236,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1emzvb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ttkciar","can_mod_post":false,"created_utc":1751681035,"send_replies":true,"parent_id":"t1_n1e2gsf","score":1,"author_fullname":"t2_cpegz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Sure, but how many customers is it having to serve with those resources?\\n\\nYour GPU only has to serve *you!*","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1emzvb","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sure, but how many customers is it having to serve with those resources?&lt;/p&gt;\\n\\n&lt;p&gt;Your GPU only has to serve &lt;em&gt;you!&lt;/em&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrpjpc","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrpjpc/can_home_sized_llms_32b_etc_or_home_gpus_ever/n1emzvb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751681035,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1e2gsf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Pale_Reputation_511","can_mod_post":false,"created_utc":1751672415,"send_replies":true,"parent_id":"t3_1lrpjpc","score":1,"author_fullname":"t2_1s2in94irg","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"a commercial agent in the cloud will always have more resources than llms in your home machine","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1e2gsf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;a commercial agent in the cloud will always have more resources than llms in your home machine&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrpjpc/can_home_sized_llms_32b_etc_or_home_gpus_ever/n1e2gsf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751672415,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrpjpc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1f2dd3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"darkhorse3141","can_mod_post":false,"created_utc":1751687963,"send_replies":true,"parent_id":"t3_1lrpjpc","score":1,"author_fullname":"t2_3z0idi9u","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"No","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1f2dd3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrpjpc/can_home_sized_llms_32b_etc_or_home_gpus_ever/n1f2dd3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751687963,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrpjpc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1h5z7c","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ykoech","can_mod_post":false,"created_utc":1751726472,"send_replies":true,"parent_id":"t3_1lrpjpc","score":1,"author_fullname":"t2_vuh1jbn","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"No.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1h5z7c","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrpjpc/can_home_sized_llms_32b_etc_or_home_gpus_ever/n1h5z7c/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751726472,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrpjpc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1fbgh9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ardalok","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1fa7xi","score":1,"author_fullname":"t2_vgnewja","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"no but there is video on youtube. look for \\"ktransformers deeepseek\\"","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1fbgh9","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;no but there is video on youtube. look for &amp;quot;ktransformers deeepseek&amp;quot;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrpjpc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrpjpc/can_home_sized_llms_32b_etc_or_home_gpus_ever/n1fbgh9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751692580,"author_flair_text":null,"treatment_tags":[],"created_utc":1751692580,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1fa7xi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Glittering-Call8746","can_mod_post":false,"created_utc":1751691920,"send_replies":true,"parent_id":"t1_n1cnaru","score":1,"author_fullname":"t2_tqwl6sawb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"U did this ?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1fa7xi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;U did this ?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrpjpc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrpjpc/can_home_sized_llms_32b_etc_or_home_gpus_ever/n1fa7xi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751691920,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1cnaru","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ardalok","can_mod_post":false,"created_utc":1751654747,"send_replies":true,"parent_id":"t3_1lrpjpc","score":1,"author_fullname":"t2_vgnewja","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You could probably run Qwen 235B or DeepSeek locally with a server CPU and a bunch of RAM ‚Äî all for about the cost of two 5090s. So we're not that far from matching cloud models.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1cnaru","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You could probably run Qwen 235B or DeepSeek locally with a server CPU and a bunch of RAM ‚Äî all for about the cost of two 5090s. So we&amp;#39;re not that far from matching cloud models.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrpjpc/can_home_sized_llms_32b_etc_or_home_gpus_ever/n1cnaru/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751654747,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrpjpc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(l,{data:t});export{n as default};
