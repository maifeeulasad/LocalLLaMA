import{j as e}from"./index-F0NXdzZX.js";import{R as t}from"./RedditPostRenderer-CoEZB1dt.js";import"./index-DrtravXm.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hey all! I'm creating a project that applies Monte Carlo Tree Search to LLM conversations. Instead of just generating the next response, it simulates entire conversation trees to find paths that achieve long-term goals. The initial draft version is up.\\n\\nGithub: [https://github.com/MVPandey/CAE](https://github.com/MVPandey/CAE)\\n\\n  \\n(Note: This is a Claude-generated mock UI. The payload is real but the UI is simulated :) I'm a terrible frontend dev)\\n\\nhttps://i.redd.it/dqws3fzgysaf1.gif\\n\\n  \\n\\n\\n**How it works:**\\n\\n* Generates multiple response candidates at each conversation state\\n* Simulates how conversations might unfold down each branch (using the LLM to predict user responses)\\n* Scores each trajectory on metrics like empathy, goal achievement, coherence\\n* Uses MCTS with UCB1 to efficiently explore the most promising paths\\n* Selects the response that leads to the best expected outcome\\n\\n**Technical implementation:**\\n\\n* FastAPI backend with async SQLAlchemy (PostgreSQL)\\n* Aggressive parallelization - all branch evaluations run concurrently with asyncio.gather()\\n* Works with any OpenAI-compatible endpoint\\n* Dual-purpose: works as both a standard chat API and on-demand analysis engine\\n* No agentic framework dependencies \\n\\n**Limitations:**\\n\\n* Scoring is done by the same LLM that generates responses (obviously bad - not very grounded or reproducible or scientific yet)\\n* Branch pruning is naive - just threshold-based instead of something smarter like progressive widening\\n* Memory usage grows with tree size - haven't implemented node recycling yet\\n* The pgvector embedding code is there but commented out (wanted semantic search over conversation history)\\n\\nOriginally thought of this to generate preference data for RL training (converting instruct/response datasets to PPO datasets) and refined the idea into code at a hackathon - the system outputs full JSON showing why certain conversation paths outperform others, with rationales and metrics. Been testing on customer support scenarios and therapeutic conversations.\\n\\nExample output shows the selected response, rejected alternatives, simulated user reactions, and scoring breakdowns. Pretty interesting to see it reason through de-escalation strategies or teaching approaches.\\n\\nCurious if anyone's tried similar approaches or has ideas for more grounded scoring methods. The LLM-as-judge problem is real here.\\n\\nAnyway, please let me know any thoughts, criticisms, feedback, etc! :) \\n\\nI also am not sure what I want this project to evolve into. This is a very crude first approach and IDK what I wanna do for next steps. ","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Created an Open Source Conversation Response Path Exploration System using Monte Carlo Tree Search","link_flair_richtext":[{"e":"text","t":"Tutorial | Guide"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":70,"top_awarded_type":null,"hide_score":false,"media_metadata":{"dqws3fzgysaf1":{"status":"valid","e":"AnimatedImage","m":"image/gif","p":[{"y":68,"x":108,"u":"https://preview.redd.it/dqws3fzgysaf1.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=681d3e8ada7686c61eb140c06d2970adf715c643"},{"y":137,"x":216,"u":"https://preview.redd.it/dqws3fzgysaf1.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=501058916599f636f9c6f39e654606a970a31607"},{"y":204,"x":320,"u":"https://preview.redd.it/dqws3fzgysaf1.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=d6716ae553702d0115496d7b4d8376216ebc6e8a"},{"y":408,"x":640,"u":"https://preview.redd.it/dqws3fzgysaf1.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=57b11dd73f09e4d90403510ca044cbf554fc102f"}],"s":{"y":511,"gif":"https://i.redd.it/dqws3fzgysaf1.gif","mp4":"https://preview.redd.it/dqws3fzgysaf1.gif?format=mp4&amp;s=98baaded5b2c4c8b2c39eb68cbe5851b9161debd","x":800},"id":"dqws3fzgysaf1"}},"name":"t3_1lrbwmz","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.98,"author_flair_background_color":null,"ups":359,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_3fx3fjwm","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Tutorial | Guide","can_mod_post":false,"score":359,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://external-preview.redd.it/g_tsBx-sQoZLGvWQ7PFRoKZ5UY7Qfo6eiDBT125d8YE.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=b817f104e6c280379522e8c2510286bb2bcf024e","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"subreddit_type":"public","created":1751610972,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey all! I&amp;#39;m creating a project that applies Monte Carlo Tree Search to LLM conversations. Instead of just generating the next response, it simulates entire conversation trees to find paths that achieve long-term goals. The initial draft version is up.&lt;/p&gt;\\n\\n&lt;p&gt;Github: &lt;a href=\\"https://github.com/MVPandey/CAE\\"&gt;https://github.com/MVPandey/CAE&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;(Note: This is a Claude-generated mock UI. The payload is real but the UI is simulated :) I&amp;#39;m a terrible frontend dev)&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://i.redd.it/dqws3fzgysaf1.gif\\"&gt;https://i.redd.it/dqws3fzgysaf1.gif&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Generates multiple response candidates at each conversation state&lt;/li&gt;\\n&lt;li&gt;Simulates how conversations might unfold down each branch (using the LLM to predict user responses)&lt;/li&gt;\\n&lt;li&gt;Scores each trajectory on metrics like empathy, goal achievement, coherence&lt;/li&gt;\\n&lt;li&gt;Uses MCTS with UCB1 to efficiently explore the most promising paths&lt;/li&gt;\\n&lt;li&gt;Selects the response that leads to the best expected outcome&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Technical implementation:&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;FastAPI backend with async SQLAlchemy (PostgreSQL)&lt;/li&gt;\\n&lt;li&gt;Aggressive parallelization - all branch evaluations run concurrently with asyncio.gather()&lt;/li&gt;\\n&lt;li&gt;Works with any OpenAI-compatible endpoint&lt;/li&gt;\\n&lt;li&gt;Dual-purpose: works as both a standard chat API and on-demand analysis engine&lt;/li&gt;\\n&lt;li&gt;No agentic framework dependencies &lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Limitations:&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Scoring is done by the same LLM that generates responses (obviously bad - not very grounded or reproducible or scientific yet)&lt;/li&gt;\\n&lt;li&gt;Branch pruning is naive - just threshold-based instead of something smarter like progressive widening&lt;/li&gt;\\n&lt;li&gt;Memory usage grows with tree size - haven&amp;#39;t implemented node recycling yet&lt;/li&gt;\\n&lt;li&gt;The pgvector embedding code is there but commented out (wanted semantic search over conversation history)&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Originally thought of this to generate preference data for RL training (converting instruct/response datasets to PPO datasets) and refined the idea into code at a hackathon - the system outputs full JSON showing why certain conversation paths outperform others, with rationales and metrics. Been testing on customer support scenarios and therapeutic conversations.&lt;/p&gt;\\n\\n&lt;p&gt;Example output shows the selected response, rejected alternatives, simulated user reactions, and scoring breakdowns. Pretty interesting to see it reason through de-escalation strategies or teaching approaches.&lt;/p&gt;\\n\\n&lt;p&gt;Curious if anyone&amp;#39;s tried similar approaches or has ideas for more grounded scoring methods. The LLM-as-judge problem is real here.&lt;/p&gt;\\n\\n&lt;p&gt;Anyway, please let me know any thoughts, criticisms, feedback, etc! :) &lt;/p&gt;\\n\\n&lt;p&gt;I also am not sure what I want this project to evolve into. This is a very crude first approach and IDK what I wanna do for next steps. &lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/g_tsBx-sQoZLGvWQ7PFRoKZ5UY7Qfo6eiDBT125d8YE.png?auto=webp&amp;s=47a3f76cc722819491300c172d752e4c070bb71e","width":1200,"height":600},"resolutions":[{"url":"https://external-preview.redd.it/g_tsBx-sQoZLGvWQ7PFRoKZ5UY7Qfo6eiDBT125d8YE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4f2b459033ec7b1c73ba64efd65042a691fe84f0","width":108,"height":54},{"url":"https://external-preview.redd.it/g_tsBx-sQoZLGvWQ7PFRoKZ5UY7Qfo6eiDBT125d8YE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f5c8d8980cb42e5790ed42559709e0a5abe078ac","width":216,"height":108},{"url":"https://external-preview.redd.it/g_tsBx-sQoZLGvWQ7PFRoKZ5UY7Qfo6eiDBT125d8YE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0235e4b5d4b4902638d04f61b768821c4f6dc438","width":320,"height":160},{"url":"https://external-preview.redd.it/g_tsBx-sQoZLGvWQ7PFRoKZ5UY7Qfo6eiDBT125d8YE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8a18a6b5aef4f1d48310d2918ee6ff6f6c5943c2","width":640,"height":320},{"url":"https://external-preview.redd.it/g_tsBx-sQoZLGvWQ7PFRoKZ5UY7Qfo6eiDBT125d8YE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a2e375eec31e7ac8f8c340f24ca723dd41a02316","width":960,"height":480},{"url":"https://external-preview.redd.it/g_tsBx-sQoZLGvWQ7PFRoKZ5UY7Qfo6eiDBT125d8YE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=059d2324da4c876d11cd8712dde3728119ec0134","width":1080,"height":540}],"variants":{},"id":"g_tsBx-sQoZLGvWQ7PFRoKZ5UY7Qfo6eiDBT125d8YE"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"449b05a6-bf8e-11ed-b4bd-66961e47bd50","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"mod_note":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"num_reports":null,"removal_reason":null,"link_flair_background_color":"#0079d3","id":"1lrbwmz","is_robot_indexable":true,"num_duplicates":1,"report_reasons":null,"author":"ManavTheWorld","discussion_type":null,"num_comments":14,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lrbwmz/created_an_open_source_conversation_response_path/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lrbwmz/created_an_open_source_conversation_response_path/","subreddit_subscribers":494987,"created_utc":1751610972,"num_crossposts":1,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1bx0q7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ManavTheWorld","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1bu9we","score":2,"author_fullname":"t2_3fx3fjwm","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thanks for the compliments and suggestions!\\n\\nTo be honest, it could be a lot cleaner. I had an issue with some of the validation/imports so some of the schema is yuck and I posted here as soon as it was end to end functional haha. I’ll update it over the coming days to be fully usable/enterprise-ready.\\n\\nAnd I agree about the embeddings! My thought was to also create functionality for learnings and the DB schema already supports vectors for the individual messages toward this end, but I haven’t yet begun to implement this or the learning functionality. I think the dataset idea is awesome though! Will look into it, thanks!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1bx0q7","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for the compliments and suggestions!&lt;/p&gt;\\n\\n&lt;p&gt;To be honest, it could be a lot cleaner. I had an issue with some of the validation/imports so some of the schema is yuck and I posted here as soon as it was end to end functional haha. I’ll update it over the coming days to be fully usable/enterprise-ready.&lt;/p&gt;\\n\\n&lt;p&gt;And I agree about the embeddings! My thought was to also create functionality for learnings and the DB schema already supports vectors for the individual messages toward this end, but I haven’t yet begun to implement this or the learning functionality. I think the dataset idea is awesome though! Will look into it, thanks!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrbwmz","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrbwmz/created_an_open_source_conversation_response_path/n1bx0q7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751646676,"author_flair_text":null,"treatment_tags":[],"created_utc":1751646676,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1bu9we","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RMCPhoto","can_mod_post":false,"send_replies":true,"parent_id":"t1_n19qkr0","score":3,"author_fullname":"t2_ehhvb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":" Cool project, love algorithmic approaches like this and it looks clean and actually usable.   \\n  \\nOne option to grasp a better idea of how a user might respond is to lean on some free datasets:   \\n\\\\- Create an embedding and find similar conversational objects / perform sentiment analysis etc.   [https://github.com/PolyAI-LDN/conversational-datasets](https://github.com/PolyAI-LDN/conversational-datasets)  [https://huggingface.co/datasets/allenai/WildChat-1M](https://huggingface.co/datasets/allenai/WildChat-1M)\\n\\nOtherwise, I would definitely recommend creating mechanisms for self improvement - if not in a live agentic loop, then by collecting the right data over time (assuming that's the goal and we don't want to actually run 5x chats for every message).\\n\\nIn which case it can be helpful to perform a clustering or statistical semantic analysis on the winners and losers and identify patterns (and/or expand on the llm as a judge and additionally export structured information that can be used to improve the prompt.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1bu9we","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Cool project, love algorithmic approaches like this and it looks clean and actually usable.   &lt;/p&gt;\\n\\n&lt;p&gt;One option to grasp a better idea of how a user might respond is to lean on some free datasets:&lt;br/&gt;\\n- Create an embedding and find similar conversational objects / perform sentiment analysis etc.   &lt;a href=\\"https://github.com/PolyAI-LDN/conversational-datasets\\"&gt;https://github.com/PolyAI-LDN/conversational-datasets&lt;/a&gt;  &lt;a href=\\"https://huggingface.co/datasets/allenai/WildChat-1M\\"&gt;https://huggingface.co/datasets/allenai/WildChat-1M&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Otherwise, I would definitely recommend creating mechanisms for self improvement - if not in a live agentic loop, then by collecting the right data over time (assuming that&amp;#39;s the goal and we don&amp;#39;t want to actually run 5x chats for every message).&lt;/p&gt;\\n\\n&lt;p&gt;In which case it can be helpful to perform a clustering or statistical semantic analysis on the winners and losers and identify patterns (and/or expand on the llm as a judge and additionally export structured information that can be used to improve the prompt.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrbwmz","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrbwmz/created_an_open_source_conversation_response_path/n1bu9we/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751645835,"author_flair_text":null,"treatment_tags":[],"created_utc":1751645835,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1a8res","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mnt_brain","can_mod_post":false,"send_replies":true,"parent_id":"t1_n19qkr0","score":1,"author_fullname":"t2_1mtt9dytfn","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Just amass enough data to make an RLHF’d user response","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1a8res","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just amass enough data to make an RLHF’d user response&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrbwmz","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrbwmz/created_an_open_source_conversation_response_path/n1a8res/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751625243,"author_flair_text":null,"treatment_tags":[],"created_utc":1751625243,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n19qkr0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ManavTheWorld","can_mod_post":false,"created_utc":1751614955,"send_replies":true,"parent_id":"t1_n19py32","score":14,"author_fullname":"t2_3fx3fjwm","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks for the feedback! And you’re absolutely right - one of the core issues I saw was that assumed user responses were kind of limited/missing a lot of unsaid context. The issue is that it’s very expensive for simulation after a certain depth, and I thought to perhaps make it a tool call that the model can invoke once it decides it has enough context, based on certain rules/guidelines.\\n\\nAnd you’re right about reasoning models! I haven’t yet benchmarked the quality of these versus simply prompting an intelligent enough CoT LLM, but I think it would be interesting to see where the value of search could come into play for something like this. Short answer is: IDK but will update here when I figure out the direction.\\n\\nP.S I can’t take credit for the Readme! That was Gemini 2.5, though I removed the LLM cringe. Thanks for the compliment about the project structure though!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n19qkr0","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for the feedback! And you’re absolutely right - one of the core issues I saw was that assumed user responses were kind of limited/missing a lot of unsaid context. The issue is that it’s very expensive for simulation after a certain depth, and I thought to perhaps make it a tool call that the model can invoke once it decides it has enough context, based on certain rules/guidelines.&lt;/p&gt;\\n\\n&lt;p&gt;And you’re right about reasoning models! I haven’t yet benchmarked the quality of these versus simply prompting an intelligent enough CoT LLM, but I think it would be interesting to see where the value of search could come into play for something like this. Short answer is: IDK but will update here when I figure out the direction.&lt;/p&gt;\\n\\n&lt;p&gt;P.S I can’t take credit for the Readme! That was Gemini 2.5, though I removed the LLM cringe. Thanks for the compliment about the project structure though!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrbwmz","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrbwmz/created_an_open_source_conversation_response_path/n19qkr0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751614955,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":14}}],"before":null}},"user_reports":[],"saved":false,"id":"n19py32","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Chromix_","can_mod_post":false,"created_utc":1751614592,"send_replies":true,"parent_id":"t3_1lrbwmz","score":28,"author_fullname":"t2_k7w2h","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This sounds like [improved beam search](https://www.reddit.com/r/LocalLLaMA/comments/1jfrwqw/comment/mitgz5v/), just for conversation turns, not for tokens.\\n\\n&gt;Scoring is done by the same LLM that generates responses (obviously bad...)\\n\\nYes, but there is more. Even the best LLM will not have enough information to accurately predict how the user might respond, as it doesn't know what the user didn't write - what's behind the request, and their current situation. This will probably improve after a few conversation turns though as it asks for more information and the predictions could become more accurate. It'd be interesting to measure the \\"difference\\" between what the user actually wrote and what was predicted. The first incorrect turn prediction will derail all those that follow.\\n\\nKeeping the LLM from jumping to conclusions, like they tend to do when trained on single Q&amp;A pairs, is already an improvement. I just wonder: Couldn't a reasoning LLM do the same when prompted for it - just in a simpler way?\\n\\nReally nice project setup by the way, with OpenAI API support, no agentic framework dependencies and a good bunch of documentation.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n19py32","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This sounds like &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1jfrwqw/comment/mitgz5v/\\"&gt;improved beam search&lt;/a&gt;, just for conversation turns, not for tokens.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Scoring is done by the same LLM that generates responses (obviously bad...)&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Yes, but there is more. Even the best LLM will not have enough information to accurately predict how the user might respond, as it doesn&amp;#39;t know what the user didn&amp;#39;t write - what&amp;#39;s behind the request, and their current situation. This will probably improve after a few conversation turns though as it asks for more information and the predictions could become more accurate. It&amp;#39;d be interesting to measure the &amp;quot;difference&amp;quot; between what the user actually wrote and what was predicted. The first incorrect turn prediction will derail all those that follow.&lt;/p&gt;\\n\\n&lt;p&gt;Keeping the LLM from jumping to conclusions, like they tend to do when trained on single Q&amp;amp;A pairs, is already an improvement. I just wonder: Couldn&amp;#39;t a reasoning LLM do the same when prompted for it - just in a simpler way?&lt;/p&gt;\\n\\n&lt;p&gt;Really nice project setup by the way, with OpenAI API support, no agentic framework dependencies and a good bunch of documentation.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrbwmz/created_an_open_source_conversation_response_path/n19py32/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751614592,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrbwmz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":28}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n19sbja","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"bdizzle146","can_mod_post":false,"created_utc":1751615940,"send_replies":true,"parent_id":"t3_1lrbwmz","score":7,"author_fullname":"t2_9g6xk0pv","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I've said for a while that, especially with agents, the important metric is correct responses per second. The ways to improve are to respond faster, and get more correct.\\n\\n\\nThis would help get more correct responses at the cost of speed, but with the advances in MoE this year, speed is no longer the constraint.\\n\\n\\nAwesome project. ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n19sbja","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve said for a while that, especially with agents, the important metric is correct responses per second. The ways to improve are to respond faster, and get more correct.&lt;/p&gt;\\n\\n&lt;p&gt;This would help get more correct responses at the cost of speed, but with the advances in MoE this year, speed is no longer the constraint.&lt;/p&gt;\\n\\n&lt;p&gt;Awesome project. &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrbwmz/created_an_open_source_conversation_response_path/n19sbja/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751615940,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrbwmz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1bxv87","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ManavTheWorld","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1bv9kg","score":1,"author_fullname":"t2_3fx3fjwm","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Agreed. Not novel! The algorithm/use-case isn’t novel, but I’m hoping it’ll evolve into an application that anyone can clone and take advantage of.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1bxv87","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Agreed. Not novel! The algorithm/use-case isn’t novel, but I’m hoping it’ll evolve into an application that anyone can clone and take advantage of.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrbwmz","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrbwmz/created_an_open_source_conversation_response_path/n1bxv87/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751646938,"author_flair_text":null,"treatment_tags":[],"created_utc":1751646938,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1bv9kg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RMCPhoto","can_mod_post":false,"created_utc":1751646136,"send_replies":true,"parent_id":"t1_n1b32sf","score":3,"author_fullname":"t2_ehhvb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I think ops implementation (and clean code that the community can use) is excellent.  \\n\\nHowever, there is a near mountain of research papers on using MCTS with LLM as judge. \\n\\n(Just a very very quick skim)\\n\\n[https://arxiv.org/abs/2505.23229](https://arxiv.org/abs/2505.23229)  \\n[https://arxiv.org/abs/2504.02426](https://arxiv.org/abs/2504.02426)  \\n[https://arxiv.org/abs/2504.11009](https://arxiv.org/abs/2504.11009)  \\n[https://arxiv.org/abs/2502.13428](https://arxiv.org/abs/2502.13428)  \\n[https://arxiv.org/abs/2503.19309](https://arxiv.org/abs/2503.19309)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1bv9kg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think ops implementation (and clean code that the community can use) is excellent.  &lt;/p&gt;\\n\\n&lt;p&gt;However, there is a near mountain of research papers on using MCTS with LLM as judge. &lt;/p&gt;\\n\\n&lt;p&gt;(Just a very very quick skim)&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://arxiv.org/abs/2505.23229\\"&gt;https://arxiv.org/abs/2505.23229&lt;/a&gt;&lt;br/&gt;\\n&lt;a href=\\"https://arxiv.org/abs/2504.02426\\"&gt;https://arxiv.org/abs/2504.02426&lt;/a&gt;&lt;br/&gt;\\n&lt;a href=\\"https://arxiv.org/abs/2504.11009\\"&gt;https://arxiv.org/abs/2504.11009&lt;/a&gt;&lt;br/&gt;\\n&lt;a href=\\"https://arxiv.org/abs/2502.13428\\"&gt;https://arxiv.org/abs/2502.13428&lt;/a&gt;&lt;br/&gt;\\n&lt;a href=\\"https://arxiv.org/abs/2503.19309\\"&gt;https://arxiv.org/abs/2503.19309&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrbwmz","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrbwmz/created_an_open_source_conversation_response_path/n1bv9kg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751646136,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1bxji8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ManavTheWorld","can_mod_post":false,"created_utc":1751646838,"send_replies":true,"parent_id":"t1_n1b32sf","score":1,"author_fullname":"t2_3fx3fjwm","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thank you for your feedback and nice words, but can’t take credit here! Not a novel approach - I just wanted to build an “engine”/backend using search-based conversation optimization, and potentially have it work as an ambient agent (asynchronously evaluating conversations in realtime), or as a tool/MCP server for giving back extended analysis, given its learnings and grounding info. Perhaps both or neither. Don’t know yet! :)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1bxji8","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you for your feedback and nice words, but can’t take credit here! Not a novel approach - I just wanted to build an “engine”/backend using search-based conversation optimization, and potentially have it work as an ambient agent (asynchronously evaluating conversations in realtime), or as a tool/MCP server for giving back extended analysis, given its learnings and grounding info. Perhaps both or neither. Don’t know yet! :)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrbwmz","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrbwmz/created_an_open_source_conversation_response_path/n1bxji8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751646838,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1b32sf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Edge2098","can_mod_post":false,"created_utc":1751637519,"send_replies":true,"parent_id":"t3_1lrbwmz","score":3,"author_fullname":"t2_uaotuj04","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Since it simulates discussion branches for long-term value rather than just next-token prediction, this is genuinely one of the most inventive uses of MCTS that I have seen in the LLM field. Even as a prototype, this seems to have a lot of potential for support bots, coaching aids, or even dialogue training. I completely agree that the scoring loop needs to be grounded. I'd love to see this develop using an external scoring model or a more intelligent pruning technique. Fantastic work!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1b32sf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Since it simulates discussion branches for long-term value rather than just next-token prediction, this is genuinely one of the most inventive uses of MCTS that I have seen in the LLM field. Even as a prototype, this seems to have a lot of potential for support bots, coaching aids, or even dialogue training. I completely agree that the scoring loop needs to be grounded. I&amp;#39;d love to see this develop using an external scoring model or a more intelligent pruning technique. Fantastic work!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrbwmz/created_an_open_source_conversation_response_path/n1b32sf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751637519,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrbwmz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1am3nv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ManavTheWorld","can_mod_post":false,"created_utc":1751631346,"send_replies":true,"parent_id":"t1_n19v9vz","score":1,"author_fullname":"t2_3fx3fjwm","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Haha yeah it’s a Uvicorn server so you can run it as a Python module. I’ll create a start script and a Dockerfile for it though, thanks for the ask!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1am3nv","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Haha yeah it’s a Uvicorn server so you can run it as a Python module. I’ll create a start script and a Dockerfile for it though, thanks for the ask!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrbwmz","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrbwmz/created_an_open_source_conversation_response_path/n1am3nv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751631346,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n19v9vz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"sammcj","can_mod_post":false,"created_utc":1751617624,"send_replies":true,"parent_id":"t3_1lrbwmz","score":1,"author_fullname":"t2_3mf7o","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Hey, looks interesting, is there a way to run the app's web interface up without having to load it up in a debug session in vscode though? That seems a little odd.","edited":1751618041,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n19v9vz","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey, looks interesting, is there a way to run the app&amp;#39;s web interface up without having to load it up in a debug session in vscode though? That seems a little odd.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrbwmz/created_an_open_source_conversation_response_path/n19v9vz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751617624,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lrbwmz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"d2642412-d9ce-11ed-ae30-32b11309f5bd","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n19x42u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Everlier","can_mod_post":false,"created_utc":1751618686,"send_replies":true,"parent_id":"t3_1lrbwmz","score":0,"author_fullname":"t2_o7p5m","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Here's a similar workflow that works as a proxy with any OpenAI-compatible LLM/Client:\\nhttps://github.com/av/harbor/wiki/5.2.-Harbor-Boost#mcts---monte-carlo-tree-search\\n\\nTo run standalone without Harbor (module is \`mcts\`)\\nhttps://github.com/av/boost-starter","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n19x42u","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Alpaca"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Here&amp;#39;s a similar workflow that works as a proxy with any OpenAI-compatible LLM/Client:\\n&lt;a href=\\"https://github.com/av/harbor/wiki/5.2.-Harbor-Boost#mcts---monte-carlo-tree-search\\"&gt;https://github.com/av/harbor/wiki/5.2.-Harbor-Boost#mcts---monte-carlo-tree-search&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;To run standalone without Harbor (module is &lt;code&gt;mcts&lt;/code&gt;)\\n&lt;a href=\\"https://github.com/av/boost-starter\\"&gt;https://github.com/av/boost-starter&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrbwmz/created_an_open_source_conversation_response_path/n19x42u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751618686,"author_flair_text":"Alpaca","treatment_tags":[],"link_id":"t3_1lrbwmz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bd9e9e","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1biex4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Neither_Lettuce_7575","can_mod_post":false,"created_utc":1751642272,"send_replies":true,"parent_id":"t3_1lrbwmz","score":-2,"author_fullname":"t2_1ot46mnpy8","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Any way to know that they are cloning my phone","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1biex4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Any way to know that they are cloning my phone&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrbwmz/created_an_open_source_conversation_response_path/n1biex4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751642272,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrbwmz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-2}}],"before":null}}]`),o=()=>e.jsx(t,{data:a});export{o as default};
