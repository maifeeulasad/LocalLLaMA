import{j as e}from"./index-cvG704yx.js";import{R as l}from"./RedditPostRenderer-CBthLTAH.js";import"./index-D-GavSZU.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Based their config.json, it is essentially a DeepSeekV3 with more experts (384 vs 256). Number of attention heads reduced from 128 to 64. Number of dense layers reduced from 3 to 1:\\n\\n|Model|dense layer#|MoE layer#|shared|active/routed|Shared|Active|Params|Active%|fp16 kv@128k|kv%|\\n|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|\\n|DeepSeek-MoE-16B|1|27|2|6/64|1.42B|2.83B|16.38B|17.28%|28GB|85.47%|\\n|DeepSeek-V2-Lite|1|26|2|6/64|1.31B|2.66B|15.71B|16.93%|3.8GB|12.09%|\\n|DeepSeek-V2|1|59|2|6/160|12.98B|21.33B|235.74B|8.41%|8.44GB|1.78%|\\n|DeepSeek-V3|3|58|1|8/256|17.01B|37.45B|671.03B|5.58%|8.578GB|0.64%|\\n|Kimi-K2|1|60|1|8/384|11.56B|32.70B|1026.41B|3.19%|8.578GB|0.42%|\\n|Qwen3-30B-A3B|0|48|0|8/128|1.53B|3.34B|30.53B|10.94%|12GB|19.65%|\\n|Qwen3-235B-A22B|0|94|0|8/128|7.95B|22.14B|235.09B|9.42%|23.5GB|4.998%|\\n|Llama-4-Scout-17B-16E|0|48|1|1/16|11.13B|17.17B|107.77B|15.93%|24GB|11.13%|\\n|Llama-4-Maverick-17B-128E|24|24|1|1/128|14.15B|17.17B|400.71B|4.28%|24GB|2.99%|\\n|Mixtral-8x7B|0|32|0|2/8|1.60B|12.88B|46.70B|27.58%|24GB|25.696%|\\n|Mixtral-8x22B|0|56|0|2/8|5.33B|39.15B|140.62B|27.84%|28GB|9.956%|\\n\\nLooks like their Kimi-Dev-72B is from Qwen2-72B. Moonlight is a small DSV3. \\n\\nModels using their own architecture is Kimi-VL and Kimi-Audio. \\n\\nEdited: Per u/Aaaaaaaaaeeeee 's request. I added a column called \\"Shared\\" which is the active params minus the routed experts params. This is the maximum amount of parameters you can offload to a GPU when you load all the routed experts to the CPU RAM using the -ot params from llama.cpp.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Kimi-K2 is a DeepSeek V3 with more experts","link_flair_richtext":[{"e":"text","t":"Resources"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lzcuom","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.97,"author_flair_background_color":null,"subreddit_type":"public","ups":214,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_s6sfw4yy","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Resources","can_mod_post":false,"score":214,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1752495937,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752466353,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Based their config.json, it is essentially a DeepSeekV3 with more experts (384 vs 256). Number of attention heads reduced from 128 to 64. Number of dense layers reduced from 3 to 1:&lt;/p&gt;\\n\\n&lt;table&gt;&lt;thead&gt;\\n&lt;tr&gt;\\n&lt;th align=\\"left\\"&gt;Model&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;dense layer#&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;MoE layer#&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;shared&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;active/routed&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;Shared&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;Active&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;Params&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;Active%&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;fp16 kv@128k&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;kv%&lt;/th&gt;\\n&lt;/tr&gt;\\n&lt;/thead&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;DeepSeek-MoE-16B&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;1&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;27&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;2&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;6/64&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;1.42B&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;2.83B&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;16.38B&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;17.28%&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;28GB&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;85.47%&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;DeepSeek-V2-Lite&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;1&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;26&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;2&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;6/64&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;1.31B&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;2.66B&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;15.71B&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;16.93%&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;3.8GB&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;12.09%&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;DeepSeek-V2&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;1&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;59&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;2&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;6/160&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;12.98B&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;21.33B&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;235.74B&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;8.41%&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;8.44GB&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;1.78%&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;DeepSeek-V3&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;3&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;58&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;1&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;8/256&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;17.01B&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;37.45B&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;671.03B&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;5.58%&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;8.578GB&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;0.64%&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;Kimi-K2&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;1&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;60&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;1&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;8/384&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;11.56B&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;32.70B&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;1026.41B&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;3.19%&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;8.578GB&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;0.42%&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;Qwen3-30B-A3B&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;0&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;48&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;0&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;8/128&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;1.53B&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;3.34B&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;30.53B&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;10.94%&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;12GB&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;19.65%&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;Qwen3-235B-A22B&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;0&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;94&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;0&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;8/128&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;7.95B&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;22.14B&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;235.09B&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;9.42%&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;23.5GB&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;4.998%&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;Llama-4-Scout-17B-16E&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;0&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;48&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;1&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;1/16&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;11.13B&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;17.17B&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;107.77B&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;15.93%&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;24GB&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;11.13%&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;Llama-4-Maverick-17B-128E&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;24&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;24&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;1&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;1/128&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;14.15B&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;17.17B&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;400.71B&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;4.28%&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;24GB&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;2.99%&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;Mixtral-8x7B&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;0&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;32&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;0&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;2/8&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;1.60B&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;12.88B&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;46.70B&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;27.58%&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;24GB&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;25.696%&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;Mixtral-8x22B&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;0&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;56&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;0&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;2/8&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;5.33B&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;39.15B&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;140.62B&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;27.84%&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;28GB&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;9.956%&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;/tbody&gt;&lt;/table&gt;\\n\\n&lt;p&gt;Looks like their Kimi-Dev-72B is from Qwen2-72B. Moonlight is a small DSV3. &lt;/p&gt;\\n\\n&lt;p&gt;Models using their own architecture is Kimi-VL and Kimi-Audio. &lt;/p&gt;\\n\\n&lt;p&gt;Edited: Per &lt;a href=\\"/u/Aaaaaaaaaeeeee\\"&gt;u/Aaaaaaaaaeeeee&lt;/a&gt; &amp;#39;s request. I added a column called &amp;quot;Shared&amp;quot; which is the active params minus the routed experts params. This is the maximum amount of parameters you can offload to a GPU when you load all the routed experts to the CPU RAM using the -ot params from llama.cpp.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#ccac2b","id":"1lzcuom","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Ok_Warning2146","discussion_type":null,"num_comments":36,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/","subreddit_subscribers":499297,"created_utc":1752466353,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n313ic1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"shing3232","can_mod_post":false,"created_utc":1752472650,"send_replies":true,"parent_id":"t1_n30wmrd","score":19,"author_fullname":"t2_ze4mg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It s a scaling method of test new optimizer  given the same amount data with bigger model and less activation","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n313ic1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It s a scaling method of test new optimizer  given the same amount data with bigger model and less activation&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzcuom","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/n313ic1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752472650,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":19}}],"before":null}},"user_reports":[],"saved":false,"id":"n30wmrd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"pigeon57434","can_mod_post":false,"created_utc":1752469179,"send_replies":true,"parent_id":"t3_1lzcuom","score":70,"author_fullname":"t2_8j5t7yjq","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"well not to mention that its also like 330B parameters larger so I'm not really surprised it outperformans deepseek and has more experts","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n30wmrd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;well not to mention that its also like 330B parameters larger so I&amp;#39;m not really surprised it outperformans deepseek and has more experts&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/n30wmrd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752469179,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzcuom","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":70}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n32bk1o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok_Warning2146","can_mod_post":false,"send_replies":true,"parent_id":"t1_n31car6","score":3,"author_fullname":"t2_s6sfw4yy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I see. I think the active params minus the 8 routed experts (DSV3 as an e.g.) is the maximum amount of params you can offload to the CPU. I added this number as a column called \\"Shared\\". This should be the maximum amount of parameters you can offload to GPU and put the routed experts to CPU RAM.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n32bk1o","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I see. I think the active params minus the 8 routed experts (DSV3 as an e.g.) is the maximum amount of params you can offload to the CPU. I added this number as a column called &amp;quot;Shared&amp;quot;. This should be the maximum amount of parameters you can offload to GPU and put the routed experts to CPU RAM.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzcuom","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/n32bk1o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752495786,"author_flair_text":null,"treatment_tags":[],"created_utc":1752495786,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n31car6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Aaaaaaaaaeeeee","can_mod_post":false,"send_replies":true,"parent_id":"t1_n317dsu","score":1,"author_fullname":"t2_el5pibmej","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":" experts I mean, sorry. \\n\\n\\nA Tensor is part of a layer right? So they can be separated and then you could use a strategy to pick what's going in RAM and VRAM. \\n\\n\\nThis would be a tensor with experts:\\n\`blk.3.ffn_down_exps.weight\`, then these are just tensors that are repeated every \`token: blk.3.attn_v_b.weight\`, \`blk.3.ffn_down_shexp.weight\`\\n\\n\\n\\n One layer is usually made of attention tensors and ffn tensors, and some ffn tensors are the experts. We just don't know the proportions of most of them, Don't worry, don't feel pressure to add anything because it's a bunch of work to calculate all of the mixture of experts models that we have.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n31car6","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt; experts I mean, sorry. &lt;/p&gt;\\n\\n&lt;p&gt;A Tensor is part of a layer right? So they can be separated and then you could use a strategy to pick what&amp;#39;s going in RAM and VRAM. &lt;/p&gt;\\n\\n&lt;p&gt;This would be a tensor with experts:\\n&lt;code&gt;blk.3.ffn_down_exps.weight&lt;/code&gt;, then these are just tensors that are repeated every &lt;code&gt;token: blk.3.attn_v_b.weight&lt;/code&gt;, &lt;code&gt;blk.3.ffn_down_shexp.weight&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt; One layer is usually made of attention tensors and ffn tensors, and some ffn tensors are the experts. We just don&amp;#39;t know the proportions of most of them, Don&amp;#39;t worry, don&amp;#39;t feel pressure to add anything because it&amp;#39;s a bunch of work to calculate all of the mixture of experts models that we have.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzcuom","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/n31car6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752477492,"author_flair_text":null,"treatment_tags":[],"created_utc":1752477492,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n317dsu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok_Warning2146","can_mod_post":false,"created_utc":1752474748,"send_replies":true,"parent_id":"t1_n310ap8","score":1,"author_fullname":"t2_s6sfw4yy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What do u mean by sparse tensor and repeating tensor? For example, which layer of DSV3 has these tensors?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n317dsu","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What do u mean by sparse tensor and repeating tensor? For example, which layer of DSV3 has these tensors?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzcuom","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/n317dsu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752474748,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n310ap8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Aaaaaaaaaeeeee","can_mod_post":false,"created_utc":1752470996,"send_replies":true,"parent_id":"t3_1lzcuom","score":10,"author_fullname":"t2_el5pibmej","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I like your MOE chart, thanks for sharing! \\nIf we have one more: repeating tensors vs \\"sparse\\", then it should be easier to estimate speed without experimentation. \\n\\n\\nWhat's great was dense layers make our asymmetric systems inference faster. Normally we'd want more of that, but we only got llama4 maverick and maybe snowflake arctic for comparison. Who knows for sure if it can be good? \\n\\n\\n ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n310ap8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I like your MOE chart, thanks for sharing! \\nIf we have one more: repeating tensors vs &amp;quot;sparse&amp;quot;, then it should be easier to estimate speed without experimentation. &lt;/p&gt;\\n\\n&lt;p&gt;What&amp;#39;s great was dense layers make our asymmetric systems inference faster. Normally we&amp;#39;d want more of that, but we only got llama4 maverick and maybe snowflake arctic for comparison. Who knows for sure if it can be good? &lt;/p&gt;\\n\\n&lt;p&gt; &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/n310ap8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752470996,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzcuom","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n33fki8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"pigeon57434","can_mod_post":false,"created_utc":1752508324,"send_replies":true,"parent_id":"t1_n32lxfj","score":1,"author_fullname":"t2_8j5t7yjq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"qwen are the only ones actually making small models seriously anymore Meta, DeepSeek, and MoonShot all only have small models really","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n33fki8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;qwen are the only ones actually making small models seriously anymore Meta, DeepSeek, and MoonShot all only have small models really&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzcuom","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/n33fki8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752508324,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n38ds9l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"chisleu","can_mod_post":false,"send_replies":true,"parent_id":"t1_n34s1bu","score":1,"author_fullname":"t2_cbxyn","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; Hunyuan\\n\\nDownloading the mlx-community 8bit version of a13b now. Fingers crossed it can handle cline.\\n\\nwait 2k context window?? I shaved my balls for this??","edited":1752573404,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n38ds9l","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Hunyuan&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Downloading the mlx-community 8bit version of a13b now. Fingers crossed it can handle cline.&lt;/p&gt;\\n\\n&lt;p&gt;wait 2k context window?? I shaved my balls for this??&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzcuom","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/n38ds9l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752572232,"author_flair_text":null,"treatment_tags":[],"created_utc":1752572232,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n34s1bu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Kooshi_Govno","can_mod_post":false,"created_utc":1752521898,"send_replies":true,"parent_id":"t1_n32lxfj","score":1,"author_fullname":"t2_7kg5p","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Hunyuan is 13/80 and is pretty solid","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n34s1bu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hunyuan is 13/80 and is pretty solid&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzcuom","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/n34s1bu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752521898,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n32lxfj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"R_Duncan","can_mod_post":false,"created_utc":1752499472,"send_replies":true,"parent_id":"t3_1lzcuom","score":5,"author_fullname":"t2_3xd4mwvn","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Still we're missing a 6B/64B MoE llm to get it Q4 and exploit 8GB GPU.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n32lxfj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Still we&amp;#39;re missing a 6B/64B MoE llm to get it Q4 and exploit 8GB GPU.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/n32lxfj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752499472,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzcuom","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n319jff","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"KillerX629","can_mod_post":false,"created_utc":1752475941,"send_replies":true,"parent_id":"t1_n30x0ed","score":12,"author_fullname":"t2_1ve6ehh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"On the contrary, it succeeded in making changes to svelte 5 files with a rust backend on tauri with me. I was impressed since it correctly used the latest syntax","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n319jff","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;On the contrary, it succeeded in making changes to svelte 5 files with a rust backend on tauri with me. I was impressed since it correctly used the latest syntax&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzcuom","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/n319jff/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752475941,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n31djo5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Corporate_Drone31","can_mod_post":false,"created_utc":1752478205,"send_replies":true,"parent_id":"t1_n30x0ed","score":19,"author_fullname":"t2_32o8hu91","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Frankly, I'm *more* impressed the more I interact with it. I don't think calling it o3-level is too inaccurate, since they are clearly within the same order of magnitude for capability on my non-public largely non-STEM questions set.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n31djo5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Frankly, I&amp;#39;m &lt;em&gt;more&lt;/em&gt; impressed the more I interact with it. I don&amp;#39;t think calling it o3-level is too inaccurate, since they are clearly within the same order of magnitude for capability on my non-public largely non-STEM questions set.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzcuom","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/n31djo5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752478205,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":19}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3152xe","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"shing3232","can_mod_post":false,"created_utc":1752473483,"send_replies":true,"parent_id":"t1_n30x0ed","score":8,"author_fullname":"t2_ze4mg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"well, It was more function call focused in its RL post training. It probably need more rl to perform well in many other task","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3152xe","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;well, It was more function call focused in its RL post training. It probably need more rl to perform well in many other task&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzcuom","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/n3152xe/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752473483,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n33y65r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"giantsparklerobot","can_mod_post":false,"send_replies":true,"parent_id":"t1_n32zzuy","score":2,"author_fullname":"t2_47gyf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; Over-confident, incorrect, poor instruction-following.\\n\\nShit. Maybe I'm an AI then. It would explain some things...","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n33y65r","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Over-confident, incorrect, poor instruction-following.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Shit. Maybe I&amp;#39;m an AI then. It would explain some things...&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzcuom","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/n33y65r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752513519,"author_flair_text":null,"treatment_tags":[],"created_utc":1752513519,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n32zzuy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"llmentry","can_mod_post":false,"created_utc":1752503850,"send_replies":true,"parent_id":"t1_n30x0ed","score":2,"author_fullname":"t2_1lufy6yx6z","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yep, me also.  I tried it out with some basic STEM knowledge questions and it was ok, but then moved on to a question about R package development, and while not-incorrect, its advice was outdated and not best-practice.  \\n\\nAt the same time, the model was highly opinionated, and also seemed to lack the ability to self-assess.  I include an \\"Always state your percent certainty\\" statement in my usual system prompt, and every model (even small models like Gemma 3) will do this except for Kimi K2.  Kimi K2 just *ignored* that aspect of the system prompt completely!\\n\\nSo, that's a hard pass from me.  Over-confident, incorrect, poor instruction-following.  Nope.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n32zzuy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yep, me also.  I tried it out with some basic STEM knowledge questions and it was ok, but then moved on to a question about R package development, and while not-incorrect, its advice was outdated and not best-practice.  &lt;/p&gt;\\n\\n&lt;p&gt;At the same time, the model was highly opinionated, and also seemed to lack the ability to self-assess.  I include an &amp;quot;Always state your percent certainty&amp;quot; statement in my usual system prompt, and every model (even small models like Gemma 3) will do this except for Kimi K2.  Kimi K2 just &lt;em&gt;ignored&lt;/em&gt; that aspect of the system prompt completely!&lt;/p&gt;\\n\\n&lt;p&gt;So, that&amp;#39;s a hard pass from me.  Over-confident, incorrect, poor instruction-following.  Nope.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzcuom","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/n32zzuy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752503850,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n31tl2h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok_Warning2146","can_mod_post":false,"send_replies":true,"parent_id":"t1_n313n8z","score":1,"author_fullname":"t2_s6sfw4yy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Dumber probably due to 5B less active params. More knowledge probably to due to 128 more experts.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n31tl2h","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Dumber probably due to 5B less active params. More knowledge probably to due to 128 more experts.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzcuom","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/n31tl2h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752487564,"author_flair_text":null,"treatment_tags":[],"created_utc":1752487564,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n313n8z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Caffeine_Monster","can_mod_post":false,"created_utc":1752472722,"send_replies":true,"parent_id":"t1_n30x0ed","score":5,"author_fullname":"t2_hg9yb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Agree. I usually do a few turns of scenario based problem solving to test coherency and logical reasoning.\\n\\nIt certainly feels like kimi-k2 has more knowledge. The text output is more varied.\\n\\nBut it feels significantly dumber and makes a fair few mistakes.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n313n8z","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Agree. I usually do a few turns of scenario based problem solving to test coherency and logical reasoning.&lt;/p&gt;\\n\\n&lt;p&gt;It certainly feels like kimi-k2 has more knowledge. The text output is more varied.&lt;/p&gt;\\n\\n&lt;p&gt;But it feels significantly dumber and makes a fair few mistakes.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzcuom","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/n313n8z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752472722,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n31gfhs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ElephantWithBlueEyes","can_mod_post":false,"created_utc":1752479899,"send_replies":true,"parent_id":"t1_n30x0ed","score":3,"author_fullname":"t2_zgb99c9ii","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Well, i asked same exact chain of questions Deepseek and Kimi K2 and they gave very similar answers except Kimi gave slightly less info.\\n\\nAs if Kimi is a Deepseek clone, indeed","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n31gfhs","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Well, i asked same exact chain of questions Deepseek and Kimi K2 and they gave very similar answers except Kimi gave slightly less info.&lt;/p&gt;\\n\\n&lt;p&gt;As if Kimi is a Deepseek clone, indeed&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzcuom","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/n31gfhs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752479899,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n326gtp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Imjustmisunderstood","can_mod_post":false,"created_utc":1752493749,"send_replies":true,"parent_id":"t1_n30x0ed","score":1,"author_fullname":"t2_s7g9g","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What kind of errors? Did it have access to up to date documentation on gradio/hf diffusers? I’ve found that no model can accurately write code for smaller (relative to, say, plotly) libraries.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n326gtp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What kind of errors? Did it have access to up to date documentation on gradio/hf diffusers? I’ve found that no model can accurately write code for smaller (relative to, say, plotly) libraries.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzcuom","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/n326gtp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752493749,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n32p8pv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"radianart","can_mod_post":false,"created_utc":1752500570,"send_replies":true,"parent_id":"t1_n30x0ed","score":1,"author_fullname":"t2_ikplmni","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I used it a little for various question and it was dumb and totally useless.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n32p8pv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I used it a little for various question and it was dumb and totally useless.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzcuom","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/n32p8pv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752500570,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n32xnqd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Informal_Librarian","can_mod_post":false,"created_utc":1752503164,"send_replies":true,"parent_id":"t1_n30x0ed","score":1,"author_fullname":"t2_obq9bdp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Be sure to check the temperature settings. 0.6 or lower seems to be the sweet spot. Higher leads to subpar results.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n32xnqd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Be sure to check the temperature settings. 0.6 or lower seems to be the sweet spot. Higher leads to subpar results.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzcuom","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/n32xnqd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752503164,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n30x0ed","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"itsmekalisyn","can_mod_post":false,"created_utc":1752469362,"send_replies":true,"parent_id":"t3_1lzcuom","score":23,"author_fullname":"t2_l3tmozxu","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Anyone feeling less impressed with Kimi-K2?\\n\\nI asked it to create a Gradio UI with HF diffusers at the backend.\\n\\nSimple pipeline with 30-40 lines of code and there were so many errors.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n30x0ed","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Anyone feeling less impressed with Kimi-K2?&lt;/p&gt;\\n\\n&lt;p&gt;I asked it to create a Gradio UI with HF diffusers at the backend.&lt;/p&gt;\\n\\n&lt;p&gt;Simple pipeline with 30-40 lines of code and there were so many errors.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/n30x0ed/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752469362,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzcuom","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":23}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n311zt1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"gabrielxdesign","can_mod_post":false,"created_utc":1752471868,"send_replies":true,"parent_id":"t3_1lzcuom","score":5,"author_fullname":"t2_261v9bd9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Kimi-Audio sounds Interesting! 🥁","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n311zt1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Kimi-Audio sounds Interesting! 🥁&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/n311zt1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752471868,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzcuom","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n32a1zo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Commercial-Celery769","can_mod_post":false,"created_utc":1752495204,"send_replies":true,"parent_id":"t3_1lzcuom","score":2,"author_fullname":"t2_zws5yqyow","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Wen 0.1 bit quant","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n32a1zo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Wen 0.1 bit quant&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/n32a1zo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752495204,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzcuom","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n32irlb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jakegh","can_mod_post":false,"created_utc":1752498399,"send_replies":true,"parent_id":"t3_1lzcuom","score":2,"author_fullname":"t2_6vt1n","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Deepseek V3 and Kimi K2 are indeed quite similar, in that they're extremely capable non-reasoning open-source models that run unusably slow for some reason.\\n\\nMuch like Deepseek R1's reasoning, I expect the primary use-case for K2 to be generating training data on its tool use to distill into other models that run at acceptable speeds.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n32irlb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Deepseek V3 and Kimi K2 are indeed quite similar, in that they&amp;#39;re extremely capable non-reasoning open-source models that run unusably slow for some reason.&lt;/p&gt;\\n\\n&lt;p&gt;Much like Deepseek R1&amp;#39;s reasoning, I expect the primary use-case for K2 to be generating training data on its tool use to distill into other models that run at acceptable speeds.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/n32irlb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752498399,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzcuom","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n34f3tk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Cadmoose","can_mod_post":false,"created_utc":1752518160,"send_replies":true,"parent_id":"t3_1lzcuom","score":2,"author_fullname":"t2_1415tq89","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"A Kimi team member (Shaowei Liu) wrote a short blogpost explaining the reasoning and process underlying their design choices - here is the English translation courtesy of Kimi 2.\\n\\nhttps://www.kimi.com/share/d1q8l75e09n7its6e7jg\\n\\nAn excerpt: \\"Each change was backed by solid theory + ablation. Once K2 is fully open-sourced, we hope the wider inference community will stress-test these claims.\\"","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n34f3tk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;A Kimi team member (Shaowei Liu) wrote a short blogpost explaining the reasoning and process underlying their design choices - here is the English translation courtesy of Kimi 2.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.kimi.com/share/d1q8l75e09n7its6e7jg\\"&gt;https://www.kimi.com/share/d1q8l75e09n7its6e7jg&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;An excerpt: &amp;quot;Each change was backed by solid theory + ablation. Once K2 is fully open-sourced, we hope the wider inference community will stress-test these claims.&amp;quot;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/n34f3tk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752518160,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzcuom","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n35cvj5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Iory1998","can_mod_post":false,"created_utc":1752527791,"send_replies":true,"parent_id":"t3_1lzcuom","score":2,"author_fullname":"t2_byt5wa14","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Deepseek will be the foundational model of choice f9r many Chinese labs in the couple of years to come, IMO. It's a good strategy if you ask me. One company focuses on training the best open-source models, while the other companies focus on building on top of it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n35cvj5","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Deepseek will be the foundational model of choice f9r many Chinese labs in the couple of years to come, IMO. It&amp;#39;s a good strategy if you ask me. One company focuses on training the best open-source models, while the other companies focus on building on top of it.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/n35cvj5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752527791,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lzcuom","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n31hm29","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Mark__27","can_mod_post":false,"created_utc":1752480585,"send_replies":true,"parent_id":"t1_n31e3ds","score":5,"author_fullname":"t2_1gxy2k6l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This sounds to be like a deliberate effort to reduce overfitting and induce more randomness into the model? Which seems to align with feedback?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n31hm29","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This sounds to be like a deliberate effort to reduce overfitting and induce more randomness into the model? Which seems to align with feedback?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzcuom","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/n31hm29/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752480585,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n31e3ds","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Afternoon_4260","can_mod_post":false,"created_utc":1752478528,"send_replies":true,"parent_id":"t3_1lzcuom","score":1,"author_fullname":"t2_cj9kap4bx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"More experts and fewer attention head","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n31e3ds","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;More experts and fewer attention head&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/n31e3ds/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752478528,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lzcuom","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n32r434","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"dhlu","can_mod_post":false,"send_replies":true,"parent_id":"t1_n32d4qi","score":1,"author_fullname":"t2_3bcperq1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I wasn't taking speed into the calculus but pure score per size, but yeah, you could make score per size per time, like point per bit per second","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n32r434","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I wasn&amp;#39;t taking speed into the calculus but pure score per size, but yeah, you could make score per size per time, like point per bit per second&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzcuom","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/n32r434/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752501170,"author_flair_text":null,"treatment_tags":[],"created_utc":1752501170,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n32d4qi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"bjodah","can_mod_post":false,"created_utc":1752496386,"send_replies":true,"parent_id":"t1_n31idm5","score":5,"author_fullname":"t2_atvy2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"fewer active parameters typically means faster inference, so it's not quite that simple for MoE models I think...","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n32d4qi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;fewer active parameters typically means faster inference, so it&amp;#39;s not quite that simple for MoE models I think...&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzcuom","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/n32d4qi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752496386,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n31idm5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"dhlu","can_mod_post":false,"created_utc":1752481032,"send_replies":true,"parent_id":"t3_1lzcuom","score":2,"author_fullname":"t2_3bcperq1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Real metric is score per size, not really the biggest the best","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n31idm5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Real metric is score per size, not really the biggest the best&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/n31idm5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752481032,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzcuom","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3534xq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BenXavier","can_mod_post":false,"send_replies":true,"parent_id":"t1_n32g2zo","score":1,"author_fullname":"t2_5w9jnbsn","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I get the idea around fine-tuning, but the line between that and continued pretraining Is blurred.\\n\\nI know It would not be a 1:1 mapping, that's why I was asking myself if It could have been done at least \\"partially\\"","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3534xq","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I get the idea around fine-tuning, but the line between that and continued pretraining Is blurred.&lt;/p&gt;\\n\\n&lt;p&gt;I know It would not be a 1:1 mapping, that&amp;#39;s why I was asking myself if It could have been done at least &amp;quot;partially&amp;quot;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzcuom","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/n3534xq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752525036,"author_flair_text":null,"treatment_tags":[],"created_utc":1752525036,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n32g2zo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"tmd_h","can_mod_post":false,"created_utc":1752497464,"send_replies":true,"parent_id":"t1_n321h28","score":1,"author_fullname":"t2_3suf0ejs","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"If you initialize a model with DeepSeek weights, then train the model, it's called fine-tuning. But Kimi K2 has a slightly different architecture than deepseek. So I don't think it's possible to initialize Kimi with DeepSeek weights. You could finetune deepseek, but then what you get is a fine-tuned model, that performs generally about the same (Or a little better if you get lucky).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n32g2zo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you initialize a model with DeepSeek weights, then train the model, it&amp;#39;s called fine-tuning. But Kimi K2 has a slightly different architecture than deepseek. So I don&amp;#39;t think it&amp;#39;s possible to initialize Kimi with DeepSeek weights. You could finetune deepseek, but then what you get is a fine-tuned model, that performs generally about the same (Or a little better if you get lucky).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzcuom","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/n32g2zo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752497464,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n321h28","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BenXavier","can_mod_post":false,"created_utc":1752491559,"send_replies":true,"parent_id":"t3_1lzcuom","score":1,"author_fullname":"t2_5w9jnbsn","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"A question for the expert ones in LLM training: was there any option to \\"smartly initialize\\" Kimi weights with deepseek ones?\\n\\nWould it have been good or detrimental?\\n\\nDo people do this kind of think in practice?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n321h28","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;A question for the expert ones in LLM training: was there any option to &amp;quot;smartly initialize&amp;quot; Kimi weights with deepseek ones?&lt;/p&gt;\\n\\n&lt;p&gt;Would it have been good or detrimental?&lt;/p&gt;\\n\\n&lt;p&gt;Do people do this kind of think in practice?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/n321h28/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752491559,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzcuom","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),o=()=>e.jsx(l,{data:t});export{o as default};
