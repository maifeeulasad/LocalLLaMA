import{j as e}from"./index-DOAmItP2.js";import{R as t}from"./RedditPostRenderer-KKgzpPpv.js";import"./index-YSfz60vQ.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I apologize if this is the Nth time something like this was posted, but I am just at my wit's end. As the title says, I need help setting up an uncensored local LLM for the purpose of running / DMing a single player text-based RPG adventure. I have tried online services like Kobold AI Lite, etc. but I always encounter issues with them (AI deciding my actions on my behalf even after numerous corrections, AI forgetting important details just after they occurred, etc.), perhaps due to my lack of knowledge and experience in this field.\\n\\nTo preface, I'm basically a boomer when it comes to AI related things. This all started when I tried a mobile app called Everweave and I was hooked immediately. Unfortunately, the monthly limit and monetization scheme is not something I am inclined to participate in. After trying online services and finding them unsatisfactory (see reasons above), I instead decided to try hosting an LLM that does the same, locally. I tried to search online and watch videos, but there is only so much I can \\"learn\\" if I couldn't even understand the terminologies being used.  I really did try to take this on by myself and be independent but my brain just could not absorb this new paradigm.\\n\\nSo far what I had done is download LM Studio and search for LLMs that would fit my intended purpose and that works with the limitations of my machine (R7 4700G 3.6 GHz, 24 GB RAM, RX 6600 8 GB VRAM). Chat GPT suggested I use Mythomist 7b and Mythomax L2 13b, so I tried both. I also wrote a long, detailed system prompt to tell it exactly what I want it to do, but the issues tend to persist.\\n\\nSo my question is, can anyone who has done the same and found it without any issues, tell me exactly what I should do? Explain it to me like I'm 5, because with all these new emerging fields I'm pretty much a child.\\n\\nThank you!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Help setting up an uncensored local LLM for a text-based RPG adventure / DMing","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lsfpi0","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.78,"author_flair_background_color":null,"subreddit_type":"public","ups":5,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_ro3h94zr","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":5,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751737829,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I apologize if this is the Nth time something like this was posted, but I am just at my wit&amp;#39;s end. As the title says, I need help setting up an uncensored local LLM for the purpose of running / DMing a single player text-based RPG adventure. I have tried online services like Kobold AI Lite, etc. but I always encounter issues with them (AI deciding my actions on my behalf even after numerous corrections, AI forgetting important details just after they occurred, etc.), perhaps due to my lack of knowledge and experience in this field.&lt;/p&gt;\\n\\n&lt;p&gt;To preface, I&amp;#39;m basically a boomer when it comes to AI related things. This all started when I tried a mobile app called Everweave and I was hooked immediately. Unfortunately, the monthly limit and monetization scheme is not something I am inclined to participate in. After trying online services and finding them unsatisfactory (see reasons above), I instead decided to try hosting an LLM that does the same, locally. I tried to search online and watch videos, but there is only so much I can &amp;quot;learn&amp;quot; if I couldn&amp;#39;t even understand the terminologies being used.  I really did try to take this on by myself and be independent but my brain just could not absorb this new paradigm.&lt;/p&gt;\\n\\n&lt;p&gt;So far what I had done is download LM Studio and search for LLMs that would fit my intended purpose and that works with the limitations of my machine (R7 4700G 3.6 GHz, 24 GB RAM, RX 6600 8 GB VRAM). Chat GPT suggested I use Mythomist 7b and Mythomax L2 13b, so I tried both. I also wrote a long, detailed system prompt to tell it exactly what I want it to do, but the issues tend to persist.&lt;/p&gt;\\n\\n&lt;p&gt;So my question is, can anyone who has done the same and found it without any issues, tell me exactly what I should do? Explain it to me like I&amp;#39;m 5, because with all these new emerging fields I&amp;#39;m pretty much a child.&lt;/p&gt;\\n\\n&lt;p&gt;Thank you!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lsfpi0","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"tac7878","discussion_type":null,"num_comments":4,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lsfpi0/help_setting_up_an_uncensored_local_llm_for_a/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lsfpi0/help_setting_up_an_uncensored_local_llm_for_a/","subreddit_subscribers":494897,"created_utc":1751737829,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1ib9sy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AverageCareful","can_mod_post":false,"created_utc":1751739469,"send_replies":true,"parent_id":"t3_1lsfpi0","score":1,"author_fullname":"t2_8ikpmh0o","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"1. You need better prompt instructions with the correct formatting for your specific use case. Inject required variables to your prompt, such as details about the player or the world itself for additional context. Don't include all instructions in a single prompt. This can cause the model to hallucinate, especially with smaller models.  \\n  \\n2. You can use a vector database to store the \\"current knowledge\\" of your actions or story. This can help the LLM \\"remember\\" the previous context. This also means you will need to use an embedding model.  \\n  \\nYou should check other examples of roleplaying prompts, e.g. SillyTavern. You can also check AIDungeon on GitHub for more references.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ib9sy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;ol&gt;\\n&lt;li&gt;&lt;p&gt;You need better prompt instructions with the correct formatting for your specific use case. Inject required variables to your prompt, such as details about the player or the world itself for additional context. Don&amp;#39;t include all instructions in a single prompt. This can cause the model to hallucinate, especially with smaller models.  &lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;You can use a vector database to store the &amp;quot;current knowledge&amp;quot; of your actions or story. This can help the LLM &amp;quot;remember&amp;quot; the previous context. This also means you will need to use an embedding model.  &lt;/p&gt;&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;You should check other examples of roleplaying prompts, e.g. SillyTavern. You can also check AIDungeon on GitHub for more references.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsfpi0/help_setting_up_an_uncensored_local_llm_for_a/n1ib9sy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751739469,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lsfpi0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1icpec","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"InternalWeather1719","can_mod_post":false,"created_utc":1751739931,"send_replies":true,"parent_id":"t3_1lsfpi0","score":1,"author_fullname":"t2_80r1tpe3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"8g vram maybe not enough for a smart uncensored model.\\nTry to improve your prompt, and then try this: https://mistral.ai/pricing#api-pricing\\nmistral api,small 3.2 is very cheap.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1icpec","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;8g vram maybe not enough for a smart uncensored model.\\nTry to improve your prompt, and then try this: &lt;a href=\\"https://mistral.ai/pricing#api-pricing\\"&gt;https://mistral.ai/pricing#api-pricing&lt;/a&gt;\\nmistral api,small 3.2 is very cheap.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsfpi0/help_setting_up_an_uncensored_local_llm_for_a/n1icpec/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751739931,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lsfpi0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1id8vx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"misterflyer","can_mod_post":false,"created_utc":1751740109,"send_replies":true,"parent_id":"t3_1lsfpi0","score":2,"author_fullname":"t2_maq0iwk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Seems like you're going to have to get around your hardware limitations first.\\n\\nWhy not actually invest in a better rig first before trying something like this?  Eg., at least \\\\~24GB VRAM.\\n\\nThat way you can run more than just 7b and 13b models... bc the thing is small models are great for certain small tasks, but eventually you'll run into limitations with them.\\n\\nAnd then you'll get extremely frustrated when they can't do what you want them to do *(eg, feeding it and having it handle lots of RAG data, and expecting the smaller models to remember everything within a limited context window)*.\\n\\nPersonally as a vanilla NSFW erotic story writer, I don't find 7b and 13b models very useful on large projects like this *(aside from Mistral Nemo -- which is great but still has its own limitations)*. From what I've seen, those smaller models only seem useful for ordinary erotic role play.\\n\\nSo at a minimum I wouldn't use anything less than a Mistral Small version or a Qwen 32b for stuff like this.  But tbh, a lot of times I still end up opting for even larger models when I need to process large amounts of data for a story.\\n\\nTrying to do stuff like this super cheaply or damn near free is next to impossible *(and IMO not worth the headaches and frustration)*. So, set a real budget for yourself *(up your hardware or run cloud GPUs)*, test out some models on SillyTavern or OpenRouter *(trial and error; running your own NSFW \\"benchmarks\\" is the best way)*. And then go from there.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1id8vx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Seems like you&amp;#39;re going to have to get around your hardware limitations first.&lt;/p&gt;\\n\\n&lt;p&gt;Why not actually invest in a better rig first before trying something like this?  Eg., at least ~24GB VRAM.&lt;/p&gt;\\n\\n&lt;p&gt;That way you can run more than just 7b and 13b models... bc the thing is small models are great for certain small tasks, but eventually you&amp;#39;ll run into limitations with them.&lt;/p&gt;\\n\\n&lt;p&gt;And then you&amp;#39;ll get extremely frustrated when they can&amp;#39;t do what you want them to do &lt;em&gt;(eg, feeding it and having it handle lots of RAG data, and expecting the smaller models to remember everything within a limited context window)&lt;/em&gt;.&lt;/p&gt;\\n\\n&lt;p&gt;Personally as a vanilla NSFW erotic story writer, I don&amp;#39;t find 7b and 13b models very useful on large projects like this &lt;em&gt;(aside from Mistral Nemo -- which is great but still has its own limitations)&lt;/em&gt;. From what I&amp;#39;ve seen, those smaller models only seem useful for ordinary erotic role play.&lt;/p&gt;\\n\\n&lt;p&gt;So at a minimum I wouldn&amp;#39;t use anything less than a Mistral Small version or a Qwen 32b for stuff like this.  But tbh, a lot of times I still end up opting for even larger models when I need to process large amounts of data for a story.&lt;/p&gt;\\n\\n&lt;p&gt;Trying to do stuff like this super cheaply or damn near free is next to impossible &lt;em&gt;(and IMO not worth the headaches and frustration)&lt;/em&gt;. So, set a real budget for yourself &lt;em&gt;(up your hardware or run cloud GPUs)&lt;/em&gt;, test out some models on SillyTavern or OpenRouter &lt;em&gt;(trial and error; running your own NSFW &amp;quot;benchmarks&amp;quot; is the best way)&lt;/em&gt;. And then go from there.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsfpi0/help_setting_up_an_uncensored_local_llm_for_a/n1id8vx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751740109,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lsfpi0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),s=()=>e.jsx(t,{data:l});export{s as default};
