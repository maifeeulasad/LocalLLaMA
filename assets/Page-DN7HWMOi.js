import{j as e}from"./index-CqAPCjw5.js";import{R as t}from"./RedditPostRenderer-4oBDAtGr.js";import"./index-D3Sdy_Op.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"https://preview.redd.it/70byco93mdaf1.png?width=2353&amp;format=png&amp;auto=webp&amp;s=226d3dc6055ad2ad9c952ed13dca4a1451ae5d2a\\n\\nit is a PR of ik\\\\_llama.cpp, by ubergarm , not yet merged.\\n\\nInstruction to compile, by ubergarm (from: [ubergarm/Hunyuan-A13B-Instruct-GGUF · Hugging Face](https://huggingface.co/ubergarm/Hunyuan-A13B-Instruct-GGUF#note-building-experimental-prs)):\\n\\n    # get the code setup\\n    cd projects\\n    git clone https://github.com/ikawrakow/ik_llama.cpp.git\\n    git ik_llama.cpp\\n    git fetch origin\\n    git remote add ubergarm https://github.com/ubergarm/ik_llama.cpp\\n    git fetch ubergarm\\n    git checkout ug/hunyuan-moe-2\\n    git checkout -b merge-stuff-here\\n    git merge ikawrakow/ik/iq3_ks_v2\\n    \\n    # build for CUDA\\n    cmake -B build -DCMAKE_BUILD_TYPE=Release -DGGML_CUDA=ON -DGGML_VULKAN=OFF -DGGML_RPC=OFF -DGGML_BLAS=OFF -DGGML_CUDA_F16=ON -DGGML_SCHED_MAX_COPIES=1\\n    cmake --build build --config Release -j $(nproc)\\n    \\n    # clean up later if things get merged into main\\n    git checkout main\\n    git branch -D merge-stuff-here\\n    \`\`\`\\n\\nGGUF download: [ubergarm/Hunyuan-A13B-Instruct-GGUF at main](https://huggingface.co/ubergarm/Hunyuan-A13B-Instruct-GGUF/tree/main)\\n\\nthe running command (better read it here, and modified by yourself):  \\n[ubergarm/Hunyuan-A13B-Instruct-GGUF · Hugging Face](https://huggingface.co/ubergarm/Hunyuan-A13B-Instruct-GGUF#note-building-experimental-prs)\\n\\na api/webui hosted by ubergarm, for early testing  \\nWebUI: [https://llm.ubergarm.com/](https://llm.ubergarm.com/)  \\nAPIEndpoint: [https://llm.ubergarm.com/](https://llm.ubergarm.com/) (it is llama-server API endpoint with no API key)","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Hosting your local Huanyuan A13B MOE","link_flair_richtext":[{"e":"text","t":"Resources"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":75,"top_awarded_type":null,"hide_score":false,"media_metadata":{"70byco93mdaf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":46,"x":108,"u":"https://preview.redd.it/70byco93mdaf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c94905bd542faf5aeeccaaf1f20d43ea5e931531"},{"y":93,"x":216,"u":"https://preview.redd.it/70byco93mdaf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a20abb731eaaec2c707ce7b00a1969911041c984"},{"y":139,"x":320,"u":"https://preview.redd.it/70byco93mdaf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=68c5198f2fec40af18896c4598f1db2fcf90ef38"},{"y":278,"x":640,"u":"https://preview.redd.it/70byco93mdaf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3865d146ff302194d485bfcde8c72cfb52f55941"},{"y":417,"x":960,"u":"https://preview.redd.it/70byco93mdaf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=913c982865a8dc27175ac09e3c93c5c499e74c69"},{"y":469,"x":1080,"u":"https://preview.redd.it/70byco93mdaf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=98a7aa248651f5bd3f39c9025ac081ce483be774"}],"s":{"y":1023,"x":2353,"u":"https://preview.redd.it/70byco93mdaf1.png?width=2353&amp;format=png&amp;auto=webp&amp;s=226d3dc6055ad2ad9c952ed13dca4a1451ae5d2a"},"id":"70byco93mdaf1"}},"name":"t3_1lpl3mv","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.89,"author_flair_background_color":null,"ups":21,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_tb0dz2ds","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Resources","can_mod_post":false,"score":21,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://external-preview.redd.it/gQRnTfcC6aPcmeuf8YDaS3B32gWvzvTmzY9W4xzLeHo.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=db0195f94ca476d95d72f0839b426bcc2113b39e","edited":1751430188,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"subreddit_type":"public","created":1751425205,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://preview.redd.it/70byco93mdaf1.png?width=2353&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=226d3dc6055ad2ad9c952ed13dca4a1451ae5d2a\\"&gt;https://preview.redd.it/70byco93mdaf1.png?width=2353&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=226d3dc6055ad2ad9c952ed13dca4a1451ae5d2a&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;it is a PR of ik_llama.cpp, by ubergarm , not yet merged.&lt;/p&gt;\\n\\n&lt;p&gt;Instruction to compile, by ubergarm (from: &lt;a href=\\"https://huggingface.co/ubergarm/Hunyuan-A13B-Instruct-GGUF#note-building-experimental-prs\\"&gt;ubergarm/Hunyuan-A13B-Instruct-GGUF · Hugging Face&lt;/a&gt;):&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;# get the code setup\\ncd projects\\ngit clone https://github.com/ikawrakow/ik_llama.cpp.git\\ngit ik_llama.cpp\\ngit fetch origin\\ngit remote add ubergarm https://github.com/ubergarm/ik_llama.cpp\\ngit fetch ubergarm\\ngit checkout ug/hunyuan-moe-2\\ngit checkout -b merge-stuff-here\\ngit merge ikawrakow/ik/iq3_ks_v2\\n\\n# build for CUDA\\ncmake -B build -DCMAKE_BUILD_TYPE=Release -DGGML_CUDA=ON -DGGML_VULKAN=OFF -DGGML_RPC=OFF -DGGML_BLAS=OFF -DGGML_CUDA_F16=ON -DGGML_SCHED_MAX_COPIES=1\\ncmake --build build --config Release -j $(nproc)\\n\\n# clean up later if things get merged into main\\ngit checkout main\\ngit branch -D merge-stuff-here\\n\`\`\`\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;GGUF download: &lt;a href=\\"https://huggingface.co/ubergarm/Hunyuan-A13B-Instruct-GGUF/tree/main\\"&gt;ubergarm/Hunyuan-A13B-Instruct-GGUF at main&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;the running command (better read it here, and modified by yourself):&lt;br/&gt;\\n&lt;a href=\\"https://huggingface.co/ubergarm/Hunyuan-A13B-Instruct-GGUF#note-building-experimental-prs\\"&gt;ubergarm/Hunyuan-A13B-Instruct-GGUF · Hugging Face&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;a api/webui hosted by ubergarm, for early testing&lt;br/&gt;\\nWebUI: &lt;a href=\\"https://llm.ubergarm.com/\\"&gt;https://llm.ubergarm.com/&lt;/a&gt;&lt;br/&gt;\\nAPIEndpoint: &lt;a href=\\"https://llm.ubergarm.com/\\"&gt;https://llm.ubergarm.com/&lt;/a&gt; (it is llama-server API endpoint with no API key)&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/gQRnTfcC6aPcmeuf8YDaS3B32gWvzvTmzY9W4xzLeHo.png?auto=webp&amp;s=65a0126fbbfded1bf2d9034e45c83e5aefd6c40d","width":1200,"height":648},"resolutions":[{"url":"https://external-preview.redd.it/gQRnTfcC6aPcmeuf8YDaS3B32gWvzvTmzY9W4xzLeHo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2a758383a47b8e50c29ab7bc70665dbff2578936","width":108,"height":58},{"url":"https://external-preview.redd.it/gQRnTfcC6aPcmeuf8YDaS3B32gWvzvTmzY9W4xzLeHo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=424b2e3f68897a62807bbaa93a663c6a511c96a9","width":216,"height":116},{"url":"https://external-preview.redd.it/gQRnTfcC6aPcmeuf8YDaS3B32gWvzvTmzY9W4xzLeHo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bd3504ce92f82931fe12a9470e4474bca8830127","width":320,"height":172},{"url":"https://external-preview.redd.it/gQRnTfcC6aPcmeuf8YDaS3B32gWvzvTmzY9W4xzLeHo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2aec10232e41be314e9b6831041b90cf9d7b4600","width":640,"height":345},{"url":"https://external-preview.redd.it/gQRnTfcC6aPcmeuf8YDaS3B32gWvzvTmzY9W4xzLeHo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b618a2c47ca135edd41e0c42cd1e1741e07e4d96","width":960,"height":518},{"url":"https://external-preview.redd.it/gQRnTfcC6aPcmeuf8YDaS3B32gWvzvTmzY9W4xzLeHo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=13903070452f80ff80364c9f43007a609885d43a","width":1080,"height":583}],"variants":{},"id":"gQRnTfcC6aPcmeuf8YDaS3B32gWvzvTmzY9W4xzLeHo"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"mod_note":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"num_reports":null,"removal_reason":null,"link_flair_background_color":"#ccac2b","id":"1lpl3mv","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"kironlau","discussion_type":null,"num_comments":15,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lpl3mv/hosting_your_local_huanyuan_a13b_moe/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lpl3mv/hosting_your_local_huanyuan_a13b_moe/","subreddit_subscribers":494001,"created_utc":1751425205,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0y8juj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"VoidAlchemy","can_mod_post":false,"created_utc":1751467325,"send_replies":true,"parent_id":"t1_n0watm0","score":2,"author_fullname":"t2_n321yfw5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks! Yeah this is an very experimental beast at the moment. Follow along the mainline llama.cpp PR for more information: https://github.com/ggml-org/llama.cpp/pull/14425#issuecomment-3026998286\\n\\nThe model is a *great size* for low VRAM rigs for hybrid CPU+GPU. However, yes I agree it is very rough around the edges.  Seems too sensitive to chat template, system prompt (or lack thereof), and does drop/goofup the \`&lt;\` in \`answer&gt;\` tags etc.\\n\\nGlad you were able to get it running and thanks for testing!\\n\\nThe good news is ik's latest \`IQ3_KS\` SOTA quant seems to up and running fine and that PR is now merged (basically an upgrade over his previous \`IQ3_XS\` implementation.)\\n\\n*EDIT* I just updated the README instructions how to pull and build the experimental PR branch.","edited":1751470772,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0y8juj","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks! Yeah this is an very experimental beast at the moment. Follow along the mainline llama.cpp PR for more information: &lt;a href=\\"https://github.com/ggml-org/llama.cpp/pull/14425#issuecomment-3026998286\\"&gt;https://github.com/ggml-org/llama.cpp/pull/14425#issuecomment-3026998286&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;The model is a &lt;em&gt;great size&lt;/em&gt; for low VRAM rigs for hybrid CPU+GPU. However, yes I agree it is very rough around the edges.  Seems too sensitive to chat template, system prompt (or lack thereof), and does drop/goofup the &lt;code&gt;&amp;lt;&lt;/code&gt; in &lt;code&gt;answer&amp;gt;&lt;/code&gt; tags etc.&lt;/p&gt;\\n\\n&lt;p&gt;Glad you were able to get it running and thanks for testing!&lt;/p&gt;\\n\\n&lt;p&gt;The good news is ik&amp;#39;s latest &lt;code&gt;IQ3_KS&lt;/code&gt; SOTA quant seems to up and running fine and that PR is now merged (basically an upgrade over his previous &lt;code&gt;IQ3_XS&lt;/code&gt; implementation.)&lt;/p&gt;\\n\\n&lt;p&gt;&lt;em&gt;EDIT&lt;/em&gt; I just updated the README instructions how to pull and build the experimental PR branch.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpl3mv","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl3mv/hosting_your_local_huanyuan_a13b_moe/n0y8juj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751467325,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0watm0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Marksta","can_mod_post":false,"created_utc":1751436072,"send_replies":true,"parent_id":"t3_1lpl3mv","score":18,"author_fullname":"t2_559a1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"**For writing:**\\n\\nIt doesn't listen to system prompt, it is the most censor heavy model I've ever seen. It likes to swap all usage of the word \\"dick\\" with a checkmark emoji.\\n\\n**For Roo code:**\\n\\nIt seemed okay before it leaked thinking tokens because it didn't put think and answer brackets, so it filled up its context fast. It was at 24k/32k-ish but then it went into a psycho loop of adding more and more junk to a file to try to fix an indentation issue it made.\\n\\nOverall, mostly useless until everyone works on it more to figure out what's wrong with it, implement whatever it needs for its chat format, de-censor it, and maybe it's a bug it completely ignores system prompt or by design but that makes it a really, really bad agentic model. I'd say for now, it's no where close to DeepSeek. But it's fast.\\n\\n\\n    ### EPYC 7702 with 256GB 3200Mhz 8 channel DDR4\\n    ### RTX 3090 + RTX 4060TI\\n    # ubergarm/Hunyuan-A13B-Instruct-IQ3_KS.gguf 34.088 GiB (3.642 BPW)\\n    ./build/bin/llama-sweep-bench \\\\\\n      --model ubergarm/Hunyuan-A13B-Instruct-IQ3_KS.gguf\\n      -fa -fmoe -rtr \\\\\\n      -c 32768 -ctk q8_0 -ctv q8_0 \\\\\\n      -ngl 99 -ub 2048 -b 2048 --threads 32 \\\\\\n      -ot \\"blk\\\\.([0-7])\\\\.ffn_.*=CUDA0\\" \\\\\\n      -ot \\"blk\\\\.([6-9]|1[0-8])\\\\.ffn_.*=CUDA1\\" \\\\\\n      -ot exps=CPU \\\\\\n      --warmup-batch\\n    main: n_kv_max = 32768, n_batch = 2048, n_ubatch = 2048, flash_attn = 1, n_gpu_layers = 99, n_threads = 32, n_threads_batch = 32\\n    |    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\\n    |-------|--------|--------|----------|----------|----------|----------|\\n    |  2048 |    512 |      0 |    5.682 |   360.45 |   18.007 |    28.43 |\\n    |  2048 |    512 |   2048 |    5.724 |   357.79 |   18.878 |    27.12 |\\n    |  2048 |    512 |   4096 |    5.762 |   355.45 |   19.625 |    26.09 |\\n\\nThank you /u/VoidAlchemy for the quant and instructions.","edited":1751436253,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0watm0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;strong&gt;For writing:&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;It doesn&amp;#39;t listen to system prompt, it is the most censor heavy model I&amp;#39;ve ever seen. It likes to swap all usage of the word &amp;quot;dick&amp;quot; with a checkmark emoji.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;For Roo code:&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;It seemed okay before it leaked thinking tokens because it didn&amp;#39;t put think and answer brackets, so it filled up its context fast. It was at 24k/32k-ish but then it went into a psycho loop of adding more and more junk to a file to try to fix an indentation issue it made.&lt;/p&gt;\\n\\n&lt;p&gt;Overall, mostly useless until everyone works on it more to figure out what&amp;#39;s wrong with it, implement whatever it needs for its chat format, de-censor it, and maybe it&amp;#39;s a bug it completely ignores system prompt or by design but that makes it a really, really bad agentic model. I&amp;#39;d say for now, it&amp;#39;s no where close to DeepSeek. But it&amp;#39;s fast.&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;### EPYC 7702 with 256GB 3200Mhz 8 channel DDR4\\n### RTX 3090 + RTX 4060TI\\n# ubergarm/Hunyuan-A13B-Instruct-IQ3_KS.gguf 34.088 GiB (3.642 BPW)\\n./build/bin/llama-sweep-bench \\\\\\n  --model ubergarm/Hunyuan-A13B-Instruct-IQ3_KS.gguf\\n  -fa -fmoe -rtr \\\\\\n  -c 32768 -ctk q8_0 -ctv q8_0 \\\\\\n  -ngl 99 -ub 2048 -b 2048 --threads 32 \\\\\\n  -ot &amp;quot;blk\\\\.([0-7])\\\\.ffn_.*=CUDA0&amp;quot; \\\\\\n  -ot &amp;quot;blk\\\\.([6-9]|1[0-8])\\\\.ffn_.*=CUDA1&amp;quot; \\\\\\n  -ot exps=CPU \\\\\\n  --warmup-batch\\nmain: n_kv_max = 32768, n_batch = 2048, n_ubatch = 2048, flash_attn = 1, n_gpu_layers = 99, n_threads = 32, n_threads_batch = 32\\n|    PP |     TG |   N_KV |   T_PP s | S_PP t/s |   T_TG s | S_TG t/s |\\n|-------|--------|--------|----------|----------|----------|----------|\\n|  2048 |    512 |      0 |    5.682 |   360.45 |   18.007 |    28.43 |\\n|  2048 |    512 |   2048 |    5.724 |   357.79 |   18.878 |    27.12 |\\n|  2048 |    512 |   4096 |    5.762 |   355.45 |   19.625 |    26.09 |\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;Thank you &lt;a href=\\"/u/VoidAlchemy\\"&gt;/u/VoidAlchemy&lt;/a&gt; for the quant and instructions.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl3mv/hosting_your_local_huanyuan_a13b_moe/n0watm0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751436072,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpl3mv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":18}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0vrg8v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kironlau","can_mod_post":false,"created_utc":1751427090,"send_replies":true,"parent_id":"t1_n0vpp97","score":3,"author_fullname":"t2_tb0dz2ds","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm compiling the ik\\\\_llama.cpp in wsl (processing....my cpu is weak... and in eco mode....)  \\nit need to fine tune the parameter afterward, with/withour optimization, the speed may vary 2 times.\\n\\nfirst of all, you may try on the [https://llm.ubergarm.com/](https://llm.ubergarm.com/),  \\nif the quality is not ok for your usage, then no waste of time\\n\\nI compare ubergarm with official huanyuan website (https://hunyuan.tencent.com/)  \\n(maybe need Chinese SMS number, which I have registered one, some hot models need login, some are not, though free)\\n\\nThe answer is not too much different in quality with the unquantized model, okay for my usage.  \\n(I just tested on Chinese: Q&amp;A on philsophy, and summarizing article)","edited":1751430330,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0vrg8v","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m compiling the ik_llama.cpp in wsl (processing....my cpu is weak... and in eco mode....)&lt;br/&gt;\\nit need to fine tune the parameter afterward, with/withour optimization, the speed may vary 2 times.&lt;/p&gt;\\n\\n&lt;p&gt;first of all, you may try on the &lt;a href=\\"https://llm.ubergarm.com/\\"&gt;https://llm.ubergarm.com/&lt;/a&gt;,&lt;br/&gt;\\nif the quality is not ok for your usage, then no waste of time&lt;/p&gt;\\n\\n&lt;p&gt;I compare ubergarm with official huanyuan website (&lt;a href=\\"https://hunyuan.tencent.com/\\"&gt;https://hunyuan.tencent.com/&lt;/a&gt;)&lt;br/&gt;\\n(maybe need Chinese SMS number, which I have registered one, some hot models need login, some are not, though free)&lt;/p&gt;\\n\\n&lt;p&gt;The answer is not too much different in quality with the unquantized model, okay for my usage.&lt;br/&gt;\\n(I just tested on Chinese: Q&amp;amp;A on philsophy, and summarizing article)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpl3mv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl3mv/hosting_your_local_huanyuan_a13b_moe/n0vrg8v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751427090,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0vpp97","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"tcpjack","can_mod_post":false,"created_utc":1751426399,"send_replies":true,"parent_id":"t3_1lpl3mv","score":3,"author_fullname":"t2_3mm65","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Awesome! Itching to give this a try. \\n\\nAnyone try this yet?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0vpp97","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Awesome! Itching to give this a try. &lt;/p&gt;\\n\\n&lt;p&gt;Anyone try this yet?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl3mv/hosting_your_local_huanyuan_a13b_moe/n0vpp97/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751426399,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpl3mv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n141i9q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Cool-Chemical-5629","can_mod_post":false,"created_utc":1751543889,"send_replies":true,"parent_id":"t1_n0wm44s","score":1,"author_fullname":"t2_qz1qjc86","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It's a sign - This LLM is a bad gateway.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n141i9q","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s a sign - This LLM is a bad gateway.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpl3mv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl3mv/hosting_your_local_huanyuan_a13b_moe/n141i9q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751543889,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0wm44s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"shing3232","can_mod_post":false,"created_utc":1751442413,"send_replies":true,"parent_id":"t3_1lpl3mv","score":3,"author_fullname":"t2_ze4mg","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"bad gateway？","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0wm44s","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;bad gateway？&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl3mv/hosting_your_local_huanyuan_a13b_moe/n0wm44s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751442413,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpl3mv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0wmdou","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"crumblix","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0wjuzd","score":2,"author_fullname":"t2_yosg8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"IQ4_XS was giving roughly 25 tokens/sec for reference (and a few GB less VRAM usage obviously as well).\\n\\nThis is a response to \\"hi\\".  I honestly haven't really tested it out much more than getting it to write a couple of simple python functions, I haven't stretched the context at all.  I had to switch to actually doing work :)\\n\\n./llama-cli -m ~/Hunyuan-A13B-Instruct-Q4_K_S.gguf --ctx-size 262144 -b 1024 --no-warmup --cache-type-k q8_0 --cache-type-v q8_0 --flash-attn --temp 0.6 --presence-penalty 0.7 --min-p 0.1 -ngl 99 --jinja\\n\\n....\\n\\n\\nllama_perf_sampler_print:    sampling time =      36.82 ms /   105 runs   (    0.35 ms per token,  2851.94 tokens per second)\\n\\nllama_perf_context_print:        load time =    8319.29 ms\\n\\nllama_perf_context_print: prompt eval time =     153.30 ms /     3 tokens (   51.10 ms per token,    19.57 tokens per second)\\n\\nllama_perf_context_print:        eval time =    4507.41 ms /   101 runs   (   44.63 ms per token,    22.41 tokens per second)\\n\\nllama_perf_context_print:       total time =   15115.74 ms /   104 tokens\\n\\nInterrupted by user","edited":1751442997,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0wmdou","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;IQ4_XS was giving roughly 25 tokens/sec for reference (and a few GB less VRAM usage obviously as well).&lt;/p&gt;\\n\\n&lt;p&gt;This is a response to &amp;quot;hi&amp;quot;.  I honestly haven&amp;#39;t really tested it out much more than getting it to write a couple of simple python functions, I haven&amp;#39;t stretched the context at all.  I had to switch to actually doing work :)&lt;/p&gt;\\n\\n&lt;p&gt;./llama-cli -m ~/Hunyuan-A13B-Instruct-Q4_K_S.gguf --ctx-size 262144 -b 1024 --no-warmup --cache-type-k q8_0 --cache-type-v q8_0 --flash-attn --temp 0.6 --presence-penalty 0.7 --min-p 0.1 -ngl 99 --jinja&lt;/p&gt;\\n\\n&lt;p&gt;....&lt;/p&gt;\\n\\n&lt;p&gt;llama_perf_sampler_print:    sampling time =      36.82 ms /   105 runs   (    0.35 ms per token,  2851.94 tokens per second)&lt;/p&gt;\\n\\n&lt;p&gt;llama_perf_context_print:        load time =    8319.29 ms&lt;/p&gt;\\n\\n&lt;p&gt;llama_perf_context_print: prompt eval time =     153.30 ms /     3 tokens (   51.10 ms per token,    19.57 tokens per second)&lt;/p&gt;\\n\\n&lt;p&gt;llama_perf_context_print:        eval time =    4507.41 ms /   101 runs   (   44.63 ms per token,    22.41 tokens per second)&lt;/p&gt;\\n\\n&lt;p&gt;llama_perf_context_print:       total time =   15115.74 ms /   104 tokens&lt;/p&gt;\\n\\n&lt;p&gt;Interrupted by user&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpl3mv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl3mv/hosting_your_local_huanyuan_a13b_moe/n0wmdou/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751442572,"author_flair_text":null,"treatment_tags":[],"created_utc":1751442572,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0wjuzd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Zyguard7777777","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0wgtgb","score":1,"author_fullname":"t2_zo1h5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Awesome, that's more than I was expecting tbh. Hopefully that will increase as software matures. What prompt processing speed are you getting? ","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0wjuzd","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Awesome, that&amp;#39;s more than I was expecting tbh. Hopefully that will increase as software matures. What prompt processing speed are you getting? &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpl3mv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl3mv/hosting_your_local_huanyuan_a13b_moe/n0wjuzd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751441079,"author_flair_text":null,"treatment_tags":[],"created_utc":1751441079,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0wmmr3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"crumblix","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0wlktj","score":1,"author_fullname":"t2_yosg8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I haven't put it through it's paces really.  Stable enough to get some numbers at least.  It may not be fully baked, but it does run and answer sensibly at least initally, not sure about long sessions though.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0wmmr3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I haven&amp;#39;t put it through it&amp;#39;s paces really.  Stable enough to get some numbers at least.  It may not be fully baked, but it does run and answer sensibly at least initally, not sure about long sessions though.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpl3mv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl3mv/hosting_your_local_huanyuan_a13b_moe/n0wmmr3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751442723,"author_flair_text":null,"treatment_tags":[],"created_utc":1751442723,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0wlktj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0wgtgb","score":1,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; (on my frankenstein build of the ngxson/llama.cpp/tree/xsn/hunyuan-moe branch\\n\\nDoes that work OK now? I've been following the PR and it still doesn't look like it's baked yet.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0wlktj","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;(on my frankenstein build of the ngxson/llama.cpp/tree/xsn/hunyuan-moe branch&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Does that work OK now? I&amp;#39;ve been following the PR and it still doesn&amp;#39;t look like it&amp;#39;s baked yet.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpl3mv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl3mv/hosting_your_local_huanyuan_a13b_moe/n0wlktj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751442090,"author_flair_text":null,"treatment_tags":[],"created_utc":1751442090,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0wgtgb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"crumblix","can_mod_post":false,"created_utc":1751439351,"send_replies":true,"parent_id":"t1_n0w6fsu","score":3,"author_fullname":"t2_yosg8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"GMKTec Ryzen AI Max+ 395.  It is using about 60GB of VRAM on Q4_K_S with 256K context on q8_0 KV, and giving roughly 22 tokens/sec (on my frankenstein build of the ngxson/llama.cpp/tree/xsn/hunyuan-moe branch with TheRock nightly ROCm 7.0 preview / 6.4.1, Ubuntu 24.04).   Thanks very much to all the amazing devs involved in getting it to this stage and creating the test GGUF's!","edited":1751440437,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0wgtgb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;GMKTec Ryzen AI Max+ 395.  It is using about 60GB of VRAM on Q4_K_S with 256K context on q8_0 KV, and giving roughly 22 tokens/sec (on my frankenstein build of the ngxson/llama.cpp/tree/xsn/hunyuan-moe branch with TheRock nightly ROCm 7.0 preview / 6.4.1, Ubuntu 24.04).   Thanks very much to all the amazing devs involved in getting it to this stage and creating the test GGUF&amp;#39;s!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpl3mv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl3mv/hosting_your_local_huanyuan_a13b_moe/n0wgtgb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751439351,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0w6fsu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Zyguard7777777","can_mod_post":false,"created_utc":1751433821,"send_replies":true,"parent_id":"t3_1lpl3mv","score":2,"author_fullname":"t2_zo1h5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'd be curious to see how this performs on amd ai 395 chip, plenty of vram to spare, I worry the memory bandwidth will still make it quite slow though despite only 13b active parameters.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0w6fsu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;d be curious to see how this performs on amd ai 395 chip, plenty of vram to spare, I worry the memory bandwidth will still make it quite slow though despite only 13b active parameters.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl3mv/hosting_your_local_huanyuan_a13b_moe/n0w6fsu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751433821,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpl3mv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0wsee5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Glittering-Bag-4662","can_mod_post":false,"created_utc":1751446161,"send_replies":true,"parent_id":"t3_1lpl3mv","score":2,"author_fullname":"t2_10rvna3i1t","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Censored? Nooooo","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0wsee5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Censored? Nooooo&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl3mv/hosting_your_local_huanyuan_a13b_moe/n0wsee5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751446161,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpl3mv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0yhsbi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1751469949,"send_replies":true,"parent_id":"t3_1lpl3mv","score":2,"author_fullname":"t2_h5utwre7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What if you use it with a different template? Those 300b MoE sound more promising, hopefully they get support. \\n\\nSized between deepseek and 235b.. maybe IK will finally have to support vision models now since there is a contender :)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0yhsbi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What if you use it with a different template? Those 300b MoE sound more promising, hopefully they get support. &lt;/p&gt;\\n\\n&lt;p&gt;Sized between deepseek and 235b.. maybe IK will finally have to support vision models now since there is a contender :)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl3mv/hosting_your_local_huanyuan_a13b_moe/n0yhsbi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751469949,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpl3mv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0vxcfg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kironlau","can_mod_post":false,"created_utc":1751429553,"send_replies":true,"parent_id":"t3_1lpl3mv","score":1,"author_fullname":"t2_tb0dz2ds","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"the first version of the post is wrong.  \\njust edited, confirmed ubergram for the instruction of compiling...  \\nI am recompiling again....","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0vxcfg","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;the first version of the post is wrong.&lt;br/&gt;\\njust edited, confirmed ubergram for the instruction of compiling...&lt;br/&gt;\\nI am recompiling again....&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl3mv/hosting_your_local_huanyuan_a13b_moe/n0vxcfg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751429553,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpl3mv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),o=()=>e.jsx(t,{data:l});export{o as default};
