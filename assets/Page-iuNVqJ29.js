import{j as e}from"./index-Cd3v0jxz.js";import{R as t}from"./RedditPostRenderer-cI96YLhy.js";import"./index-DGBoKyQm.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hey guys, do you have any idea how vibe coding platforms like Replit and Lovable fine tune their code generation algorithms?\\n\\nIt's unclear to me how their core product look like!\\n\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Finetuning for code generation","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m7hvxz","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.6,"author_flair_background_color":null,"subreddit_type":"public","ups":1,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1np6q3qbry","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":1,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753297107,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey guys, do you have any idea how vibe coding platforms like Replit and Lovable fine tune their code generation algorithms?&lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s unclear to me how their core product look like!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1m7hvxz","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"gpt_devastation","discussion_type":null,"num_comments":7,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m7hvxz/finetuning_for_code_generation/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m7hvxz/finetuning_for_code_generation/","subreddit_subscribers":503758,"created_utc":1753297107,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4rjizu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fp4guru","can_mod_post":false,"created_utc":1753297310,"send_replies":true,"parent_id":"t3_1m7hvxz","score":3,"author_fullname":"t2_1tp8zldw5g","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"how could we possibly know?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4rjizu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;how could we possibly know?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7hvxz/finetuning_for_code_generation/n4rjizu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753297310,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7hvxz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4rl0ca","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"blankboy2022","can_mod_post":false,"created_utc":1753297728,"send_replies":true,"parent_id":"t3_1m7hvxz","score":2,"author_fullname":"t2_42ba8hq5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"By putting relevant data into finetuning good models? Like, if you know, you know.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4rl0ca","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;By putting relevant data into finetuning good models? Like, if you know, you know.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7hvxz/finetuning_for_code_generation/n4rl0ca/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753297728,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7hvxz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4rmci6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"tillybowman","can_mod_post":false,"created_utc":1753298107,"send_replies":true,"parent_id":"t3_1m7hvxz","score":1,"author_fullname":"t2_mdv6krfz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"i've not read a single thing about replit. but my guess would be, they don't?  \\n\\nreplit is basically just an army of agents running different (non-finetuned) models like gpt or claude with tons of custom tools and custom system prompts attached no?\\n\\ni mean, they might host things like qwen coder for some agents but i doubt they fine tune.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4rmci6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;i&amp;#39;ve not read a single thing about replit. but my guess would be, they don&amp;#39;t?  &lt;/p&gt;\\n\\n&lt;p&gt;replit is basically just an army of agents running different (non-finetuned) models like gpt or claude with tons of custom tools and custom system prompts attached no?&lt;/p&gt;\\n\\n&lt;p&gt;i mean, they might host things like qwen coder for some agents but i doubt they fine tune.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7hvxz/finetuning_for_code_generation/n4rmci6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753298107,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7hvxz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4rv6u8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"____vladrad","can_mod_post":false,"created_utc":1753300633,"send_replies":true,"parent_id":"t3_1m7hvxz","score":1,"author_fullname":"t2_u6i8a0ay","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I do this with great success at home. Write a script that loops through GitHub history. For each pr generate the question to it and then ask it to generate a agentic dataset without referencing the answer. It does a very good job. You can do a little bit of editing to add in readme calls etc. you then finetune that.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4rv6u8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I do this with great success at home. Write a script that loops through GitHub history. For each pr generate the question to it and then ask it to generate a agentic dataset without referencing the answer. It does a very good job. You can do a little bit of editing to add in readme calls etc. you then finetune that.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7hvxz/finetuning_for_code_generation/n4rv6u8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753300633,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7hvxz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4rzbru","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Efficiency_1144","can_mod_post":false,"created_utc":1753301789,"send_replies":true,"parent_id":"t3_1m7hvxz","score":1,"author_fullname":"t2_1nkj9l14b0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I don’t know these models but the massive increase in coding abilities in the big LLMs recently came from reinforcement learning, mostly PPO and GRPO. Before PPO there was TRPO and REINFORCE which you still see sometimes. Newer ones are DAPP and CISPO.\\n\\n\\nThis is just one type of reinforcement learning which is suited to LLMs. For diffusion models there are some which know that it’s going step by step and give different rewards per step. For robots there are ones where the robot “dreams” a fake world and then does its robot things in there. The “dream” is just a separate diffusion model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4rzbru","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don’t know these models but the massive increase in coding abilities in the big LLMs recently came from reinforcement learning, mostly PPO and GRPO. Before PPO there was TRPO and REINFORCE which you still see sometimes. Newer ones are DAPP and CISPO.&lt;/p&gt;\\n\\n&lt;p&gt;This is just one type of reinforcement learning which is suited to LLMs. For diffusion models there are some which know that it’s going step by step and give different rewards per step. For robots there are ones where the robot “dreams” a fake world and then does its robot things in there. The “dream” is just a separate diffusion model.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7hvxz/finetuning_for_code_generation/n4rzbru/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753301789,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7hvxz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(t,{data:l});export{n as default};
