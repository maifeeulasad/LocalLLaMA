import{j as e}from"./index-CqAPCjw5.js";import{R as l}from"./RedditPostRenderer-4oBDAtGr.js";import"./index-D3Sdy_Op.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I’m working on a science project at a University of Applied Sciences. We plan to purchase a server with an NVIDIA H200 GPU. This system will host LLM services for students.\\n\\nFor development purposes, we’d like to have a second system where speed isn’t critical, but it should still be capable of running the same models we plan to use in production (probably up to 70B parameters). We don’t have the budget to simply replicate the production system — ideally, the dev system should be under €10k.\\n\\nMy research led me to the NVIDIA DGX Spark and similar solutions from other vendors, but none of the resellers I contacted had any idea when these systems will be available. (Paper launch?)\\n\\nI also found the GMKtec EVO-X2, which seems to be the AMD equivalent of the Spark. It’s cheap and available, but I don’t have any experience with ROCm, and developing on an AMD machine for a CUDA-based production system seems like an odd choice. On the other hand, we don’t plan to develop at the CUDA level, but rather focus on pipelines and orchestration.\\n\\nA third option would be to build a system with a few older cards like K40s or something similar.\\n\\nWhat would you advise?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Affordable dev system (spark alternative?)","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lo35gq","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.77,"author_flair_background_color":null,"subreddit_type":"public","ups":7,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_9mg5e1cr","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":7,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751275859,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I’m working on a science project at a University of Applied Sciences. We plan to purchase a server with an NVIDIA H200 GPU. This system will host LLM services for students.&lt;/p&gt;\\n\\n&lt;p&gt;For development purposes, we’d like to have a second system where speed isn’t critical, but it should still be capable of running the same models we plan to use in production (probably up to 70B parameters). We don’t have the budget to simply replicate the production system — ideally, the dev system should be under €10k.&lt;/p&gt;\\n\\n&lt;p&gt;My research led me to the NVIDIA DGX Spark and similar solutions from other vendors, but none of the resellers I contacted had any idea when these systems will be available. (Paper launch?)&lt;/p&gt;\\n\\n&lt;p&gt;I also found the GMKtec EVO-X2, which seems to be the AMD equivalent of the Spark. It’s cheap and available, but I don’t have any experience with ROCm, and developing on an AMD machine for a CUDA-based production system seems like an odd choice. On the other hand, we don’t plan to develop at the CUDA level, but rather focus on pipelines and orchestration.&lt;/p&gt;\\n\\n&lt;p&gt;A third option would be to build a system with a few older cards like K40s or something similar.&lt;/p&gt;\\n\\n&lt;p&gt;What would you advise?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lo35gq","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"_camera_up","discussion_type":null,"num_comments":14,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lo35gq/affordable_dev_system_spark_alternative/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lo35gq/affordable_dev_system_spark_alternative/","subreddit_subscribers":493242,"created_utc":1751275859,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0njsl2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"emprahsFury","can_mod_post":false,"created_utc":1751320373,"send_replies":false,"parent_id":"t1_n0mna5g","score":2,"author_fullname":"t2_177r8n","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Really the only sensible option. A dgx spark isn't going to enable anything an rtx pro cannot do except an llm that's &gt;200B and won't even scale past 300B. Maybe if you buy 2-4 as Jensen claimed you'd be able to, but even then it's not going to run such large models at anything more than tokens per second","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0njsl2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Really the only sensible option. A dgx spark isn&amp;#39;t going to enable anything an rtx pro cannot do except an llm that&amp;#39;s &amp;gt;200B and won&amp;#39;t even scale past 300B. Maybe if you buy 2-4 as Jensen claimed you&amp;#39;d be able to, but even then it&amp;#39;s not going to run such large models at anything more than tokens per second&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lo35gq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo35gq/affordable_dev_system_spark_alternative/n0njsl2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751320373,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0mna5g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"NoVibeCoding","can_mod_post":false,"created_utc":1751310765,"send_replies":true,"parent_id":"t3_1lo35gq","score":4,"author_fullname":"t2_1neapdttam","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The RTX PRO 6000 workstation might do the trick. It is about €10K. With 96GB of VRAM, you will be able to run numerous models.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0mna5g","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The RTX PRO 6000 workstation might do the trick. It is about €10K. With 96GB of VRAM, you will be able to run numerous models.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo35gq/affordable_dev_system_spark_alternative/n0mna5g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751310765,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lo35gq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ldwdf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Noxusequal","can_mod_post":false,"created_utc":1751297908,"send_replies":true,"parent_id":"t3_1lo35gq","score":2,"author_fullname":"t2_b3etlj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I would say using amd isn't a problem depending on what you test ? I meannif you finetune models.on the big server and then run inference tests on the small.one it kinda doesn't matter as much. \\n\\nGenerally I would say if what you do on the small system is inference once you set up an inference engine. It doesn't matter if its amd, apple or nvidia. Since in the api all is the same.  However if you want to do model training or specific model.modefications on the small system as well using a different vendor is not a good idea. So what is the exact use case you have ?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ldwdf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I would say using amd isn&amp;#39;t a problem depending on what you test ? I meannif you finetune models.on the big server and then run inference tests on the small.one it kinda doesn&amp;#39;t matter as much. &lt;/p&gt;\\n\\n&lt;p&gt;Generally I would say if what you do on the small system is inference once you set up an inference engine. It doesn&amp;#39;t matter if its amd, apple or nvidia. Since in the api all is the same.  However if you want to do model training or specific model.modefications on the small system as well using a different vendor is not a good idea. So what is the exact use case you have ?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo35gq/affordable_dev_system_spark_alternative/n0ldwdf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751297908,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lo35gq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0os72b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SkyFeistyLlama8","can_mod_post":false,"created_utc":1751335463,"send_replies":true,"parent_id":"t1_n0l3gur","score":1,"author_fullname":"t2_1hgbaqgbnq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah, Nvidia nailed it by identifying a market segment that competitors haven't tried entering. A proper AI workstation doesn't exist yet and a Mac Studio sure as hell ain't it.\\n\\nI'd just wait for a Spark. You could technically run inference on AMD, Apple or even Snapdragon X but you'd be using bleeding-edge packages and there would be little support for finetuning or building new models from scratch, for the sake of learning. It's still CUDA or nothing.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0os72b","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, Nvidia nailed it by identifying a market segment that competitors haven&amp;#39;t tried entering. A proper AI workstation doesn&amp;#39;t exist yet and a Mac Studio sure as hell ain&amp;#39;t it.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;d just wait for a Spark. You could technically run inference on AMD, Apple or even Snapdragon X but you&amp;#39;d be using bleeding-edge packages and there would be little support for finetuning or building new models from scratch, for the sake of learning. It&amp;#39;s still CUDA or nothing.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lo35gq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo35gq/affordable_dev_system_spark_alternative/n0os72b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751335463,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0l3gur","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"mtmttuan","can_mod_post":false,"created_utc":1751294859,"send_replies":true,"parent_id":"t3_1lo35gq","score":4,"author_fullname":"t2_6mjqz0at","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Lol OP's school want a server stacked with H200 freaking GPUs and 10k$ of additional compute and people here are recommending Mac Studio and laptops lol","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0l3gur","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Lol OP&amp;#39;s school want a server stacked with H200 freaking GPUs and 10k$ of additional compute and people here are recommending Mac Studio and laptops lol&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo35gq/affordable_dev_system_spark_alternative/n0l3gur/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751294859,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lo35gq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0lewf5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"pmv143","can_mod_post":false,"created_utc":1751298200,"send_replies":true,"parent_id":"t3_1lo35gq","score":1,"author_fullname":"t2_teiqn5f1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You could also explore runtime platforms that support model snapshots and orchestration without replicating full production hardware. We’re building InferX for exactly this . loading large models dynamically, orchestrating on shared GPUs, and testing flows without needing the full infra every time. Might be worth chatting if dev-test efficiency is a blocker.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0lewf5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You could also explore runtime platforms that support model snapshots and orchestration without replicating full production hardware. We’re building InferX for exactly this . loading large models dynamically, orchestrating on shared GPUs, and testing flows without needing the full infra every time. Might be worth chatting if dev-test efficiency is a blocker.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo35gq/affordable_dev_system_spark_alternative/n0lewf5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751298200,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lo35gq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0mc9as","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FlexFreak","can_mod_post":false,"created_utc":1751307578,"send_replies":true,"parent_id":"t3_1lo35gq","score":1,"author_fullname":"t2_98uikklu","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Just test in production","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0mc9as","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just test in production&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo35gq/affordable_dev_system_spark_alternative/n0mc9as/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751307578,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lo35gq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0p9rmw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Double_Cause4609","can_mod_post":false,"created_utc":1751342200,"send_replies":true,"parent_id":"t3_1lo35gq","score":2,"author_fullname":"t2_1kubzxt2ww","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I apologize greatly if this is an unsuitable suggestion, but had you considered CPU inference?\\n\\nAs a rule, CPU inference is the cheapest way to run a model if your metric for running is \\\\[yes/no\\\\], and there's this weird thing that happens with inference at smaller scales (that is to say, fewer than 200 concurrent requests), where CPUs actually end up performing pretty similarly to GPUs for the same price.\\n\\nIf you're considering systems like Spark and Strix Halo APU based systems, you're probably already looking at limited use of the system.\\n\\nEpyc 9124 processors for instance go for not that much more than consumer variants, and in your budget getting up to enough RAM to run models like Deepseek is not at all out of the question. Depending on the specifics you could handle some of the more popular big MoE models at around \\\\~10-20 tokens per second I believe (single-user), and you'd probably scale fairly gracefully in throughput with higher loads.\\n\\nFor models up to 70B (dense), I think you'd be looking at around \\\\~5 tokens per second (single-user), and with enough memory you might be able to get it up to around 70-120 tokens per second in total with high concurrency (particularly on vLLM's CPU inference backend).\\n\\nThere's a lot of room open for interesting research projects involving optimizing CPU inference, as a lot of things like sparsity are more tenable there.\\n\\nAdditionally, down the line adding in a GPU is possibly viable depending on the exact workloads, and while I don't know how well LlamaCPP scales in its current form to concurrent requests with hybrid inference, I know that KTransformers handles it well for low concurrency (4-16 requests), and hybrid inference tends to offer the most reasonable balance of low user inference per dollar. There may be further advancements in hybrid inference down the line, too.\\n\\nAnother note: There's a lot of opportunity for research projects optimizing CPU + NPU inference. Alternatives to autoregressive inference are starting to show up, like Diffusion, Parallel Scaling, possibly things like energy based models under JEPA or Active Inference, etc etc. These alternatives are more balanced in their compute / bandwidth ratio and favor the use of compute dense architectures. That end of the field is fairly green in terms of available low hanging fruit and it's exactly the sort of thing I'd be interested in participating in if I were in academia.\\n\\nAdditionally, add-in NPUs are very cheap comparatively; I can't imagine it would hurt to throw in a fairly cheap Hailo NPU to see if anyone can get anything useful done with it in low-bit operations (ie: int8, etc). Even without new types of models or objectives, having a compute dense piece of hardware to handle Attention operations alone is super valuable.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0p9rmw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I apologize greatly if this is an unsuitable suggestion, but had you considered CPU inference?&lt;/p&gt;\\n\\n&lt;p&gt;As a rule, CPU inference is the cheapest way to run a model if your metric for running is [yes/no], and there&amp;#39;s this weird thing that happens with inference at smaller scales (that is to say, fewer than 200 concurrent requests), where CPUs actually end up performing pretty similarly to GPUs for the same price.&lt;/p&gt;\\n\\n&lt;p&gt;If you&amp;#39;re considering systems like Spark and Strix Halo APU based systems, you&amp;#39;re probably already looking at limited use of the system.&lt;/p&gt;\\n\\n&lt;p&gt;Epyc 9124 processors for instance go for not that much more than consumer variants, and in your budget getting up to enough RAM to run models like Deepseek is not at all out of the question. Depending on the specifics you could handle some of the more popular big MoE models at around ~10-20 tokens per second I believe (single-user), and you&amp;#39;d probably scale fairly gracefully in throughput with higher loads.&lt;/p&gt;\\n\\n&lt;p&gt;For models up to 70B (dense), I think you&amp;#39;d be looking at around ~5 tokens per second (single-user), and with enough memory you might be able to get it up to around 70-120 tokens per second in total with high concurrency (particularly on vLLM&amp;#39;s CPU inference backend).&lt;/p&gt;\\n\\n&lt;p&gt;There&amp;#39;s a lot of room open for interesting research projects involving optimizing CPU inference, as a lot of things like sparsity are more tenable there.&lt;/p&gt;\\n\\n&lt;p&gt;Additionally, down the line adding in a GPU is possibly viable depending on the exact workloads, and while I don&amp;#39;t know how well LlamaCPP scales in its current form to concurrent requests with hybrid inference, I know that KTransformers handles it well for low concurrency (4-16 requests), and hybrid inference tends to offer the most reasonable balance of low user inference per dollar. There may be further advancements in hybrid inference down the line, too.&lt;/p&gt;\\n\\n&lt;p&gt;Another note: There&amp;#39;s a lot of opportunity for research projects optimizing CPU + NPU inference. Alternatives to autoregressive inference are starting to show up, like Diffusion, Parallel Scaling, possibly things like energy based models under JEPA or Active Inference, etc etc. These alternatives are more balanced in their compute / bandwidth ratio and favor the use of compute dense architectures. That end of the field is fairly green in terms of available low hanging fruit and it&amp;#39;s exactly the sort of thing I&amp;#39;d be interested in participating in if I were in academia.&lt;/p&gt;\\n\\n&lt;p&gt;Additionally, add-in NPUs are very cheap comparatively; I can&amp;#39;t imagine it would hurt to throw in a fairly cheap Hailo NPU to see if anyone can get anything useful done with it in low-bit operations (ie: int8, etc). Even without new types of models or objectives, having a compute dense piece of hardware to handle Attention operations alone is super valuable.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo35gq/affordable_dev_system_spark_alternative/n0p9rmw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751342200,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lo35gq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0k96ex","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok_Hope_4007","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0k3gr2","score":-2,"author_fullname":"t2_pjzish3n","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I would disagree. It just depends on what your development focus is.\\nThe only major difference is the inference engine for your llm. You can ground your llm service stack on an openai compatible inference endpoint which could be llama.cpp on mac and llama.cpp/vllm/slang etc on your linux h200 server or even a third party subscription...\\n\\nBut i assume that the actual 'development' is the pipeline/services that define what you use the LLm for and this stack is most likely built on top of some kind of framework and custom code combination which i see not being any different on mac than linux.\\n\\nI suggested this as an alternative because one could develop your service stack AND host a variety of LLMs on a single machine. Once you are happy you would swap out the api_url from a slow mac to a fast h200. \\n\\nBut you are right if the majority of your focus is on how to setup/configure  a runtime environment for the llm.","edited":1751284668,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0k96ex","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I would disagree. It just depends on what your development focus is.\\nThe only major difference is the inference engine for your llm. You can ground your llm service stack on an openai compatible inference endpoint which could be llama.cpp on mac and llama.cpp/vllm/slang etc on your linux h200 server or even a third party subscription...&lt;/p&gt;\\n\\n&lt;p&gt;But i assume that the actual &amp;#39;development&amp;#39; is the pipeline/services that define what you use the LLm for and this stack is most likely built on top of some kind of framework and custom code combination which i see not being any different on mac than linux.&lt;/p&gt;\\n\\n&lt;p&gt;I suggested this as an alternative because one could develop your service stack AND host a variety of LLMs on a single machine. Once you are happy you would swap out the api_url from a slow mac to a fast h200. &lt;/p&gt;\\n\\n&lt;p&gt;But you are right if the majority of your focus is on how to setup/configure  a runtime environment for the llm.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lo35gq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo35gq/affordable_dev_system_spark_alternative/n0k96ex/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751284363,"author_flair_text":null,"treatment_tags":[],"created_utc":1751284363,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0k3gr2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FullstackSensei","can_mod_post":false,"created_utc":1751281754,"send_replies":true,"parent_id":"t1_n0junq6","score":4,"author_fullname":"t2_17n3nqtj56","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"OP is literally saying they want a development system for an H200 production system. Buying a mac means literally everything is different.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0k3gr2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;OP is literally saying they want a development system for an H200 production system. Buying a mac means literally everything is different.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lo35gq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo35gq/affordable_dev_system_spark_alternative/n0k3gr2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751281754,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n0junq6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok_Hope_4007","can_mod_post":false,"created_utc":1751277027,"send_replies":true,"parent_id":"t3_1lo35gq","score":-4,"author_fullname":"t2_pjzish3n","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Have you considered a Mac Studio M4/M3 ? If you are not relying on fiddeling with CUDA and need to run LLMs for Prototyping/Development then these will fit perfectly iny opinion. The 96/128GB variant will probably be sufficient and most likely within your budget. \\nOf course prompt processing is relatively slow but that might not be an issue on a development machine. \\nI like to link to the llama.cpp [Benchmark](https://github.com/ggml-org/llama.cpp/discussions/4167) \\nIt will give you at least a hint on a baseline of llm performance for different macs.\\n\\nEDIT\\n\\n[This post](https://www.reddit.com/r/LocalLLaMA/s/sclrqRR4F3) lists performance for larger llms and an m4 max chip.","edited":1751277292,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0junq6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Have you considered a Mac Studio M4/M3 ? If you are not relying on fiddeling with CUDA and need to run LLMs for Prototyping/Development then these will fit perfectly iny opinion. The 96/128GB variant will probably be sufficient and most likely within your budget. \\nOf course prompt processing is relatively slow but that might not be an issue on a development machine. \\nI like to link to the llama.cpp &lt;a href=\\"https://github.com/ggml-org/llama.cpp/discussions/4167\\"&gt;Benchmark&lt;/a&gt; \\nIt will give you at least a hint on a baseline of llm performance for different macs.&lt;/p&gt;\\n\\n&lt;p&gt;EDIT&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/s/sclrqRR4F3\\"&gt;This post&lt;/a&gt; lists performance for larger llms and an m4 max chip.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo35gq/affordable_dev_system_spark_alternative/n0junq6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751277027,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lo35gq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0l65bo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Herr_Drosselmeyer","can_mod_post":false,"created_utc":1751295644,"send_replies":true,"parent_id":"t3_1lo35gq","score":0,"author_fullname":"t2_1zr9gwsn","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The DGX Spark would be ideal for your purpose. Going with an AMD or Mac based rig makes no sense, since you'll have to use entirely different software from what you'll be using on your production server.\\n\\n&gt;DGX Spark will be available from Acer, ASUS, Dell Technologies, GIGABYTE, HP, Lenovo and MSI, as well as global channel partners, starting in July.\\n\\nWe're not yet in July and many vendors have accepted preorders, so availability could be tight for a month or two.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0l65bo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The DGX Spark would be ideal for your purpose. Going with an AMD or Mac based rig makes no sense, since you&amp;#39;ll have to use entirely different software from what you&amp;#39;ll be using on your production server.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;DGX Spark will be available from Acer, ASUS, Dell Technologies, GIGABYTE, HP, Lenovo and MSI, as well as global channel partners, starting in July.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;We&amp;#39;re not yet in July and many vendors have accepted preorders, so availability could be tight for a month or two.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo35gq/affordable_dev_system_spark_alternative/n0l65bo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751295644,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lo35gq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0k39kx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullstackSensei","can_mod_post":false,"created_utc":1751281657,"send_replies":true,"parent_id":"t3_1lo35gq","score":-2,"author_fullname":"t2_17n3nqtj56","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Why not get some laptops with the RTX 5090? Those come with 24GB of VRAM. Not exactly 70B territory (unless you're fine with Q2/iQ2 quants), but that's probably the easiest way to have an integrated solution with as close CUDA-features support as the H200.\\n\\nAlternatively, build a desktop with a desktop 5090. Will probably cost the same as the laptop and have better performance and more VRAM (32GB vs 24GB). The only question is availability to buy as a whole system with warranty and support for the university, which will greatly depend on where you live.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0k39kx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why not get some laptops with the RTX 5090? Those come with 24GB of VRAM. Not exactly 70B territory (unless you&amp;#39;re fine with Q2/iQ2 quants), but that&amp;#39;s probably the easiest way to have an integrated solution with as close CUDA-features support as the H200.&lt;/p&gt;\\n\\n&lt;p&gt;Alternatively, build a desktop with a desktop 5090. Will probably cost the same as the laptop and have better performance and more VRAM (32GB vs 24GB). The only question is availability to buy as a whole system with warranty and support for the university, which will greatly depend on where you live.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo35gq/affordable_dev_system_spark_alternative/n0k39kx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751281657,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lo35gq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0knxcc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"secopsml","can_mod_post":false,"created_utc":1751289973,"send_replies":true,"parent_id":"t3_1lo35gq","score":-2,"author_fullname":"t2_pmniwf57y","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"3090/4090? 4x3090?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0knxcc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;3090/4090? 4x3090?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo35gq/affordable_dev_system_spark_alternative/n0knxcc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751289973,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lo35gq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-2}}],"before":null}}]`),r=()=>e.jsx(l,{data:t});export{r as default};
