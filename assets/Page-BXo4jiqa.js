import{j as e}from"./index-CqAPCjw5.js";import{R as t}from"./RedditPostRenderer-4oBDAtGr.js";import"./index-D3Sdy_Op.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I'm working on a privacy sensitive usecase that needs a LLM. Instead of relaying the entire prompt to the server, I want to run a few layers in the client and then send the intermediate state to the server to be run until completion.  \\nWhile I understand this doesn't exactly solve the privacy issue, this level of information loss is enough for my usecase.\\n\\nMy questions:  \\n1. Is something like this even possible? Has anybody done something like this before?  \\n2. If this is possible, will the resulting clients-side model be runnable with limited hardware (rephrase: Does running a partial model going to require enough hardware power as much as running a full model?)","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"I want to split a model to run a portion of it on client and run the remaining layers on server. Is that possible?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lqo1bt","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.67,"author_flair_background_color":null,"subreddit_type":"public","ups":1,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_2ue366bm","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":1,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751544887,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m working on a privacy sensitive usecase that needs a LLM. Instead of relaying the entire prompt to the server, I want to run a few layers in the client and then send the intermediate state to the server to be run until completion.&lt;br/&gt;\\nWhile I understand this doesn&amp;#39;t exactly solve the privacy issue, this level of information loss is enough for my usecase.&lt;/p&gt;\\n\\n&lt;p&gt;My questions:&lt;br/&gt;\\n1. Is something like this even possible? Has anybody done something like this before?&lt;br/&gt;\\n2. If this is possible, will the resulting clients-side model be runnable with limited hardware (rephrase: Does running a partial model going to require enough hardware power as much as running a full model?)&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lqo1bt","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"crazycodemonkey","discussion_type":null,"num_comments":4,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lqo1bt/i_want_to_split_a_model_to_run_a_portion_of_it_on/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lqo1bt/i_want_to_split_a_model_to_run_a_portion_of_it_on/","subreddit_subscribers":494198,"created_utc":1751544887,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n15k9fy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Pedalnomica","can_mod_post":false,"created_utc":1751560919,"send_replies":true,"parent_id":"t3_1lqo1bt","score":2,"author_fullname":"t2_b0d7j6x9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'm not an expert, but I'm pretty sure the transformers architecture uses the KV cache with every layer. Since that is basically your prompt, I think what you're suggesting is impossible.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n15k9fy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m not an expert, but I&amp;#39;m pretty sure the transformers architecture uses the KV cache with every layer. Since that is basically your prompt, I think what you&amp;#39;re suggesting is impossible.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqo1bt/i_want_to_split_a_model_to_run_a_portion_of_it_on/n15k9fy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751560919,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqo1bt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n14dryv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rusty_fans","can_mod_post":false,"created_utc":1751548397,"send_replies":true,"parent_id":"t3_1lqo1bt","score":1,"author_fullname":"t2_9ntkbp4y3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Possible? Likely yes.\\nPractical ? No, not at all. \\nYou'd be massively limited by the bandwidth &amp; latency between client &amp; server. \\n\\nPerformance would be abysmal, (rough guess minutes to hours per token, instead of tens-hundreds tokens per seconds)...\\n\\nNobody has done it because of the expected horrible performance.\\n\\nWhat you could do and is quite practical if optimized well, is run a very lightweight/small LLM like Qwen3-4B on the client to pre-process stuff and delegate to a bigger more capable model on the server for harder parts of the problem.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n14dryv","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Possible? Likely yes.\\nPractical ? No, not at all. \\nYou&amp;#39;d be massively limited by the bandwidth &amp;amp; latency between client &amp;amp; server. &lt;/p&gt;\\n\\n&lt;p&gt;Performance would be abysmal, (rough guess minutes to hours per token, instead of tens-hundreds tokens per seconds)...&lt;/p&gt;\\n\\n&lt;p&gt;Nobody has done it because of the expected horrible performance.&lt;/p&gt;\\n\\n&lt;p&gt;What you could do and is quite practical if optimized well, is run a very lightweight/small LLM like Qwen3-4B on the client to pre-process stuff and delegate to a bigger more capable model on the server for harder parts of the problem.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqo1bt/i_want_to_split_a_model_to_run_a_portion_of_it_on/n14dryv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751548397,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lqo1bt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n15vpm9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Conscious_Cut_6144","can_mod_post":false,"created_utc":1751564088,"send_replies":true,"parent_id":"t3_1lqo1bt","score":1,"author_fullname":"t2_9hl4ymvj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Llama.cpp supports RPC where some layers are on one server and some are on another.  \\nHowever I don't think you actually achieve anything privacy wise depending on what you are trying to protect. \\n\\nIf it's the actual model you are worried about someone stealing this might stand a tiny chance of working.\\n\\nBut if it's the prompt with sensitive data then no, each layer processes the KV cache, so any server in the chain sees the whole prompt.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n15vpm9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Llama.cpp supports RPC where some layers are on one server and some are on another.&lt;br/&gt;\\nHowever I don&amp;#39;t think you actually achieve anything privacy wise depending on what you are trying to protect. &lt;/p&gt;\\n\\n&lt;p&gt;If it&amp;#39;s the actual model you are worried about someone stealing this might stand a tiny chance of working.&lt;/p&gt;\\n\\n&lt;p&gt;But if it&amp;#39;s the prompt with sensitive data then no, each layer processes the KV cache, so any server in the chain sees the whole prompt.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqo1bt/i_want_to_split_a_model_to_run_a_portion_of_it_on/n15vpm9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751564088,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqo1bt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1943ox","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"townofsalemfangay","can_mod_post":false,"created_utc":1751603516,"send_replies":true,"parent_id":"t3_1lqo1bt","score":1,"author_fullname":"t2_122dsg","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What’s the specific use case here?  \\n  \\nFrom what you’ve described, it sounds like you want the client to retain potentially sensitive PII while still leveraging the full power of a larger model on the server, but without sending raw PII downstream. If that’s the goal, I’d suggest rethinking the partial model execution idea.\\n\\nInstead, consider building a client-side preprocessing layer with a lightweight model that strips or masks PII before passing the cleaned context to the server. The larger model can then generate the response, which the client wraps or reintegrates with the original context as needed.\\n\\nIs this feasible? Yes, but it’s complex, and only really justifiable in niche, high-security scenarios. You’d need model compatibility between client and server, custom layer-splitting, and a carefully managed state handoff (especially around KV cache if you're using attention). And even then, the privacy gains are marginal; early layers often encode surprisingly interpretable information.\\n\\nFor the edge layer, you could look at modern compact models like SmolLM2 (1.7B), Gemma 3 (1B), and others. Personally, I’d go with something like Qwen 3 (0.6B), arguably the best low-parameter instruct model right now, with solid reasoning ability and a lightweight enough footprint to slot neatly into a preprocessing setup.\\n\\nIdeally, though, unless you have strong reasons to avoid sending *any* PII, the cleaner solution is just to encrypt end-to-end and handle sanitisation at the prompt layer.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1943ox","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What’s the specific use case here?  &lt;/p&gt;\\n\\n&lt;p&gt;From what you’ve described, it sounds like you want the client to retain potentially sensitive PII while still leveraging the full power of a larger model on the server, but without sending raw PII downstream. If that’s the goal, I’d suggest rethinking the partial model execution idea.&lt;/p&gt;\\n\\n&lt;p&gt;Instead, consider building a client-side preprocessing layer with a lightweight model that strips or masks PII before passing the cleaned context to the server. The larger model can then generate the response, which the client wraps or reintegrates with the original context as needed.&lt;/p&gt;\\n\\n&lt;p&gt;Is this feasible? Yes, but it’s complex, and only really justifiable in niche, high-security scenarios. You’d need model compatibility between client and server, custom layer-splitting, and a carefully managed state handoff (especially around KV cache if you&amp;#39;re using attention). And even then, the privacy gains are marginal; early layers often encode surprisingly interpretable information.&lt;/p&gt;\\n\\n&lt;p&gt;For the edge layer, you could look at modern compact models like SmolLM2 (1.7B), Gemma 3 (1B), and others. Personally, I’d go with something like Qwen 3 (0.6B), arguably the best low-parameter instruct model right now, with solid reasoning ability and a lightweight enough footprint to slot neatly into a preprocessing setup.&lt;/p&gt;\\n\\n&lt;p&gt;Ideally, though, unless you have strong reasons to avoid sending &lt;em&gt;any&lt;/em&gt; PII, the cleaner solution is just to encrypt end-to-end and handle sanitisation at the prompt layer.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqo1bt/i_want_to_split_a_model_to_run_a_portion_of_it_on/n1943ox/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751603516,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqo1bt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(t,{data:l});export{r as default};
