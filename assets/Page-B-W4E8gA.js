import{j as e}from"./index-Bqs-ekb2.js";import{R as l}from"./RedditPostRenderer-DUVdf0-i.js";import"./index-D52ORTDm.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hardware is a mini PC with AMD's Ryzen AI MAX 395 APU with 128GB RAM. Model is llama-4-scout, which is an MOE with 16B active and 109B total parameters.\\n\\nUI: GAIA, our fork of Open WebUI, that offers out-of-box Lemonade integration, a one-click installer, and electron.js app experience. [https://github.com/amd/gaia](https://github.com/amd/gaia)\\n\\nInference server: Lemonade, our AMD-first OpenAI compatible server, running llama.cpp+Vulkan in the backend on the APU's Radeon 8060S GPU. [https://github.com/lemonade-sdk/lemonade](https://github.com/lemonade-sdk/lemonade)\\n\\nI found it cool that a model of this size with VLM capability could achieve usable TPS on a mini PC and wanted to see if others were excited as well.\\n\\nFull disclosure: prompt processing time (pp) was 13 seconds, and I edited that part out when making the video. Mentioned this in the post title and video caption for maximum transparency. I find 13 seconds usable for this model+usecase, but not very entertaining in a Reddit video.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"llama-4-scout-17B-16E GGUF running on Strix Halo (Ryzen AI MAX 395 + 128GB) (13s prompt processing edited out)","link_flair_richtext":[{"e":"text","t":"Resources"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":78,"top_awarded_type":null,"hide_score":false,"name":"t3_1lpy8nv","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.88,"author_flair_background_color":null,"ups":72,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1m2ckixcqh","secure_media":{"reddit_video":{"bitrate_kbps":5000,"fallback_url":"https://v.redd.it/e6ao7yjh5haf1/DASH_1080.mp4?source=fallback","has_audio":true,"height":1080,"width":1920,"scrubber_media_url":"https://v.redd.it/e6ao7yjh5haf1/DASH_96.mp4","dash_url":"https://v.redd.it/e6ao7yjh5haf1/DASHPlaylist.mpd?a=1754149874%2COWI2NzNkZTdmZmRmZThjMGZhMDQ5MGNjMDY4ZGJmZmYzNjk2MzlkYjQ4ZDBlZDU2ZTEwNGZiZGJiNjU3YzFmYQ%3D%3D&amp;v=1&amp;f=sd","duration":22,"hls_url":"https://v.redd.it/e6ao7yjh5haf1/HLSPlaylist.m3u8?a=1754149874%2CNWEyMWM4YmI1NGM2NTgyMGU2MmU3ZjM3YTZkNTBjMTlmM2QwMTAwZGI1YWFkYzAwYjkxMGU2YzQzODRiNmU0Zg%3D%3D&amp;v=1&amp;f=sd","is_gif":false,"transcoding_status":"completed"}},"is_reddit_media_domain":true,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Resources","can_mod_post":false,"score":72,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://external-preview.redd.it/am1mM2cxa2g1aGFmMQV6SVFBQzrbFE16bYnvbQZAeh3r0dX_wLOv44vX6S3R.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=e1f718d7079c60f94f735410900431972636cede","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"hosted:video","content_categories":null,"is_self":false,"subreddit_type":"public","created":1751468754,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"v.redd.it","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hardware is a mini PC with AMD&amp;#39;s Ryzen AI MAX 395 APU with 128GB RAM. Model is llama-4-scout, which is an MOE with 16B active and 109B total parameters.&lt;/p&gt;\\n\\n&lt;p&gt;UI: GAIA, our fork of Open WebUI, that offers out-of-box Lemonade integration, a one-click installer, and electron.js app experience. &lt;a href=\\"https://github.com/amd/gaia\\"&gt;https://github.com/amd/gaia&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Inference server: Lemonade, our AMD-first OpenAI compatible server, running llama.cpp+Vulkan in the backend on the APU&amp;#39;s Radeon 8060S GPU. &lt;a href=\\"https://github.com/lemonade-sdk/lemonade\\"&gt;https://github.com/lemonade-sdk/lemonade&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;I found it cool that a model of this size with VLM capability could achieve usable TPS on a mini PC and wanted to see if others were excited as well.&lt;/p&gt;\\n\\n&lt;p&gt;Full disclosure: prompt processing time (pp) was 13 seconds, and I edited that part out when making the video. Mentioned this in the post title and video caption for maximum transparency. I find 13 seconds usable for this model+usecase, but not very entertaining in a Reddit video.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"url_overridden_by_dest":"https://v.redd.it/e6ao7yjh5haf1","view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/am1mM2cxa2g1aGFmMQV6SVFBQzrbFE16bYnvbQZAeh3r0dX_wLOv44vX6S3R.png?format=pjpg&amp;auto=webp&amp;s=4b1c4b76e0dc99f437a47284c476d6bb6d814dc1","width":1920,"height":1080},"resolutions":[{"url":"https://external-preview.redd.it/am1mM2cxa2g1aGFmMQV6SVFBQzrbFE16bYnvbQZAeh3r0dX_wLOv44vX6S3R.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=7edee4961fc0944fa68d82b0e8f7d4f6a9b508f6","width":108,"height":60},{"url":"https://external-preview.redd.it/am1mM2cxa2g1aGFmMQV6SVFBQzrbFE16bYnvbQZAeh3r0dX_wLOv44vX6S3R.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=95d1a5633971ac0518c4154e32d5c957e8e0c415","width":216,"height":121},{"url":"https://external-preview.redd.it/am1mM2cxa2g1aGFmMQV6SVFBQzrbFE16bYnvbQZAeh3r0dX_wLOv44vX6S3R.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d4b2d3baafd68805600c14522f04b267109c6bd9","width":320,"height":180},{"url":"https://external-preview.redd.it/am1mM2cxa2g1aGFmMQV6SVFBQzrbFE16bYnvbQZAeh3r0dX_wLOv44vX6S3R.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d3ba311dbd4239ec1df6efeb77952b592fa1f126","width":640,"height":360},{"url":"https://external-preview.redd.it/am1mM2cxa2g1aGFmMQV6SVFBQzrbFE16bYnvbQZAeh3r0dX_wLOv44vX6S3R.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=7c1f74c006b8d794c8ce3922815de33fb5439bf6","width":960,"height":540},{"url":"https://external-preview.redd.it/am1mM2cxa2g1aGFmMQV6SVFBQzrbFE16bYnvbQZAeh3r0dX_wLOv44vX6S3R.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=739c5db6d12b6704b26154959af635576548016f","width":1080,"height":607}],"variants":{},"id":"am1mM2cxa2g1aGFmMQV6SVFBQzrbFE16bYnvbQZAeh3r0dX_wLOv44vX6S3R"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"mod_note":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"num_reports":null,"removal_reason":null,"link_flair_background_color":"#ccac2b","id":"1lpy8nv","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"jfowers_amd","discussion_type":null,"num_comments":42,"send_replies":true,"media":{"reddit_video":{"bitrate_kbps":5000,"fallback_url":"https://v.redd.it/e6ao7yjh5haf1/DASH_1080.mp4?source=fallback","has_audio":true,"height":1080,"width":1920,"scrubber_media_url":"https://v.redd.it/e6ao7yjh5haf1/DASH_96.mp4","dash_url":"https://v.redd.it/e6ao7yjh5haf1/DASHPlaylist.mpd?a=1754149874%2COWI2NzNkZTdmZmRmZThjMGZhMDQ5MGNjMDY4ZGJmZmYzNjk2MzlkYjQ4ZDBlZDU2ZTEwNGZiZGJiNjU3YzFmYQ%3D%3D&amp;v=1&amp;f=sd","duration":22,"hls_url":"https://v.redd.it/e6ao7yjh5haf1/HLSPlaylist.m3u8?a=1754149874%2CNWEyMWM4YmI1NGM2NTgyMGU2MmU3ZjM3YTZkNTBjMTlmM2QwMTAwZGI1YWFkYzAwYjkxMGU2YzQzODRiNmU0Zg%3D%3D&amp;v=1&amp;f=sd","is_gif":false,"transcoding_status":"completed"}},"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/","stickied":false,"url":"https://v.redd.it/e6ao7yjh5haf1","subreddit_subscribers":494001,"created_utc":1751468754,"num_crossposts":0,"mod_reports":[],"is_video":true}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n114pha","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"simracerman","can_mod_post":false,"send_replies":true,"parent_id":"t1_n10yzsv","score":2,"author_fullname":"t2_vbzgnic","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I don’t disagree. Requirements and budget are main factors, if you’re flexible on budget, don’t ever limit yourself. Otherwise, budget is the defining factor, especially for most of us hobbyists here.","edited":false,"author_flair_css_class":null,"name":"t1_n114pha","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don’t disagree. Requirements and budget are main factors, if you’re flexible on budget, don’t ever limit yourself. Otherwise, budget is the defining factor, especially for most of us hobbyists here.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lpy8nv","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/n114pha/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751498024,"author_flair_text":null,"collapsed":false,"created_utc":1751498024,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n10yzsv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Freonr2","can_mod_post":false,"send_replies":true,"parent_id":"t1_n10s7cz","score":5,"author_fullname":"t2_8xi6x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Its just to give you an idea what you get with a full fledged GPU and what you're leaving on the table.  Cost isn't always everything.\\n\\nThere are a bunch of other alternatives. CPU with 2/4/8/12 channel mem, multiple consumer level GPUs, and soforth. They all have trade offs. \\n\\nThe card is ~$8500.  It's probably more than 4x faster for 4x the price. Maybe about ~4x on tok/s but I dunno, 10x or 20x faster on prompt processing?\\n\\nThe info is probably useful to at least some people here.","edited":1751496366,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n10yzsv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Its just to give you an idea what you get with a full fledged GPU and what you&amp;#39;re leaving on the table.  Cost isn&amp;#39;t always everything.&lt;/p&gt;\\n\\n&lt;p&gt;There are a bunch of other alternatives. CPU with 2/4/8/12 channel mem, multiple consumer level GPUs, and soforth. They all have trade offs. &lt;/p&gt;\\n\\n&lt;p&gt;The card is ~$8500.  It&amp;#39;s probably more than 4x faster for 4x the price. Maybe about ~4x on tok/s but I dunno, 10x or 20x faster on prompt processing?&lt;/p&gt;\\n\\n&lt;p&gt;The info is probably useful to at least some people here.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpy8nv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/n10yzsv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751496150,"author_flair_text":null,"treatment_tags":[],"created_utc":1751496150,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n10s7cz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"simracerman","can_mod_post":false,"send_replies":true,"parent_id":"t1_n101y6i","score":4,"author_fullname":"t2_vbzgnic","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I like the “for reference, but man that’s an Apples to Cinderblocks type comparison.\\n\\nOne is  $2k full system, and the other is $10k for the card only. ","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n10s7cz","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I like the “for reference, but man that’s an Apples to Cinderblocks type comparison.&lt;/p&gt;\\n\\n&lt;p&gt;One is  $2k full system, and the other is $10k for the card only. &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpy8nv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/n10s7cz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751493923,"author_flair_text":null,"treatment_tags":[],"created_utc":1751493923,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n101y6i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Freonr2","can_mod_post":false,"created_utc":1751486142,"send_replies":true,"parent_id":"t1_n0zpvmi","score":4,"author_fullname":"t2_8xi6x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"For reference, RTX 6000 Pro Blackwell, L4 Scout Q5_K_M with fp16 kv cache in LM Studio which probably isn't the most optimized app:\\n\\nhttps://imgur.com/a/mV2MtYd\\n\\nI pasted in a large \\"codex\\" of every character from the game describing their appearance/outfits, along with a screenshot of Aerith and asked it to identify the character in the screenshot.  The codex is just under 3k tokens by itself, plus the image is whatever number of vision tokens.  It's a 4k image but I assume the preprocessor is resizing that down significantly (448x448? and with 16x16patch probably means 768 tokens for the image?)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n101y6i","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For reference, RTX 6000 Pro Blackwell, L4 Scout Q5_K_M with fp16 kv cache in LM Studio which probably isn&amp;#39;t the most optimized app:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://imgur.com/a/mV2MtYd\\"&gt;https://imgur.com/a/mV2MtYd&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;I pasted in a large &amp;quot;codex&amp;quot; of every character from the game describing their appearance/outfits, along with a screenshot of Aerith and asked it to identify the character in the screenshot.  The codex is just under 3k tokens by itself, plus the image is whatever number of vision tokens.  It&amp;#39;s a 4k image but I assume the preprocessor is resizing that down significantly (448x448? and with 16x16patch probably means 768 tokens for the image?)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpy8nv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/n101y6i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751486142,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n0zpvmi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Freonr2","can_mod_post":false,"created_utc":1751482497,"send_replies":true,"parent_id":"t3_1lpy8nv","score":8,"author_fullname":"t2_8xi6x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Definitely think MOE models are the right target for the 395 boxes, and glad we're seeing more come out, especially in the ~70-80B total and 10-20 active range, which seems nearly ideal for the hardware. 395 is going to struggle on inference speed for dense models that actually use all that VRAM, or even a significant portion of it.\\n\\nI hope AMD succeeds with the product and we see more of this type of thing in the future.","edited":1751482753,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0zpvmi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Definitely think MOE models are the right target for the 395 boxes, and glad we&amp;#39;re seeing more come out, especially in the ~70-80B total and 10-20 active range, which seems nearly ideal for the hardware. 395 is going to struggle on inference speed for dense models that actually use all that VRAM, or even a significant portion of it.&lt;/p&gt;\\n\\n&lt;p&gt;I hope AMD succeeds with the product and we see more of this type of thing in the future.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/n0zpvmi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751482497,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpy8nv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0zk6mz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jfowers_amd","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0z9fqg","score":2,"author_fullname":"t2_1m2ckixcqh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"No worries! I'm grateful you shared all of this data since a few other people have been asking about speeds on the post.\\n\\nIn terms of capabilities, my 2 cents is that we're still in very early days of matching applications with models and hardware, and in terms of optimizing the software and hardware. That's why I thought to make this post: to celebrate that a certain level capability is available on this particular HW/model/UI/server combo, even if it isn't for everyone (or anyone) yet. As MKBHD says... \\"this is the worst it will ever be.\\"","edited":false,"author_flair_css_class":null,"name":"t1_n0zk6mz","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No worries! I&amp;#39;m grateful you shared all of this data since a few other people have been asking about speeds on the post.&lt;/p&gt;\\n\\n&lt;p&gt;In terms of capabilities, my 2 cents is that we&amp;#39;re still in very early days of matching applications with models and hardware, and in terms of optimizing the software and hardware. That&amp;#39;s why I thought to make this post: to celebrate that a certain level capability is available on this particular HW/model/UI/server combo, even if it isn&amp;#39;t for everyone (or anyone) yet. As MKBHD says... &amp;quot;this is the worst it will ever be.&amp;quot;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lpy8nv","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/n0zk6mz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751480802,"author_flair_text":null,"collapsed":false,"created_utc":1751480802,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0z9fqg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"thomthehound","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0z7h7i","score":11,"author_fullname":"t2_vxbs7cf4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I apologize if the snark in my reply came off as offensive. I meant nothing by it aside from good-natured fun. I can understand these Strix Halo boxes not being for everyone, and I can certainly understand power-users finding them frustratingly slow. But, on the other hand, the massive amount of relatively fast RAM in a box that pulls only 120 W (sustained) at full-tilt, max speed, with desktop compute and productivity within only a few percent of a 9950X, AND the ability to run (albeit neither optimally nor especially poorly) essentially any consumer-level AI model locally... surely you can see how some people might also find that appealing.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0z9fqg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I apologize if the snark in my reply came off as offensive. I meant nothing by it aside from good-natured fun. I can understand these Strix Halo boxes not being for everyone, and I can certainly understand power-users finding them frustratingly slow. But, on the other hand, the massive amount of relatively fast RAM in a box that pulls only 120 W (sustained) at full-tilt, max speed, with desktop compute and productivity within only a few percent of a 9950X, AND the ability to run (albeit neither optimally nor especially poorly) essentially any consumer-level AI model locally... surely you can see how some people might also find that appealing.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpy8nv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/n0z9fqg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751477711,"author_flair_text":null,"treatment_tags":[],"created_utc":1751477711,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}}],"before":null}},"user_reports":[],"saved":false,"id":"n0z7h7i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"tomz17","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0z2de0","score":8,"author_fullname":"t2_1mhx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"So for comparison, a single 3090 + a 2-channel DDR4 3600 desktop system from 5 years ago on similar context (9K / 128K) can pretty much match that on that same model (Llama-4-Scout-17B-16E-Instruct-UD-Q4\\\\_K\\\\_XL).  This is kind of the worst case for a system like this, as most of the MOE layers are offloaded to CPU, and generation is entirely 100% limited by the (relatively) slow system ram.\\n\\n    prompt eval time =   63365.24 ms /  9000 tokens (    7.04 ms per token,   142.03 tokens per second)\\n           eval time =   28843.37 ms /   357 tokens (   80.79 ms per token,    12.38 tokens per second)\\n          total time =   92208.61 ms /  9357 tokens\\n\\nEither way these new unified memory systems are only really useful in a very niche set of cases, which is why it's maddening to see everyone recommend apple silicon &amp; AI max solutions to novices online.  Yeah, it's simple to get started, but it's also important to understand the real-world limits before putting down a lot of cash for them.   If you are using any appreciable amount of context (e.g. most tool use, most retrieval use, most code use, etc.), you will be waiting quite a while for that first token.  If you want to do any training/tuning, you are also completely out of luck.  \\n\\nFor anything interactive / iterative, I only use models that fit in VRAM of a GPU.  If I don't have a system large enough for that task, I just pay for API access.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0z7h7i","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So for comparison, a single 3090 + a 2-channel DDR4 3600 desktop system from 5 years ago on similar context (9K / 128K) can pretty much match that on that same model (Llama-4-Scout-17B-16E-Instruct-UD-Q4_K_XL).  This is kind of the worst case for a system like this, as most of the MOE layers are offloaded to CPU, and generation is entirely 100% limited by the (relatively) slow system ram.&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;prompt eval time =   63365.24 ms /  9000 tokens (    7.04 ms per token,   142.03 tokens per second)\\n       eval time =   28843.37 ms /   357 tokens (   80.79 ms per token,    12.38 tokens per second)\\n      total time =   92208.61 ms /  9357 tokens\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;Either way these new unified memory systems are only really useful in a very niche set of cases, which is why it&amp;#39;s maddening to see everyone recommend apple silicon &amp;amp; AI max solutions to novices online.  Yeah, it&amp;#39;s simple to get started, but it&amp;#39;s also important to understand the real-world limits before putting down a lot of cash for them.   If you are using any appreciable amount of context (e.g. most tool use, most retrieval use, most code use, etc.), you will be waiting quite a while for that first token.  If you want to do any training/tuning, you are also completely out of luck.  &lt;/p&gt;\\n\\n&lt;p&gt;For anything interactive / iterative, I only use models that fit in VRAM of a GPU.  If I don&amp;#39;t have a system large enough for that task, I just pay for API access.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpy8nv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/n0z7h7i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751477157,"author_flair_text":null,"treatment_tags":[],"created_utc":1751477157,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}}],"before":null}},"user_reports":[],"saved":false,"id":"n0z2de0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"thomthehound","can_mod_post":false,"created_utc":1751475742,"send_replies":true,"parent_id":"t1_n0ykx6l","score":11,"author_fullname":"t2_vxbs7cf4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Just in case anybody is interested, I just benched koboldcpp (Unsloth Llama-4-Scout-17B-16E-Instruct-UD-Q4\\\\_K\\\\_XL) against a context of 10240 (full FP16, flash attention nominally \\"on\\") tokens on my box (GMKtex Evo X-2). Prompt processing takes \\\\~90 seconds, \\\\~112 T/s. Offloading all of the layers gives a comfortable (to me at least) generation speed of \\\\~13.5 T/s. Dropping down to Q8 KV boosts that to \\\\~124 T/s pp and \\\\~14.2 T/s gen.\\n\\nI should probably point out that flash attention has a \\\\*large\\\\* negative impact for pp on this hardware. Prompt processing for FP16 rockets up to \\\\~138 T/s when it is off (and Q8 crashes, which I expected it would). On the other hand, gen speed drops to \\\\~10.5 T/s. So pick your poison.\\n\\nRegardless, I find these speeds to be acceptable. But I understand why other people might not. A 'big boy' GPU (or array of such GPUs) should be able to get 2-3x these speeds while simultaneously saving money on sauna visits. And a professional GPU would provide 8x, while also making walking easier due to carrying a much lighter wallet.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0z2de0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just in case anybody is interested, I just benched koboldcpp (Unsloth Llama-4-Scout-17B-16E-Instruct-UD-Q4_K_XL) against a context of 10240 (full FP16, flash attention nominally &amp;quot;on&amp;quot;) tokens on my box (GMKtex Evo X-2). Prompt processing takes ~90 seconds, ~112 T/s. Offloading all of the layers gives a comfortable (to me at least) generation speed of ~13.5 T/s. Dropping down to Q8 KV boosts that to ~124 T/s pp and ~14.2 T/s gen.&lt;/p&gt;\\n\\n&lt;p&gt;I should probably point out that flash attention has a *large* negative impact for pp on this hardware. Prompt processing for FP16 rockets up to ~138 T/s when it is off (and Q8 crashes, which I expected it would). On the other hand, gen speed drops to ~10.5 T/s. So pick your poison.&lt;/p&gt;\\n\\n&lt;p&gt;Regardless, I find these speeds to be acceptable. But I understand why other people might not. A &amp;#39;big boy&amp;#39; GPU (or array of such GPUs) should be able to get 2-3x these speeds while simultaneously saving money on sauna visits. And a professional GPU would provide 8x, while also making walking easier due to carrying a much lighter wallet.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpy8nv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/n0z2de0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751475742,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0zdxjx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"fallingdowndizzyvr","can_mod_post":false,"created_utc":1751478968,"send_replies":true,"parent_id":"t1_n0ykx6l","score":5,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; It is until you start putting actual context into the query. Then the initial wait (due to low prompt processing speeds) gets really onerous for most interactive workflows.\\n\\nI benched Scout Q3 with a 10,000 tk context earlier on my Max+ 395. If you want to get a feel for it.\\n\\nhttps://www.reddit.com/r/LocalLLaMA/comments/1le951x/gmk_x2amd_max_395_w128gb_first_impressions/","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0zdxjx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;It is until you start putting actual context into the query. Then the initial wait (due to low prompt processing speeds) gets really onerous for most interactive workflows.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I benched Scout Q3 with a 10,000 tk context earlier on my Max+ 395. If you want to get a feel for it.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1le951x/gmk_x2amd_max_395_w128gb_first_impressions/\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1le951x/gmk_x2amd_max_395_w128gb_first_impressions/&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpy8nv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/n0zdxjx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751478968,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ykx6l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"tomz17","can_mod_post":false,"created_utc":1751470815,"send_replies":true,"parent_id":"t3_1lpy8nv","score":23,"author_fullname":"t2_1mhx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt; I find 13 seconds usable for this model+usecase\\n\\nIt is until you start putting actual context into the query.  Then the initial wait (due to low prompt processing speeds) gets really onerous for most interactive workflows.  It's the same exact problem that apple silicon suffers from.  Adequate for messing around with LLM inferencing, but prohibitively annoying once you want to get actual stuff done (e.g. give the LLM a context to actually work on).\\n\\nI have an M1 Max 64gb, and very rarely use it for LLM inferencing, in favor of actual GPU's.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ykx6l","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;I find 13 seconds usable for this model+usecase&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;It is until you start putting actual context into the query.  Then the initial wait (due to low prompt processing speeds) gets really onerous for most interactive workflows.  It&amp;#39;s the same exact problem that apple silicon suffers from.  Adequate for messing around with LLM inferencing, but prohibitively annoying once you want to get actual stuff done (e.g. give the LLM a context to actually work on).&lt;/p&gt;\\n\\n&lt;p&gt;I have an M1 Max 64gb, and very rarely use it for LLM inferencing, in favor of actual GPU&amp;#39;s.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/n0ykx6l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751470815,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpy8nv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":23}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ylfnk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"jfowers_amd","can_mod_post":false,"created_utc":1751470956,"send_replies":true,"parent_id":"t1_n0yl8nl","score":6,"author_fullname":"t2_1m2ckixcqh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF:Q4\\\\_K\\\\_S","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ylfnk","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF:Q4_K_S&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpy8nv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/n0ylfnk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751470956,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n0yl8nl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"mr-claesson","can_mod_post":false,"created_utc":1751470903,"send_replies":true,"parent_id":"t3_1lpy8nv","score":6,"author_fullname":"t2_qlpdano","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Quant?  \\nContext size?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0yl8nl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Quant?&lt;br/&gt;\\nContext size?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/n0yl8nl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751470903,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpy8nv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n109gxg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MIXEDGREENS","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0z3gvq","score":0,"author_fullname":"t2_4tbel","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Just don't break its \\"nice\\" persona or it will literally go \\"eldritch horror\\" mode on you and start making threats.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n109gxg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just don&amp;#39;t break its &amp;quot;nice&amp;quot; persona or it will literally go &amp;quot;eldritch horror&amp;quot; mode on you and start making threats.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpy8nv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/n109gxg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751488338,"author_flair_text":null,"treatment_tags":[],"created_utc":1751488338,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n0z3gvq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"thomthehound","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0yyip4","score":1,"author_fullname":"t2_vxbs7cf4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thank you for that. I hadn't heard of it yet. I'm excited to check it out.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0z3gvq","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you for that. I hadn&amp;#39;t heard of it yet. I&amp;#39;m excited to check it out.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpy8nv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/n0z3gvq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751476048,"author_flair_text":null,"treatment_tags":[],"created_utc":1751476048,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0yyip4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"TaroOk7112","can_mod_post":false,"created_utc":1751474661,"send_replies":true,"parent_id":"t1_n0yf08z","score":7,"author_fullname":"t2_tsjh0dua","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Instead of using Llama 4, I would use dots.llm1. I tested it locally, but I had to use very low quants (UD-IQ2\\\\_M). Then I thought of 128GB Strix Halo with this quants:\\n\\nUD-Q4\\\\_K\\\\_XL: 86.8GB\\n\\nQ5\\\\_K\\\\_S: 101GB\\n\\n[https://huggingface.co/unsloth/dots.llm1.inst-GGUF](https://huggingface.co/unsloth/dots.llm1.inst-GGUF)","edited":1751475744,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0yyip4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Instead of using Llama 4, I would use dots.llm1. I tested it locally, but I had to use very low quants (UD-IQ2_M). Then I thought of 128GB Strix Halo with this quants:&lt;/p&gt;\\n\\n&lt;p&gt;UD-Q4_K_XL: 86.8GB&lt;/p&gt;\\n\\n&lt;p&gt;Q5_K_S: 101GB&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/unsloth/dots.llm1.inst-GGUF\\"&gt;https://huggingface.co/unsloth/dots.llm1.inst-GGUF&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpy8nv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/n0yyip4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751474661,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ze9ck","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"fallingdowndizzyvr","can_mod_post":false,"created_utc":1751479062,"send_replies":true,"parent_id":"t1_n0yf08z","score":6,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; So far, I am extremely pleased with the performance of my Strix Halo box\\n\\nMe as well. It's a keeper.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ze9ck","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;So far, I am extremely pleased with the performance of my Strix Halo box&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Me as well. It&amp;#39;s a keeper.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpy8nv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/n0ze9ck/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751479062,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0zd5ru","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Expensive-Apricot-25","can_mod_post":false,"created_utc":1751478753,"send_replies":true,"parent_id":"t1_n0yf08z","score":4,"author_fullname":"t2_idqkwio0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Llama 4 gets too much hate for what it is\\n\\nIt’s just as good as llama3.3 70b, but FAR faster, and it currently has the best vision out of ANY local models by a long shot, including Gemma.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0zd5ru","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Llama 4 gets too much hate for what it is&lt;/p&gt;\\n\\n&lt;p&gt;It’s just as good as llama3.3 70b, but FAR faster, and it currently has the best vision out of ANY local models by a long shot, including Gemma.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpy8nv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/n0zd5ru/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751478753,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n0yf08z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"thomthehound","can_mod_post":false,"created_utc":1751469165,"send_replies":true,"parent_id":"t3_1lpy8nv","score":17,"author_fullname":"t2_vxbs7cf4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"So far, I am extremely pleased with the performance of my Strix Halo box and, as you pointed out, it is perfect for running MoEs like Scout (it's just a shame that Llama 4 is kind of a disappointment).\\n\\nI take it you work for AMD. Is there any way you could get somebody to explain exactly *why* it is that the memory speed is locked at 8000 MT/s, even on boards that ship with 8533 MT/s chips?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0yf08z","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So far, I am extremely pleased with the performance of my Strix Halo box and, as you pointed out, it is perfect for running MoEs like Scout (it&amp;#39;s just a shame that Llama 4 is kind of a disappointment).&lt;/p&gt;\\n\\n&lt;p&gt;I take it you work for AMD. Is there any way you could get somebody to explain exactly &lt;em&gt;why&lt;/em&gt; it is that the memory speed is locked at 8000 MT/s, even on boards that ship with 8533 MT/s chips?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/n0yf08z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751469165,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpy8nv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":17}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0z2kr3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jfowers_amd","can_mod_post":false,"created_utc":1751475800,"send_replies":true,"parent_id":"t1_n0z21pj","score":3,"author_fullname":"t2_1m2ckixcqh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Definitely, that would rock!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0z2kr3","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Definitely, that would rock!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpy8nv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/n0z2kr3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751475800,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0z21pj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"zelkovamoon","can_mod_post":false,"created_utc":1751475650,"send_replies":true,"parent_id":"t3_1lpy8nv","score":4,"author_fullname":"t2_9yxfq","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Pretty decent tps on that. Glad to see AMD is doing stuff - I'll be honest though, you know what would make me *really* consider AMD?\\n\\nI have a DGX GB10 coming. It was a genuinely good idea on Nvidia and friends part to offer a Blackwell basically datacenter grade variant for AI workstation scale instead of rack scale - I couldn't justify an AI max setup given that at that price, you might as well spend a little more and get *way more* compute.\\n\\nIf you guys could offer a product that competes equally in terms of vram, tops, and FP4 support but maybe with a lower price; or more vram, that would be a real contender. If you toss a product together with double the vram and comparable compute, that would do gangbusters.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0z21pj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Pretty decent tps on that. Glad to see AMD is doing stuff - I&amp;#39;ll be honest though, you know what would make me &lt;em&gt;really&lt;/em&gt; consider AMD?&lt;/p&gt;\\n\\n&lt;p&gt;I have a DGX GB10 coming. It was a genuinely good idea on Nvidia and friends part to offer a Blackwell basically datacenter grade variant for AI workstation scale instead of rack scale - I couldn&amp;#39;t justify an AI max setup given that at that price, you might as well spend a little more and get &lt;em&gt;way more&lt;/em&gt; compute.&lt;/p&gt;\\n\\n&lt;p&gt;If you guys could offer a product that competes equally in terms of vram, tops, and FP4 support but maybe with a lower price; or more vram, that would be a real contender. If you toss a product together with double the vram and comparable compute, that would do gangbusters.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/n0z21pj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751475650,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpy8nv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0yg4dg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Agitated_Camel1886","can_mod_post":false,"created_utc":1751469480,"send_replies":true,"parent_id":"t3_1lpy8nv","score":3,"author_fullname":"t2_1jf10fah7i","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What's the model of the mini pc?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0yg4dg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What&amp;#39;s the model of the mini pc?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/n0yg4dg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751469480,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpy8nv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0zksh5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"jfowers_amd","can_mod_post":false,"created_utc":1751480984,"send_replies":true,"parent_id":"t1_n0yzl8i","score":4,"author_fullname":"t2_1m2ckixcqh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Would you believe that there are 8 computers turned on in my home office right now, but I don't have an eGPU on hand yet? If I had one I'd test it for you.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0zksh5","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Would you believe that there are 8 computers turned on in my home office right now, but I don&amp;#39;t have an eGPU on hand yet? If I had one I&amp;#39;d test it for you.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpy8nv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/n0zksh5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751480984,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n0yzl8i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"TaroOk7112","can_mod_post":false,"created_utc":1751474965,"send_replies":true,"parent_id":"t3_1lpy8nv","score":3,"author_fullname":"t2_tsjh0dua","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Could you connect an external GPU and test overall speed? I mean, PP speed must be improved considerably making this machine incredible. I know it's been asked several times, but I haven't seen any tests with Strix Halo + eGPUs.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0yzl8i","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Could you connect an external GPU and test overall speed? I mean, PP speed must be improved considerably making this machine incredible. I know it&amp;#39;s been asked several times, but I haven&amp;#39;t seen any tests with Strix Halo + eGPUs.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/n0yzl8i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751474965,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpy8nv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0z5p72","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jfowers_amd","can_mod_post":false,"created_utc":1751476662,"send_replies":true,"parent_id":"t1_n0yugv6","score":3,"author_fullname":"t2_1m2ckixcqh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"u/thomthehound did us a solid and measured on theirs! See their comment on this post.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0z5p72","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"/u/thomthehound\\"&gt;u/thomthehound&lt;/a&gt; did us a solid and measured on theirs! See their comment on this post.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpy8nv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/n0z5p72/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751476662,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0yugv6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Normal-Ad-7114","can_mod_post":false,"created_utc":1751473492,"send_replies":true,"parent_id":"t3_1lpy8nv","score":2,"author_fullname":"t2_8fu8sqhz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Can you measure how long would it take to process a moderately large prompt (let's say 5K tokens)?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0yugv6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Can you measure how long would it take to process a moderately large prompt (let&amp;#39;s say 5K tokens)?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/n0yugv6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751473492,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpy8nv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0zkley","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jfowers_amd","can_mod_post":false,"created_utc":1751480925,"send_replies":true,"parent_id":"t1_n0yulax","score":3,"author_fullname":"t2_1m2ckixcqh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Prompt processing for this image and short text prompt was about 13 seconds. Other users on the post are commenting with some great data of their own!\\n\\n\\n\\nFYI my data is with Vulkan, out-of-box, without any optimization effort on my part. ROCm may be better, and I am working on bringing that up next.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0zkley","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Prompt processing for this image and short text prompt was about 13 seconds. Other users on the post are commenting with some great data of their own!&lt;/p&gt;\\n\\n&lt;p&gt;FYI my data is with Vulkan, out-of-box, without any optimization effort on my part. ROCm may be better, and I am working on bringing that up next.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpy8nv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/n0zkley/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751480925,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0yulax","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Zyguard7777777","can_mod_post":false,"created_utc":1751473527,"send_replies":true,"parent_id":"t3_1lpy8nv","score":2,"author_fullname":"t2_zo1h5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Looks plenty fast enough for most use cases. Can you show some prompt processing speeds?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0yulax","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Looks plenty fast enough for most use cases. Can you show some prompt processing speeds?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/n0yulax/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751473527,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpy8nv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0z4fbo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Cool-Chemical-5629","can_mod_post":false,"created_utc":1751476312,"send_replies":true,"parent_id":"t3_1lpy8nv","score":2,"author_fullname":"t2_qz1qjc86","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This is cool and I always support advancements of AMD - I am all AMD currently. However honestly if I was to buy a new hardware for AI right now, I'd go with AMD CPU and Nvidia GPU.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0z4fbo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is cool and I always support advancements of AMD - I am all AMD currently. However honestly if I was to buy a new hardware for AI right now, I&amp;#39;d go with AMD CPU and Nvidia GPU.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/n0z4fbo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751476312,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpy8nv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n103d2h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Freonr2","can_mod_post":false,"created_utc":1751486558,"send_replies":true,"parent_id":"t1_n0zdcdb","score":4,"author_fullname":"t2_8xi6x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"GMktek and Framework desktop Ryzen AI 395+ 128GB are generally $1999, keep in mind that's ae complete system that include SSD, case, PSU, cooler--so everything but monitor/keyboard/mouse.  There were some deals here and there on the GMKtek for slightly less but unsure if they reliably available anymore.  GMKtek box (EVO-X2) is on amazon or their own website. Framework Desktop version with identical specs should be out \\"Q3\\".\\n\\nFramework desktop looks to be a bit nicer and better built than the GMKtek (IMO) if you don't mind waiting another month or three.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n103d2h","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;GMktek and Framework desktop Ryzen AI 395+ 128GB are generally $1999, keep in mind that&amp;#39;s ae complete system that include SSD, case, PSU, cooler--so everything but monitor/keyboard/mouse.  There were some deals here and there on the GMKtek for slightly less but unsure if they reliably available anymore.  GMKtek box (EVO-X2) is on amazon or their own website. Framework Desktop version with identical specs should be out &amp;quot;Q3&amp;quot;.&lt;/p&gt;\\n\\n&lt;p&gt;Framework desktop looks to be a bit nicer and better built than the GMKtek (IMO) if you don&amp;#39;t mind waiting another month or three.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpy8nv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/n103d2h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751486558,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0zl869","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jfowers_amd","can_mod_post":false,"created_utc":1751481117,"send_replies":true,"parent_id":"t1_n0zdcdb","score":2,"author_fullname":"t2_1m2ckixcqh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I can't comment specifically on cost, but you can look up the STX Halo mini PCs from popular OEMs and retailers. IMO they are nicely affordable.\\n\\nThere are some other commenters in this post sharing performance data - I will refer you to them for prompt processing speeds.\\n\\nThe generation speed in my video was about 17 tokens/s. Keep in mind this is out-of-box Vulkan performance on a brand new PC and I didn't put any effort towards optimization.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0zl869","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I can&amp;#39;t comment specifically on cost, but you can look up the STX Halo mini PCs from popular OEMs and retailers. IMO they are nicely affordable.&lt;/p&gt;\\n\\n&lt;p&gt;There are some other commenters in this post sharing performance data - I will refer you to them for prompt processing speeds.&lt;/p&gt;\\n\\n&lt;p&gt;The generation speed in my video was about 17 tokens/s. Keep in mind this is out-of-box Vulkan performance on a brand new PC and I didn&amp;#39;t put any effort towards optimization.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpy8nv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/n0zl869/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751481117,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0zdcdb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Expensive-Apricot-25","can_mod_post":false,"created_utc":1751478804,"send_replies":true,"parent_id":"t3_1lpy8nv","score":2,"author_fullname":"t2_idqkwio0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"How much would a rig like this cost?\\n\\nAlso when you start to fill up the context window, like 16k tokens in, what’s the prompt processing speed there?\\n\\nAlso what is the actual generation speed? I don’t think it was mentioned any where.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0zdcdb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How much would a rig like this cost?&lt;/p&gt;\\n\\n&lt;p&gt;Also when you start to fill up the context window, like 16k tokens in, what’s the prompt processing speed there?&lt;/p&gt;\\n\\n&lt;p&gt;Also what is the actual generation speed? I don’t think it was mentioned any where.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/n0zdcdb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751478804,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpy8nv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n11p8zs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"simracerman","can_mod_post":false,"created_utc":1751504715,"send_replies":true,"parent_id":"t1_n11ixym","score":1,"author_fullname":"t2_vbzgnic","approved_by":null,"mod_note":null,"all_awardings":[],"body":"You just made my consideration for a 395+ machine much higher!","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n11p8zs","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You just made my consideration for a 395+ machine much higher!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/n11p8zs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751504715,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpy8nv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":8,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n11ixym","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jfowers_amd","can_mod_post":false,"created_utc":1751502660,"send_replies":true,"parent_id":"t1_n10u4cz","score":2,"author_fullname":"t2_1m2ckixcqh","approved_by":null,"mod_note":null,"all_awardings":[],"body":"My team is building Lemonade to automate all of that! Right now the underlying inference engines are a little fragmented, so what we can do is:\\n\\n1. llamacpp can offload to GPU or CPU\\n\\n2. OGA can offload to NPU+GPU simultaneously, or to CPU\\n\\nEither way we'll get you up and running in minutes with fully automated installation, check out [https://lemonade-server.ai](https://lemonade-server.ai)","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n11ixym","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;My team is building Lemonade to automate all of that! Right now the underlying inference engines are a little fragmented, so what we can do is:&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;&lt;p&gt;llamacpp can offload to GPU or CPU&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;OGA can offload to NPU+GPU simultaneously, or to CPU&lt;/p&gt;&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;Either way we&amp;#39;ll get you up and running in minutes with fully automated installation, check out &lt;a href=\\"https://lemonade-server.ai\\"&gt;https://lemonade-server.ai&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lpy8nv","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/n11ixym/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751502660,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n10u4cz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"simracerman","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0zx54o","score":2,"author_fullname":"t2_vbzgnic","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I’m currently evaluating the purchase of a 395+ box and super interested in using all the 126TOPS this box can offer. Would be wonderful if there’s a guide to setup llama.cpp to offload layers to GPU —&gt; NPU —&gt; CPU, in that order.\\n\\nThis would make a killer machine!","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n10u4cz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I’m currently evaluating the purchase of a 395+ box and super interested in using all the 126TOPS this box can offer. Would be wonderful if there’s a guide to setup llama.cpp to offload layers to GPU —&amp;gt; NPU —&amp;gt; CPU, in that order.&lt;/p&gt;\\n\\n&lt;p&gt;This would make a killer machine!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lpy8nv","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/n10u4cz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751494550,"author_flair_text":null,"treatment_tags":[],"created_utc":1751494550,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0zx54o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jfowers_amd","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0zw75m","score":3,"author_fullname":"t2_1m2ckixcqh","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Gotcha. The GPU on your Max+ 395 is relatively strong, so it doesn't necessarily need help from the NPU.\\n\\nFYI, the NPU is the same size (50 TOPS compute) on the entire Ryzen AI 300-series lineup, but the GPU size changes significantly from the 350 to 395.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0zx54o","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Gotcha. The GPU on your Max+ 395 is relatively strong, so it doesn&amp;#39;t necessarily need help from the NPU.&lt;/p&gt;\\n\\n&lt;p&gt;FYI, the NPU is the same size (50 TOPS compute) on the entire Ryzen AI 300-series lineup, but the GPU size changes significantly from the 350 to 395.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lpy8nv","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/n0zx54o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751484689,"author_flair_text":null,"treatment_tags":[],"created_utc":1751484689,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0zw75m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0zv6ep","score":2,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm running it on a GMK X2, so a Max+ 395.","edited":false,"author_flair_css_class":null,"name":"t1_n0zw75m","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m running it on a GMK X2, so a Max+ 395.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lpy8nv","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/n0zw75m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751484400,"author_flair_text":null,"collapsed":false,"created_utc":1751484400,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0zv6ep","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jfowers_amd","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ztqxx","score":2,"author_fullname":"t2_1m2ckixcqh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"\\\\&gt; But offhand it doesn't seem to be as fast as llama.cpp.\\n\\n  \\nMay I ask what hardware you're on? Hybrid (OGA+NPU+GPU) has its biggest advantage over llamacpp+GPU-only on systems where the GPU has less compute.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0zv6ep","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&amp;gt; But offhand it doesn&amp;#39;t seem to be as fast as llama.cpp.&lt;/p&gt;\\n\\n&lt;p&gt;May I ask what hardware you&amp;#39;re on? Hybrid (OGA+NPU+GPU) has its biggest advantage over llamacpp+GPU-only on systems where the GPU has less compute.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpy8nv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/n0zv6ep/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751484090,"author_flair_text":null,"treatment_tags":[],"created_utc":1751484090,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ztqxx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0zlqv7","score":3,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; For NPU+GPU mode, right now you would need to use the OGA inference engine, which is also supported by Lemonade.\\n\\nThat's what I've been trying. But offhand it doesn't seem to be as fast as llama.cpp. That's why I'd like to see stats.\\n\\n&gt; Lemonade has a stats endpoint that you can use to query performance information about the last completions request: \`curl http://localhost:8000/api/v1/stats\`\\n\\nNice. Thanks.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0ztqxx","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;For NPU+GPU mode, right now you would need to use the OGA inference engine, which is also supported by Lemonade.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;That&amp;#39;s what I&amp;#39;ve been trying. But offhand it doesn&amp;#39;t seem to be as fast as llama.cpp. That&amp;#39;s why I&amp;#39;d like to see stats.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Lemonade has a stats endpoint that you can use to query performance information about the last completions request: &lt;code&gt;curl http://localhost:8000/api/v1/stats&lt;/code&gt;&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Nice. Thanks.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpy8nv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/n0ztqxx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751483659,"author_flair_text":null,"treatment_tags":[],"created_utc":1751483659,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0zlqv7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jfowers_amd","can_mod_post":false,"created_utc":1751481272,"send_replies":true,"parent_id":"t1_n0zegcu","score":3,"author_fullname":"t2_1m2ckixcqh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This video uses llama.cpp's official Vulkan binaries as the inference engine, no optimization on my part.\\n\\nFor NPU+GPU mode, right now you would need to use the OGA inference engine, which is also supported by Lemonade.\\n\\n\\\\&gt; Also, is it possible for lemonade to print out stats like tokens per second?\\n\\nLemonade has a stats endpoint that you can use to query performance information about the last completions request: \\\\\`curl [http://localhost:8000/api/v1/stats\\\\\`](http://localhost:8000/api/v1/stats\`)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0zlqv7","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This video uses llama.cpp&amp;#39;s official Vulkan binaries as the inference engine, no optimization on my part.&lt;/p&gt;\\n\\n&lt;p&gt;For NPU+GPU mode, right now you would need to use the OGA inference engine, which is also supported by Lemonade.&lt;/p&gt;\\n\\n&lt;p&gt;&amp;gt; Also, is it possible for lemonade to print out stats like tokens per second?&lt;/p&gt;\\n\\n&lt;p&gt;Lemonade has a stats endpoint that you can use to query performance information about the last completions request: \`curl &lt;a href=\\"http://localhost:8000/api/v1/stats%60\\"&gt;http://localhost:8000/api/v1/stats\`&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpy8nv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/n0zlqv7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751481272,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0zegcu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"created_utc":1751479117,"send_replies":true,"parent_id":"t3_1lpy8nv","score":2,"author_fullname":"t2_o65i6kx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt; Inference server: Lemonade, our AMD-first OpenAI compatible server, running llama.cpp+Vulkan in the backend on the APU's Radeon 8060S GPU\\n\\nIs it just using llama.cpp dead stock. Or is this a version you guys made that supports hybrid NPU+GPU mode?\\n\\nAlso, is it possible for lemonade to print out stats like tokens per second?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0zegcu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Inference server: Lemonade, our AMD-first OpenAI compatible server, running llama.cpp+Vulkan in the backend on the APU&amp;#39;s Radeon 8060S GPU&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Is it just using llama.cpp dead stock. Or is this a version you guys made that supports hybrid NPU+GPU mode?&lt;/p&gt;\\n\\n&lt;p&gt;Also, is it possible for lemonade to print out stats like tokens per second?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/n0zegcu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751479117,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpy8nv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n112fmw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"synn89","can_mod_post":false,"created_utc":1751497280,"send_replies":true,"parent_id":"t3_1lpy8nv","score":1,"author_fullname":"t2_3jm4t","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This is pretty interesting. So a used Ebay Mac M1 Ultra 128GB is around $2700-3k. On that device, using the prompt from https://docs.google.com/document/d/1qPad75t_4ex99tbHsHTGhAH7i5JGUDPc-TKRfoiKFJI/edit?tab=t.0\\n\\nLlama-4-Scout-17B-16E-Instruct-Q4_K_S.gguf with a recent git pull of text-generation-webui:\\n\\n    prompt processing progress, n_past = 7323, n_tokens = 150, progress = 0.999317\\n    prompt eval time =   31764.47 ms /  7318 tokens (    4.34 ms per token,   230.38 tokens per second)\\n           eval time =   19447.79 ms /   342 tokens (   56.86 ms per token,    17.59 tokens per second)\\n          total time =   51212.26 ms /  7660 tokens\\n    18:52:50-790912 INFO     Output generated in 51.24 seconds (6.67 tokens/s, 342 tokens, context 7323, seed 1221920105)    \\n\\nSo the Ryzen 395 is slower, but not by much and costs $1k less, new. The Mac will get better, likely, with MLX but that was acting up for me today(MLX in general can be finicky and GGUF is just a lot easier). I also wonder once you got Linux onto a Ryzen and more of them get out into the wild, if we see some optimization for the hardware.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n112fmw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is pretty interesting. So a used Ebay Mac M1 Ultra 128GB is around $2700-3k. On that device, using the prompt from &lt;a href=\\"https://docs.google.com/document/d/1qPad75t_4ex99tbHsHTGhAH7i5JGUDPc-TKRfoiKFJI/edit?tab=t.0\\"&gt;https://docs.google.com/document/d/1qPad75t_4ex99tbHsHTGhAH7i5JGUDPc-TKRfoiKFJI/edit?tab=t.0&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Llama-4-Scout-17B-16E-Instruct-Q4_K_S.gguf with a recent git pull of text-generation-webui:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;prompt processing progress, n_past = 7323, n_tokens = 150, progress = 0.999317\\nprompt eval time =   31764.47 ms /  7318 tokens (    4.34 ms per token,   230.38 tokens per second)\\n       eval time =   19447.79 ms /   342 tokens (   56.86 ms per token,    17.59 tokens per second)\\n      total time =   51212.26 ms /  7660 tokens\\n18:52:50-790912 INFO     Output generated in 51.24 seconds (6.67 tokens/s, 342 tokens, context 7323, seed 1221920105)    \\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;So the Ryzen 395 is slower, but not by much and costs $1k less, new. The Mac will get better, likely, with MLX but that was acting up for me today(MLX in general can be finicky and GGUF is just a lot easier). I also wonder once you got Linux onto a Ryzen and more of them get out into the wild, if we see some optimization for the hardware.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/n112fmw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751497280,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpy8nv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(l,{data:t});export{r as default};
