import{j as e}from"./index-CqAPCjw5.js";import{R as l}from"./RedditPostRenderer-4oBDAtGr.js";import"./index-D3Sdy_Op.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I want to eventually build my own pc and host locally, mostly for the sake of reliability and not being reliant on the big guys in the bizz.\\n\\nMy main issue is that models such as Sonnet and Opus 4, even sonnet 3.5 performs so much better when it comes to coding, than what I've seen any locally run models being capable of. Not talking about open-source, as the new kimi model has shown a lot of promise, but it is too big to run locally.\\n\\n\\nBut I am curious if it is possible to have specialized models which run locally, but perform equally to the big dogs.\\n\\nFor instance, if I train one local model to be my Python specialist, another for Flutter etc. Then I simply use the model I need, depending on the project.\\n\\nIs such a thing possible, to train local models like this and have them perform equally to the great Sonnet and Opus models, for programming purposes? Has anyone tried something similar already?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Is it possible to have a specialized local llm perform at the level of cloud based models?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m5fwpz","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.38,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_7kei19es","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753095737,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I want to eventually build my own pc and host locally, mostly for the sake of reliability and not being reliant on the big guys in the bizz.&lt;/p&gt;\\n\\n&lt;p&gt;My main issue is that models such as Sonnet and Opus 4, even sonnet 3.5 performs so much better when it comes to coding, than what I&amp;#39;ve seen any locally run models being capable of. Not talking about open-source, as the new kimi model has shown a lot of promise, but it is too big to run locally.&lt;/p&gt;\\n\\n&lt;p&gt;But I am curious if it is possible to have specialized models which run locally, but perform equally to the big dogs.&lt;/p&gt;\\n\\n&lt;p&gt;For instance, if I train one local model to be my Python specialist, another for Flutter etc. Then I simply use the model I need, depending on the project.&lt;/p&gt;\\n\\n&lt;p&gt;Is such a thing possible, to train local models like this and have them perform equally to the great Sonnet and Opus models, for programming purposes? Has anyone tried something similar already?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m5fwpz","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Relative_Mouse7680","discussion_type":null,"num_comments":13,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m5fwpz/is_it_possible_to_have_a_specialized_local_llm/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m5fwpz/is_it_possible_to_have_a_specialized_local_llm/","subreddit_subscribers":502516,"created_utc":1753095737,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4bskns","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Zealousideal-Bug1837","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4boa0m","score":2,"author_fullname":"t2_d18mespd","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Determine the correct category from the available categories to place this item in:\\n\\nScore this comment as positive or negative:\\n\\netc etc. You can get near magical results from small fine tuned models for very very specific tasks.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4bskns","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Determine the correct category from the available categories to place this item in:&lt;/p&gt;\\n\\n&lt;p&gt;Score this comment as positive or negative:&lt;/p&gt;\\n\\n&lt;p&gt;etc etc. You can get near magical results from small fine tuned models for very very specific tasks.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5fwpz","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5fwpz/is_it_possible_to_have_a_specialized_local_llm/n4bskns/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753099641,"author_flair_text":null,"treatment_tags":[],"created_utc":1753099641,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4boa0m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Relative_Mouse7680","can_mod_post":false,"created_utc":1753097875,"send_replies":true,"parent_id":"t1_n4bkw8q","score":1,"author_fullname":"t2_7kei19es","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I understand, so Python is too big and ambitious. Thanks for clarifying. How specific does it have to be then?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4boa0m","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I understand, so Python is too big and ambitious. Thanks for clarifying. How specific does it have to be then?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5fwpz","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5fwpz/is_it_possible_to_have_a_specialized_local_llm/n4boa0m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753097875,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4bkw8q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Zealousideal-Bug1837","can_mod_post":false,"created_utc":1753096357,"send_replies":true,"parent_id":"t3_1m5fwpz","score":3,"author_fullname":"t2_d18mespd","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"for very specific tasks, yes, potentially. 'python' is not a specific task however.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4bkw8q","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;for very specific tasks, yes, potentially. &amp;#39;python&amp;#39; is not a specific task however.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5fwpz/is_it_possible_to_have_a_specialized_local_llm/n4bkw8q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753096357,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5fwpz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4bp2iz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Relative_Mouse7680","can_mod_post":false,"created_utc":1753098217,"send_replies":true,"parent_id":"t1_n4blgvk","score":1,"author_fullname":"t2_7kei19es","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Do you know if anyone has done something similar with Deepseek already?\\n\\nPrivacy is only partially the reason, independence and reliability are the main reasons. Sonnet 4 has a reasonable price, but they have had a lot of issues with down time the past year. Which is okey for now, I'm mostly thinking long term.\\n\\nCloud solution with open source models is actually a good option for now, as they allow access to bigger models than I could ever afford myself. But I've not managed to find a cloud platform which is reliable when it comes to privacy. Do you have any which you could recommend?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4bp2iz","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Do you know if anyone has done something similar with Deepseek already?&lt;/p&gt;\\n\\n&lt;p&gt;Privacy is only partially the reason, independence and reliability are the main reasons. Sonnet 4 has a reasonable price, but they have had a lot of issues with down time the past year. Which is okey for now, I&amp;#39;m mostly thinking long term.&lt;/p&gt;\\n\\n&lt;p&gt;Cloud solution with open source models is actually a good option for now, as they allow access to bigger models than I could ever afford myself. But I&amp;#39;ve not managed to find a cloud platform which is reliable when it comes to privacy. Do you have any which you could recommend?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5fwpz","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5fwpz/is_it_possible_to_have_a_specialized_local_llm/n4bp2iz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753098217,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4blgvk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mobileJay77","can_mod_post":false,"created_utc":1753096619,"send_replies":true,"parent_id":"t3_1m5fwpz","score":3,"author_fullname":"t2_q9ojhw3l","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Theoretically yes. It won't be easy and it won't be cheap. \\n\\nBut if you have the hardware to train, you can  run a large open source model like Deepseek.\\n\\nWhat is your motivation? \\n\\nPrivacy, sensitive data- go local and compromise quality vs cost. \\n\\nPrice and independence? Use any of the cloud model providers, they are dirt cheap.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4blgvk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Theoretically yes. It won&amp;#39;t be easy and it won&amp;#39;t be cheap. &lt;/p&gt;\\n\\n&lt;p&gt;But if you have the hardware to train, you can  run a large open source model like Deepseek.&lt;/p&gt;\\n\\n&lt;p&gt;What is your motivation? &lt;/p&gt;\\n\\n&lt;p&gt;Privacy, sensitive data- go local and compromise quality vs cost. &lt;/p&gt;\\n\\n&lt;p&gt;Price and independence? Use any of the cloud model providers, they are dirt cheap.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5fwpz/is_it_possible_to_have_a_specialized_local_llm/n4blgvk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753096619,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5fwpz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4bpcl0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Relative_Mouse7680","can_mod_post":false,"created_utc":1753098335,"send_replies":true,"parent_id":"t1_n4bm00o","score":1,"author_fullname":"t2_7kei19es","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Interesting. Have you trained the model for your specific task?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4bpcl0","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Interesting. Have you trained the model for your specific task?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5fwpz","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5fwpz/is_it_possible_to_have_a_specialized_local_llm/n4bpcl0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753098335,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4bm00o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"dheetoo","can_mod_post":false,"created_utc":1753096859,"send_replies":true,"parent_id":"t3_1m5fwpz","score":2,"author_fullname":"t2_c5n1x183x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Devstral is pretty good for my use case (debugging, test driven where you write a test and let llm figure out the implement)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4bm00o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Devstral is pretty good for my use case (debugging, test driven where you write a test and let llm figure out the implement)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5fwpz/is_it_possible_to_have_a_specialized_local_llm/n4bm00o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753096859,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5fwpz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4bm38s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"dheetoo","can_mod_post":false,"created_utc":1753096899,"send_replies":true,"parent_id":"t3_1m5fwpz","score":2,"author_fullname":"t2_c5n1x183x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"qwen 2.5 coder maybe ?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4bm38s","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;qwen 2.5 coder maybe ?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5fwpz/is_it_possible_to_have_a_specialized_local_llm/n4bm38s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753096899,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5fwpz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4bmi1p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"johnkapolos","can_mod_post":false,"created_utc":1753097083,"send_replies":true,"parent_id":"t3_1m5fwpz","score":2,"author_fullname":"t2_te4dl","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"No is the short answer.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4bmi1p","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No is the short answer.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5fwpz/is_it_possible_to_have_a_specialized_local_llm/n4bmi1p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753097083,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5fwpz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4bpahb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Relative_Mouse7680","can_mod_post":false,"created_utc":1753098310,"send_replies":true,"parent_id":"t1_n4bluco","score":1,"author_fullname":"t2_7kei19es","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks for being honest and straightforward. After reading yours and the other replies, I realize now that what I was after was probably too big and ambitious on a limited budget.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4bpahb","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for being honest and straightforward. After reading yours and the other replies, I realize now that what I was after was probably too big and ambitious on a limited budget.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5fwpz","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5fwpz/is_it_possible_to_have_a_specialized_local_llm/n4bpahb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753098310,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4bluco","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CBW1255","can_mod_post":false,"created_utc":1753096788,"send_replies":true,"parent_id":"t3_1m5fwpz","score":1,"author_fullname":"t2_uprpjkzls","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I truly hate to say this but no, not in the foreseeable future for what you want to accomplish.\\n\\nI know from first hand experience that it is hard to accept but it doesn’t make it less true.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4bluco","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I truly hate to say this but no, not in the foreseeable future for what you want to accomplish.&lt;/p&gt;\\n\\n&lt;p&gt;I know from first hand experience that it is hard to accept but it doesn’t make it less true.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5fwpz/is_it_possible_to_have_a_specialized_local_llm/n4bluco/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753096788,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5fwpz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),s=()=>e.jsx(l,{data:a});export{s as default};
