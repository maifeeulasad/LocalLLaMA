import{j as e}from"./index-Bu7qcPAU.js";import{R as t}from"./RedditPostRenderer-CbHA7O5q.js";import"./index-BKgbfxhf.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Lately I am trying to setup a home-assistant like system (will be interfaced with STT/TTS). I was hoping a small model like Qwen3 4B@Q4 will be sufficient for some contextual understanding which allows it to provide advices when the question is not \\"straight-forward\\". However, it seems this is not working by default.\\n\\nFor example, I provided the model with a simple prompt and a set of test data, to make it know it should report weather.\\n\\n&gt;You will now act as an agent for home assistant like Alexa or Siri. As your response will be turned into speech by another TTS model, you keep your response concise. When you are asked about weather information, you will use the pre-fetched weather forecast to answer questions. The below is a test.\\n\\n&gt;Weather information:\\n\\n&gt;{ \\"location\\": \\"Tokyo, Japan\\", \\"units\\": { \\"temperature\\": \\"°C\\", \\"wind\\\\_speed\\": \\"km/h\\" }, \\"forecast\\": \\\\[ { \\"date\\": \\"2025-07-08\\", \\"weekday\\": \\"Tuesday\\", \\"condition\\": \\"Hazy Sun\\", \\"high\\": 36, \\"low\\": 26, \\"precipitation\\": \\"0%\\", \\"wind\\": \\"Light breeze\\", \\"advisory\\": \\"Very hot; limit outdoor activities\\" }, { \\"date\\": \\"2025-07-09\\", \\"weekday\\": \\"Wednesday\\", \\"condition\\": \\"Hazy Sun, Breezy\\", \\"high\\": 36, \\"low\\": 26, \\"precipitation\\": \\"10%\\", \\"wind\\": \\"Breezy PM\\", \\"advisory\\": \\"Heat stress risk; caution advised\\" }, { \\"date\\": \\"2025-07-10\\", \\"weekday\\": \\"Thursday\\", \\"condition\\": \\"Afternoon Thunderstorms\\", \\"high\\": 34, \\"low\\": 22, \\"precipitation\\": \\"60%\\", \\"wind\\": \\"Moderate\\", \\"advisory\\": \\"Rain and thunderstorms expected; stay indoors if possible\\" }, { \\"date\\": \\"2025-07-11\\", \\"weekday\\": \\"Friday\\", \\"condition\\": \\"Cloudy, Cooler\\", \\"high\\": 28, \\"low\\": 21, \\"precipitation\\": \\"20%\\", \\"wind\\": \\"Light\\", \\"advisory\\": \\"Much more comfortable; good for outdoor plans\\" }, { \\"date\\": \\"2025-07-12\\", \\"weekday\\": \\"Saturday\\", \\"condition\\": \\"Partly Cloudy\\", \\"high\\": 30, \\"low\\": 22, \\"precipitation\\": \\"10%\\", \\"wind\\": \\"Light\\", \\"advisory\\": \\"Mild and pleasant\\" }, { \\"date\\": \\"2025-07-13\\", \\"weekday\\": \\"Sunday\\", \\"condition\\": \\"Mostly Cloudy, Light Rain\\", \\"high\\": 31, \\"low\\": 24, \\"precipitation\\": \\"40%\\", \\"wind\\": \\"Light\\", \\"advisory\\": \\"Carry umbrella; sticky humidity\\" }, { \\"date\\": \\"2025-07-14\\", \\"weekday\\": \\"Monday\\", \\"condition\\": \\"AM Thunderstorms, Cloudy PM\\", \\"high\\": 31, \\"low\\": 25, \\"precipitation\\": \\"50%\\", \\"wind\\": \\"Moderate\\", \\"advisory\\": \\"Rain in the morning; watch for thunder\\" } \\\\], \\"alerts\\": \\\\[ { \\"type\\": \\"Thunderstorm Advisory\\", \\"region\\": \\"Wako-shi (Tokyo area)\\", \\"start\\": \\"2025-07-08T12:00:00+09:00\\", \\"end\\": \\"2025-07-08T21:00:00+09:00\\", \\"source\\": \\"Japan Meteorological Agency\\" } \\\\] }\\n\\nThen, when the LLM is asked \\"on which day should I bring an umbrella\\", despite July-10th has a thunderstorm and a 60% chance of precipitation, it often miss that out, while still being able to point out I should bring one for July-13th. I tested Qwen 30B-A3, it does not suffer from the same, and usually is aware of July-10th being a day I should bring my umbrella.\\n\\nGoing back to Qwen3 4B, In one conversation it even said:\\n\\n    Agent: No, you don't need an umbrella on July 10th. You'll need it only on July 13th and 14th.\\n    \\n    Me: why\\n    \\n    Agent: Because July 10th has thunderstorms, not rain. You’ll need an umbrella only on July 13th and 14th when there’s light rain.\\n\\nI am guessing I can fine-tune the model to \\"fix\\" this? Any advice are appreciated.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"How to get small models (&lt;= 4B) to have better \\"common sense\\" for use with daily conversations?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m31p47","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.5,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_10icmj","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1752866729,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752843628,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Lately I am trying to setup a home-assistant like system (will be interfaced with STT/TTS). I was hoping a small model like Qwen3 4B@Q4 will be sufficient for some contextual understanding which allows it to provide advices when the question is not &amp;quot;straight-forward&amp;quot;. However, it seems this is not working by default.&lt;/p&gt;\\n\\n&lt;p&gt;For example, I provided the model with a simple prompt and a set of test data, to make it know it should report weather.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;You will now act as an agent for home assistant like Alexa or Siri. As your response will be turned into speech by another TTS model, you keep your response concise. When you are asked about weather information, you will use the pre-fetched weather forecast to answer questions. The below is a test.&lt;/p&gt;\\n\\n&lt;p&gt;Weather information:&lt;/p&gt;\\n\\n&lt;p&gt;{ &amp;quot;location&amp;quot;: &amp;quot;Tokyo, Japan&amp;quot;, &amp;quot;units&amp;quot;: { &amp;quot;temperature&amp;quot;: &amp;quot;°C&amp;quot;, &amp;quot;wind_speed&amp;quot;: &amp;quot;km/h&amp;quot; }, &amp;quot;forecast&amp;quot;: [ { &amp;quot;date&amp;quot;: &amp;quot;2025-07-08&amp;quot;, &amp;quot;weekday&amp;quot;: &amp;quot;Tuesday&amp;quot;, &amp;quot;condition&amp;quot;: &amp;quot;Hazy Sun&amp;quot;, &amp;quot;high&amp;quot;: 36, &amp;quot;low&amp;quot;: 26, &amp;quot;precipitation&amp;quot;: &amp;quot;0%&amp;quot;, &amp;quot;wind&amp;quot;: &amp;quot;Light breeze&amp;quot;, &amp;quot;advisory&amp;quot;: &amp;quot;Very hot; limit outdoor activities&amp;quot; }, { &amp;quot;date&amp;quot;: &amp;quot;2025-07-09&amp;quot;, &amp;quot;weekday&amp;quot;: &amp;quot;Wednesday&amp;quot;, &amp;quot;condition&amp;quot;: &amp;quot;Hazy Sun, Breezy&amp;quot;, &amp;quot;high&amp;quot;: 36, &amp;quot;low&amp;quot;: 26, &amp;quot;precipitation&amp;quot;: &amp;quot;10%&amp;quot;, &amp;quot;wind&amp;quot;: &amp;quot;Breezy PM&amp;quot;, &amp;quot;advisory&amp;quot;: &amp;quot;Heat stress risk; caution advised&amp;quot; }, { &amp;quot;date&amp;quot;: &amp;quot;2025-07-10&amp;quot;, &amp;quot;weekday&amp;quot;: &amp;quot;Thursday&amp;quot;, &amp;quot;condition&amp;quot;: &amp;quot;Afternoon Thunderstorms&amp;quot;, &amp;quot;high&amp;quot;: 34, &amp;quot;low&amp;quot;: 22, &amp;quot;precipitation&amp;quot;: &amp;quot;60%&amp;quot;, &amp;quot;wind&amp;quot;: &amp;quot;Moderate&amp;quot;, &amp;quot;advisory&amp;quot;: &amp;quot;Rain and thunderstorms expected; stay indoors if possible&amp;quot; }, { &amp;quot;date&amp;quot;: &amp;quot;2025-07-11&amp;quot;, &amp;quot;weekday&amp;quot;: &amp;quot;Friday&amp;quot;, &amp;quot;condition&amp;quot;: &amp;quot;Cloudy, Cooler&amp;quot;, &amp;quot;high&amp;quot;: 28, &amp;quot;low&amp;quot;: 21, &amp;quot;precipitation&amp;quot;: &amp;quot;20%&amp;quot;, &amp;quot;wind&amp;quot;: &amp;quot;Light&amp;quot;, &amp;quot;advisory&amp;quot;: &amp;quot;Much more comfortable; good for outdoor plans&amp;quot; }, { &amp;quot;date&amp;quot;: &amp;quot;2025-07-12&amp;quot;, &amp;quot;weekday&amp;quot;: &amp;quot;Saturday&amp;quot;, &amp;quot;condition&amp;quot;: &amp;quot;Partly Cloudy&amp;quot;, &amp;quot;high&amp;quot;: 30, &amp;quot;low&amp;quot;: 22, &amp;quot;precipitation&amp;quot;: &amp;quot;10%&amp;quot;, &amp;quot;wind&amp;quot;: &amp;quot;Light&amp;quot;, &amp;quot;advisory&amp;quot;: &amp;quot;Mild and pleasant&amp;quot; }, { &amp;quot;date&amp;quot;: &amp;quot;2025-07-13&amp;quot;, &amp;quot;weekday&amp;quot;: &amp;quot;Sunday&amp;quot;, &amp;quot;condition&amp;quot;: &amp;quot;Mostly Cloudy, Light Rain&amp;quot;, &amp;quot;high&amp;quot;: 31, &amp;quot;low&amp;quot;: 24, &amp;quot;precipitation&amp;quot;: &amp;quot;40%&amp;quot;, &amp;quot;wind&amp;quot;: &amp;quot;Light&amp;quot;, &amp;quot;advisory&amp;quot;: &amp;quot;Carry umbrella; sticky humidity&amp;quot; }, { &amp;quot;date&amp;quot;: &amp;quot;2025-07-14&amp;quot;, &amp;quot;weekday&amp;quot;: &amp;quot;Monday&amp;quot;, &amp;quot;condition&amp;quot;: &amp;quot;AM Thunderstorms, Cloudy PM&amp;quot;, &amp;quot;high&amp;quot;: 31, &amp;quot;low&amp;quot;: 25, &amp;quot;precipitation&amp;quot;: &amp;quot;50%&amp;quot;, &amp;quot;wind&amp;quot;: &amp;quot;Moderate&amp;quot;, &amp;quot;advisory&amp;quot;: &amp;quot;Rain in the morning; watch for thunder&amp;quot; } ], &amp;quot;alerts&amp;quot;: [ { &amp;quot;type&amp;quot;: &amp;quot;Thunderstorm Advisory&amp;quot;, &amp;quot;region&amp;quot;: &amp;quot;Wako-shi (Tokyo area)&amp;quot;, &amp;quot;start&amp;quot;: &amp;quot;2025-07-08T12:00:00+09:00&amp;quot;, &amp;quot;end&amp;quot;: &amp;quot;2025-07-08T21:00:00+09:00&amp;quot;, &amp;quot;source&amp;quot;: &amp;quot;Japan Meteorological Agency&amp;quot; } ] }&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Then, when the LLM is asked &amp;quot;on which day should I bring an umbrella&amp;quot;, despite July-10th has a thunderstorm and a 60% chance of precipitation, it often miss that out, while still being able to point out I should bring one for July-13th. I tested Qwen 30B-A3, it does not suffer from the same, and usually is aware of July-10th being a day I should bring my umbrella.&lt;/p&gt;\\n\\n&lt;p&gt;Going back to Qwen3 4B, In one conversation it even said:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;Agent: No, you don&amp;#39;t need an umbrella on July 10th. You&amp;#39;ll need it only on July 13th and 14th.\\n\\nMe: why\\n\\nAgent: Because July 10th has thunderstorms, not rain. You’ll need an umbrella only on July 13th and 14th when there’s light rain.\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;I am guessing I can fine-tune the model to &amp;quot;fix&amp;quot; this? Any advice are appreciated.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m31p47","is_robot_indexable":true,"num_duplicates":1,"report_reasons":null,"author":"SandboChang","discussion_type":null,"num_comments":23,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m31p47/how_to_get_small_models_4b_to_have_better_common/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m31p47/how_to_get_small_models_4b_to_have_better_common/","subreddit_subscribers":501232,"created_utc":1752843628,"num_crossposts":1,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3u9ujk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SandboChang","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3u8g11","score":1,"author_fullname":"t2_10icmj","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks, it’s interesting to know and I will explore that more","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3u9ujk","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks, it’s interesting to know and I will explore that more&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m31p47","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31p47/how_to_get_small_models_4b_to_have_better_common/n3u9ujk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752855482,"author_flair_text":null,"treatment_tags":[],"created_utc":1752855482,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3u8g11","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Red_Redditor_Reddit","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3u5ocl","score":3,"author_fullname":"t2_8eelmfjg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I use markup language for mine.\\n\\n&gt;it’s about whether it was trained that way.\\n\\nThat's the thing, it's kinda not.  It's trained on human (hopefully) generated text.  Also, while the model runs on a computer, that doesn't mean that the model itself is a computer.  If we scanned your brain and ran it as a model in a computer, that model wouldn't be any better at machine encoding.  I hope that makes sense.","edited":false,"author_flair_css_class":null,"name":"t1_n3u8g11","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I use markup language for mine.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;it’s about whether it was trained that way.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;That&amp;#39;s the thing, it&amp;#39;s kinda not.  It&amp;#39;s trained on human (hopefully) generated text.  Also, while the model runs on a computer, that doesn&amp;#39;t mean that the model itself is a computer.  If we scanned your brain and ran it as a model in a computer, that model wouldn&amp;#39;t be any better at machine encoding.  I hope that makes sense.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m31p47","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31p47/how_to_get_small_models_4b_to_have_better_common/n3u8g11/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752855081,"author_flair_text":null,"collapsed":false,"created_utc":1752855081,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n3u5ocl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SandboChang","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3u4q8e","score":1,"author_fullname":"t2_10icmj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"So you are suggesting maybe more verbose format is better suited for LLM? Guess I need to understand what maybe the optimal way for the LLM to digest the data. \\n\\nWhile the above looks like crap for normal people, these are well separated from a coding point of view and it is relatively simple to write a script to extract data precisely for every day for what information. I thought this is the preferred format of most LLM but I guess it’s about whether it was trained that way.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3u5ocl","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So you are suggesting maybe more verbose format is better suited for LLM? Guess I need to understand what maybe the optimal way for the LLM to digest the data. &lt;/p&gt;\\n\\n&lt;p&gt;While the above looks like crap for normal people, these are well separated from a coding point of view and it is relatively simple to write a script to extract data precisely for every day for what information. I thought this is the preferred format of most LLM but I guess it’s about whether it was trained that way.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m31p47","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31p47/how_to_get_small_models_4b_to_have_better_common/n3u5ocl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752854290,"author_flair_text":null,"treatment_tags":[],"created_utc":1752854290,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3u4q8e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Red_Redditor_Reddit","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3txj4s","score":1,"author_fullname":"t2_8eelmfjg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm not a genius when it comes to LLM's, but I'm pretty sure they don't see the world the same way a normal algorithm does. I look at that data and all I see is a jumble of crap. It needs to be easy enough for like a 1st grade kid could figure out. ","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3u4q8e","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m not a genius when it comes to LLM&amp;#39;s, but I&amp;#39;m pretty sure they don&amp;#39;t see the world the same way a normal algorithm does. I look at that data and all I see is a jumble of crap. It needs to be easy enough for like a 1st grade kid could figure out. &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m31p47","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31p47/how_to_get_small_models_4b_to_have_better_common/n3u4q8e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752854022,"author_flair_text":null,"treatment_tags":[],"created_utc":1752854022,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3txj4s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SandboChang","can_mod_post":false,"created_utc":1752852014,"send_replies":true,"parent_id":"t1_n3tx029","score":0,"author_fullname":"t2_10icmj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"May I know which part of it is problematic? I am using a json like format and each day has its own separator, containing with a fixed format the basic weather information and possibly an advisory. \\n\\nI will look up how other weather websites present their fetched data, but I thought this is more or less like the above? If not then maybe I was solving a wrong problem.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3txj4s","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;May I know which part of it is problematic? I am using a json like format and each day has its own separator, containing with a fixed format the basic weather information and possibly an advisory. &lt;/p&gt;\\n\\n&lt;p&gt;I will look up how other weather websites present their fetched data, but I thought this is more or less like the above? If not then maybe I was solving a wrong problem.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m31p47","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31p47/how_to_get_small_models_4b_to_have_better_common/n3txj4s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752852014,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n3tx029","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Red_Redditor_Reddit","can_mod_post":false,"created_utc":1752851865,"send_replies":true,"parent_id":"t3_1m31p47","score":4,"author_fullname":"t2_8eelmfjg","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I don't know if this is relevant, but I'm confused by the prompt.  I'm not going to fault the LLM when I'm having trouble understanding it.  You might want to try just organizing the data a bit more clearly.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3tx029","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t know if this is relevant, but I&amp;#39;m confused by the prompt.  I&amp;#39;m not going to fault the LLM when I&amp;#39;m having trouble understanding it.  You might want to try just organizing the data a bit more clearly.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31p47/how_to_get_small_models_4b_to_have_better_common/n3tx029/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752851865,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m31p47","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3xrcbr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SandboChang","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3vtadx","score":1,"author_fullname":"t2_10icmj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yeah I found 32B models to do much better in this; I will likely also try using my 5090 and Qwen3 32B to generate a bunch of training data to fine tune the small models.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3xrcbr","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah I found 32B models to do much better in this; I will likely also try using my 5090 and Qwen3 32B to generate a bunch of training data to fine tune the small models.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m31p47","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31p47/how_to_get_small_models_4b_to_have_better_common/n3xrcbr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752896584,"author_flair_text":null,"treatment_tags":[],"created_utc":1752896584,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3vtadx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"nore_se_kra","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3voxag","score":1,"author_fullname":"t2_1bpvzzmckh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Check miprov2 optimizer or similar. They are kinda easy to use - issue is that most people dont have proper training data. But thats probably easy to get for your use vase by using a foundation model. U should use a good model anyway as teacher for the small one you want to use.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3vtadx","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Check miprov2 optimizer or similar. They are kinda easy to use - issue is that most people dont have proper training data. But thats probably easy to get for your use vase by using a foundation model. U should use a good model anyway as teacher for the small one you want to use.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m31p47","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31p47/how_to_get_small_models_4b_to_have_better_common/n3vtadx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752871501,"author_flair_text":null,"treatment_tags":[],"created_utc":1752871501,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3voxag","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SandboChang","can_mod_post":false,"created_utc":1752870223,"send_replies":true,"parent_id":"t1_n3vlqsk","score":1,"author_fullname":"t2_10icmj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What did you mean by training without fine-tuning? Is it by better system prompts?\\n\\nRegarding tests, I am considering script some automated test with a bunch of small models, but out of some “vibe testing” I did find Falcon-H1 even at 0.5B is doing very well in capturing the idea in a couple weather reporting scenarios, might just stick with it for now though it seems relatively slow when inferencing through llama.cpp in LMStudio due to its different architecture.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3voxag","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What did you mean by training without fine-tuning? Is it by better system prompts?&lt;/p&gt;\\n\\n&lt;p&gt;Regarding tests, I am considering script some automated test with a bunch of small models, but out of some “vibe testing” I did find Falcon-H1 even at 0.5B is doing very well in capturing the idea in a couple weather reporting scenarios, might just stick with it for now though it seems relatively slow when inferencing through llama.cpp in LMStudio due to its different architecture.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m31p47","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31p47/how_to_get_small_models_4b_to_have_better_common/n3voxag/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752870223,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3vlqsk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"nore_se_kra","can_mod_post":false,"created_utc":1752869290,"send_replies":true,"parent_id":"t3_1m31p47","score":2,"author_fullname":"t2_1bpvzzmckh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It doesnt always have to be a finetune - for well defined scenarios + eval/trainings sets you can easily train a much better prompt - or just eval alot of different models instead of \\"vibe testing\\".","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3vlqsk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It doesnt always have to be a finetune - for well defined scenarios + eval/trainings sets you can easily train a much better prompt - or just eval alot of different models instead of &amp;quot;vibe testing&amp;quot;.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31p47/how_to_get_small_models_4b_to_have_better_common/n3vlqsk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752869290,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m31p47","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3u5nc8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Clear-Ad-9312","can_mod_post":false,"created_utc":1752854282,"send_replies":true,"parent_id":"t1_n3u2d3f","score":1,"author_fullname":"t2_13gn4f8kdq","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I imagine weather data won't change too much that you will need to worry about reprocessing new data every time someone would make a query to it. the only unfortunate part is that you might have moments where you will preprocess the data and end up not using before needing to refresh it with new data. that is just a trade off needed to be made. it is possible you might have some moments where you will voice activate it but it is preprocessing new data. that is called a race condition, and that would be something to plan for.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3u5nc8","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I imagine weather data won&amp;#39;t change too much that you will need to worry about reprocessing new data every time someone would make a query to it. the only unfortunate part is that you might have moments where you will preprocess the data and end up not using before needing to refresh it with new data. that is just a trade off needed to be made. it is possible you might have some moments where you will voice activate it but it is preprocessing new data. that is called a race condition, and that would be something to plan for.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31p47/how_to_get_small_models_4b_to_have_better_common/n3u5nc8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752854282,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m31p47","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":8,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3u2d3f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SandboChang","can_mod_post":false,"created_utc":1752853361,"send_replies":true,"parent_id":"t1_n3u1pit","score":1,"author_fullname":"t2_10icmj","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks, these are great ideas. As it will be part of a home assistant pipeline, I will likely pre-fetch these data and can process them in the background regularly, and they will be used as context of every new conversation as a voice trigger is received.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n3u2d3f","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks, these are great ideas. As it will be part of a home assistant pipeline, I will likely pre-fetch these data and can process them in the background regularly, and they will be used as context of every new conversation as a voice trigger is received.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m31p47","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31p47/how_to_get_small_models_4b_to_have_better_common/n3u2d3f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752853361,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3u1pit","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Clear-Ad-9312","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3ty7ch","score":1,"author_fullname":"t2_13gn4f8kdq","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What I am saying is that in your code, you should parse the JSON a little bit. that means you need to get the data and analyze the JSON. sorry but perfection doesnt exist, sometimes you will have to put some work in when the provider you get data from changes it. however, they typically dont want to change the data format because that is just not nice.\\n\\nlets take your test input your post above had. each day will be its own conversation(keeping context fresh to prevent context growth from reducing LLM performance). have the LLM convert each day into a short sentence in layman's terms or scientific terms, whatever you decide. store the new sentences you got in memory/storage.\\n\\nnow when you ask the LLM a query you can use these new sentences as context. the LLM will understand it better and not get confused as easily.\\n\\nmake it too verbose and the context fills way too much way too fast. LLMs also can degrade after a certain size of context, like 12k+ context, plus bigger context means slower inference speeds.\\n\\nyou should tinker and test how you see fit with what you like.\\n\\nothers have already recommended fine-tuning. I will let you know that is very much a viable option as it can help, but beware that it is more work and you will need to fine tune each time you move to a new llm.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n3u1pit","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What I am saying is that in your code, you should parse the JSON a little bit. that means you need to get the data and analyze the JSON. sorry but perfection doesnt exist, sometimes you will have to put some work in when the provider you get data from changes it. however, they typically dont want to change the data format because that is just not nice.&lt;/p&gt;\\n\\n&lt;p&gt;lets take your test input your post above had. each day will be its own conversation(keeping context fresh to prevent context growth from reducing LLM performance). have the LLM convert each day into a short sentence in layman&amp;#39;s terms or scientific terms, whatever you decide. store the new sentences you got in memory/storage.&lt;/p&gt;\\n\\n&lt;p&gt;now when you ask the LLM a query you can use these new sentences as context. the LLM will understand it better and not get confused as easily.&lt;/p&gt;\\n\\n&lt;p&gt;make it too verbose and the context fills way too much way too fast. LLMs also can degrade after a certain size of context, like 12k+ context, plus bigger context means slower inference speeds.&lt;/p&gt;\\n\\n&lt;p&gt;you should tinker and test how you see fit with what you like.&lt;/p&gt;\\n\\n&lt;p&gt;others have already recommended fine-tuning. I will let you know that is very much a viable option as it can help, but beware that it is more work and you will need to fine tune each time you move to a new llm.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m31p47","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31p47/how_to_get_small_models_4b_to_have_better_common/n3u1pit/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752853179,"author_flair_text":null,"treatment_tags":[],"created_utc":1752853179,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ty7ch","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SandboChang","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3txhs2","score":1,"author_fullname":"t2_10icmj","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Indeed I am just feeding the above as a prompt. \\n\\nIn that case, would a more verbose format be actually preferred? Like maybe individual sentences describing a day with certain format?\\n\\nNot sure if I can really get data to be presented in a way I want, I will check what data does the local observatory actually give.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3ty7ch","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Indeed I am just feeding the above as a prompt. &lt;/p&gt;\\n\\n&lt;p&gt;In that case, would a more verbose format be actually preferred? Like maybe individual sentences describing a day with certain format?&lt;/p&gt;\\n\\n&lt;p&gt;Not sure if I can really get data to be presented in a way I want, I will check what data does the local observatory actually give.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m31p47","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31p47/how_to_get_small_models_4b_to_have_better_common/n3ty7ch/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752852204,"author_flair_text":null,"treatment_tags":[],"created_utc":1752852204,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3txhs2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Clear-Ad-9312","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3tuv33","score":1,"author_fullname":"t2_13gn4f8kdq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"depends, is the json parsed by the inference engine/provider to feed the llm discretely or is the json just added to the input as plain text? the latter is more commonly seen done because the former requires a bit more integration and parsing logic. I think you might had the wrong impression about LLMs. they can handle the data as json format, but smaller LLMs are more easily overwhelmed by chaotic/varied data and can't properly connect many data points together. you might be thinking of tool calling or structured outputs for LLMs as that is json, but I doubt the LLM should be fed straight JSON unless it is small enough.\\n\\nAll in all, if you are building the code, then it would be prudent of you to break the data down in code to something more digestible. the LLM likely got lost and gaslighted it self into thinking the 10th had some unknown hallucinated data. I am glad to share my 2 cents. good luck on your efforts","edited":false,"author_flair_css_class":null,"name":"t1_n3txhs2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;depends, is the json parsed by the inference engine/provider to feed the llm discretely or is the json just added to the input as plain text? the latter is more commonly seen done because the former requires a bit more integration and parsing logic. I think you might had the wrong impression about LLMs. they can handle the data as json format, but smaller LLMs are more easily overwhelmed by chaotic/varied data and can&amp;#39;t properly connect many data points together. you might be thinking of tool calling or structured outputs for LLMs as that is json, but I doubt the LLM should be fed straight JSON unless it is small enough.&lt;/p&gt;\\n\\n&lt;p&gt;All in all, if you are building the code, then it would be prudent of you to break the data down in code to something more digestible. the LLM likely got lost and gaslighted it self into thinking the 10th had some unknown hallucinated data. I am glad to share my 2 cents. good luck on your efforts&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m31p47","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31p47/how_to_get_small_models_4b_to_have_better_common/n3txhs2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752852004,"author_flair_text":null,"collapsed":false,"created_utc":1752852004,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3tuv33","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SandboChang","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3ts903","score":1,"author_fullname":"t2_10icmj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thanks for the insights. I am new to this and was only asking ChatGPT to provide me with some mock-up data, just those in the original post. I chose json as it seems to be a preferred format for LLM to more easily understand the data, but I maybe wrong.\\n\\nBesides the word \\"Thunderstorm\\", each day does have a bunch of information, in particular precipitation chance was actually given. And on July 10th it was objectively higher comparing to July 13th/14th, still it more often than not missed 10th comparing to 13th/14th.\\n\\nFrom the above, this is for 10th (60% chance):\\n\\n { \\"date\\": \\"2025-07-10\\", \\"weekday\\": \\"Thursday\\", \\"condition\\": \\"Afternoon Thunderstorms\\", \\"high\\": 34, \\"low\\": 22, \\"precipitation\\": \\"60%\\", \\"wind\\": \\"Moderate\\", \\"advisory\\": \\"Rain and thunderstorms expected; stay indoors if possible\\" }, \\n\\nAnd for 13th it is only 40%:\\n\\n { \\"date\\": \\"2025-07-13\\", \\"weekday\\": \\"Sunday\\", \\"condition\\": \\"Mostly Cloudy, Light Rain\\", \\"high\\": 31, \\"low\\": 24, \\"precipitation\\": \\"40%\\", \\"wind\\": \\"Light\\", \\"advisory\\": \\"Carry umbrella; sticky humidity\\" }, ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3tuv33","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for the insights. I am new to this and was only asking ChatGPT to provide me with some mock-up data, just those in the original post. I chose json as it seems to be a preferred format for LLM to more easily understand the data, but I maybe wrong.&lt;/p&gt;\\n\\n&lt;p&gt;Besides the word &amp;quot;Thunderstorm&amp;quot;, each day does have a bunch of information, in particular precipitation chance was actually given. And on July 10th it was objectively higher comparing to July 13th/14th, still it more often than not missed 10th comparing to 13th/14th.&lt;/p&gt;\\n\\n&lt;p&gt;From the above, this is for 10th (60% chance):&lt;/p&gt;\\n\\n&lt;p&gt; { &amp;quot;date&amp;quot;: &amp;quot;2025-07-10&amp;quot;, &amp;quot;weekday&amp;quot;: &amp;quot;Thursday&amp;quot;, &amp;quot;condition&amp;quot;: &amp;quot;Afternoon Thunderstorms&amp;quot;, &amp;quot;high&amp;quot;: 34, &amp;quot;low&amp;quot;: 22, &amp;quot;precipitation&amp;quot;: &amp;quot;60%&amp;quot;, &amp;quot;wind&amp;quot;: &amp;quot;Moderate&amp;quot;, &amp;quot;advisory&amp;quot;: &amp;quot;Rain and thunderstorms expected; stay indoors if possible&amp;quot; }, &lt;/p&gt;\\n\\n&lt;p&gt;And for 13th it is only 40%:&lt;/p&gt;\\n\\n&lt;p&gt; { &amp;quot;date&amp;quot;: &amp;quot;2025-07-13&amp;quot;, &amp;quot;weekday&amp;quot;: &amp;quot;Sunday&amp;quot;, &amp;quot;condition&amp;quot;: &amp;quot;Mostly Cloudy, Light Rain&amp;quot;, &amp;quot;high&amp;quot;: 31, &amp;quot;low&amp;quot;: 24, &amp;quot;precipitation&amp;quot;: &amp;quot;40%&amp;quot;, &amp;quot;wind&amp;quot;: &amp;quot;Light&amp;quot;, &amp;quot;advisory&amp;quot;: &amp;quot;Carry umbrella; sticky humidity&amp;quot; }, &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m31p47","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31p47/how_to_get_small_models_4b_to_have_better_common/n3tuv33/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752851269,"author_flair_text":null,"treatment_tags":[],"created_utc":1752851269,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ts903","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Clear-Ad-9312","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3tbzpa","score":1,"author_fullname":"t2_13gn4f8kdq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thunderstorms don't always mean there will be rain because thunderstorms are classified as having thunder and lightning. however, it is odd that the llm doesn't err on the side of it being likely to have rain, especially since it was high precipitation.\\n\\nI am curious what specific data points are you feeding to the llm. if it is just the straight json as text then I think that is your main issue. if you are only feeding the llm specific data points such as only condition, temperature, etc., but leaving out some precipitation or advisory or what not, then that could be an issue. If you want to work with smaller LLMs you really need to break each data point down to as small as possible and do more automated supervising.\\n\\nEach data point for each day will be handled separately. change the formatting to more plain text style, rather than forcing the LLM to attempt to parse the JSON. I would use the LLM to convert some data points to something more coherent, like I would take the single data point \`\\"precipitation\\": \\"10%\\"\` and have the llm decide what to say about it or take the whole day's data point and see if it understands them. if you notice it can't handle a full day then do the former and give it each data point formatted differently.\\n\\ninput: \`convert { \\"date\\": \\"2025-07-12\\", \\"weekday\\": \\"Saturday\\", \\"condition\\": \\"Partly Cloudy\\", \\"high\\": 30, \\"low\\": 22, \\"precipitation\\": \\"10%\\", \\"wind\\": \\"Light\\", \\"advisory\\": \\"Mild and pleasant\\" }\`  \\n\`to a short sentence in laymans terms\`  \\noutput: \`On Saturday, July 12, 2025, it will be partly cloudy with a high of 30°C and a low of 22°C, a 10% chance of rain, light wind, and mild, pleasant weather.\`\\n\\nmaybe it will trip up on a day, but I haven't really seen it happen.\\n\\nafterwards, I can use this output as basis of what I would query to the LLM with. it would be able to understand the input better that way since you are preprocessing the data to something more digestible.\\n\\ninput: \`On Saturday, July 12, 2025, it will be partly cloudy with a high of 30°C and a low of 22°C, a 10% chance of rain, light wind, and mild, pleasant weather. should I bring an umbrella?\`\\n\\noutput: \`Yes, you should bring an umbrella.\` (llm outputs a lot more text but this is first sentence and plenty to get my point across, llm does mention that it is not necessary but good idea to bring umbrella)\\n\\nremember, throwing too much at the small llm is not good. you should throw smaller inputs and try to get small/concise outputs.","edited":1752851318,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3ts903","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thunderstorms don&amp;#39;t always mean there will be rain because thunderstorms are classified as having thunder and lightning. however, it is odd that the llm doesn&amp;#39;t err on the side of it being likely to have rain, especially since it was high precipitation.&lt;/p&gt;\\n\\n&lt;p&gt;I am curious what specific data points are you feeding to the llm. if it is just the straight json as text then I think that is your main issue. if you are only feeding the llm specific data points such as only condition, temperature, etc., but leaving out some precipitation or advisory or what not, then that could be an issue. If you want to work with smaller LLMs you really need to break each data point down to as small as possible and do more automated supervising.&lt;/p&gt;\\n\\n&lt;p&gt;Each data point for each day will be handled separately. change the formatting to more plain text style, rather than forcing the LLM to attempt to parse the JSON. I would use the LLM to convert some data points to something more coherent, like I would take the single data point &lt;code&gt;&amp;quot;precipitation&amp;quot;: &amp;quot;10%&amp;quot;&lt;/code&gt; and have the llm decide what to say about it or take the whole day&amp;#39;s data point and see if it understands them. if you notice it can&amp;#39;t handle a full day then do the former and give it each data point formatted differently.&lt;/p&gt;\\n\\n&lt;p&gt;input: &lt;code&gt;convert { &amp;quot;date&amp;quot;: &amp;quot;2025-07-12&amp;quot;, &amp;quot;weekday&amp;quot;: &amp;quot;Saturday&amp;quot;, &amp;quot;condition&amp;quot;: &amp;quot;Partly Cloudy&amp;quot;, &amp;quot;high&amp;quot;: 30, &amp;quot;low&amp;quot;: 22, &amp;quot;precipitation&amp;quot;: &amp;quot;10%&amp;quot;, &amp;quot;wind&amp;quot;: &amp;quot;Light&amp;quot;, &amp;quot;advisory&amp;quot;: &amp;quot;Mild and pleasant&amp;quot; }&lt;/code&gt;&lt;br/&gt;\\n&lt;code&gt;to a short sentence in laymans terms&lt;/code&gt;&lt;br/&gt;\\noutput: &lt;code&gt;On Saturday, July 12, 2025, it will be partly cloudy with a high of 30°C and a low of 22°C, a 10% chance of rain, light wind, and mild, pleasant weather.&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;maybe it will trip up on a day, but I haven&amp;#39;t really seen it happen.&lt;/p&gt;\\n\\n&lt;p&gt;afterwards, I can use this output as basis of what I would query to the LLM with. it would be able to understand the input better that way since you are preprocessing the data to something more digestible.&lt;/p&gt;\\n\\n&lt;p&gt;input: &lt;code&gt;On Saturday, July 12, 2025, it will be partly cloudy with a high of 30°C and a low of 22°C, a 10% chance of rain, light wind, and mild, pleasant weather. should I bring an umbrella?&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;output: &lt;code&gt;Yes, you should bring an umbrella.&lt;/code&gt; (llm outputs a lot more text but this is first sentence and plenty to get my point across, llm does mention that it is not necessary but good idea to bring umbrella)&lt;/p&gt;\\n\\n&lt;p&gt;remember, throwing too much at the small llm is not good. you should throw smaller inputs and try to get small/concise outputs.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m31p47","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31p47/how_to_get_small_models_4b_to_have_better_common/n3ts903/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752850530,"author_flair_text":null,"treatment_tags":[],"created_utc":1752850530,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3xriua","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SandboChang","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3ty582","score":1,"author_fullname":"t2_10icmj","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I haven’t tried unsloth Colab, but their blogs and tutorials have been amazing. I do have some access to large VRAM GPU myself so I might start by trying it locally first.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3xriua","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I haven’t tried unsloth Colab, but their blogs and tutorials have been amazing. I do have some access to large VRAM GPU myself so I might start by trying it locally first.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m31p47","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31p47/how_to_get_small_models_4b_to_have_better_common/n3xriua/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752896662,"author_flair_text":null,"treatment_tags":[],"created_utc":1752896662,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ty582","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"bralynn2222","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3tv9bk","score":1,"author_fullname":"t2_769j0jzd","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Certainly man the understanding of these systems and how to tune them to your needs is one of the most valuable modern skills today! , If you fine tune your own models, I highly recommend you do it through a un-sloth Collab instead of locally that way, you can fine-tune the full 4 billion parameters for free rather than degrading to the less than 1 billion,  as model adoption/learning becomes much more difficult as model size decreases as well as informational forgetting side effects increasing","edited":1752897373,"author_flair_css_class":null,"name":"t1_n3ty582","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Certainly man the understanding of these systems and how to tune them to your needs is one of the most valuable modern skills today! , If you fine tune your own models, I highly recommend you do it through a un-sloth Collab instead of locally that way, you can fine-tune the full 4 billion parameters for free rather than degrading to the less than 1 billion,  as model adoption/learning becomes much more difficult as model size decreases as well as informational forgetting side effects increasing&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m31p47","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31p47/how_to_get_small_models_4b_to_have_better_common/n3ty582/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752852188,"author_flair_text":null,"collapsed":false,"created_utc":1752852188,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3tv9bk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SandboChang","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3ttgel","score":2,"author_fullname":"t2_10icmj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Right, guess it's a good opportunity to try some fine-tuning as well. I might tinker with the 0.6B model first to see if I can indeed make it better aware of judging weather information like this. In general, though, I then should figure out what other information in daily life I might need the LLM to get better at.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3tv9bk","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Right, guess it&amp;#39;s a good opportunity to try some fine-tuning as well. I might tinker with the 0.6B model first to see if I can indeed make it better aware of judging weather information like this. In general, though, I then should figure out what other information in daily life I might need the LLM to get better at.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m31p47","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31p47/how_to_get_small_models_4b_to_have_better_common/n3tv9bk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752851379,"author_flair_text":null,"treatment_tags":[],"created_utc":1752851379,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ttgel","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"bralynn2222","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3tbzpa","score":1,"author_fullname":"t2_769j0jzd","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"While models below 7B tend to not have these abilities emergently depending on the time and effort you’re willing to invest you can certainly fine tune a model like llama 3.2 3B for example using unsloth Collabs. You can either manually ask state-of-the-art models the types of questions you want to it be consistently better at and save the questions and answers to a Json file. Or you can use an API provider and set up a synthetic data generation loop using a basic Python script that communicates between an API  to generate the question based on the criteria that you define and another to generate the answer, then extract the responses we have using Python to a json","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3ttgel","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;While models below 7B tend to not have these abilities emergently depending on the time and effort you’re willing to invest you can certainly fine tune a model like llama 3.2 3B for example using unsloth Collabs. You can either manually ask state-of-the-art models the types of questions you want to it be consistently better at and save the questions and answers to a Json file. Or you can use an API provider and set up a synthetic data generation loop using a basic Python script that communicates between an API  to generate the question based on the criteria that you define and another to generate the answer, then extract the responses we have using Python to a json&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m31p47","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31p47/how_to_get_small_models_4b_to_have_better_common/n3ttgel/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752850871,"author_flair_text":null,"treatment_tags":[],"created_utc":1752850871,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3tbzpa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SandboChang","can_mod_post":false,"created_utc":1752845752,"send_replies":true,"parent_id":"t1_n3tbb79","score":0,"author_fullname":"t2_10icmj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I see, thanks for the explanation. This capability isn’t strictly needed for it to function as a home assistant to be fair, just a bit surprised as I thought this was relatively basic reasoning. \\n\\nUnfortunately I am trying to use a Jetson Nano for the task so using beyond 4B may not be feasible for now.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3tbzpa","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I see, thanks for the explanation. This capability isn’t strictly needed for it to function as a home assistant to be fair, just a bit surprised as I thought this was relatively basic reasoning. &lt;/p&gt;\\n\\n&lt;p&gt;Unfortunately I am trying to use a Jetson Nano for the task so using beyond 4B may not be feasible for now.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m31p47","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31p47/how_to_get_small_models_4b_to_have_better_common/n3tbzpa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752845752,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n3tbb79","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"codegolf-guru","can_mod_post":false,"created_utc":1752845538,"send_replies":true,"parent_id":"t3_1m31p47","score":1,"author_fullname":"t2_x1ml507r5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Models like Qwen 4B, even at Q4 quantization, are not strong at semantic abstractino unless heavily supervised for it. In your example, the model fails to infer that “thunderstorm” implies “rain” and therefore an umbrella. This kind of reasoning is considered a second order inference - and it's not well emergent bellow 7B without fine-tuning.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3tbb79","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Models like Qwen 4B, even at Q4 quantization, are not strong at semantic abstractino unless heavily supervised for it. In your example, the model fails to infer that “thunderstorm” implies “rain” and therefore an umbrella. This kind of reasoning is considered a second order inference - and it&amp;#39;s not well emergent bellow 7B without fine-tuning.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31p47/how_to_get_small_models_4b_to_have_better_common/n3tbb79/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752845538,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m31p47","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(t,{data:a});export{r as default};
