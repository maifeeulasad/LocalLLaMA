import{j as e}from"./index-CWmJdUH_.js";import{R as t}from"./RedditPostRenderer-D2iunoQ9.js";import"./index-BCg9RP6g.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hi, I'm a newb, please forgive me if I'm missing some obvious documentation.\\n\\nFor the sake of fun and learning, I'd like to fine-tune a local model (haven't decided which one yet), as some kind of writing assistant. My mid-term goal is to have a local VSCode extension that will rewrite e.g. doc comments or CVs as shakespearian sonnets, but we're not there yet.\\n\\nRight now, I'd like to start by fine-tuning a model, just to see how this works and how this influences the results. However, it's not clear to me where to start. I'm not afraid of Python or PyTorch (or Rust, or C++), but I'm entirely lost on the process.\\n\\n1. Any suggestion for a model to use as base? I'd like to be able to run the result on a recent MacBook or on my 3060. For a first attempt, I don't need something particularly fancy.\\n2. How large a corpus do I need to get started?\\n3. Let's assume that I have a corpus of data. What do I do next? Do I need to tokenize it myself? Or should I use some well-known tokenizer?\\n4. How do I even run this fine-tuning? Which tools? Can I run it on my 12Gb 3060 or do I need to rent some GPU time?\\n5. Do I need to quantize myself? Which tools do I need for that? How do I determine to which size I need to quantize?\\n6. Once I have my fine-tuning, how do I deliver it to users? Can I use lama.cpp or do I need to embed Python?\\n7. What else am I missing?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"So how do I fine-time a local model?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":true,"name":"t3_1m19upn","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":1,"author_flair_background_color":null,"subreddit_type":"public","ups":7,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_388ox","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":7,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752664497,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi, I&amp;#39;m a newb, please forgive me if I&amp;#39;m missing some obvious documentation.&lt;/p&gt;\\n\\n&lt;p&gt;For the sake of fun and learning, I&amp;#39;d like to fine-tune a local model (haven&amp;#39;t decided which one yet), as some kind of writing assistant. My mid-term goal is to have a local VSCode extension that will rewrite e.g. doc comments or CVs as shakespearian sonnets, but we&amp;#39;re not there yet.&lt;/p&gt;\\n\\n&lt;p&gt;Right now, I&amp;#39;d like to start by fine-tuning a model, just to see how this works and how this influences the results. However, it&amp;#39;s not clear to me where to start. I&amp;#39;m not afraid of Python or PyTorch (or Rust, or C++), but I&amp;#39;m entirely lost on the process.&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;Any suggestion for a model to use as base? I&amp;#39;d like to be able to run the result on a recent MacBook or on my 3060. For a first attempt, I don&amp;#39;t need something particularly fancy.&lt;/li&gt;\\n&lt;li&gt;How large a corpus do I need to get started?&lt;/li&gt;\\n&lt;li&gt;Let&amp;#39;s assume that I have a corpus of data. What do I do next? Do I need to tokenize it myself? Or should I use some well-known tokenizer?&lt;/li&gt;\\n&lt;li&gt;How do I even run this fine-tuning? Which tools? Can I run it on my 12Gb 3060 or do I need to rent some GPU time?&lt;/li&gt;\\n&lt;li&gt;Do I need to quantize myself? Which tools do I need for that? How do I determine to which size I need to quantize?&lt;/li&gt;\\n&lt;li&gt;Once I have my fine-tuning, how do I deliver it to users? Can I use lama.cpp or do I need to embed Python?&lt;/li&gt;\\n&lt;li&gt;What else am I missing?&lt;/li&gt;\\n&lt;/ol&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m19upn","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"ImYoric","discussion_type":null,"num_comments":3,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m19upn/so_how_do_i_finetime_a_local_model/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m19upn/so_how_do_i_finetime_a_local_model/","subreddit_subscribers":499773,"created_utc":1752664497,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3fjok0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Lissanro","can_mod_post":false,"created_utc":1752667116,"send_replies":true,"parent_id":"t3_1m19upn","score":4,"author_fullname":"t2_fpfao9g","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For your first model, I suggest Qwen3 0.6B - even if you require a smarter model, you still can experiment with it and see:\\n\\n\\\\- If you set up fine-tuning correctly and made no obvious mistakes\\n\\n\\\\- If your fine-tuning makes it pick up style you want\\n\\n\\\\- Does it provide any improvement over prompt with instructions and few examples\\n\\n\\\\- And does the model preserve its knowledge and intelligence. A small model is quick to test on benchmarks, but to keep it simple you could just use subset of MMLU Pro for example instead of running the whole thing.\\n\\n\\\\- Most likely you will be able to experiment with 0.6B model without resorting to renting GPUs\\n\\n\\\\- You can easily do multiple runs with smaller and bigger portions of your dataset to see if it really helps and by how much\\n\\nOnce you feel you fine tuned it as good as it gets, you will feel much more confident fine-tuning a larger model (such as 7B). My guess, for your purposes the final model of 7B-14B will be sufficient. Possibly even 3B if the goal is to just rewrite in a different style short text such as documentation comments.\\n\\nTo get started with fine-tuning, I suggest visiting this page: [https://docs.unsloth.ai/get-started/beginner-start-here](https://docs.unsloth.ai/get-started/beginner-start-here) \\\\- it contains well organized table of content where you can find everything you need\\n\\nIf you do not have dataset or it is to small, you can use large SOTA models like DeepSeek R1 or V3 (both 671B) or Kimi K2 (1T size) to generate synthetic dataset, based on prompt engineering with examples, and some quality control.","edited":1752667946,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3fjok0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For your first model, I suggest Qwen3 0.6B - even if you require a smarter model, you still can experiment with it and see:&lt;/p&gt;\\n\\n&lt;p&gt;- If you set up fine-tuning correctly and made no obvious mistakes&lt;/p&gt;\\n\\n&lt;p&gt;- If your fine-tuning makes it pick up style you want&lt;/p&gt;\\n\\n&lt;p&gt;- Does it provide any improvement over prompt with instructions and few examples&lt;/p&gt;\\n\\n&lt;p&gt;- And does the model preserve its knowledge and intelligence. A small model is quick to test on benchmarks, but to keep it simple you could just use subset of MMLU Pro for example instead of running the whole thing.&lt;/p&gt;\\n\\n&lt;p&gt;- Most likely you will be able to experiment with 0.6B model without resorting to renting GPUs&lt;/p&gt;\\n\\n&lt;p&gt;- You can easily do multiple runs with smaller and bigger portions of your dataset to see if it really helps and by how much&lt;/p&gt;\\n\\n&lt;p&gt;Once you feel you fine tuned it as good as it gets, you will feel much more confident fine-tuning a larger model (such as 7B). My guess, for your purposes the final model of 7B-14B will be sufficient. Possibly even 3B if the goal is to just rewrite in a different style short text such as documentation comments.&lt;/p&gt;\\n\\n&lt;p&gt;To get started with fine-tuning, I suggest visiting this page: &lt;a href=\\"https://docs.unsloth.ai/get-started/beginner-start-here\\"&gt;https://docs.unsloth.ai/get-started/beginner-start-here&lt;/a&gt; - it contains well organized table of content where you can find everything you need&lt;/p&gt;\\n\\n&lt;p&gt;If you do not have dataset or it is to small, you can use large SOTA models like DeepSeek R1 or V3 (both 671B) or Kimi K2 (1T size) to generate synthetic dataset, based on prompt engineering with examples, and some quality control.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m19upn/so_how_do_i_finetime_a_local_model/n3fjok0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752667116,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m19upn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ff44w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MaybeIWasTheBot","can_mod_post":false,"created_utc":1752665227,"send_replies":true,"parent_id":"t3_1m19upn","score":3,"author_fullname":"t2_1tnwyfcaff","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Check out [Unsloth](https://unsloth.ai). Their documentation answers a lot of your questions.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ff44w","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Check out &lt;a href=\\"https://unsloth.ai\\"&gt;Unsloth&lt;/a&gt;. Their documentation answers a lot of your questions.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m19upn/so_how_do_i_finetime_a_local_model/n3ff44w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752665227,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m19upn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3fo22v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rnosov","can_mod_post":false,"created_utc":1752668781,"send_replies":true,"parent_id":"t3_1m19upn","score":2,"author_fullname":"t2_18x6fa","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"1. 4b Gemma - pleasant writing style, easy to fine-tune. Good first model to experiment.\\n2. At least 1 sample, but if you really want to change it - maybe corpus of a few million words would be a good start.\\n3. Model comes with a tokenizer. Normally, it's all done automatically.\\n4. HF PEFT, Unsloth, countless other projects should do the trick. 3060 should be good enough for LoRA.\\n5. Don't worry about the rest - it's all very simple compared to running training successfully.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3fo22v","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;ol&gt;\\n&lt;li&gt;4b Gemma - pleasant writing style, easy to fine-tune. Good first model to experiment.&lt;/li&gt;\\n&lt;li&gt;At least 1 sample, but if you really want to change it - maybe corpus of a few million words would be a good start.&lt;/li&gt;\\n&lt;li&gt;Model comes with a tokenizer. Normally, it&amp;#39;s all done automatically.&lt;/li&gt;\\n&lt;li&gt;HF PEFT, Unsloth, countless other projects should do the trick. 3060 should be good enough for LoRA.&lt;/li&gt;\\n&lt;li&gt;Don&amp;#39;t worry about the rest - it&amp;#39;s all very simple compared to running training successfully.&lt;/li&gt;\\n&lt;/ol&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m19upn/so_how_do_i_finetime_a_local_model/n3fo22v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752668781,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m19upn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),s=()=>e.jsx(t,{data:l});export{s as default};
