import{j as e}from"./index-BOnf-UhU.js";import{R as l}from"./RedditPostRenderer-Ce39qICS.js";import"./index-CDase6VD.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"For those who may be interested, a free-time project that I've now put up on Github: [https://github.com/adriancable/qwen3.c](https://github.com/adriancable/qwen3.c)\\n\\nRun Qwen3-architecture models (like Qwen3-4B, or DeepSeek-R1-0528-Qwen3-8B) locally, no GPU required, using an LLM inference engine you build yourself from just 1 file of C source, with no dependencies. Only requirement is enough RAM to load the models. Think llama.cpp but 100X smaller and simpler, although it's still very functional: multi-language input/output, multi-core CPU support, supports reasoning/thinking models etc.\\n\\nAll you need to build and run is Python3 and a C compiler. The C source is so small, it compiles in around a second. Then, go have fun with the models!\\n\\nAfter you've played around for a bit, if you already understand a bit about how transformers work but want to really learn the detail, the inference engine's C source (unlike llama.cpp) is small enough to dig into without getting a heart attack. Once you've understood how it ticks, you're a transformers expert! 😃\\n\\nNot intended to compete with 'heavyweight' engines like llama.cpp, rather, the focus is on being (fun)ctional and educational.\\n\\nMIT license so you can do whatever you want with the source, no restrictions.\\n\\nProject will be a success if at least one person here enjoys it!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Qwen3 inference engine in C: simple, educational, fun","link_flair_richtext":[{"e":"text","t":"Generation"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lpejnj","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.98,"author_flair_background_color":null,"subreddit_type":"public","ups":167,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_fyyk012qp","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Generation","can_mod_post":false,"score":167,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1751406598,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;For those who may be interested, a free-time project that I&amp;#39;ve now put up on Github: &lt;a href=\\"https://github.com/adriancable/qwen3.c\\"&gt;https://github.com/adriancable/qwen3.c&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Run Qwen3-architecture models (like Qwen3-4B, or DeepSeek-R1-0528-Qwen3-8B) locally, no GPU required, using an LLM inference engine you build yourself from just 1 file of C source, with no dependencies. Only requirement is enough RAM to load the models. Think llama.cpp but 100X smaller and simpler, although it&amp;#39;s still very functional: multi-language input/output, multi-core CPU support, supports reasoning/thinking models etc.&lt;/p&gt;\\n\\n&lt;p&gt;All you need to build and run is Python3 and a C compiler. The C source is so small, it compiles in around a second. Then, go have fun with the models!&lt;/p&gt;\\n\\n&lt;p&gt;After you&amp;#39;ve played around for a bit, if you already understand a bit about how transformers work but want to really learn the detail, the inference engine&amp;#39;s C source (unlike llama.cpp) is small enough to dig into without getting a heart attack. Once you&amp;#39;ve understood how it ticks, you&amp;#39;re a transformers expert! 😃&lt;/p&gt;\\n\\n&lt;p&gt;Not intended to compete with &amp;#39;heavyweight&amp;#39; engines like llama.cpp, rather, the focus is on being (fun)ctional and educational.&lt;/p&gt;\\n\\n&lt;p&gt;MIT license so you can do whatever you want with the source, no restrictions.&lt;/p&gt;\\n\\n&lt;p&gt;Project will be a success if at least one person here enjoys it!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/LxoqZs4q3Osj78IVaTXSrgUKqNHcOujrOF1Tg6_GYA4.jpeg?auto=webp&amp;s=f64a6eef9fb25bb8dece4a00b49169cb6de85df2","width":640,"height":640},"resolutions":[{"url":"https://external-preview.redd.it/LxoqZs4q3Osj78IVaTXSrgUKqNHcOujrOF1Tg6_GYA4.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b6a4a1ab699ce9984d57b0696bdd1f873de9e614","width":108,"height":108},{"url":"https://external-preview.redd.it/LxoqZs4q3Osj78IVaTXSrgUKqNHcOujrOF1Tg6_GYA4.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=80ccbeb83c907fb5b897374c139c51e76825ec00","width":216,"height":216},{"url":"https://external-preview.redd.it/LxoqZs4q3Osj78IVaTXSrgUKqNHcOujrOF1Tg6_GYA4.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f5ad415a9157f412849b8def8bc5c576f5d41217","width":320,"height":320},{"url":"https://external-preview.redd.it/LxoqZs4q3Osj78IVaTXSrgUKqNHcOujrOF1Tg6_GYA4.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3ac79d8600937790d6301fdd4917b87eabf6336a","width":640,"height":640}],"variants":{},"id":"LxoqZs4q3Osj78IVaTXSrgUKqNHcOujrOF1Tg6_GYA4"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"23bddba8-ff56-11ed-9688-1a11994b71f7","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#b5a3d0","id":"1lpejnj","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"adrian-cable","discussion_type":null,"num_comments":33,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/","subreddit_subscribers":494001,"created_utc":1751406598,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ud3v1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Agreeable-Prompt-666","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ucleq","score":5,"author_fullname":"t2_1l3z4stvkq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"For sure. I just see huge possibilities with this.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0ud3v1","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For sure. I just see huge possibilities with this.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpejnj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n0ud3v1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751409373,"author_flair_text":null,"treatment_tags":[],"created_utc":1751409373,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0uytrx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"adrian-cable","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0uo4in","score":8,"author_fullname":"t2_fyyk012qp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Potentially. The project is only a day old so I’m really appreciative of any feedback and thoughts on directions I can take it. Thank you!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0uytrx","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Potentially. The project is only a day old so I’m really appreciative of any feedback and thoughts on directions I can take it. Thank you!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpejnj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n0uytrx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751416759,"author_flair_text":null,"treatment_tags":[],"created_utc":1751416759,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}}],"before":null}},"user_reports":[],"saved":false,"id":"n0uo4in","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Accomplished_Mode170","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ucleq","score":3,"author_fullname":"t2_4hfmiefj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Any interest in supporting ‘commodity compute’ on something like tenstorrent?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0uo4in","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Any interest in supporting ‘commodity compute’ on something like tenstorrent?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpejnj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n0uo4in/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751413013,"author_flair_text":null,"treatment_tags":[],"created_utc":1751413013,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n12qyx6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"adrian-cable","can_mod_post":false,"send_replies":true,"parent_id":"t1_n12hjew","score":3,"author_fullname":"t2_fyyk012qp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"That's great! Most of the runtime is spent inside matmul, so that's definitely the one to optimize. If you can do it without increasing the complexity of the code, please submit a PR. Otherwise feel free to make a fork, and let me know and I'm happy to link to it from my README.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n12qyx6","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s great! Most of the runtime is spent inside matmul, so that&amp;#39;s definitely the one to optimize. If you can do it without increasing the complexity of the code, please submit a PR. Otherwise feel free to make a fork, and let me know and I&amp;#39;m happy to link to it from my README.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpejnj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n12qyx6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751519271,"author_flair_text":null,"treatment_tags":[],"created_utc":1751519271,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n12hjew","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Agreeable-Prompt-666","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ucleq","score":3,"author_fullname":"t2_1l3z4stvkq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Qwen/qwen4B:q8\\n\\nllama-bench \\\\~9 tok/sec\\n\\nrunq \\\\~15tok/sec\\n\\nreally nice job\\n\\nim going to try and optimize matmul and rmsnorm functions for fun\\n\\nif you think there's a heavier function that would be better for optimization please let me know.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n12hjew","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen/qwen4B:q8&lt;/p&gt;\\n\\n&lt;p&gt;llama-bench ~9 tok/sec&lt;/p&gt;\\n\\n&lt;p&gt;runq ~15tok/sec&lt;/p&gt;\\n\\n&lt;p&gt;really nice job&lt;/p&gt;\\n\\n&lt;p&gt;im going to try and optimize matmul and rmsnorm functions for fun&lt;/p&gt;\\n\\n&lt;p&gt;if you think there&amp;#39;s a heavier function that would be better for optimization please let me know.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpejnj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n12hjew/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751515030,"author_flair_text":null,"treatment_tags":[],"created_utc":1751515030,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ucleq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"adrian-cable","can_mod_post":false,"created_utc":1751409205,"send_replies":true,"parent_id":"t1_n0ub1tk","score":16,"author_fullname":"t2_fyyk012qp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Running the same quantisation (Q8_0) it’s within the same ballpark, generally within a factor of 2. It’s optimized for simplicity not performance, but it still runs at a very usable speed.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ucleq","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Running the same quantisation (Q8_0) it’s within the same ballpark, generally within a factor of 2. It’s optimized for simplicity not performance, but it still runs at a very usable speed.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpejnj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n0ucleq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751409205,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":16}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ub1tk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Agreeable-Prompt-666","can_mod_post":false,"created_utc":1751408697,"send_replies":true,"parent_id":"t3_1lpejnj","score":22,"author_fullname":"t2_1l3z4stvkq","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Amazing and thank you, looking forward to learning.\\n\\nQuick q , really curious, how's speed relative to llamacpp :D","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ub1tk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Amazing and thank you, looking forward to learning.&lt;/p&gt;\\n\\n&lt;p&gt;Quick q , really curious, how&amp;#39;s speed relative to llamacpp :D&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n0ub1tk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751408697,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpejnj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":22}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0z1i14","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"adrian-cable","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0y72dc","score":3,"author_fullname":"t2_fyyk012qp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"As with any LLM inference engine, the vast majority of the execution time is spent within the matmul function, and this (on most systems) is limited by memory bandwidth rather than computation.\\n\\nSo my expectation is that any gains would need to come from micro-optimizing things to specific CPUs (for example, prefetch just the right amount of data from RAM to CPU cache) which probably moves things very quickly away from simplicity. But I'm very open to trying!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0z1i14","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;As with any LLM inference engine, the vast majority of the execution time is spent within the matmul function, and this (on most systems) is limited by memory bandwidth rather than computation.&lt;/p&gt;\\n\\n&lt;p&gt;So my expectation is that any gains would need to come from micro-optimizing things to specific CPUs (for example, prefetch just the right amount of data from RAM to CPU cache) which probably moves things very quickly away from simplicity. But I&amp;#39;m very open to trying!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpejnj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n0z1i14/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751475499,"author_flair_text":null,"treatment_tags":[],"created_utc":1751475499,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0y72dc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"yeah-ok","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0udetj","score":2,"author_fullname":"t2_3xlrs","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"And I guess the simplicity also allows for easier (initial) performance gain via gprof or Valgrind sooo, exciting times!","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0y72dc","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;And I guess the simplicity also allows for easier (initial) performance gain via gprof or Valgrind sooo, exciting times!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpejnj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n0y72dc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751466898,"author_flair_text":null,"treatment_tags":[],"created_utc":1751466898,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0udetj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"adrian-cable","can_mod_post":false,"created_utc":1751409473,"send_replies":true,"parent_id":"t1_n0ucorh","score":4,"author_fullname":"t2_fyyk012qp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Not as fast since it prioritises simplicity over performance, but with everything else equal within 2X.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0udetj","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not as fast since it prioritises simplicity over performance, but with everything else equal within 2X.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpejnj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n0udetj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751409473,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ucorh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"yeah-ok","can_mod_post":false,"created_utc":1751409236,"send_replies":true,"parent_id":"t3_1lpejnj","score":8,"author_fullname":"t2_3xlrs","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Very impressive work, had a browse through runq.c and indeed it is, as c goes, digestible!👍\\n\\nHave you done any, however rudimentary, comparison benchmarks in terms of qwen3.c vs llama.cpp?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ucorh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Very impressive work, had a browse through runq.c and indeed it is, as c goes, digestible!👍&lt;/p&gt;\\n\\n&lt;p&gt;Have you done any, however rudimentary, comparison benchmarks in terms of qwen3.c vs llama.cpp?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n0ucorh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751409236,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpejnj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ughhw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"althalusian","can_mod_post":false,"created_utc":1751410488,"send_replies":true,"parent_id":"t1_n0ubrl7","score":5,"author_fullname":"t2_1qrqmhyn","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Careers not Carter’s?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ughhw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Careers not Carter’s?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpejnj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n0ughhw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751410488,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ubrl7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"_moria_","can_mod_post":false,"created_utc":1751408932,"send_replies":true,"parent_id":"t3_1lpejnj","score":6,"author_fullname":"t2_10lot4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"My humble opinion is that this is a critical objective. Understanding is a critical aspect of forming new people and ideas. Think about netbsd. The best? No, but surely the most clear code for an operating system, I know a lot of people for which clear simple code has opened high profile Carter's in os development.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ubrl7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;My humble opinion is that this is a critical objective. Understanding is a critical aspect of forming new people and ideas. Think about netbsd. The best? No, but surely the most clear code for an operating system, I know a lot of people for which clear simple code has opened high profile Carter&amp;#39;s in os development.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n0ubrl7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751408932,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpejnj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0vyezf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Traditional_Tap1708","can_mod_post":false,"created_utc":1751430023,"send_replies":true,"parent_id":"t3_1lpejnj","score":3,"author_fullname":"t2_aejvth7b","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Really cool","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0vyezf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Really cool&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n0vyezf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751430023,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpejnj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0y1v0l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jsllls","can_mod_post":false,"created_utc":1751465372,"send_replies":true,"parent_id":"t3_1lpejnj","score":3,"author_fullname":"t2_ekgpz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Nice, I’m currently in the middle of a similar project, but built to run baremetal on my risc-v simulator with vector extensions. Inference engine and cpu sim both written in C++, no external dependencies other than STL.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0y1v0l","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nice, I’m currently in the middle of a similar project, but built to run baremetal on my risc-v simulator with vector extensions. Inference engine and cpu sim both written in C++, no external dependencies other than STL.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n0y1v0l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751465372,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpejnj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0y2nku","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"adrian-cable","can_mod_post":false,"created_utc":1751465607,"send_replies":true,"parent_id":"t1_n0wiokb","score":3,"author_fullname":"t2_fyyk012qp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That's right, quantization is done in blocks (like Q8\\\\_0), with each block of 64 floats being scaled to 64 8-bit ints, and 1 float scale factor.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0y2nku","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s right, quantization is done in blocks (like Q8_0), with each block of 64 floats being scaled to 64 8-bit ints, and 1 float scale factor.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpejnj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n0y2nku/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751465607,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0wiokb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Confident_Pi","can_mod_post":false,"created_utc":1751440397,"send_replies":true,"parent_id":"t3_1lpejnj","score":2,"author_fullname":"t2_3qyy1yzk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Amazing work, congrats! How did you handle quantization? I see that you support Q8\\\\_0 and your matmuls run in 8 bit?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0wiokb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Amazing work, congrats! How did you handle quantization? I see that you support Q8_0 and your matmuls run in 8 bit?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n0wiokb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751440397,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpejnj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n11o0f8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"adrian-cable","can_mod_post":false,"created_utc":1751504314,"send_replies":true,"parent_id":"t1_n10w58w","score":3,"author_fullname":"t2_fyyk012qp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That’s totally fine! Enjoy.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n11o0f8","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That’s totally fine! Enjoy.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpejnj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n11o0f8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751504314,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n10w58w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"teleprint-me","can_mod_post":false,"created_utc":1751495214,"send_replies":true,"parent_id":"t3_1lpejnj","score":2,"author_fullname":"t2_slcrtxpr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This is very cool. It's like the fates were like, \\"we bestow you this wonderful gift.\\"\\n\\n\\nI've been considering what model I wanted to focus on and Qwen3 seemed like the perfect candidate.\\n\\n\\nI wanted to learn how the [Vulkan compute pipeline](https://github.com/teleprint-me/vk.c/blob/main/examples/vk.c) worked since I have an AMD stack and torch is hit or miss for me as a result (it has improved a lot, but it needs a lot of work still).\\n\\n\\nMind if I use this as a base in the future?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n10w58w","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is very cool. It&amp;#39;s like the fates were like, &amp;quot;we bestow you this wonderful gift.&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ve been considering what model I wanted to focus on and Qwen3 seemed like the perfect candidate.&lt;/p&gt;\\n\\n&lt;p&gt;I wanted to learn how the &lt;a href=\\"https://github.com/teleprint-me/vk.c/blob/main/examples/vk.c\\"&gt;Vulkan compute pipeline&lt;/a&gt; worked since I have an AMD stack and torch is hit or miss for me as a result (it has improved a lot, but it needs a lot of work still).&lt;/p&gt;\\n\\n&lt;p&gt;Mind if I use this as a base in the future?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n10w58w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751495214,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpejnj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n14fmua","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"adrian-cable","can_mod_post":false,"send_replies":true,"parent_id":"t1_n145s66","score":1,"author_fullname":"t2_fyyk012qp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"That’s a good catch!\\n\\nWith that said, I’m thinking (in the spirit of simplicity) of removing the generate mode entirely. As far as I can tell, all Qwen3 models are ‘instruct’ models and don’t work properly in generate mode. Are there any exceptions you’re aware of?\\n\\nEdit to add: there are the Base versions of Qwen3 available. So I won’t remove generate.","edited":1751549816,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n14fmua","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That’s a good catch!&lt;/p&gt;\\n\\n&lt;p&gt;With that said, I’m thinking (in the spirit of simplicity) of removing the generate mode entirely. As far as I can tell, all Qwen3 models are ‘instruct’ models and don’t work properly in generate mode. Are there any exceptions you’re aware of?&lt;/p&gt;\\n\\n&lt;p&gt;Edit to add: there are the Base versions of Qwen3 available. So I won’t remove generate.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpejnj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n14fmua/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751549027,"author_flair_text":null,"treatment_tags":[],"created_utc":1751549027,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n145s66","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Agreeable-Prompt-666","can_mod_post":false,"send_replies":true,"parent_id":"t1_n12rims","score":1,"author_fullname":"t2_1l3z4stvkq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"generate","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n145s66","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;generate&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpejnj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n145s66/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751545558,"author_flair_text":null,"treatment_tags":[],"created_utc":1751545558,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n12rims","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"adrian-cable","can_mod_post":false,"created_utc":1751519537,"send_replies":true,"parent_id":"t1_n1250f7","score":2,"author_fullname":"t2_fyyk012qp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That's in the 'generate' function, right, and the 'chat' function is correct?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n12rims","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s in the &amp;#39;generate&amp;#39; function, right, and the &amp;#39;chat&amp;#39; function is correct?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpejnj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n12rims/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751519537,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1250f7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Agreeable-Prompt-666","can_mod_post":false,"created_utc":1751510062,"send_replies":true,"parent_id":"t3_1lpejnj","score":2,"author_fullname":"t2_1l3z4stvkq","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"quick bug fix, it's leaving out the last char at the absolute end of its output; here's the fix(just move one line down.\\n\\n\\n\\n\\n\\n// data-dependent terminating condition: the BOS token delimits sequences\\n\\nif (pos &gt;= \\\\*num\\\\_prompt\\\\_tokens) (\\\\*generated\\\\_tokens)++;\\n\\n\\n\\n**DELETE THIS LINE-&gt;** **~~if (pos &gt;= \\\\*num\\\\_prompt\\\\_tokens &amp;&amp; (next == tokenizer-&gt;bos\\\\_token\\\\_id || next == tokenizer-&gt;eos\\\\_token\\\\_id)) { break; }~~**\\n\\n\\n\\n// print the token as string, decode it with the Tokenizer object\\n\\nif (pos &gt;= \\\\*num\\\\_prompt\\\\_tokens) {\\n\\nprintf(\\"%s\\", decode(tokenizer, token));\\n\\nfflush(stdout);\\n\\n} else if (debug) {\\n\\nprintf(\\"%s\\", decode(tokenizer, token));\\n\\nfflush(stdout);}\\n\\n// check termination condition afterprinting the current token\\n\\n\\n\\n**ADD THIS LINE: if (pos &gt;= \\\\*num\\\\_prompt\\\\_tokens &amp;&amp; (next == tokenizer-&gt;bos\\\\_token\\\\_id || next == tokenizer-&gt;eos\\\\_token\\\\_id)) { break; }**\\n\\n\\n\\ntoken = next;}\\n\\nif (debug) printf(\\"\\\\\\\\n\\");","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1250f7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;quick bug fix, it&amp;#39;s leaving out the last char at the absolute end of its output; here&amp;#39;s the fix(just move one line down.&lt;/p&gt;\\n\\n&lt;p&gt;// data-dependent terminating condition: the BOS token delimits sequences&lt;/p&gt;\\n\\n&lt;p&gt;if (pos &amp;gt;= *num_prompt_tokens) (*generated_tokens)++;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;DELETE THIS LINE-&amp;gt;&lt;/strong&gt; &lt;strong&gt;&lt;del&gt;if (pos &amp;gt;= *num_prompt_tokens &amp;amp;&amp;amp; (next == tokenizer-&amp;gt;bos_token_id || next == tokenizer-&amp;gt;eos_token_id)) { break; }&lt;/del&gt;&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;// print the token as string, decode it with the Tokenizer object&lt;/p&gt;\\n\\n&lt;p&gt;if (pos &amp;gt;= *num_prompt_tokens) {&lt;/p&gt;\\n\\n&lt;p&gt;printf(&amp;quot;%s&amp;quot;, decode(tokenizer, token));&lt;/p&gt;\\n\\n&lt;p&gt;fflush(stdout);&lt;/p&gt;\\n\\n&lt;p&gt;} else if (debug) {&lt;/p&gt;\\n\\n&lt;p&gt;printf(&amp;quot;%s&amp;quot;, decode(tokenizer, token));&lt;/p&gt;\\n\\n&lt;p&gt;fflush(stdout);}&lt;/p&gt;\\n\\n&lt;p&gt;// check termination condition afterprinting the current token&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;ADD THIS LINE: if (pos &amp;gt;= *num_prompt_tokens &amp;amp;&amp;amp; (next == tokenizer-&amp;gt;bos_token_id || next == tokenizer-&amp;gt;eos_token_id)) { break; }&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;token = next;}&lt;/p&gt;\\n\\n&lt;p&gt;if (debug) printf(&amp;quot;\\\\n&amp;quot;);&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n1250f7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751510062,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpejnj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n14ic6x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"bigattichouse","can_mod_post":false,"created_utc":1751549913,"send_replies":true,"parent_id":"t3_1lpejnj","score":2,"author_fullname":"t2_7s6m4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Good work! Been looking for a simple C program to run Qwen","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n14ic6x","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Good work! Been looking for a simple C program to run Qwen&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n14ic6x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751549913,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpejnj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0uuamk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Languages_Learner","can_mod_post":false,"created_utc":1751415130,"send_replies":true,"parent_id":"t3_1lpejnj","score":3,"author_fullname":"t2_v9x8tm7u","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thanks for great implementation. It reminds me another pure C llm cpu inference engine which supports different models: [pierrel55/llama\\\\_st: Load and run Llama from safetensors files in C](https://github.com/pierrel55/llama_st)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0uuamk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for great implementation. It reminds me another pure C llm cpu inference engine which supports different models: &lt;a href=\\"https://github.com/pierrel55/llama_st\\"&gt;pierrel55/llama_st: Load and run Llama from safetensors files in C&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n0uuamk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751415130,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpejnj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1596qu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"althalusian","can_mod_post":false,"send_replies":true,"parent_id":"t1_n155b9n","score":1,"author_fullname":"t2_1qrqmhyn","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Environment is Win11 WSL2, with 96GB RAM (and RTX 3080).\\n\\nInitially I did the installation like in the instructions (I used same conda env I use for llama.cpp so it had most of the tools ready):\\n\\n    git clone https://github.com/adriancable/qwen3.c\\n    cd qwen3.c\\n    make openmp\\n\\nThen adding git lfs to download the model files (already had git):\\n\\n    conda install git-lfs\\n    git lfs install\\n\\nthen downloading the models, 8B in this example:\\n\\n    git clone https://huggingface.co/Qwen/Qwen3-8B\\n\\nexporting the model:\\n\\n    python export.py Qwen3-8B.bin ./Qwen3-8B\\n    Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 5/5 [00:07&lt;00:00,  1.59s/it]\\n    ModelArgs(dim=4096, n_layers=36, n_heads=32, n_kv_heads=8, head_dim=128, vocab_size=151936, hidden_dim=12288, multiple_of=256, norm_eps=1e-06, max_seq_len=40960, dropout=0.0)\\n    Written tokenizer model to Qwen3-8B.bin.tokenizer\\n    Written prompt templates to Qwen3-8B.bin.template.*\\n    1/254 quantized (151936, 4096) to Q8_0 with max error 0.00385975\\n    ...\\n    254/254 quantized (151936, 4096) to Q8_0 with max error 0.00143553\\n    max quantization group error across all weights: 0.01134389\\n    Written model checkpoint to Qwen3-8B.bin\\n\\nand finally running:\\n\\n\`./runq Qwen3-8B.bin\`\\n\\nGives the result (using the example prompt):\\n\\n    ./runq Qwen3-8B.bin -r 1\\n    hidden_size=4096, intermediate_size=12288, num_hidden_layers=36, num_attention_heads=32, num_kv_heads=8, head_dim=128, ctx_length=40960, vocab_size=151936, shared_classifier=0, quantization_block_size=64\\n    \\n    &gt; What is 19673261 * 1842.64?\\n    !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!^C","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1596qu","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Environment is Win11 WSL2, with 96GB RAM (and RTX 3080).&lt;/p&gt;\\n\\n&lt;p&gt;Initially I did the installation like in the instructions (I used same conda env I use for llama.cpp so it had most of the tools ready):&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;git clone https://github.com/adriancable/qwen3.c\\ncd qwen3.c\\nmake openmp\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;Then adding git lfs to download the model files (already had git):&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;conda install git-lfs\\ngit lfs install\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;then downloading the models, 8B in this example:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;git clone https://huggingface.co/Qwen/Qwen3-8B\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;exporting the model:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;python export.py Qwen3-8B.bin ./Qwen3-8B\\nLoading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 5/5 [00:07&amp;lt;00:00,  1.59s/it]\\nModelArgs(dim=4096, n_layers=36, n_heads=32, n_kv_heads=8, head_dim=128, vocab_size=151936, hidden_dim=12288, multiple_of=256, norm_eps=1e-06, max_seq_len=40960, dropout=0.0)\\nWritten tokenizer model to Qwen3-8B.bin.tokenizer\\nWritten prompt templates to Qwen3-8B.bin.template.*\\n1/254 quantized (151936, 4096) to Q8_0 with max error 0.00385975\\n...\\n254/254 quantized (151936, 4096) to Q8_0 with max error 0.00143553\\nmax quantization group error across all weights: 0.01134389\\nWritten model checkpoint to Qwen3-8B.bin\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;and finally running:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;./runq Qwen3-8B.bin&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Gives the result (using the example prompt):&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;./runq Qwen3-8B.bin -r 1\\nhidden_size=4096, intermediate_size=12288, num_hidden_layers=36, num_attention_heads=32, num_kv_heads=8, head_dim=128, ctx_length=40960, vocab_size=151936, shared_classifier=0, quantization_block_size=64\\n\\n&amp;gt; What is 19673261 * 1842.64?\\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!^C\\n&lt;/code&gt;&lt;/pre&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpejnj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n1596qu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751557758,"author_flair_text":null,"treatment_tags":[],"created_utc":1751557758,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n155b9n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"adrian-cable","can_mod_post":false,"created_utc":1751556675,"send_replies":true,"parent_id":"t1_n14wpwg","score":1,"author_fullname":"t2_fyyk012qp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Can you tell me the exact sequence of commands you're using to download, export and run the Qwen3-8B model? Also, how much RAM do you have, and what platform are you using (Linux, macOS etc.)?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n155b9n","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Can you tell me the exact sequence of commands you&amp;#39;re using to download, export and run the Qwen3-8B model? Also, how much RAM do you have, and what platform are you using (Linux, macOS etc.)?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpejnj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n155b9n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751556675,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n14wpwg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"althalusian","can_mod_post":false,"created_utc":1751554252,"send_replies":true,"parent_id":"t3_1lpejnj","score":1,"author_fullname":"t2_1qrqmhyn","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Still trying to get this to work; export.py dies when trying Qwen3-32B, and managed to go through on Qwen3-8B but the output is only ! -characters… Well, I guess troubleshooting is part of the learning experience.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n14wpwg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Still trying to get this to work; export.py dies when trying Qwen3-32B, and managed to go through on Qwen3-8B but the output is only ! -characters… Well, I guess troubleshooting is part of the learning experience.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n14wpwg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751554252,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpejnj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0uvhpi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok_Cow1976","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0uc6xo","score":2,"author_fullname":"t2_3pwbsmdr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks a lot for explanations.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0uvhpi","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks a lot for explanations.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpejnj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n0uvhpi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751415556,"author_flair_text":null,"treatment_tags":[],"created_utc":1751415556,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0uc6xo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"adrian-cable","can_mod_post":false,"created_utc":1751409072,"send_replies":true,"parent_id":"t1_n0u8ovm","score":19,"author_fullname":"t2_fyyk012qp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Everything’s relative, but llama.cpp is pretty heavy, at around 400,000 lines of code, compared with 1,500 lines of code for this project. (Verify for yourself on codetabs.com)\\n\\nThe idea here is to make an inference engine whose source is small and simple enough so that, if you already understand C/C++, you can quickly understand how inference works in depth. You can’t do that with a 400KLOC project.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0uc6xo","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Everything’s relative, but llama.cpp is pretty heavy, at around 400,000 lines of code, compared with 1,500 lines of code for this project. (Verify for yourself on codetabs.com)&lt;/p&gt;\\n\\n&lt;p&gt;The idea here is to make an inference engine whose source is small and simple enough so that, if you already understand C/C++, you can quickly understand how inference works in depth. You can’t do that with a 400KLOC project.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpejnj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n0uc6xo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751409072,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":19}}],"before":null}},"user_reports":[],"saved":false,"id":"n0u8ovm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok_Cow1976","can_mod_post":false,"created_utc":1751407939,"send_replies":true,"parent_id":"t3_1lpejnj","score":1,"author_fullname":"t2_3pwbsmdr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Llama.cpp is not heavy. Vllm is huge and heavy. But nice to see alternatives.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0u8ovm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Llama.cpp is not heavy. Vllm is huge and heavy. But nice to see alternatives.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n0u8ovm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751407939,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpejnj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"c07aa42e-51fe-11f0-afcc-462aad931709","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0uy5k2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"created_utc":1751416520,"send_replies":true,"parent_id":"t3_1lpejnj","score":-4,"author_fullname":"t2_1a48h7vf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Masochist.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0uy5k2","is_submitter":false,"downs":0,"author_flair_richtext":[{"a":":X:","u":"https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X","e":"emoji"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Masochist.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/n0uy5k2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751416520,"author_flair_text":":X:","treatment_tags":[],"link_id":"t3_1lpejnj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"transparent","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-4}}],"before":null}}]`),o=()=>e.jsx(l,{data:t});export{o as default};
