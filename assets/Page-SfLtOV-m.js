import{j as e}from"./index-Bu7qcPAU.js";import{R as l}from"./RedditPostRenderer-CbHA7O5q.js";import"./index-BKgbfxhf.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"The Qwen3 MoE model (and all other MoE models) in HF Transformers is notoriously slow, because it uses a for loop to access the experts, resulting in &lt; 20% GPU usage. It's been two months and there are still very few LoRAs of Qwen3-30B-A3B in the public. (If you search 'qwen3 30b a3b lora' on HuggingFace, that's... interesting)\\n\\nThis should be made easier. I've made a fused version of Qwen3 MoE Layer that's much faster, while being compatible with the HF Transformers ecosystem, such as LoRA, bitsandbytes 4-bit quantization, and Unsloth. On a single GPU with 24GB VRAM, it reaches 100% GPU usage and 5x speedup of training compared to the unfused model.\\n\\nThere is still room for further optimization, but you can try it now and train your own LoRA.\\n\\nAlso, please help if you know how to upstream this to Transformers or Unsloth. (Transformers itself never includes Triton or CUDA kernels in the package, but they have a HuggingFace Kernels project to do so.)","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Fused Qwen3 MoE layer for faster training Qwen3-30B-A3B LoRA","link_flair_richtext":[{"e":"text","t":"Resources"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":70,"top_awarded_type":null,"hide_score":false,"name":"t3_1ltgayn","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.97,"author_flair_background_color":null,"ups":92,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1rq55mepz8","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Resources","can_mod_post":false,"score":92,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://external-preview.redd.it/HSpEkOVbwqk3BvJQ60pY-PTeM7wLxNZ1qE0Ade0f5cc.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=4c8616ae35c9e35f518e997ac58630a64deac92f","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"link","content_categories":null,"is_self":false,"subreddit_type":"public","created":1751847508,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"github.com","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;The Qwen3 MoE model (and all other MoE models) in HF Transformers is notoriously slow, because it uses a for loop to access the experts, resulting in &amp;lt; 20% GPU usage. It&amp;#39;s been two months and there are still very few LoRAs of Qwen3-30B-A3B in the public. (If you search &amp;#39;qwen3 30b a3b lora&amp;#39; on HuggingFace, that&amp;#39;s... interesting)&lt;/p&gt;\\n\\n&lt;p&gt;This should be made easier. I&amp;#39;ve made a fused version of Qwen3 MoE Layer that&amp;#39;s much faster, while being compatible with the HF Transformers ecosystem, such as LoRA, bitsandbytes 4-bit quantization, and Unsloth. On a single GPU with 24GB VRAM, it reaches 100% GPU usage and 5x speedup of training compared to the unfused model.&lt;/p&gt;\\n\\n&lt;p&gt;There is still room for further optimization, but you can try it now and train your own LoRA.&lt;/p&gt;\\n\\n&lt;p&gt;Also, please help if you know how to upstream this to Transformers or Unsloth. (Transformers itself never includes Triton or CUDA kernels in the package, but they have a HuggingFace Kernels project to do so.)&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"url_overridden_by_dest":"https://github.com/woct0rdho/transformers-qwen3-moe-fused","view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/HSpEkOVbwqk3BvJQ60pY-PTeM7wLxNZ1qE0Ade0f5cc.png?auto=webp&amp;s=a967129ff5ab80fff515506e789647ed640bbd2f","width":1200,"height":600},"resolutions":[{"url":"https://external-preview.redd.it/HSpEkOVbwqk3BvJQ60pY-PTeM7wLxNZ1qE0Ade0f5cc.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a3be8044143b537d63e288d3975bce8cd7e0836b","width":108,"height":54},{"url":"https://external-preview.redd.it/HSpEkOVbwqk3BvJQ60pY-PTeM7wLxNZ1qE0Ade0f5cc.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2c1301694d705875bed40d97a02b2f4d4f11005f","width":216,"height":108},{"url":"https://external-preview.redd.it/HSpEkOVbwqk3BvJQ60pY-PTeM7wLxNZ1qE0Ade0f5cc.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f81de294025d9ab45618aca8ab4acb37ea5cfcfa","width":320,"height":160},{"url":"https://external-preview.redd.it/HSpEkOVbwqk3BvJQ60pY-PTeM7wLxNZ1qE0Ade0f5cc.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=094a72eeef2838a876e8835f533b8146c2ead2a9","width":640,"height":320},{"url":"https://external-preview.redd.it/HSpEkOVbwqk3BvJQ60pY-PTeM7wLxNZ1qE0Ade0f5cc.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e882735fafc07d7af0f34739ba0a85a5fed48802","width":960,"height":480},{"url":"https://external-preview.redd.it/HSpEkOVbwqk3BvJQ60pY-PTeM7wLxNZ1qE0Ade0f5cc.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8874406a247b374af4f7df5cc87dde23ebe726e2","width":1080,"height":540}],"variants":{},"id":"HSpEkOVbwqk3BvJQ60pY-PTeM7wLxNZ1qE0Ade0f5cc"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"mod_note":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"num_reports":null,"removal_reason":null,"link_flair_background_color":"#ccac2b","id":"1ltgayn","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"woct0rdho","discussion_type":null,"num_comments":12,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1ltgayn/fused_qwen3_moe_layer_for_faster_training/","stickied":false,"url":"https://github.com/woct0rdho/transformers-qwen3-moe-fused","subreddit_subscribers":496034,"created_utc":1751847508,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1rciyz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"danielhanchen","can_mod_post":false,"created_utc":1751865332,"send_replies":true,"parent_id":"t3_1ltgayn","score":39,"author_fullname":"t2_5wukhd4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Oh hi again! Great work! Thanks for utilizing the Unsloth kernels! We haven't yet released or announced MoE atuff for Unsloth since unfortunately we're a bit behind schedule and we need more helping hands!\\n\\nMore than happy for an Unsloth PR and I can help!\\n\\nJust note the kernels are placed under an agplv3 license since unfortunately we had multiple companies and packages copy and paste our kernels without crediting us in the license header nor acknowledgements - we tried lgplv3 to no avail since some would sneakily fork the repo and link it to theirs.\\n\\nWe'll be communicating this with the community in the following days!\\n\\nAgain great work and excited to work together in stuff!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1rciyz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Oh hi again! Great work! Thanks for utilizing the Unsloth kernels! We haven&amp;#39;t yet released or announced MoE atuff for Unsloth since unfortunately we&amp;#39;re a bit behind schedule and we need more helping hands!&lt;/p&gt;\\n\\n&lt;p&gt;More than happy for an Unsloth PR and I can help!&lt;/p&gt;\\n\\n&lt;p&gt;Just note the kernels are placed under an agplv3 license since unfortunately we had multiple companies and packages copy and paste our kernels without crediting us in the license header nor acknowledgements - we tried lgplv3 to no avail since some would sneakily fork the repo and link it to theirs.&lt;/p&gt;\\n\\n&lt;p&gt;We&amp;#39;ll be communicating this with the community in the following days!&lt;/p&gt;\\n\\n&lt;p&gt;Again great work and excited to work together in stuff!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltgayn/fused_qwen3_moe_layer_for_faster_training/n1rciyz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751865332,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltgayn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":39}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1qgfce","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"woct0rdho","can_mod_post":false,"created_utc":1751851820,"send_replies":true,"parent_id":"t1_n1qec32","score":11,"author_fullname":"t2_1rq55mepz8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"GPU is fast only if you let it process a lot of numbers at once. The MoE (mixture of experts) model has many 'experts' (Qwen3-30B-A3B has 128 experts in each layer), and each expert only has a small amount of parameters, so it's slow if you access them separately. 'Fused' means some clever code to access them at once.\\n\\nFor 8GB VRAM, I guess the fuse will not help. Even after 4-bit quantization, Qwen3-30B-A3B takes 16GB memory, so you need to offload to CPU memory, and the speed is limited by the memory transfer between CPU and GPU rather than the computation on GPU. This kind of memory offload is optimized in Unsloth and you can try it.","edited":1751939714,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1qgfce","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;GPU is fast only if you let it process a lot of numbers at once. The MoE (mixture of experts) model has many &amp;#39;experts&amp;#39; (Qwen3-30B-A3B has 128 experts in each layer), and each expert only has a small amount of parameters, so it&amp;#39;s slow if you access them separately. &amp;#39;Fused&amp;#39; means some clever code to access them at once.&lt;/p&gt;\\n\\n&lt;p&gt;For 8GB VRAM, I guess the fuse will not help. Even after 4-bit quantization, Qwen3-30B-A3B takes 16GB memory, so you need to offload to CPU memory, and the speed is limited by the memory transfer between CPU and GPU rather than the computation on GPU. This kind of memory offload is optimized in Unsloth and you can try it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltgayn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltgayn/fused_qwen3_moe_layer_for_faster_training/n1qgfce/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751851820,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}}],"before":null}},"user_reports":[],"saved":false,"id":"n1qec32","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"True_Requirement_891","can_mod_post":false,"created_utc":1751851050,"send_replies":true,"parent_id":"t3_1ltgayn","score":6,"author_fullname":"t2_yfi9sqrzf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Damn son, can you explain this in more simpler terms? Also, can I benefit with this on 8gb vram?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1qec32","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Damn son, can you explain this in more simpler terms? Also, can I benefit with this on 8gb vram?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltgayn/fused_qwen3_moe_layer_for_faster_training/n1qec32/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751851050,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltgayn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1snzlm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"woct0rdho","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1sd9j4","score":5,"author_fullname":"t2_1rq55mepz8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"In principle a lora should not significantly change the expert usage. Also it depends on whether you create a lora on the routing gate.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1snzlm","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;In principle a lora should not significantly change the expert usage. Also it depends on whether you create a lora on the routing gate.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltgayn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltgayn/fused_qwen3_moe_layer_for_faster_training/n1snzlm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751890923,"author_flair_text":null,"treatment_tags":[],"created_utc":1751890923,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n1sd9j4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Zc5Gwu","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1radni","score":6,"author_fullname":"t2_67qrvlir","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Won’t expert usage be unbalanced when you reseparate?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1sd9j4","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Won’t expert usage be unbalanced when you reseparate?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltgayn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltgayn/fused_qwen3_moe_layer_for_faster_training/n1sd9j4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751886158,"author_flair_text":null,"treatment_tags":[],"created_utc":1751886158,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1vope1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"woct0rdho","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1u7uw9","score":3,"author_fullname":"t2_1rq55mepz8","approved_by":null,"mod_note":null,"all_awardings":[],"body":"235B cannot fit in a single GPU, and my code may need some modification to run on multiple GPUs (such as moving the tensors to the correct device.)\\n\\nvllm already has the fused MoE kernels that support multiple GPUs, and I guess my code will not be faster than theirs. It's just because no one did this for training (in open source code I could find) so I did it.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n1vope1","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;235B cannot fit in a single GPU, and my code may need some modification to run on multiple GPUs (such as moving the tensors to the correct device.)&lt;/p&gt;\\n\\n&lt;p&gt;vllm already has the fused MoE kernels that support multiple GPUs, and I guess my code will not be faster than theirs. It&amp;#39;s just because no one did this for training (in open source code I could find) so I did it.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1ltgayn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltgayn/fused_qwen3_moe_layer_for_faster_training/n1vope1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751925437,"author_flair_text":null,"treatment_tags":[],"created_utc":1751925437,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1vcz4n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"shing3232","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1u7uw9","score":1,"author_fullname":"t2_ze4mg","approved_by":null,"mod_note":null,"all_awardings":[],"body":"technically, it should. Qwen3 moe is pretty sparse","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n1vcz4n","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;technically, it should. Qwen3 moe is pretty sparse&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1ltgayn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltgayn/fused_qwen3_moe_layer_for_faster_training/n1vcz4n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751921514,"author_flair_text":null,"treatment_tags":[],"created_utc":1751921514,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1u7uw9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"__JockY__","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1rnzge","score":1,"author_fullname":"t2_qf8h7ka8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Does it follow that this technique could be applied to Qwen3 235B A22B for faster inference also? \\n\\nI have access to a quad RTX A6000 rig that runs 4-bit quants of 235B model in vLLM and I’d be very interested in ways to make it faster.","edited":false,"author_flair_css_class":null,"name":"t1_n1u7uw9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Does it follow that this technique could be applied to Qwen3 235B A22B for faster inference also? &lt;/p&gt;\\n\\n&lt;p&gt;I have access to a quad RTX A6000 rig that runs 4-bit quants of 235B model in vLLM and I’d be very interested in ways to make it faster.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1ltgayn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltgayn/fused_qwen3_moe_layer_for_faster_training/n1u7uw9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751908346,"author_flair_text":null,"collapsed":false,"created_utc":1751908346,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1rnzge","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"woct0rdho","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1rhu75","score":5,"author_fullname":"t2_1rq55mepz8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Sure, there's also \`example_infer_30b_a3b.py\`. Inference using the original HF Transformers is slow, but projects like llama.cpp and vllm already have this kind of fused kernels.","edited":1751871916,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1rnzge","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sure, there&amp;#39;s also &lt;code&gt;example_infer_30b_a3b.py&lt;/code&gt;. Inference using the original HF Transformers is slow, but projects like llama.cpp and vllm already have this kind of fused kernels.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltgayn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltgayn/fused_qwen3_moe_layer_for_faster_training/n1rnzge/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751871565,"author_flair_text":null,"treatment_tags":[],"created_utc":1751871565,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n1rhu75","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"shing3232","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1radni","score":2,"author_fullname":"t2_ze4mg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Can you fused moe layer for inference as well？ irs kind of slow for batching","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1rhu75","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Can you fused moe layer for inference as well？ irs kind of slow for batching&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltgayn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltgayn/fused_qwen3_moe_layer_for_faster_training/n1rhu75/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751868135,"author_flair_text":null,"treatment_tags":[],"created_utc":1751868135,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1radni","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"woct0rdho","can_mod_post":false,"created_utc":1751864249,"send_replies":true,"parent_id":"t1_n1raa1t","score":9,"author_fullname":"t2_1rq55mepz8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes. The conversion between the fused and the unfused formats is lossless.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1radni","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes. The conversion between the fused and the unfused formats is lossless.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltgayn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltgayn/fused_qwen3_moe_layer_for_faster_training/n1radni/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751864249,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}}],"before":null}},"user_reports":[],"saved":false,"id":"n1raa1t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Desperate-Sir-5088","can_mod_post":false,"created_utc":1751864200,"send_replies":true,"parent_id":"t3_1ltgayn","score":4,"author_fullname":"t2_1dhesoqqtu","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Would you confirm that my understand is correct?\\n\\n\\n- By using fused-MOE, Effectively tune QWEN3 30B-A3B with unsloth.\\n\\n\\n- Restore it to its original tensor, to convert GGUF and serving them  under llama.cpp or vllm.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1raa1t","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Would you confirm that my understand is correct?&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;p&gt;By using fused-MOE, Effectively tune QWEN3 30B-A3B with unsloth.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Restore it to its original tensor, to convert GGUF and serving them  under llama.cpp or vllm.&lt;/p&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltgayn/fused_qwen3_moe_layer_for_faster_training/n1raa1t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751864200,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltgayn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}}]`),o=()=>e.jsx(l,{data:a});export{o as default};
