import{j as e}from"./index-Cd3v0jxz.js";import{R as l}from"./RedditPostRenderer-cI96YLhy.js";import"./index-DGBoKyQm.js";const a=JSON.parse('[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I have a 7900xt and 32gb of ddr5, I am planning on adding an mi50 32gb to my system, do I need to upgrade my ram for this?\\n\\nWeird situation but my knowledge of pc building is mostly centred around gaming hardware, and this scenario basically never happens in that context.\\n\\nWill I need to upgrade my ram in order for llms to load properly? I’ve heard that the model is loaded into system ram then into vram, if I don’t have enough system ram, does it just not work?\\n\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Can you have more vram than system ram?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m0jeyu","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":1,"author_flair_background_color":null,"subreddit_type":"public","ups":2,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_rn6co7q5m","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":2,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752590118,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I have a 7900xt and 32gb of ddr5, I am planning on adding an mi50 32gb to my system, do I need to upgrade my ram for this?&lt;/p&gt;\\n\\n&lt;p&gt;Weird situation but my knowledge of pc building is mostly centred around gaming hardware, and this scenario basically never happens in that context.&lt;/p&gt;\\n\\n&lt;p&gt;Will I need to upgrade my ram in order for llms to load properly? I’ve heard that the model is loaded into system ram then into vram, if I don’t have enough system ram, does it just not work?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m0jeyu","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"opoot_","discussion_type":null,"num_comments":10,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m0jeyu/can_you_have_more_vram_than_system_ram/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m0jeyu/can_you_have_more_vram_than_system_ram/","subreddit_subscribers":499773,"created_utc":1752590118,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n39ukh2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"segmond","can_mod_post":false,"send_replies":true,"parent_id":"t1_n39qylc","score":2,"author_fullname":"t2_ah13x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"look at my profile, i believe it\'s one of my pinned posts.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n39ukh2","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;look at my profile, i believe it&amp;#39;s one of my pinned posts.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0jeyu","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0jeyu/can_you_have_more_vram_than_system_ram/n39ukh2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752592047,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752592047,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n39qylc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Willing_Landscape_61","can_mod_post":false,"created_utc":1752591033,"send_replies":true,"parent_id":"t1_n39q0u3","score":1,"author_fullname":"t2_8lvrytgw","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I\'m intrigued!\\nWould you mind telling more about this cluster? Hardware, software and usage?\\nThx!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n39qylc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m intrigued!\\nWould you mind telling more about this cluster? Hardware, software and usage?\\nThx!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0jeyu","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0jeyu/can_you_have_more_vram_than_system_ram/n39qylc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752591033,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n39q0u3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"segmond","can_mod_post":false,"created_utc":1752590766,"send_replies":true,"parent_id":"t3_1m0jeyu","score":5,"author_fullname":"t2_ah13x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yes you can have more, my AMD cluster has 160gb VRAM and 16gb system ram.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n39q0u3","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes you can have more, my AMD cluster has 160gb VRAM and 16gb system ram.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0jeyu/can_you_have_more_vram_than_system_ram/n39q0u3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752590766,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m0jeyu","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n39slxt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AdamDhahabi","can_mod_post":false,"send_replies":true,"parent_id":"t1_n39pr5f","score":1,"author_fullname":"t2_x5lnbc2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Many of us here are running dual GPU and multi GPU setups. Your inference engine like llama.cpp just splits the model weights to all available GPUs, at limited speed penalty. Hence you can run a 24GB model even if only 16GB system memory. In my example you can even run a 30GB model with partial offloading, what does not fit in GPU memory will be fit in system memory. That last scenario is costing you a lot of speed as system memory is slow.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n39slxt","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Many of us here are running dual GPU and multi GPU setups. Your inference engine like llama.cpp just splits the model weights to all available GPUs, at limited speed penalty. Hence you can run a 24GB model even if only 16GB system memory. In my example you can even run a 30GB model with partial offloading, what does not fit in GPU memory will be fit in system memory. That last scenario is costing you a lot of speed as system memory is slow.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0jeyu","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0jeyu/can_you_have_more_vram_than_system_ram/n39slxt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752591496,"author_flair_text":null,"treatment_tags":[],"created_utc":1752591496,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n39pr5f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"opoot_","can_mod_post":false,"created_utc":1752590689,"send_replies":true,"parent_id":"t1_n39oq61","score":1,"author_fullname":"t2_rn6co7q5m","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Eh? Could you explain a bit more? Do models that exceed 16 gb in size work at all? Are there big performance hits compared to systems that have more ram?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n39pr5f","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Eh? Could you explain a bit more? Do models that exceed 16 gb in size work at all? Are there big performance hits compared to systems that have more ram?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0jeyu","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0jeyu/can_you_have_more_vram_than_system_ram/n39pr5f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752590689,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n39oq61","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AdamDhahabi","can_mod_post":false,"created_utc":1752590393,"send_replies":true,"parent_id":"t3_1m0jeyu","score":1,"author_fullname":"t2_x5lnbc2","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I previously had a dual GPU setup with 24 GB VRAM and only 16GB system memory. Don\'t mind Windows keeping the weights in system memory after offloading to GPU, you would think your system is almost on its knees but everything goes fine.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n39oq61","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I previously had a dual GPU setup with 24 GB VRAM and only 16GB system memory. Don&amp;#39;t mind Windows keeping the weights in system memory after offloading to GPU, you would think your system is almost on its knees but everything goes fine.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0jeyu/can_you_have_more_vram_than_system_ram/n39oq61/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752590393,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0jeyu","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3a8ndo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fizzy1242","can_mod_post":false,"created_utc":1752595978,"send_replies":true,"parent_id":"t3_1m0jeyu","score":1,"author_fullname":"t2_16zcsx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I did. I had 32 ram and 72 vram. There were no real issues, other than sometimes models would load onto vram slightly slower due to paging.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3a8ndo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I did. I had 32 ram and 72 vram. There were no real issues, other than sometimes models would load onto vram slightly slower due to paging.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0jeyu/can_you_have_more_vram_than_system_ram/n3a8ndo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752595978,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0jeyu","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3aolru","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"panchovix","can_mod_post":false,"created_utc":1752600316,"send_replies":true,"parent_id":"t3_1m0jeyu","score":2,"author_fullname":"t2_j1kqr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yes, no issues. I have 208GB VRAM and 192GB RAM.\\n\\nTo load models bigger than RAM though (i.e. a 300GB model) you need to have enough swap to load the model with RAM + swap. Then it moves to VRAM + RAM.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3aolru","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, no issues. I have 208GB VRAM and 192GB RAM.&lt;/p&gt;\\n\\n&lt;p&gt;To load models bigger than RAM though (i.e. a 300GB model) you need to have enough swap to load the model with RAM + swap. Then it moves to VRAM + RAM.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0jeyu/can_you_have_more_vram_than_system_ram/n3aolru/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752600316,"author_flair_text":"Llama 405B","treatment_tags":[],"link_id":"t3_1m0jeyu","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ashw8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ravage382","can_mod_post":false,"created_utc":1752601357,"send_replies":true,"parent_id":"t1_n39s7a1","score":1,"author_fullname":"t2_9sf41","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I have 96gb of ram allocated as vram, 32gb of ram as system ram and a 1gb swap file, where its using 102.3mb of swap and its running fine with unsloth/dots.llm1.inst-GGUF:Q4\\\\_K\\\\_XL . \\n\\nThe only initial hurdle was I had to offload 2 layers to system ram and that used up most of the remainder of the system ram, but released it afterwards. I hit 98% ram utilization and then that leveled back out when the model was fully loaded in llama.cpp.\\n\\n  \\n`root@balthasar:~/vulkan/bin# free`\\n\\n`total        used        free      shared  buff/cache   available`\\n\\n`Mem:        32494744     8032956      231848         120    24699440    24461788`\\n\\n`Swap:        1000444      104708      895736`\\n\\n`root@balthasar:~/vulkan/bin# rocm-smi` \\n\\n\\n\\n`======================================== ROCm System Management Interface ========================================`\\n\\n`================================================== Concise Info ==================================================`\\n\\n`Device  Node  IDs              Temp    Power     Partitions          SCLK  MCLK  Fan  Perf  PwrCap  VRAM%  GPU%`  \\n\\n`(DID,     GUID)  (Edge)  (Socket)  (Mem, Compute, ID)`                                              \\n\\n`==================================================================================================================`\\n\\n`0       1     0x1586,   15162  27.0°C  3.052W    N/A, N/A, 0         N/A   N/A   0%   auto  N/A     96%    0%`    \\n\\n`==================================================================================================================`\\n\\n`============================================== End of ROCm SMI Log ===============================================`","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ashw8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have 96gb of ram allocated as vram, 32gb of ram as system ram and a 1gb swap file, where its using 102.3mb of swap and its running fine with unsloth/dots.llm1.inst-GGUF:Q4_K_XL . &lt;/p&gt;\\n\\n&lt;p&gt;The only initial hurdle was I had to offload 2 layers to system ram and that used up most of the remainder of the system ram, but released it afterwards. I hit 98% ram utilization and then that leveled back out when the model was fully loaded in llama.cpp.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;root@balthasar:~/vulkan/bin# free&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;total        used        free      shared  buff/cache   available&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;Mem:        32494744     8032956      231848         120    24699440    24461788&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;Swap:        1000444      104708      895736&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;root@balthasar:~/vulkan/bin# rocm-smi&lt;/code&gt; &lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;======================================== ROCm System Management Interface ========================================&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;================================================== Concise Info ==================================================&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;Device  Node  IDs              Temp    Power     Partitions          SCLK  MCLK  Fan  Perf  PwrCap  VRAM%  GPU%&lt;/code&gt;  &lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;(DID,     GUID)  (Edge)  (Socket)  (Mem, Compute, ID)&lt;/code&gt;                                              &lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;==================================================================================================================&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;0       1     0x1586,   15162  27.0°C  3.052W    N/A, N/A, 0         N/A   N/A   0%   auto  N/A     96%    0%&lt;/code&gt;    &lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;==================================================================================================================&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;============================================== End of ROCm SMI Log ===============================================&lt;/code&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0jeyu","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0jeyu/can_you_have_more_vram_than_system_ram/n3ashw8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752601357,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n39s7a1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Nepherpitu","can_mod_post":false,"created_utc":1752591382,"send_replies":true,"parent_id":"t3_1m0jeyu","score":1,"author_fullname":"t2_plp1w","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You can have more VRAM than RAM, BUT. BUUUUUT. REALLY HUGE BUT.\\nBut: you need to have more virtual memory than VRAM. Enable swap. You don\'t need swap to be fast, you only need RAM address space to be bigger than VRAM address space. At least for windows, but I suspect it\'s same for linux. \\n\\nIn my case I wasn\'t able to utilize 3 x 24Gb GPU with 64 RAM + 32 SWAP - driver was out of memory trying to allocate `72Gb` VRAM addresses into `96 - (docker + wsl + browser + system overhead) Gb` of available system memory.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n39s7a1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can have more VRAM than RAM, BUT. BUUUUUT. REALLY HUGE BUT.\\nBut: you need to have more virtual memory than VRAM. Enable swap. You don&amp;#39;t need swap to be fast, you only need RAM address space to be bigger than VRAM address space. At least for windows, but I suspect it&amp;#39;s same for linux. &lt;/p&gt;\\n\\n&lt;p&gt;In my case I wasn&amp;#39;t able to utilize 3 x 24Gb GPU with 64 RAM + 32 SWAP - driver was out of memory trying to allocate &lt;code&gt;72Gb&lt;/code&gt; VRAM addresses into &lt;code&gt;96 - (docker + wsl + browser + system overhead) Gb&lt;/code&gt; of available system memory.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0jeyu/can_you_have_more_vram_than_system_ram/n39s7a1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752591382,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0jeyu","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]'),n=()=>e.jsx(l,{data:a});export{n as default};
