import{j as e}from"./index-MXAMZFAj.js";import{R as l}from"./RedditPostRenderer-AiHEQBDd.js";import"./index-B-YL2gc7.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I am working on porting a GPT-4.1 project over to an open-source model to deal with a GDPR-compliant client. The task is basically fine-tuning the model to classify text in a western European language.\\n\\nI tried Qwen3 (0.6B, 1.7B, 8B) without making much progress (the fine-tuned model is far behind GPT-4.1) and finally went back to Llama-3.1-8B, which was what worked for me over a year ago. This is super surprising to me, because Qwen3's zero-shot performance in English is almost 2x that of Llama's for similar model sizes.\\n\\nDoes anyone else run fine-tuning heavy workloads in European languages? What's the best model for this workload that I can fine-tune on an H100 96GB (note: I don't do PEFT)?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"I keep returning to Llama-3.1-8B","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lm9012","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.9,"author_flair_background_color":null,"subreddit_type":"public","ups":52,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1a48h7vf","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":52,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751068694,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I am working on porting a GPT-4.1 project over to an open-source model to deal with a GDPR-compliant client. The task is basically fine-tuning the model to classify text in a western European language.&lt;/p&gt;\\n\\n&lt;p&gt;I tried Qwen3 (0.6B, 1.7B, 8B) without making much progress (the fine-tuned model is far behind GPT-4.1) and finally went back to Llama-3.1-8B, which was what worked for me over a year ago. This is super surprising to me, because Qwen3&amp;#39;s zero-shot performance in English is almost 2x that of Llama&amp;#39;s for similar model sizes.&lt;/p&gt;\\n\\n&lt;p&gt;Does anyone else run fine-tuning heavy workloads in European languages? What&amp;#39;s the best model for this workload that I can fine-tune on an H100 96GB (note: I don&amp;#39;t do PEFT)?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lm9012","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"entsnack","discussion_type":null,"num_comments":27,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lm9012/i_keep_returning_to_llama318b/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lm9012/i_keep_returning_to_llama318b/","subreddit_subscribers":492627,"created_utc":1751068694,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n06n7pc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ThinkExtension2328","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0602rd","score":4,"author_fullname":"t2_8eneodlk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yea If it was me I‚Äôd go the gmma or qwen flavors , llama is good but these two just edge it out.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n06n7pc","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yea If it was me I‚Äôd go the gmma or qwen flavors , llama is good but these two just edge it out.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm9012","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm9012/i_keep_returning_to_llama318b/n06n7pc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751081746,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1751081746,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n0602rd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"entsnack","can_mod_post":false,"created_utc":1751072590,"send_replies":true,"parent_id":"t1_n05z14j","score":5,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Excellent suggestion, added to my cart.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0602rd","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Excellent suggestion, added to my cart.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm9012","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm9012/i_keep_returning_to_llama318b/n0602rd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751072590,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0d4v51","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Snirlavi5","can_mod_post":false,"send_replies":true,"parent_id":"t1_n091gbe","score":1,"author_fullname":"t2_6pj8r10c","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Could you recommend a decent vision model you're using?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0d4v51","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Could you recommend a decent vision model you&amp;#39;re using?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm9012","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm9012/i_keep_returning_to_llama318b/n0d4v51/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751176015,"author_flair_text":null,"treatment_tags":[],"created_utc":1751176015,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n091gbe","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"gdzzzz","can_mod_post":false,"created_utc":1751122880,"send_replies":true,"parent_id":"t1_n05z14j","score":5,"author_fullname":"t2_15r5g8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Allow me to disagree :   \\n\\\\- local vision models are getting much better to the point where I'm actually starting used them in production.   \\n\\\\- until now I was using small models for specific tasks, with new models like gemma3, I'm giving larger tasks  \\n\\\\- there's a whole set of new models with reasoning and tool calling that are coming, still not optimal, but the trend is clealry there, similar to vision models which started 1 year ago before reaching a satisfactory maturity","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n091gbe","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Allow me to disagree :&lt;br/&gt;\\n- local vision models are getting much better to the point where I&amp;#39;m actually starting used them in production.&lt;br/&gt;\\n- until now I was using small models for specific tasks, with new models like gemma3, I&amp;#39;m giving larger tasks&lt;br/&gt;\\n- there&amp;#39;s a whole set of new models with reasoning and tool calling that are coming, still not optimal, but the trend is clealry there, similar to vision models which started 1 year ago before reaching a satisfactory maturity&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm9012","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm9012/i_keep_returning_to_llama318b/n091gbe/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751122880,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n05z14j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ArsNeph","can_mod_post":false,"created_utc":1751072202,"send_replies":true,"parent_id":"t3_1lm9012","score":39,"author_fullname":"t2_vt0xkv60d","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Unfortunately, there hasn't been much happening in the small model space, but you might want to try Gemma 3 12B, as it's very good at multilingual, including European languages. The Google team also said it's easy to fine tune, though I'm not sure how true that is.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n05z14j","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Unfortunately, there hasn&amp;#39;t been much happening in the small model space, but you might want to try Gemma 3 12B, as it&amp;#39;s very good at multilingual, including European languages. The Google team also said it&amp;#39;s easy to fine tune, though I&amp;#39;m not sure how true that is.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm9012/i_keep_returning_to_llama318b/n05z14j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751072202,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lm9012","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":39}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n06dwbu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"My_Unbiased_Opinion","can_mod_post":false,"send_replies":true,"parent_id":"t1_n06bond","score":11,"author_fullname":"t2_esiyl0yb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Oh yeah you are gonna love Mistral. Their stuff doesn't score the highest in benchmarks, but their practical usability and effectiveness is top tier.¬†","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n06dwbu","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Oh yeah you are gonna love Mistral. Their stuff doesn&amp;#39;t score the highest in benchmarks, but their practical usability and effectiveness is top tier.¬†&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm9012","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm9012/i_keep_returning_to_llama318b/n06dwbu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751077856,"author_flair_text":null,"treatment_tags":[],"created_utc":1751077856,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n07s020","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"send_replies":true,"parent_id":"t1_n076mnc","score":1,"author_fullname":"t2_1a48h7vf","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I didn't know unsloth does full fine-tuning, I'll check. My setup is just TRL SFTTrainer. The reason I don't use PEFT is because I have an internal benchmark that needs to compare with reinforcement fine-tuning, and PEFT with reinforcement learning doesn't work well.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n07s020","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I didn&amp;#39;t know unsloth does full fine-tuning, I&amp;#39;ll check. My setup is just TRL SFTTrainer. The reason I don&amp;#39;t use PEFT is because I have an internal benchmark that needs to compare with reinforcement fine-tuning, and PEFT with reinforcement learning doesn&amp;#39;t work well.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lm9012","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm9012/i_keep_returning_to_llama318b/n07s020/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751103576,"author_flair_text":null,"treatment_tags":[],"created_utc":1751103576,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n076mnc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok_Appearance3584","can_mod_post":false,"send_replies":true,"parent_id":"t1_n06xcz6","score":3,"author_fullname":"t2_oyxj85n1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What's your full finetuning setup? Just transformers or have you tried unsloth? I hear they have support for full finetuning and they do memory optimizations (especially if you install the variant with ampere-specific optimizations) - I'd give it a go in a new environment. Maybe you could fit 12b into it.","edited":false,"author_flair_css_class":null,"name":"t1_n076mnc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What&amp;#39;s your full finetuning setup? Just transformers or have you tried unsloth? I hear they have support for full finetuning and they do memory optimizations (especially if you install the variant with ampere-specific optimizations) - I&amp;#39;d give it a go in a new environment. Maybe you could fit 12b into it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lm9012","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm9012/i_keep_returning_to_llama318b/n076mnc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751091259,"author_flair_text":null,"collapsed":false,"created_utc":1751091259,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n06xcz6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"send_replies":true,"parent_id":"t1_n06vyni","score":2,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Very cool! 8B is the largest that seems to fit on my H100.\\n\\nOne thing I haven't tried is supervised fine-tuning a reasoning model, not sure if that would work (and it would take a really long time).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n06xcz6","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Very cool! 8B is the largest that seems to fit on my H100.&lt;/p&gt;\\n\\n&lt;p&gt;One thing I haven&amp;#39;t tried is supervised fine-tuning a reasoning model, not sure if that would work (and it would take a really long time).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm9012","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm9012/i_keep_returning_to_llama318b/n06xcz6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751086468,"author_flair_text":null,"treatment_tags":[],"created_utc":1751086468,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n06vyni","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"GlowingPulsar","can_mod_post":false,"send_replies":true,"parent_id":"t1_n06bond","score":5,"author_fullname":"t2_1eeuvibnme","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Mistral AI released [Ministral](https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF) last October, it's a solid 8b model that you may like if you want to try something a little smaller than Nemo.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n06vyni","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Mistral AI released &lt;a href=\\"https://huggingface.co/bartowski/Ministral-8B-Instruct-2410-GGUF\\"&gt;Ministral&lt;/a&gt; last October, it&amp;#39;s a solid 8b model that you may like if you want to try something a little smaller than Nemo.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm9012","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm9012/i_keep_returning_to_llama318b/n06vyni/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751085780,"author_flair_text":null,"treatment_tags":[],"created_utc":1751085780,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n06bond","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"entsnack","can_mod_post":false,"created_utc":1751076994,"send_replies":true,"parent_id":"t1_n067pi6","score":4,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"No I've overlooked Mistral so far, but it seems perfect given it's from Europe. I'm going to try that before the other Llama fine-tunes.\\n\\nI do feel like Llama-3.1 was peak open-source LLM versatility. It's been my workhorse model for too long and I'm planning to switch to Qwen eventually.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n06bond","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No I&amp;#39;ve overlooked Mistral so far, but it seems perfect given it&amp;#39;s from Europe. I&amp;#39;m going to try that before the other Llama fine-tunes.&lt;/p&gt;\\n\\n&lt;p&gt;I do feel like Llama-3.1 was peak open-source LLM versatility. It&amp;#39;s been my workhorse model for too long and I&amp;#39;m planning to switch to Qwen eventually.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm9012","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm9012/i_keep_returning_to_llama318b/n06bond/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751076994,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n07muo4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"loadsamuny","can_mod_post":false,"created_utc":1751100479,"send_replies":true,"parent_id":"t1_n067pi6","score":2,"author_fullname":"t2_10p7p3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"nemo is good at consistency üëç","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n07muo4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;nemo is good at consistency üëç&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm9012","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm9012/i_keep_returning_to_llama318b/n07muo4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751100479,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n067pi6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"My_Unbiased_Opinion","can_mod_post":false,"created_utc":1751075472,"send_replies":true,"parent_id":"t3_1lm9012","score":19,"author_fullname":"t2_esiyl0yb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Llama models have this thing about them where they are just a breeze to work with. They arnt so focused on maxing benchmarks. It's why I like Mistral so much as well. Same philosophy.¬†\\n\\n\\nHave you tried one of the newer Mistral 12B models like Mistral nemo?\\n\\n\\nAlso, check out NeuralDaredevil-abliterated 8B as well. That model hits hard for an 8B Llama finetune.¬†","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n067pi6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Llama models have this thing about them where they are just a breeze to work with. They arnt so focused on maxing benchmarks. It&amp;#39;s why I like Mistral so much as well. Same philosophy.¬†&lt;/p&gt;\\n\\n&lt;p&gt;Have you tried one of the newer Mistral 12B models like Mistral nemo?&lt;/p&gt;\\n\\n&lt;p&gt;Also, check out NeuralDaredevil-abliterated 8B as well. That model hits hard for an 8B Llama finetune.¬†&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm9012/i_keep_returning_to_llama318b/n067pi6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751075472,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lm9012","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":19}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n07s1ff","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"created_utc":1751103599,"send_replies":true,"parent_id":"t1_n079652","score":1,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I did indeed discount Qwen 2.5, going to add it to my list.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n07s1ff","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I did indeed discount Qwen 2.5, going to add it to my list.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm9012","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm9012/i_keep_returning_to_llama318b/n07s1ff/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751103599,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n079652","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Mushoz","can_mod_post":false,"created_utc":1751092619,"send_replies":true,"parent_id":"t3_1lm9012","score":3,"author_fullname":"t2_gwpq7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Don't discount Qwen2.5. It's often easier to finetune than Qwen3.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n079652","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Don&amp;#39;t discount Qwen2.5. It&amp;#39;s often easier to finetune than Qwen3.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm9012/i_keep_returning_to_llama318b/n079652/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751092619,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lm9012","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n07ad12","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Top_Extent_765","can_mod_post":false,"created_utc":1751093262,"send_replies":true,"parent_id":"t3_1lm9012","score":3,"author_fullname":"t2_1gifxs174a","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Try gemma3 12b, we were surprised recently. Or even the new 3n, didn‚Äôt try it yet though","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n07ad12","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Try gemma3 12b, we were surprised recently. Or even the new 3n, didn‚Äôt try it yet though&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm9012/i_keep_returning_to_llama318b/n07ad12/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751093262,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lm9012","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n08nihq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"randomfoo2","can_mod_post":false,"created_utc":1751118218,"send_replies":true,"parent_id":"t3_1lm9012","score":3,"author_fullname":"t2_eztox","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you are fine-tuning Qwen 3, be sure to modify the chat\\\\_template so that you are using a nothink (empty think tags with proper line breaks) for training and output. In my recent testing I found it makes a huge difference in task performance.\\n\\nAs others have mentioned, the Mistral models are worth trying (Ministral, Nemo) although if you're going to 12B class check out Phi4 14B as well.\\n\\nOne thing you should definitely try is Unsloth. It can do FFT but it can reduce memory usage and increase tuning speed by a fair amount so for a single GPU use case it should be quite a bit better than TRL. You can also check out Axolotl which has similar optimizations - big ones include using Liger, support for 8 bit/4bit AdamW optimizer (much less memory usage, basically no quality difference) and gradient checkpointing. If necessary you can use DeepSpeed ZeRO 3 w/ optimizer/gradient offload (or paged\\\\_adamw\\\\_8bit might be good enough) for speed hits. Also using accelerate (Transformer Engine) you may be able to leverage FP8 mixed precision training as well.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n08nihq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you are fine-tuning Qwen 3, be sure to modify the chat_template so that you are using a nothink (empty think tags with proper line breaks) for training and output. In my recent testing I found it makes a huge difference in task performance.&lt;/p&gt;\\n\\n&lt;p&gt;As others have mentioned, the Mistral models are worth trying (Ministral, Nemo) although if you&amp;#39;re going to 12B class check out Phi4 14B as well.&lt;/p&gt;\\n\\n&lt;p&gt;One thing you should definitely try is Unsloth. It can do FFT but it can reduce memory usage and increase tuning speed by a fair amount so for a single GPU use case it should be quite a bit better than TRL. You can also check out Axolotl which has similar optimizations - big ones include using Liger, support for 8 bit/4bit AdamW optimizer (much less memory usage, basically no quality difference) and gradient checkpointing. If necessary you can use DeepSpeed ZeRO 3 w/ optimizer/gradient offload (or paged_adamw_8bit might be good enough) for speed hits. Also using accelerate (Transformer Engine) you may be able to leverage FP8 mixed precision training as well.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm9012/i_keep_returning_to_llama318b/n08nihq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751118218,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lm9012","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n05r2q8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jacek2023","can_mod_post":false,"send_replies":true,"parent_id":"t1_n05quxn","score":3,"author_fullname":"t2_vqgbql9w","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"if I remember correctly they used Mistral as a base, that make sense, because Mistral is from Europe :)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n05r2q8","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;if I remember correctly they used Mistral as a base, that make sense, because Mistral is from Europe :)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm9012","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm9012/i_keep_returning_to_llama318b/n05r2q8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751069271,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1751069271,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n05quxn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"created_utc":1751069193,"send_replies":true,"parent_id":"t1_n05qchz","score":1,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks, going to try this.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n05quxn","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks, going to try this.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm9012","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm9012/i_keep_returning_to_llama318b/n05quxn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751069193,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n05qchz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jacek2023","can_mod_post":false,"created_utc":1751069004,"send_replies":true,"parent_id":"t3_1lm9012","score":2,"author_fullname":"t2_vqgbql9w","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"look at Bielik","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n05qchz","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;look at Bielik&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm9012/i_keep_returning_to_llama318b/n05qchz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751069004,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lm9012","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0601bx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"created_utc":1751072575,"send_replies":true,"parent_id":"t1_n05x7y5","score":1,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah things are different on fine-tuning workloads, it's a less well benchmarked setup.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0601bx","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah things are different on fine-tuning workloads, it&amp;#39;s a less well benchmarked setup.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm9012","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm9012/i_keep_returning_to_llama318b/n0601bx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751072575,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n05x7y5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MengerianMango","can_mod_post":false,"created_utc":1751071533,"send_replies":true,"parent_id":"t3_1lm9012","score":2,"author_fullname":"t2_mcvyi","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Qwen models and deepseek distills give odd results for me on programmatic tasks. I used those and llama/mistral/phi for a quantitative sentiment analysis task. The latter 3 had high correlation with gpt. Qwen and deepseek distills had near 0 correlation.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n05x7y5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen models and deepseek distills give odd results for me on programmatic tasks. I used those and llama/mistral/phi for a quantitative sentiment analysis task. The latter 3 had high correlation with gpt. Qwen and deepseek distills had near 0 correlation.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm9012/i_keep_returning_to_llama318b/n05x7y5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751071533,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lm9012","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n06qo0q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"oldschooldaw","can_mod_post":false,"created_utc":1751083282,"send_replies":true,"parent_id":"t3_1lm9012","score":2,"author_fullname":"t2_9jna1rq","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I too really love llama 3.1 8b for specific tasks. Some I have been able to offhand to Gemma 3 4b, others I have to keep on llama because Gemma is trying to be too helpful and in doing so poisons the output with its suggestions. Honestly I don‚Äôt know if there‚Äôs any other strict replacement for 3.1, it just works.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n06qo0q","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I too really love llama 3.1 8b for specific tasks. Some I have been able to offhand to Gemma 3 4b, others I have to keep on llama because Gemma is trying to be too helpful and in doing so poisons the output with its suggestions. Honestly I don‚Äôt know if there‚Äôs any other strict replacement for 3.1, it just works.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm9012/i_keep_returning_to_llama318b/n06qo0q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751083282,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lm9012","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0abcbc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AdministrationOk9523","can_mod_post":false,"created_utc":1751137230,"send_replies":true,"parent_id":"t3_1lm9012","score":2,"author_fullname":"t2_7rnqz97x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"OpenEuroLLM series covers most of the EU languages and is based on the Gemma 3 12b model. I believe it could be useful to you.\\n\\nIt is licensed as CC BY-NC-SA 4.0.\\n\\nAlso, Aya Expanse is quite nice if you don't mind the non-commercial license.\\n\\nOtherwise, just stick with Gemma 3; it is really nice in multilingual stuff.\\n\\nMistral-small or Phi could also yield usable results. Good luck!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0abcbc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;OpenEuroLLM series covers most of the EU languages and is based on the Gemma 3 12b model. I believe it could be useful to you.&lt;/p&gt;\\n\\n&lt;p&gt;It is licensed as CC BY-NC-SA 4.0.&lt;/p&gt;\\n\\n&lt;p&gt;Also, Aya Expanse is quite nice if you don&amp;#39;t mind the non-commercial license.&lt;/p&gt;\\n\\n&lt;p&gt;Otherwise, just stick with Gemma 3; it is really nice in multilingual stuff.&lt;/p&gt;\\n\\n&lt;p&gt;Mistral-small or Phi could also yield usable results. Good luck!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm9012/i_keep_returning_to_llama318b/n0abcbc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751137230,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lm9012","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0dcyxq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"liquid_bee_3","can_mod_post":false,"created_utc":1751180537,"send_replies":true,"parent_id":"t3_1lm9012","score":2,"author_fullname":"t2_18ijyp2nfl","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"ive done so many things with this model training wise. its prob the hardest model to tune but gets the best results for me as well.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0dcyxq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;ive done so many things with this model training wise. its prob the hardest model to tune but gets the best results for me as well.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm9012/i_keep_returning_to_llama318b/n0dcyxq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751180537,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lm9012","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ccbyd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Rich_Artist_8327","can_mod_post":false,"created_utc":1751163088,"send_replies":true,"parent_id":"t3_1lm9012","score":1,"author_fullname":"t2_1jk2ep8a52","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Depends of the language. If its Finnish then poro2 beats gemma3","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ccbyd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Depends of the language. If its Finnish then poro2 beats gemma3&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm9012/i_keep_returning_to_llama318b/n0ccbyd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751163088,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lm9012","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0df27j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"dimkaNORD","can_mod_post":false,"created_utc":1751181757,"send_replies":true,"parent_id":"t3_1lm9012","score":1,"author_fullname":"t2_2lt7nkof","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"1. Gemma3n (e4b or maybe e2b) ‚Äî it's a newest model... I try it and it's a brilliant!\\n2. Phi4-mini ‚Äî it's another good choice, I think.\\n\\nGood luck! :)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0df27j","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;ol&gt;\\n&lt;li&gt;Gemma3n (e4b or maybe e2b) ‚Äî it&amp;#39;s a newest model... I try it and it&amp;#39;s a brilliant!&lt;/li&gt;\\n&lt;li&gt;Phi4-mini ‚Äî it&amp;#39;s another good choice, I think.&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;Good luck! :)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm9012/i_keep_returning_to_llama318b/n0df27j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751181757,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lm9012","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),o=()=>e.jsx(l,{data:a});export{o as default};
