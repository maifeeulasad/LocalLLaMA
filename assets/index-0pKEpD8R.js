const e=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Just trying to get some ideas from actual people ( already went the AI route ) for what to get...\\n\\nI have a Gigabyte M32 AR3 a 7xx2 64 core cpu, requisite ram, and PSU.\\n\\nThe above budget is strictly for GPUs and can be up to $5500 or more if the best suggestion is to just wait.\\n\\nUse cases mostly involve fine tuning and / or training smaller specialized models, mostly for breaking down and outlining technical documents. \\n\\nI would go the cloud route but we are looking at 500+ pages, possibly needing OCR ( or similar ), some layout retention, up to 40 individual sections in each and doing ~100 a week.\\n\\nI am looking for recommendations on GPUs mostly and what would be an effective rig I could build.\\n\\nYes I priced the cloud and yes I think it will be more cost effective to build this in-house, rather than go pure cloud rental.\\n\\nThe above is the primary driver, it would be cool to integrate web search and other things into the system, and I am not really 100% sure what it will look like, tbh it is quite overwhelming with so many options and everything that is out there.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"$5k budget for Local AI","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1louk6a","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.67,"author_flair_background_color":null,"subreddit_type":"public","ups":4,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_r783n0ey","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":4,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751351301,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Just trying to get some ideas from actual people ( already went the AI route ) for what to get...&lt;/p&gt;\\n\\n&lt;p&gt;I have a Gigabyte M32 AR3 a 7xx2 64 core cpu, requisite ram, and PSU.&lt;/p&gt;\\n\\n&lt;p&gt;The above budget is strictly for GPUs and can be up to $5500 or more if the best suggestion is to just wait.&lt;/p&gt;\\n\\n&lt;p&gt;Use cases mostly involve fine tuning and / or training smaller specialized models, mostly for breaking down and outlining technical documents. &lt;/p&gt;\\n\\n&lt;p&gt;I would go the cloud route but we are looking at 500+ pages, possibly needing OCR ( or similar ), some layout retention, up to 40 individual sections in each and doing ~100 a week.&lt;/p&gt;\\n\\n&lt;p&gt;I am looking for recommendations on GPUs mostly and what would be an effective rig I could build.&lt;/p&gt;\\n\\n&lt;p&gt;Yes I priced the cloud and yes I think it will be more cost effective to build this in-house, rather than go pure cloud rental.&lt;/p&gt;\\n\\n&lt;p&gt;The above is the primary driver, it would be cool to integrate web search and other things into the system, and I am not really 100% sure what it will look like, tbh it is quite overwhelming with so many options and everything that is out there.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1louk6a","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Unlikely_Track_5154","discussion_type":null,"num_comments":38,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1louk6a/5k_budget_for_local_ai/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1louk6a/5k_budget_for_local_ai/","subreddit_subscribers":493458,"created_utc":1751351301,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0pyyv3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DeltaSqueezer","can_mod_post":false,"created_utc":1751355157,"send_replies":true,"parent_id":"t3_1louk6a","score":10,"author_fullname":"t2_8jqx3m14","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Just test on cloud GPUs and then decide. I don't think you can even buy an A100 for $5500.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0pyyv3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just test on cloud GPUs and then decide. I don&amp;#39;t think you can even buy an A100 for $5500.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1louk6a/5k_budget_for_local_ai/n0pyyv3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751355157,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1louk6a","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0szyhs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CrescendollsFan","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0sycys","score":1,"author_fullname":"t2_17ipi9avq8","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah, Daniel Han-Chen is a math genius. They must have so many offer to acquire them with huge amounts of cash. I bet everyone is after him and his brother right now.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0szyhs","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, Daniel Han-Chen is a math genius. They must have so many offer to acquire them with huge amounts of cash. I bet everyone is after him and his brother right now.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1louk6a","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1louk6a/5k_budget_for_local_ai/n0szyhs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751394923,"author_flair_text":null,"treatment_tags":[],"created_utc":1751394923,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0sycys","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Afternoon_4260","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0sxg9g","score":1,"author_fullname":"t2_cj9kap4bx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Wow they did optimise a few things","edited":false,"author_flair_css_class":null,"name":"t1_n0sycys","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Wow they did optimise a few things&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1louk6a","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1louk6a/5k_budget_for_local_ai/n0sycys/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751394468,"author_flair_text":"llama.cpp","collapsed":false,"created_utc":1751394468,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0sxg9g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CrescendollsFan","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0rwyp0","score":1,"author_fullname":"t2_17ipi9avq8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"https://github.com/unslothai/notebooks","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0sxg9g","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://github.com/unslothai/notebooks\\"&gt;https://github.com/unslothai/notebooks&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1louk6a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1louk6a/5k_budget_for_local_ai/n0sxg9g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751394214,"author_flair_text":null,"treatment_tags":[],"created_utc":1751394214,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0rwyp0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Afternoon_4260","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0r3fy8","score":1,"author_fullname":"t2_cj9kap4bx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"To finetune what on a colab t4?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0rwyp0","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;To finetune what on a colab t4?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1louk6a","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1louk6a/5k_budget_for_local_ai/n0rwyp0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751384169,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1751384169,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0r3fy8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CrescendollsFan","can_mod_post":false,"created_utc":1751375225,"send_replies":true,"parent_id":"t1_n0q3bf3","score":1,"author_fullname":"t2_17ipi9avq8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"\\\\&gt; Neither will be enough for tuning/training.\\n\\nHave you looked at what is possible with unsloth? the optimizations they have made make it quite viable to finetune on a free tier google colab t4","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0r3fy8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&amp;gt; Neither will be enough for tuning/training.&lt;/p&gt;\\n\\n&lt;p&gt;Have you looked at what is possible with unsloth? the optimizations they have made make it quite viable to finetune on a free tier google colab t4&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1louk6a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1louk6a/5k_budget_for_local_ai/n0r3fy8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751375225,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0vh73u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Technical_Bar_1908","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0vfj0o","score":1,"author_fullname":"t2_f8arhws9t","approved_by":null,"mod_note":null,"all_awardings":[],"body":"PS I took $160 out of the ATM at the club last night and hit 4 majors on the pokies and walked after 3 hr session on $4800. So have my 4tb Samsung pcie4 to add and jumping on my second 5080 today. Xtreme waterforce ofc. Some of my interest is ECDSA so the dual 5080 is probably better for me than a 5090 as it enables parallel processing","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0vh73u","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;PS I took $160 out of the ATM at the club last night and hit 4 majors on the pokies and walked after 3 hr session on $4800. So have my 4tb Samsung pcie4 to add and jumping on my second 5080 today. Xtreme waterforce ofc. Some of my interest is ECDSA so the dual 5080 is probably better for me than a 5090 as it enables parallel processing&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1louk6a","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1louk6a/5k_budget_for_local_ai/n0vh73u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751423218,"author_flair_text":null,"treatment_tags":[],"created_utc":1751423218,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0vfj0o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Technical_Bar_1908","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0uxvi2","score":1,"author_fullname":"t2_f8arhws9t","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"They have some listed as nvlink that have only one socket of the the two populated with hardware","edited":false,"author_flair_css_class":null,"name":"t1_n0vfj0o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They have some listed as nvlink that have only one socket of the the two populated with hardware&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1louk6a","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1louk6a/5k_budget_for_local_ai/n0vfj0o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751422635,"author_flair_text":null,"collapsed":false,"created_utc":1751422635,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0uxvi2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Unlikely_Track_5154","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0sdxpu","score":1,"author_fullname":"t2_r783n0ey","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What about them looks like shit?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0uxvi2","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What about them looks like shit?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1louk6a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1louk6a/5k_budget_for_local_ai/n0uxvi2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751416420,"author_flair_text":null,"treatment_tags":[],"created_utc":1751416420,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0sdxpu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Technical_Bar_1908","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0qbpqc","score":2,"author_fullname":"t2_f8arhws9t","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Same. But half the adapter boards look like shit. I wonder if maybe on one of the Facebook hardware selling groups or even on Reddit might be able to organise some kind of way to do a group buy of some dope hardware for some enthusiasts.\\n\\nI already have AI Top x870 and a 5080 but would love to add a trx50 AI Top with a 7960/70 with four  16/32 GB nvlinked hbm2 SXME2 v100's on risers. I'm pretty sure I can even run it off my current build. But with the way pcie5 lanes are allocated I think bifurcation on my am5 looks like this pcie5 x8 &gt; pcie4 x4 x 2 &gt; pcie3 x16 x 2 &gt; 2 sxme2 X2 + 1300w psu and on eBay that would cost me under $2000 without buying pre adapted pcie GPU's.\\n\\nBut my current build is $6000aud already with the PNY OC 5080, 9900x, 128gb TForce @6000, 4tb 9100 pro, Gigabyte Aorus Xtreme AI Top x870 (used from auction from Israeli store ksmtop on eBay roughly 30% retail, ex display, damaged heatsink clip confirmed working suits my purpose with pcie risers and ssd heatsinks) , 1300w PSU, InWin Dubuli Gold.\\n\\nOptions from here for me is to spend another $5000aud on a second 5080 or buy the smxe2 set up or buy a TR CPU and board + a 3090 MAYBE for $6000 and still have to work towards the built iteratively as I can afford the rest of the components and make do with the 5080 + 3090 on these x870 until it's finished.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0sdxpu","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Same. But half the adapter boards look like shit. I wonder if maybe on one of the Facebook hardware selling groups or even on Reddit might be able to organise some kind of way to do a group buy of some dope hardware for some enthusiasts.&lt;/p&gt;\\n\\n&lt;p&gt;I already have AI Top x870 and a 5080 but would love to add a trx50 AI Top with a 7960/70 with four  16/32 GB nvlinked hbm2 SXME2 v100&amp;#39;s on risers. I&amp;#39;m pretty sure I can even run it off my current build. But with the way pcie5 lanes are allocated I think bifurcation on my am5 looks like this pcie5 x8 &amp;gt; pcie4 x4 x 2 &amp;gt; pcie3 x16 x 2 &amp;gt; 2 sxme2 X2 + 1300w psu and on eBay that would cost me under $2000 without buying pre adapted pcie GPU&amp;#39;s.&lt;/p&gt;\\n\\n&lt;p&gt;But my current build is $6000aud already with the PNY OC 5080, 9900x, 128gb TForce @6000, 4tb 9100 pro, Gigabyte Aorus Xtreme AI Top x870 (used from auction from Israeli store ksmtop on eBay roughly 30% retail, ex display, damaged heatsink clip confirmed working suits my purpose with pcie risers and ssd heatsinks) , 1300w PSU, InWin Dubuli Gold.&lt;/p&gt;\\n\\n&lt;p&gt;Options from here for me is to spend another $5000aud on a second 5080 or buy the smxe2 set up or buy a TR CPU and board + a 3090 MAYBE for $6000 and still have to work towards the built iteratively as I can afford the rest of the components and make do with the 5080 + 3090 on these x870 until it&amp;#39;s finished.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1louk6a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1louk6a/5k_budget_for_local_ai/n0sdxpu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751388905,"author_flair_text":null,"treatment_tags":[],"created_utc":1751388905,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0szamv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Unlikely_Track_5154","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0qi2g0","score":1,"author_fullname":"t2_r783n0ey","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What does 8.6 get me that the other things don't?\\n\\nI understand it is a prehistoric card, 32gb of vram for that price = low demand plus ancient technology.\\n\\nI am a window shopper right now, a tire kicker if you will.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0szamv","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What does 8.6 get me that the other things don&amp;#39;t?&lt;/p&gt;\\n\\n&lt;p&gt;I understand it is a prehistoric card, 32gb of vram for that price = low demand plus ancient technology.&lt;/p&gt;\\n\\n&lt;p&gt;I am a window shopper right now, a tire kicker if you will.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1louk6a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1louk6a/5k_budget_for_local_ai/n0szamv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751394734,"author_flair_text":null,"treatment_tags":[],"created_utc":1751394734,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0qi2g0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MelodicRecognition7","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0qbpqc","score":1,"author_fullname":"t2_1eex9ug5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"do not even think about V100, it is a prehistoric card. Check here: https://developer.nvidia.com/cuda-gpus you need Compute Capability 8.6 and above.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0qi2g0","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;do not even think about V100, it is a prehistoric card. Check here: &lt;a href=\\"https://developer.nvidia.com/cuda-gpus\\"&gt;https://developer.nvidia.com/cuda-gpus&lt;/a&gt; you need Compute Capability 8.6 and above.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1louk6a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1louk6a/5k_budget_for_local_ai/n0qi2g0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751366363,"author_flair_text":null,"treatment_tags":[],"created_utc":1751366363,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0t0f0j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Unlikely_Track_5154","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0shlu0","score":2,"author_fullname":"t2_r783n0ey","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I have ChatGPT pro, one of the first things I did was make a token counting / message database system, so I have 10s of thousands of messages specifically related to what I plan to do with local.\\n\\nThis isn't a spur of the moment thing, I have been planning and saving etc, but I never really paused to look at specifically GPUs, so now I am at that stage, and I need some help ( most of which the nice people of localllama can't provide, but I think yall got me on the GPUs).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0t0f0j","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have ChatGPT pro, one of the first things I did was make a token counting / message database system, so I have 10s of thousands of messages specifically related to what I plan to do with local.&lt;/p&gt;\\n\\n&lt;p&gt;This isn&amp;#39;t a spur of the moment thing, I have been planning and saving etc, but I never really paused to look at specifically GPUs, so now I am at that stage, and I need some help ( most of which the nice people of localllama can&amp;#39;t provide, but I think yall got me on the GPUs).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1louk6a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1louk6a/5k_budget_for_local_ai/n0t0f0j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751395052,"author_flair_text":null,"treatment_tags":[],"created_utc":1751395052,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0shlu0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Massive-Question-550","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0qbpqc","score":1,"author_fullname":"t2_72xxv3wb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"If your usage really is that heavy then maybe a home setup can be worth it but I would first test what your workload actually is for a month just to get an idea of real world cost and then go from there.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0shlu0","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If your usage really is that heavy then maybe a home setup can be worth it but I would first test what your workload actually is for a month just to get an idea of real world cost and then go from there.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1louk6a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1louk6a/5k_budget_for_local_ai/n0shlu0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751389899,"author_flair_text":null,"treatment_tags":[],"created_utc":1751389899,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0qbpqc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Unlikely_Track_5154","can_mod_post":false,"created_utc":1751362878,"send_replies":true,"parent_id":"t1_n0q3bf3","score":0,"author_fullname":"t2_r783n0ey","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Idk that is why I am asking.\\n\\nIt is probably like 60 / week plus data transfer at $4 / gpu hr, and then I am pretty sure gpt4.1 / gemini whatever / others are going to be around 60 to 100 a week, inference only.\\n\\nI was looking at v100 maybe some amd type cards, idk though I am just kind of gathering ideas here. I am not committed to any path yet, other than I have a server board and ram and all that stuff that I use for other stuff, and I can repurpose it to this or maybe even extend it into this.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0qbpqc","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Idk that is why I am asking.&lt;/p&gt;\\n\\n&lt;p&gt;It is probably like 60 / week plus data transfer at $4 / gpu hr, and then I am pretty sure gpt4.1 / gemini whatever / others are going to be around 60 to 100 a week, inference only.&lt;/p&gt;\\n\\n&lt;p&gt;I was looking at v100 maybe some amd type cards, idk though I am just kind of gathering ideas here. I am not committed to any path yet, other than I have a server board and ram and all that stuff that I use for other stuff, and I can repurpose it to this or maybe even extend it into this.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1louk6a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1louk6a/5k_budget_for_local_ai/n0qbpqc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751362878,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n0q3bf3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MelodicRecognition7","can_mod_post":false,"created_utc":1751357791,"send_replies":true,"parent_id":"t3_1louk6a","score":7,"author_fullname":"t2_1eex9ug5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think you've done your math wrong, there is a very low chance that a local build will be cheaper than the cloud. Finetuning at home is also very unlikely, you need hundreds of gigabytes of VRAM for that, and for just $5k budget you could get only 64 GB new or 96 GB used hardware.\\n\\nAnyway if you insist then for 5k you could buy either a used \\"6000 Ada\\" (not to be confused with \\"A6000\\") or try to catch a new RTX Pro 5000 before scalpers do, or get 2x new 5090, or 4x used 3090 if you enjoy messing with the hardware.  Or 2x chinese modded 4090 48GB if you are feeling lucky.\\n\\nNeither will be enough for tuning/training.","edited":1751358501,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0q3bf3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think you&amp;#39;ve done your math wrong, there is a very low chance that a local build will be cheaper than the cloud. Finetuning at home is also very unlikely, you need hundreds of gigabytes of VRAM for that, and for just $5k budget you could get only 64 GB new or 96 GB used hardware.&lt;/p&gt;\\n\\n&lt;p&gt;Anyway if you insist then for 5k you could buy either a used &amp;quot;6000 Ada&amp;quot; (not to be confused with &amp;quot;A6000&amp;quot;) or try to catch a new RTX Pro 5000 before scalpers do, or get 2x new 5090, or 4x used 3090 if you enjoy messing with the hardware.  Or 2x chinese modded 4090 48GB if you are feeling lucky.&lt;/p&gt;\\n\\n&lt;p&gt;Neither will be enough for tuning/training.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1louk6a/5k_budget_for_local_ai/n0q3bf3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751357791,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1louk6a","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0q95dg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SuperSimpSons","can_mod_post":false,"created_utc":1751361347,"send_replies":true,"parent_id":"t3_1louk6a","score":2,"author_fullname":"t2_12s3sunmlo","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Since you already use Gigabyte, how about a pre-built home server from them, save yourself the hassle. Last year iirc they launched something called AI TOP www.gigabyte.com/Consumer/AI-TOP/?lan=en that's a desktop PC designed for local AI fine-tuning, basically right up your alley. Might make a nifty gift for yourself.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0q95dg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Since you already use Gigabyte, how about a pre-built home server from them, save yourself the hassle. Last year iirc they launched something called AI TOP &lt;a href=\\"http://www.gigabyte.com/Consumer/AI-TOP/?lan=en\\"&gt;www.gigabyte.com/Consumer/AI-TOP/?lan=en&lt;/a&gt; that&amp;#39;s a desktop PC designed for local AI fine-tuning, basically right up your alley. Might make a nifty gift for yourself.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1louk6a/5k_budget_for_local_ai/n0q95dg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751361347,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1louk6a","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0s3x96","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CorpusculantCortex","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0qkel5","score":1,"author_fullname":"t2_a6hf5ncx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Damn, okay thank you!! Guess I will need to find a practical use case for this to justify some costs","edited":false,"author_flair_css_class":null,"name":"t1_n0s3x96","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Damn, okay thank you!! Guess I will need to find a practical use case for this to justify some costs&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1louk6a","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1louk6a/5k_budget_for_local_ai/n0s3x96/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751386091,"author_flair_text":null,"collapsed":false,"created_utc":1751386091,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0qkel5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok_Appearance3584","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0qiy9o","score":1,"author_fullname":"t2_oyxj85n1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Depends on how big the model you're training, how big the context of the dataset is and batch size.\\n\\n\\nFor example, I full finetuned 1B model with about 2k context length with a low batch size on an A100 for about 8 hours and I got maybe 100k steps. The dataset was about 300k steps I think.\\n\\n\\nSo you need a lot of time. On the other hand, I did Llama 3.1 8B QLoRa finetuning with unsloth on T4, pretty low rank, with a similar dataset and it took a couple days I think.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0qkel5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Depends on how big the model you&amp;#39;re training, how big the context of the dataset is and batch size.&lt;/p&gt;\\n\\n&lt;p&gt;For example, I full finetuned 1B model with about 2k context length with a low batch size on an A100 for about 8 hours and I got maybe 100k steps. The dataset was about 300k steps I think.&lt;/p&gt;\\n\\n&lt;p&gt;So you need a lot of time. On the other hand, I did Llama 3.1 8B QLoRa finetuning with unsloth on T4, pretty low rank, with a similar dataset and it took a couple days I think.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1louk6a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1louk6a/5k_budget_for_local_ai/n0qkel5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751367541,"author_flair_text":null,"treatment_tags":[],"created_utc":1751367541,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0qiy9o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CorpusculantCortex","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0qeats","score":1,"author_fullname":"t2_a6hf5ncx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Can I ask a stupid unrelated question as someone who has never finetuned? How long does it take? I see the /hrs pricing, but i am curious what that translates into in an absolute cost sense. I recognize this is undoubtedly dependent on a lot, bit even just one example. Im just curious what it would look like in terms of cloud costs for this.\\n\\n(I am not op, I am just interested in finetuning and curious if it is beyond my hobby budget or not to explore as a novice)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0qiy9o","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Can I ask a stupid unrelated question as someone who has never finetuned? How long does it take? I see the /hrs pricing, but i am curious what that translates into in an absolute cost sense. I recognize this is undoubtedly dependent on a lot, bit even just one example. Im just curious what it would look like in terms of cloud costs for this.&lt;/p&gt;\\n\\n&lt;p&gt;(I am not op, I am just interested in finetuning and curious if it is beyond my hobby budget or not to explore as a novice)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1louk6a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1louk6a/5k_budget_for_local_ai/n0qiy9o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751366819,"author_flair_text":null,"treatment_tags":[],"created_utc":1751366819,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0qvquo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Unlikely_Track_5154","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0qeats","score":1,"author_fullname":"t2_r783n0ey","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I have GPT Pro, the first thing i did was make a token counter / message database and i have 10s of thousands of messages recorded for what i want to do. I am pretty sure I will be somewhere around $100 per week if I use gpt 4.1 / whatever mid range model OAI is offering.\\n\\nThe other ones I do not know about like Gemini etc.\\n\\nI appreciate your help.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0qvquo","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have GPT Pro, the first thing i did was make a token counter / message database and i have 10s of thousands of messages recorded for what i want to do. I am pretty sure I will be somewhere around $100 per week if I use gpt 4.1 / whatever mid range model OAI is offering.&lt;/p&gt;\\n\\n&lt;p&gt;The other ones I do not know about like Gemini etc.&lt;/p&gt;\\n\\n&lt;p&gt;I appreciate your help.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1louk6a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1louk6a/5k_budget_for_local_ai/n0qvquo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751372419,"author_flair_text":null,"treatment_tags":[],"created_utc":1751372419,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0qeats","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Azuriteh","can_mod_post":false,"created_utc":1751364335,"send_replies":true,"parent_id":"t1_n0qdh8u","score":1,"author_fullname":"t2_wmv41","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Also, maybe take a look at using API solutions for OCR for let's say Gemma 3, which are an order of magnitude inferior in cost compared to the main contenders like Gemini Flash 2.5:  \\n[https://openrouter.ai/google/gemma-3-27b-it](https://openrouter.ai/google/gemma-3-27b-it)\\n\\nI'd recommend you to test these models for a month and see how much do you spend and see if it's worth it... and if you see that it's not worth it completely but you still want to play around... get 2x RTX 3090 and call it a day.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0qeats","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Also, maybe take a look at using API solutions for OCR for let&amp;#39;s say Gemma 3, which are an order of magnitude inferior in cost compared to the main contenders like Gemini Flash 2.5:&lt;br/&gt;\\n&lt;a href=\\"https://openrouter.ai/google/gemma-3-27b-it\\"&gt;https://openrouter.ai/google/gemma-3-27b-it&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;d recommend you to test these models for a month and see how much do you spend and see if it&amp;#39;s worth it... and if you see that it&amp;#39;s not worth it completely but you still want to play around... get 2x RTX 3090 and call it a day.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1louk6a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1louk6a/5k_budget_for_local_ai/n0qeats/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751364335,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0qdh8u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Azuriteh","can_mod_post":false,"created_utc":1751363874,"send_replies":true,"parent_id":"t3_1louk6a","score":2,"author_fullname":"t2_wmv41","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think you should switch your approach here. If it's only for serving then I can definitely see the benefit of a custom rig. For your budget the big-VRAM GPUs will be out of question, but you can definitely get a few RTX 3090 cards which I think are the best deal right now for inference.\\n\\n  \\nAs for fine-tuning, you'll need to rent on the cloud, there's no other reliable way. For my projects I always use Unsloth, with QLoRa and a small dataset you might be able to fine-tune a 32b model in your local setup but it'll be extremely limited (&amp; they only support single-gpu systems), but for $1/hr you can easily rent an A100 GPU on specific providers like TensorDock... or if you get lucky you might catch a $1.5/hr B200 GPU that has 180GB of VRAM (with that much VRAM you can full fine-tune a 27b model like Gemma 3 with a modest dataset).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0qdh8u","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think you should switch your approach here. If it&amp;#39;s only for serving then I can definitely see the benefit of a custom rig. For your budget the big-VRAM GPUs will be out of question, but you can definitely get a few RTX 3090 cards which I think are the best deal right now for inference.&lt;/p&gt;\\n\\n&lt;p&gt;As for fine-tuning, you&amp;#39;ll need to rent on the cloud, there&amp;#39;s no other reliable way. For my projects I always use Unsloth, with QLoRa and a small dataset you might be able to fine-tune a 32b model in your local setup but it&amp;#39;ll be extremely limited (&amp;amp; they only support single-gpu systems), but for $1/hr you can easily rent an A100 GPU on specific providers like TensorDock... or if you get lucky you might catch a $1.5/hr B200 GPU that has 180GB of VRAM (with that much VRAM you can full fine-tune a 27b model like Gemma 3 with a modest dataset).&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1louk6a/5k_budget_for_local_ai/n0qdh8u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751363874,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1louk6a","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0qv7ac","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Unlikely_Track_5154","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0quidt","score":1,"author_fullname":"t2_r783n0ey","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thank you for your help.\\n\\nI was talking to another guy in here, and he said it would probably be more effective to rent cloud gpu for the training portion.\\n\\nIs that what you were referring to, and when I say training, I mean training and / or fine tuning.\\n\\nHe made it seem like you had to upload all of the training data at once, which I was under the impression that you could slowly feed the model the training set, is that accurate?","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0qv7ac","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you for your help.&lt;/p&gt;\\n\\n&lt;p&gt;I was talking to another guy in here, and he said it would probably be more effective to rent cloud gpu for the training portion.&lt;/p&gt;\\n\\n&lt;p&gt;Is that what you were referring to, and when I say training, I mean training and / or fine tuning.&lt;/p&gt;\\n\\n&lt;p&gt;He made it seem like you had to upload all of the training data at once, which I was under the impression that you could slowly feed the model the training set, is that accurate?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1louk6a","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1louk6a/5k_budget_for_local_ai/n0qv7ac/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751372208,"author_flair_text":null,"treatment_tags":[],"created_utc":1751372208,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0quidt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0qc069","score":2,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Then just buy 2 of these 4090D 48gb for $5200 total:\\n\\nhttps://www.alibaba.com/x/B00hA7","edited":false,"author_flair_css_class":null,"name":"t1_n0quidt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Then just buy 2 of these 4090D 48gb for $5200 total:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.alibaba.com/x/B00hA7\\"&gt;https://www.alibaba.com/x/B00hA7&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1louk6a","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1louk6a/5k_budget_for_local_ai/n0quidt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751371936,"author_flair_text":null,"collapsed":false,"created_utc":1751371936,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0qc069","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Unlikely_Track_5154","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0q2xz6","score":1,"author_fullname":"t2_r783n0ey","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I am not worried about the Chinese, what are they going to do, steal my publicly available data and reverse engineer my super simple idea.\\n\\nWhatever they can have it, I don't think it will be the next big thing anywhere, ever.\\n\\nAs far as the cards go idk, that is why I am asking.\\n\\nThe nvidia xx90s did not seem to be the value oriented play at least to me.\\n\\nThe 4080 does look interesting for the inference side, but not the training side, (imo ,which is about worthless mind you ).\\n\\nOther than that I was looking at v100 maybe some amd type cards, 3090s always apparently...\\n\\nYeah, idk like I said it is overwhelming tbh.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0qc069","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I am not worried about the Chinese, what are they going to do, steal my publicly available data and reverse engineer my super simple idea.&lt;/p&gt;\\n\\n&lt;p&gt;Whatever they can have it, I don&amp;#39;t think it will be the next big thing anywhere, ever.&lt;/p&gt;\\n\\n&lt;p&gt;As far as the cards go idk, that is why I am asking.&lt;/p&gt;\\n\\n&lt;p&gt;The nvidia xx90s did not seem to be the value oriented play at least to me.&lt;/p&gt;\\n\\n&lt;p&gt;The 4080 does look interesting for the inference side, but not the training side, (imo ,which is about worthless mind you ).&lt;/p&gt;\\n\\n&lt;p&gt;Other than that I was looking at v100 maybe some amd type cards, 3090s always apparently...&lt;/p&gt;\\n\\n&lt;p&gt;Yeah, idk like I said it is overwhelming tbh.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1louk6a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1louk6a/5k_budget_for_local_ai/n0qc069/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751363042,"author_flair_text":null,"treatment_tags":[],"created_utc":1751363042,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0sy7g6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Unlikely_Track_5154","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0sg5sm","score":1,"author_fullname":"t2_r783n0ey","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Define getting scalped?","edited":false,"author_flair_css_class":null,"name":"t1_n0sy7g6","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Define getting scalped?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1louk6a","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1louk6a/5k_budget_for_local_ai/n0sy7g6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751394425,"author_flair_text":null,"collapsed":false,"created_utc":1751394425,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0sg5sm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Technical_Bar_1908","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0q2xz6","score":1,"author_fullname":"t2_f8arhws9t","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I would like to know more about the 5070ti and how we can get 2 without being scalped. Also I wonder if NVidia can get behind the consumer hardware accelerated decentralized  computation and home AI market and enable it for NVLINK on pcie 5","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0sg5sm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I would like to know more about the 5070ti and how we can get 2 without being scalped. Also I wonder if NVidia can get behind the consumer hardware accelerated decentralized  computation and home AI market and enable it for NVLINK on pcie 5&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1louk6a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1louk6a/5k_budget_for_local_ai/n0sg5sm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751389510,"author_flair_text":null,"treatment_tags":[],"created_utc":1751389510,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0shwtv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Massive-Question-550","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0q2xz6","score":1,"author_fullname":"t2_72xxv3wb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If that GPU is true and not the cost of a 5080 then that's a pretty nice option.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0shwtv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If that GPU is true and not the cost of a 5080 then that&amp;#39;s a pretty nice option.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1louk6a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1louk6a/5k_budget_for_local_ai/n0shwtv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751389980,"author_flair_text":null,"treatment_tags":[],"created_utc":1751389980,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0q2xz6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DepthHour1669","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0q0otj","score":6,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You would need a bit more than 64gb of vram to finetune a 32b model. \\n\\nBest bet is something like 4x 3090 at 96gb nvlinked together. \\n\\nDual 5090s is a bit out of your budget and not enough vram, you’re cutting it close. The 4090 24gb isn’t really price competitive with the 3090, but might be an option. You might also consider 2x chinese 4090 48gb, that might be a great option for you but corporate types may balk at the chinese source. You’re finetuning, so you’d want to stick with nvidia, but if you’re just running inference AMD/Intel may work as well. \\n\\nIf you can wait a few months, maybe the 5070 Ti Super 24gb that’s coming out is a good option.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0q2xz6","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You would need a bit more than 64gb of vram to finetune a 32b model. &lt;/p&gt;\\n\\n&lt;p&gt;Best bet is something like 4x 3090 at 96gb nvlinked together. &lt;/p&gt;\\n\\n&lt;p&gt;Dual 5090s is a bit out of your budget and not enough vram, you’re cutting it close. The 4090 24gb isn’t really price competitive with the 3090, but might be an option. You might also consider 2x chinese 4090 48gb, that might be a great option for you but corporate types may balk at the chinese source. You’re finetuning, so you’d want to stick with nvidia, but if you’re just running inference AMD/Intel may work as well. &lt;/p&gt;\\n\\n&lt;p&gt;If you can wait a few months, maybe the 5070 Ti Super 24gb that’s coming out is a good option.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1louk6a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1louk6a/5k_budget_for_local_ai/n0q2xz6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751357560,"author_flair_text":null,"treatment_tags":[],"created_utc":1751357560,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n0q0otj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Unlikely_Track_5154","can_mod_post":false,"created_utc":1751356203,"send_replies":true,"parent_id":"t1_n0pyjfy","score":2,"author_fullname":"t2_r783n0ey","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"5 at most...\\n\\nIt will not be high concurrency in terms of users, and I am not trying to be the next OAI.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0q0otj","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;5 at most...&lt;/p&gt;\\n\\n&lt;p&gt;It will not be high concurrency in terms of users, and I am not trying to be the next OAI.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1louk6a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1louk6a/5k_budget_for_local_ai/n0q0otj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751356203,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0pyjfy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"created_utc":1751354895,"send_replies":true,"parent_id":"t3_1louk6a","score":1,"author_fullname":"t2_t6glzswk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"How many users?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0pyjfy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How many users?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1louk6a/5k_budget_for_local_ai/n0pyjfy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751354895,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1louk6a","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0qwe5d","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"__JockY__","can_mod_post":false,"created_utc":1751372669,"send_replies":true,"parent_id":"t3_1louk6a","score":1,"author_fullname":"t2_qf8h7ka8","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You can get a 48GB RTX A6000 Ampere for that price. Older gen, but fantastic GPUs: fast, 2-slot, 300W. Job done.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0qwe5d","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can get a 48GB RTX A6000 Ampere for that price. Older gen, but fantastic GPUs: fast, 2-slot, 300W. Job done.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1louk6a/5k_budget_for_local_ai/n0qwe5d/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751372669,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1louk6a","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0syrtw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Unlikely_Track_5154","can_mod_post":false,"created_utc":1751394586,"send_replies":true,"parent_id":"t1_n0sae3g","score":1,"author_fullname":"t2_r783n0ey","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"OK, fair enough, I was talking to a different guy, and he said basically the same thing.\\n\\nMakes sense to me, since most of my heavy lifting is inference and not training, training is more of a side gig to the main gig.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0syrtw","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;OK, fair enough, I was talking to a different guy, and he said basically the same thing.&lt;/p&gt;\\n\\n&lt;p&gt;Makes sense to me, since most of my heavy lifting is inference and not training, training is more of a side gig to the main gig.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1louk6a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1louk6a/5k_budget_for_local_ai/n0syrtw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751394586,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0sae3g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Massive-Question-550","can_mod_post":false,"created_utc":1751387916,"send_replies":true,"parent_id":"t3_1louk6a","score":1,"author_fullname":"t2_72xxv3wb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For most fine tuning and especially training, cloud services are cheaper by a lot. since you will have a pretty beefy setup and if you want to run inference locally it's only really cost effective if the largest model you are fine tuning is around 32b parameters and it could take a while. For example I have 2 3090's and 64 GB of ram and can only fine tune 8-12b models. Unless things have changed significantly you need roughly 8-10 x the memory vs the size of the model loaded up and that's just for fine tuning.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0sae3g","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For most fine tuning and especially training, cloud services are cheaper by a lot. since you will have a pretty beefy setup and if you want to run inference locally it&amp;#39;s only really cost effective if the largest model you are fine tuning is around 32b parameters and it could take a while. For example I have 2 3090&amp;#39;s and 64 GB of ram and can only fine tune 8-12b models. Unless things have changed significantly you need roughly 8-10 x the memory vs the size of the model loaded up and that&amp;#39;s just for fine tuning.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1louk6a/5k_budget_for_local_ai/n0sae3g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751387916,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1louk6a","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0w22j4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Turbulent_Pin7635","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ur1ie","score":1,"author_fullname":"t2_1hra1kibwa","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I truly hate apple, believe me. Even the phenotype of the people that go inside the store, the way the push and discard iPhones, the programed obsolescence, the inability to repair... The list go on. This was my first and only apple, I have 40 years. After I have analysed the pro and cons SEVERAL times I had to decided between apple quiet, portable and powerful machine that give me the possibility to access and use all models (quantified) and a noisy rig, with domestic GPUs, with an inflated price by company and second-hand users. I opt by the first. And I have learned that if your enemy drops an AK-47, I won't wonder if holding it will improve the Russian industry. I'll just use the damn thing to kill an enemy. \\n\\nWe are consumers, we are in the cage, there is no good to refuse something good, what we can do is to accept whatever is useful and press the government to impose harsh regulations on the motherfuckers. \\n\\nAnyway I understand the conflitant feeling. I can ensure you that the answers provided by larger models are way better than the ones you get from paid services. =) \\n\\nIf you want help or doubts feel free to ask me. =)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0w22j4","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I truly hate apple, believe me. Even the phenotype of the people that go inside the store, the way the push and discard iPhones, the programed obsolescence, the inability to repair... The list go on. This was my first and only apple, I have 40 years. After I have analysed the pro and cons SEVERAL times I had to decided between apple quiet, portable and powerful machine that give me the possibility to access and use all models (quantified) and a noisy rig, with domestic GPUs, with an inflated price by company and second-hand users. I opt by the first. And I have learned that if your enemy drops an AK-47, I won&amp;#39;t wonder if holding it will improve the Russian industry. I&amp;#39;ll just use the damn thing to kill an enemy. &lt;/p&gt;\\n\\n&lt;p&gt;We are consumers, we are in the cage, there is no good to refuse something good, what we can do is to accept whatever is useful and press the government to impose harsh regulations on the motherfuckers. &lt;/p&gt;\\n\\n&lt;p&gt;Anyway I understand the conflitant feeling. I can ensure you that the answers provided by larger models are way better than the ones you get from paid services. =) &lt;/p&gt;\\n\\n&lt;p&gt;If you want help or doubts feel free to ask me. =)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1louk6a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1louk6a/5k_budget_for_local_ai/n0w22j4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751431701,"author_flair_text":null,"treatment_tags":[],"created_utc":1751431701,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ur1ie","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Unlikely_Track_5154","can_mod_post":false,"created_utc":1751413992,"send_replies":true,"parent_id":"t1_n0upt1q","score":1,"author_fullname":"t2_r783n0ey","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Absolutely 100% unequivocally will never purchase an apple product for tye rest of my miserable existence.\\n\\nI appreciate you taking the time to post this, that is not a dig on your suggestion, I am just an active boycotted of all things apple ( that I can control nor buying )","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ur1ie","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Absolutely 100% unequivocally will never purchase an apple product for tye rest of my miserable existence.&lt;/p&gt;\\n\\n&lt;p&gt;I appreciate you taking the time to post this, that is not a dig on your suggestion, I am just an active boycotted of all things apple ( that I can control nor buying )&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1louk6a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1louk6a/5k_budget_for_local_ai/n0ur1ie/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751413992,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0upt1q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Turbulent_Pin7635","can_mod_post":false,"created_utc":1751413573,"send_replies":true,"parent_id":"t3_1louk6a","score":1,"author_fullname":"t2_1hra1kibwa","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Go for a Mac Studio","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0upt1q","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Go for a Mac Studio&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1louk6a/5k_budget_for_local_ai/n0upt1q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751413573,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1louk6a","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`);export{e as default};
