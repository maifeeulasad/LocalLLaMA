import{j as e}from"./index-F0NXdzZX.js";import{R as l}from"./RedditPostRenderer-CoEZB1dt.js";import"./index-DrtravXm.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hey folks, I wanted to share a tool I built out of frustration with existing prompt evaluation tools.\\n\\n**Problem:**  \\nMost prompt testing tools are either:\\n\\n* Cloud-locked\\n* Too academic\\n* Don’t support function-calling or tool-using agents\\n\\n**RawBench is:**\\n\\n* YAML-first — define models, prompts, and tests cleanly\\n* Supports **tool mocking**, even recursive calls (for agent workflows)\\n* Measures latency, token usage, cost\\n* Has a clean local dashboard (no cloud BS)\\n* Works for multiple models, prompts, and variables\\n\\nYou just:\\n\\n    rawbench init &amp;&amp; rawbench run\\n\\nand browse the results on a local dashboard. Built this for myself while working on LLM agents. Now it's open-source.\\n\\nGitHub: [https://github.com/0xsomesh/rawbench](https://github.com/0xsomesh/rawbench)\\n\\nWould love to know if anyone here finds this useful or has feedback!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"I built RawBench — an LLM prompt + agent testing tool with YAML config and tool mocking","link_flair_richtext":[{"e":"text","t":"Resources"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lqwt0v","quarantine":false,"link_flair_text_color":"light","upvote_ratio":1,"author_flair_background_color":null,"subreddit_type":"public","ups":3,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_13ngefnj8u","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Resources","can_mod_post":false,"score":3,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1751566749,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey folks, I wanted to share a tool I built out of frustration with existing prompt evaluation tools.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt;&lt;br/&gt;\\nMost prompt testing tools are either:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Cloud-locked&lt;/li&gt;\\n&lt;li&gt;Too academic&lt;/li&gt;\\n&lt;li&gt;Don’t support function-calling or tool-using agents&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;&lt;strong&gt;RawBench is:&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;YAML-first — define models, prompts, and tests cleanly&lt;/li&gt;\\n&lt;li&gt;Supports &lt;strong&gt;tool mocking&lt;/strong&gt;, even recursive calls (for agent workflows)&lt;/li&gt;\\n&lt;li&gt;Measures latency, token usage, cost&lt;/li&gt;\\n&lt;li&gt;Has a clean local dashboard (no cloud BS)&lt;/li&gt;\\n&lt;li&gt;Works for multiple models, prompts, and variables&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;You just:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;rawbench init &amp;amp;&amp;amp; rawbench run\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;and browse the results on a local dashboard. Built this for myself while working on LLM agents. Now it&amp;#39;s open-source.&lt;/p&gt;\\n\\n&lt;p&gt;GitHub: &lt;a href=\\"https://github.com/0xsomesh/rawbench\\"&gt;https://github.com/0xsomesh/rawbench&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Would love to know if anyone here finds this useful or has feedback!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/3wPVQ1NECuerGxKriWYbQg_skoF_J9GxR6VzquKW5SU.png?auto=webp&amp;s=a078a018c74c21a71f75ac5a98882aad4e86b0b7","width":1200,"height":600},"resolutions":[{"url":"https://external-preview.redd.it/3wPVQ1NECuerGxKriWYbQg_skoF_J9GxR6VzquKW5SU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3d4232591b620a69a45cb2c9ee5419af11da1e7c","width":108,"height":54},{"url":"https://external-preview.redd.it/3wPVQ1NECuerGxKriWYbQg_skoF_J9GxR6VzquKW5SU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f8497b6fc8dc1e5d892f5baecffd41774d44c84c","width":216,"height":108},{"url":"https://external-preview.redd.it/3wPVQ1NECuerGxKriWYbQg_skoF_J9GxR6VzquKW5SU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9934ec85ac5d27ff15eedf26d4480648f0b13674","width":320,"height":160},{"url":"https://external-preview.redd.it/3wPVQ1NECuerGxKriWYbQg_skoF_J9GxR6VzquKW5SU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d2d02e6b8566e053a430c340eb14b30df9bc09f8","width":640,"height":320},{"url":"https://external-preview.redd.it/3wPVQ1NECuerGxKriWYbQg_skoF_J9GxR6VzquKW5SU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=60faea70a472592a81c6a854713a06f0bf9577a8","width":960,"height":480},{"url":"https://external-preview.redd.it/3wPVQ1NECuerGxKriWYbQg_skoF_J9GxR6VzquKW5SU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=348303cec615839c34f2bcef0afcc7e1f37d80c5","width":1080,"height":540}],"variants":{},"id":"3wPVQ1NECuerGxKriWYbQg_skoF_J9GxR6VzquKW5SU"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#ccac2b","id":"1lqwt0v","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"0xsomesh","discussion_type":null,"num_comments":4,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lqwt0v/i_built_rawbench_an_llm_prompt_agent_testing_tool/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lqwt0v/i_built_rawbench_an_llm_prompt_agent_testing_tool/","subreddit_subscribers":494198,"created_utc":1751566749,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n19ah4d","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"lemon07r","can_mod_post":false,"send_replies":true,"parent_id":"t1_n18hykg","score":1,"author_fullname":"t2_i697e","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Why ollama instead of openai compatible api? Ollama only allows local llms.. openai compatible api would allow other service providers AND local models.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n19ah4d","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why ollama instead of openai compatible api? Ollama only allows local llms.. openai compatible api would allow other service providers AND local models.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqwt0v","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqwt0v/i_built_rawbench_an_llm_prompt_agent_testing_tool/n19ah4d/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751606485,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1751606485,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n18hykg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"0xsomesh","can_mod_post":false,"created_utc":1751594540,"send_replies":true,"parent_id":"t1_n16hlpy","score":1,"author_fullname":"t2_13ngefnj8u","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes adding ollama support today. Thanks for the feedback","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n18hykg","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes adding ollama support today. Thanks for the feedback&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqwt0v","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqwt0v/i_built_rawbench_an_llm_prompt_agent_testing_tool/n18hykg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751594540,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n16hlpy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"lemon07r","can_mod_post":false,"created_utc":1751570424,"send_replies":true,"parent_id":"t3_1lqwt0v","score":1,"author_fullname":"t2_i697e","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Does this only support chatgpt models? Would like to see the option to use openai compatible api endpoints. This would open up the ability to test models from almost any provider, and locally run models.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n16hlpy","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Does this only support chatgpt models? Would like to see the option to use openai compatible api endpoints. This would open up the ability to test models from almost any provider, and locally run models.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqwt0v/i_built_rawbench_an_llm_prompt_agent_testing_tool/n16hlpy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751570424,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lqwt0v","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n19bi7j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"0xsomesh","can_mod_post":false,"created_utc":1751606988,"send_replies":true,"parent_id":"t3_1lqwt0v","score":1,"author_fullname":"t2_13ngefnj8u","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think I got confused. In the current version you can use any provider. Although the env file it generates on \`rawbench init\` only has the key for openai as of now. I'll be expanding this for users to use other providers as well. Internally I'm using litellm for integration with providers. \\n\\nHope that answers your question.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n19bi7j","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think I got confused. In the current version you can use any provider. Although the env file it generates on &lt;code&gt;rawbench init&lt;/code&gt; only has the key for openai as of now. I&amp;#39;ll be expanding this for users to use other providers as well. Internally I&amp;#39;m using litellm for integration with providers. &lt;/p&gt;\\n\\n&lt;p&gt;Hope that answers your question.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqwt0v/i_built_rawbench_an_llm_prompt_agent_testing_tool/n19bi7j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751606988,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqwt0v","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(l,{data:t});export{n as default};
