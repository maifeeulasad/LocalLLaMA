import{j as e}from"./index-Bu7qcPAU.js";import{R as l}from"./RedditPostRenderer-CbHA7O5q.js";import"./index-BKgbfxhf.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I currently have an ASUS TUF Gaming F15, and before people start telling me to give up on local models, let me just say that I have currently been able to successfully run various LLMs and even Images Diffusion models locally with very little issues (mainly just speed and sometimes lag due to OOM). I can easily run 7B Q4_K_Ms and Stable Diffusion/Flux. However, my RAM and GPU max out during such tasks and even sometimes when opening chrome with multiple tabs.\\n\\nSo I was thinking of upgrading my RAM (since upgrading my GPU is not an option). I currently have 16 GB built-in with an upgrade slot in which I plan on adding 32 GB. Is this a wise decision? Would it be better to have matching RAMs? (16&amp;16/32&amp;32)","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"I have a Laptop with 3050 Ti 4GB VRAM, will upgrading my RAM from 16 to 48 help?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lymewq","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.66,"author_flair_background_color":null,"subreddit_type":"public","ups":1,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_2l48bxrf","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":1,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752389844,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I currently have an ASUS TUF Gaming F15, and before people start telling me to give up on local models, let me just say that I have currently been able to successfully run various LLMs and even Images Diffusion models locally with very little issues (mainly just speed and sometimes lag due to OOM). I can easily run 7B Q4_K_Ms and Stable Diffusion/Flux. However, my RAM and GPU max out during such tasks and even sometimes when opening chrome with multiple tabs.&lt;/p&gt;\\n\\n&lt;p&gt;So I was thinking of upgrading my RAM (since upgrading my GPU is not an option). I currently have 16 GB built-in with an upgrade slot in which I plan on adding 32 GB. Is this a wise decision? Would it be better to have matching RAMs? (16&amp;amp;16/32&amp;amp;32)&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lymewq","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"GamerWael","discussion_type":null,"num_comments":4,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lymewq/i_have_a_laptop_with_3050_ti_4gb_vram_will/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lymewq/i_have_a_laptop_with_3050_ti_4gb_vram_will/","subreddit_subscribers":498344,"created_utc":1752389844,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2v92yg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ivari","can_mod_post":false,"created_utc":1752396070,"send_replies":true,"parent_id":"t3_1lymewq","score":7,"author_fullname":"t2_707lw","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"better on Qwen3 30A3B","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2v92yg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;better on Qwen3 30A3B&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lymewq/i_have_a_laptop_with_3050_ti_4gb_vram_will/n2v92yg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752396070,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lymewq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2v11bg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RhubarbSimilar1683","can_mod_post":false,"created_utc":1752391398,"send_replies":true,"parent_id":"t3_1lymewq","score":1,"author_fullname":"t2_1k4sjdwzk2","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It will help you get rid of OOM, were you using most of your swap? If so it will help with performance but if you weren't using swap it will only get rid of OOM. You don't need to match ram by capacity but you do need to match speed","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2v11bg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It will help you get rid of OOM, were you using most of your swap? If so it will help with performance but if you weren&amp;#39;t using swap it will only get rid of OOM. You don&amp;#39;t need to match ram by capacity but you do need to match speed&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lymewq/i_have_a_laptop_with_3050_ti_4gb_vram_will/n2v11bg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752391398,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lymewq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2vmbic","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Specialist8602","can_mod_post":false,"created_utc":1752403770,"send_replies":true,"parent_id":"t3_1lymewq","score":1,"author_fullname":"t2_ecdkrq9p","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yes, but not the way you may think. Ram is way slower. Video card ram (vram) is fast. So yes, increasing your ram helps, but if you load a model onto it, it will impact performance.  Try and have same size/ speed ram sticks also. If you can get two 32gb sticks of ram cheap, that would be good.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2vmbic","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, but not the way you may think. Ram is way slower. Video card ram (vram) is fast. So yes, increasing your ram helps, but if you load a model onto it, it will impact performance.  Try and have same size/ speed ram sticks also. If you can get two 32gb sticks of ram cheap, that would be good.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lymewq/i_have_a_laptop_with_3050_ti_4gb_vram_will/n2vmbic/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752403770,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lymewq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2v3iu9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"sunshinecheung","can_mod_post":false,"created_utc":1752392823,"send_replies":true,"parent_id":"t3_1lymewq","score":-2,"author_fullname":"t2_u398xzta","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"maybe upgrade your vram into 8gb","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2v3iu9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;maybe upgrade your vram into 8gb&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lymewq/i_have_a_laptop_with_3050_ti_4gb_vram_will/n2v3iu9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752392823,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lymewq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-2}}],"before":null}}]`),s=()=>e.jsx(l,{data:a});export{s as default};
