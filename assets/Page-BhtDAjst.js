import{j as e}from"./index-CWmJdUH_.js";import{R as l}from"./RedditPostRenderer-D2iunoQ9.js";import"./index-BCg9RP6g.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I'm currently working on setting up an **in-house Large Language Model (LLM) system** for internal organizational projects. Given the rapid advancements in AI technology, I’d greatly value your professional insights and recommendations to ensure we're leveraging the latest tools and methods effectively.\\n\\n**Here's our current plan and key considerations:**\\n\\n**1. Model Selection:** We're considering open-source models such as GPT-3 (EleutherAI), T5, or FLAN-T5. Are there any standout alternatives or specific models you've successfully implemented lately?\\n\\n**2. Data Pipeline:** We’re using Apache Kafka for real-time data ingestion and Apache Spark for batch processing. Have you come across any newer or more efficient tools and practices beneficial for handling large-scale datasets?\\n\\n**3. Training &amp; Fine-Tuning:** Planning to utilize Ray Tune and Weights &amp; Biases for hyperparameter optimization and experiment tracking. GPU costs remain a concern—any advice on cost-effective or emerging platforms for fine-tuning large models?\\n\\n**4. Deployment &amp; Serving:** Considering Kubernetes, Docker, and FastAPI for deployment. Would you recommend NVIDIA Triton Server or TensorRT for better performance? What has your experience been?\\n\\n**5. Performance &amp; Scalability:** Ensuring real-time scalability and minimal latency is crucial. How do you efficiently manage scalability and parallel inference when deploying multiple models concurrently?\\n\\n**6. Ethics &amp; Bias Mitigation:** Effective bias detection and mitigation frameworks are essential for us. Can you suggest recent effective tools or methods for ethical AI deployment?\\n\\n**We'd appreciate your input on:**\\n\\n* Key tools or strategies that significantly improved your LLM workflows in 2025.\\n* Recommendations for cost-effective GPU management and training setups.\\n* Preferred tools for robust monitoring, logging, and performance analysis (e.g., Prometheus, Grafana).","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Advice Needed: Building an In-House LLM System Using Latest Tech — Recommendations?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lsx9pn","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.25,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_uaotuj04","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751794548,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m currently working on setting up an &lt;strong&gt;in-house Large Language Model (LLM) system&lt;/strong&gt; for internal organizational projects. Given the rapid advancements in AI technology, I’d greatly value your professional insights and recommendations to ensure we&amp;#39;re leveraging the latest tools and methods effectively.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Here&amp;#39;s our current plan and key considerations:&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;1. Model Selection:&lt;/strong&gt; We&amp;#39;re considering open-source models such as GPT-3 (EleutherAI), T5, or FLAN-T5. Are there any standout alternatives or specific models you&amp;#39;ve successfully implemented lately?&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;2. Data Pipeline:&lt;/strong&gt; We’re using Apache Kafka for real-time data ingestion and Apache Spark for batch processing. Have you come across any newer or more efficient tools and practices beneficial for handling large-scale datasets?&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;3. Training &amp;amp; Fine-Tuning:&lt;/strong&gt; Planning to utilize Ray Tune and Weights &amp;amp; Biases for hyperparameter optimization and experiment tracking. GPU costs remain a concern—any advice on cost-effective or emerging platforms for fine-tuning large models?&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;4. Deployment &amp;amp; Serving:&lt;/strong&gt; Considering Kubernetes, Docker, and FastAPI for deployment. Would you recommend NVIDIA Triton Server or TensorRT for better performance? What has your experience been?&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;5. Performance &amp;amp; Scalability:&lt;/strong&gt; Ensuring real-time scalability and minimal latency is crucial. How do you efficiently manage scalability and parallel inference when deploying multiple models concurrently?&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;6. Ethics &amp;amp; Bias Mitigation:&lt;/strong&gt; Effective bias detection and mitigation frameworks are essential for us. Can you suggest recent effective tools or methods for ethical AI deployment?&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;We&amp;#39;d appreciate your input on:&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Key tools or strategies that significantly improved your LLM workflows in 2025.&lt;/li&gt;\\n&lt;li&gt;Recommendations for cost-effective GPU management and training setups.&lt;/li&gt;\\n&lt;li&gt;Preferred tools for robust monitoring, logging, and performance analysis (e.g., Prometheus, Grafana).&lt;/li&gt;\\n&lt;/ul&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lsx9pn","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"No_Edge2098","discussion_type":null,"num_comments":4,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lsx9pn/advice_needed_building_an_inhouse_llm_system/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lsx9pn/advice_needed_building_an_inhouse_llm_system/","subreddit_subscribers":495651,"created_utc":1751794548,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1nimrt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"sh4rksh4d0w","can_mod_post":false,"created_utc":1751817468,"send_replies":true,"parent_id":"t3_1lsx9pn","score":4,"author_fullname":"t2_15jwtv","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This reads like it was written by AI. Have you tried asking AI chatbots this question?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1nimrt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This reads like it was written by AI. Have you tried asking AI chatbots this question?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsx9pn/advice_needed_building_an_inhouse_llm_system/n1nimrt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751817468,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lsx9pn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1lzfe2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Ok_Hope_4007","can_mod_post":false,"created_utc":1751795409,"send_replies":true,"parent_id":"t3_1lsx9pn","score":5,"author_fullname":"t2_pjzish3n","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The choice of model selection is a bit dated. I would strongly recommend considering more recent llms. I don't know the exact use case so it's hard to put the finger on specific models but have a look at the Gemma3/Llama3 or Qwen3 Family as well as Models like Mistral/Magistral or Devstral. They all have their cons and pros but are without doubt far superior to t5/gpt3 and everything else from the years 2022 and earlier.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1lzfe2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The choice of model selection is a bit dated. I would strongly recommend considering more recent llms. I don&amp;#39;t know the exact use case so it&amp;#39;s hard to put the finger on specific models but have a look at the Gemma3/Llama3 or Qwen3 Family as well as Models like Mistral/Magistral or Devstral. They all have their cons and pros but are without doubt far superior to t5/gpt3 and everything else from the years 2022 and earlier.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsx9pn/advice_needed_building_an_inhouse_llm_system/n1lzfe2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751795409,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lsx9pn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1pd08f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Fit-Produce420","can_mod_post":false,"created_utc":1751837867,"send_replies":true,"parent_id":"t3_1lsx9pn","score":1,"author_fullname":"t2_tewf9bdwg","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Do your job for you?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1pd08f","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Do your job for you?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsx9pn/advice_needed_building_an_inhouse_llm_system/n1pd08f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751837867,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lsx9pn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1n13qa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Conscious_Cut_6144","can_mod_post":false,"created_utc":1751812111,"send_replies":true,"parent_id":"t3_1lsx9pn","score":2,"author_fullname":"t2_9hl4ymvj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Can you elaborate on use case? Those models are prehistoric by LLM standards.\\nGemma3 and Qwen3 are where you should be looking for general purpose llms (or deepseek if you need high performance and have a big budget)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1n13qa","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Can you elaborate on use case? Those models are prehistoric by LLM standards.\\nGemma3 and Qwen3 are where you should be looking for general purpose llms (or deepseek if you need high performance and have a big budget)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsx9pn/advice_needed_building_an_inhouse_llm_system/n1n13qa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751812111,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lsx9pn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),o=()=>e.jsx(l,{data:t});export{o as default};
