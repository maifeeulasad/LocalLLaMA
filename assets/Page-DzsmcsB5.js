import{j as e}from"./index-BesAfWr7.js";import{R as l}from"./RedditPostRenderer-BdTjz6Pg.js";import"./index-fSlfMVR-.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Not sure if you've noticed, but a lot of model providers no longer explicitly note that their models are reasoning models (on benchmarks in particular). Reasoning models aren't ideal for every application.\\n\\nI looked at the non-reasoning benchmarks on [Artificial Analysis](https://artificialanalysis.ai/models/llama-4-maverick?model-filters=open-source%2Cnon-reasoning-models#artificial-analysis-intelligence-index-by-model-type) today and the top 2 models (performing comparable) are DeepSeek v3 and Llama 4 Maverick (which I heard was a flop?). I was surprised to see these 2 at the top.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Progress stalled in non-reasoning open-source models?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":79,"top_awarded_type":null,"hide_score":false,"name":"t3_1lmk2dj","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.86,"author_flair_background_color":null,"ups":249,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1a48h7vf","secure_media":null,"is_reddit_media_domain":true,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":249,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://external-preview.redd.it/hy_9GqJ2O_8sGhhuyCns6yeVFjNJeFPspRVlVN08J6o.png?width=140&amp;height=79&amp;crop=140:79,smart&amp;auto=webp&amp;s=0c26a41ba15afee80f3b3690229a42577afa4c1e","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"image","content_categories":null,"is_self":false,"subreddit_type":"public","created":1751108315,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"i.redd.it","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Not sure if you&amp;#39;ve noticed, but a lot of model providers no longer explicitly note that their models are reasoning models (on benchmarks in particular). Reasoning models aren&amp;#39;t ideal for every application.&lt;/p&gt;\\n\\n&lt;p&gt;I looked at the non-reasoning benchmarks on &lt;a href=\\"https://artificialanalysis.ai/models/llama-4-maverick?model-filters=open-source%2Cnon-reasoning-models#artificial-analysis-intelligence-index-by-model-type\\"&gt;Artificial Analysis&lt;/a&gt; today and the top 2 models (performing comparable) are DeepSeek v3 and Llama 4 Maverick (which I heard was a flop?). I was surprised to see these 2 at the top.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"url_overridden_by_dest":"https://i.redd.it/q53t8do2fn9f1.png","view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://preview.redd.it/q53t8do2fn9f1.png?auto=webp&amp;s=63814db4b942f4e5d54f75d31c55fc061cc3744a","width":1706,"height":972},"resolutions":[{"url":"https://preview.redd.it/q53t8do2fn9f1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=12e5ec06f68e30807329419d5fe1dbf669b5da76","width":108,"height":61},{"url":"https://preview.redd.it/q53t8do2fn9f1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7f0377fa6cd364ad361be1634a72279983524dc1","width":216,"height":123},{"url":"https://preview.redd.it/q53t8do2fn9f1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d98a3b3b3d13ccb479144cfd0d5bdf61305fc630","width":320,"height":182},{"url":"https://preview.redd.it/q53t8do2fn9f1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=dbf6bcfd1d93bd65c875ca994b48c3b38839c958","width":640,"height":364},{"url":"https://preview.redd.it/q53t8do2fn9f1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8e510afc5c65f9ee299ba5ad78f94fc8d6a32a42","width":960,"height":546},{"url":"https://preview.redd.it/q53t8do2fn9f1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d2d71c1d41fc743dd7632574990820c4fee67d63","width":1080,"height":615}],"variants":{},"id":"hy_9GqJ2O_8sGhhuyCns6yeVFjNJeFPspRVlVN08J6o"}],"enabled":true},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"mod_note":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"num_reports":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lmk2dj","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"entsnack","discussion_type":null,"num_comments":136,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/","stickied":false,"url":"https://i.redd.it/q53t8do2fn9f1.png","subreddit_subscribers":492929,"created_utc":1751108315,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0b97og","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Sea-Rope-31","can_mod_post":false,"send_replies":true,"parent_id":"t1_n09vqz2","score":9,"author_fullname":"t2_1sdssbj1gj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Guilty!","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0b97og","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Guilty!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0b97og/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751148401,"author_flair_text":null,"treatment_tags":[],"created_utc":1751148401,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0cxoq5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Inaeipathy","can_mod_post":false,"send_replies":true,"parent_id":"t1_n09vqz2","score":6,"author_fullname":"t2_e5t3kbfb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"AGI 2025 is coming guys, any day now.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0cxoq5","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;AGI 2025 is coming guys, any day now.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0cxoq5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751172426,"author_flair_text":null,"treatment_tags":[],"created_utc":1751172426,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n09vqz2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Ansible32","can_mod_post":false,"created_utc":1751132376,"send_replies":true,"parent_id":"t1_n084h9h","score":54,"author_fullname":"t2_72uqo","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Among people who genuinely look at every release and extrapolate an exponential graph from the past 5 linear datapoints, yes.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n09vqz2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Among people who genuinely look at every release and extrapolate an exponential graph from the past 5 linear datapoints, yes.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n09vqz2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751132376,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":54}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0eglna","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Django_McFly","can_mod_post":false,"created_utc":1751202150,"send_replies":true,"parent_id":"t1_n084h9h","score":2,"author_fullname":"t2_g35vlbqkh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; Y'all aren't inventing brand new technology fast enough.  It should take like 3 weeks top to crack all issues and hit a massive new milestone.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0eglna","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Y&amp;#39;all aren&amp;#39;t inventing brand new technology fast enough.  It should take like 3 weeks top to crack all issues and hit a massive new milestone.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0eglna/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751202150,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n08jmem","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"b3081a","can_mod_post":false,"send_replies":true,"parent_id":"t1_n08a7v2","score":9,"author_fullname":"t2_17n5yh7l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The so-called \\"non-reasoning\\" model is non-existent long ago. Both Qwen 2.5 and Llama 4 tries to \\"think\\" in steps when you ask them complex questions that requires some logical steps to resolve. If you specifically prompt them to answer the question without any intermediate thoughts, the accuracy of their answers will be all over the place.","edited":false,"author_flair_css_class":null,"name":"t1_n08jmem","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The so-called &amp;quot;non-reasoning&amp;quot; model is non-existent long ago. Both Qwen 2.5 and Llama 4 tries to &amp;quot;think&amp;quot; in steps when you ask them complex questions that requires some logical steps to resolve. If you specifically prompt them to answer the question without any intermediate thoughts, the accuracy of their answers will be all over the place.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lmk2dj","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n08jmem/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751116763,"author_flair_text":"llama.cpp","collapsed":false,"created_utc":1751116763,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}}],"before":null}},"user_reports":[],"saved":false,"id":"n08a7v2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"entsnack","can_mod_post":false,"send_replies":true,"parent_id":"t1_n088soi","score":-8,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":true,"body":"Valid thoughts. I have seen papers on hybrid models (i.e., thinking fast and slow), so I agree that the era of fully non-reasoning models is slipping away.\\n\\nI'm an academic and benchmarks are at the core of the scientific method, so I'm not going to write them off wholesale yet. We will come up with better benchmarks as the field matures. Feels isn't going to cut it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n08a7v2","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Valid thoughts. I have seen papers on hybrid models (i.e., thinking fast and slow), so I agree that the era of fully non-reasoning models is slipping away.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m an academic and benchmarks are at the core of the scientific method, so I&amp;#39;m not going to write them off wholesale yet. We will come up with better benchmarks as the field matures. Feels isn&amp;#39;t going to cut it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n08a7v2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751112970,"author_flair_text":null,"treatment_tags":[],"created_utc":1751112970,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-8}}],"before":null}},"user_reports":[],"saved":false,"id":"n088soi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"-dysangel-","can_mod_post":false,"send_replies":true,"parent_id":"t1_n084qyu","score":46,"author_fullname":"t2_12ggykute6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It's maybe because the main benchmarks increasingly don't seem to reflect real life performance, ie some models may be being trained on the benchmarks to fudge their performance. What matters is how the models feel for real world use cases.\\n\\nRegarding your point in general - yes maybe the base line understanding is stalling out. That's interesting. We as humans also have limits to our intuition. Sometimes you just need to think something out rather than intuit/guess. Also models are increasingly becoming a mix of reasoning and non reasoning, either explicitly setting the mode on or off, or the model deciding if it needs to reason. So I think we are naturally going to increasingly see the \\"non-reasoning\\" models lag behind, because they are becoming outdated.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n088soi","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s maybe because the main benchmarks increasingly don&amp;#39;t seem to reflect real life performance, ie some models may be being trained on the benchmarks to fudge their performance. What matters is how the models feel for real world use cases.&lt;/p&gt;\\n\\n&lt;p&gt;Regarding your point in general - yes maybe the base line understanding is stalling out. That&amp;#39;s interesting. We as humans also have limits to our intuition. Sometimes you just need to think something out rather than intuit/guess. Also models are increasingly becoming a mix of reasoning and non reasoning, either explicitly setting the mode on or off, or the model deciding if it needs to reason. So I think we are naturally going to increasingly see the &amp;quot;non-reasoning&amp;quot; models lag behind, because they are becoming outdated.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n088soi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751112345,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1751112345,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":46}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0arjvq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0abgai","score":-3,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I don't have daily driver LLMs, I code in vim, and that's not the Llama 4 use case anyway. You're better off with a stupider model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0arjvq","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t have daily driver LLMs, I code in vim, and that&amp;#39;s not the Llama 4 use case anyway. You&amp;#39;re better off with a stupider model.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0arjvq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751142490,"author_flair_text":null,"treatment_tags":[],"created_utc":1751142490,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0abgai","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"IrisColt","can_mod_post":false,"send_replies":true,"parent_id":"t1_n084qyu","score":5,"author_fullname":"t2_c2f558x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt;I also don't get the negativity here for Llama 4\\n\\n\\nGive it a spin as your daily driver, spoiler: it’s downright annoying","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0abgai","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;I also don&amp;#39;t get the negativity here for Llama 4&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Give it a spin as your daily driver, spoiler: it’s downright annoying&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0abgai/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751137265,"author_flair_text":null,"treatment_tags":[],"created_utc":1751137265,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0coc6b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"JustImmunity","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0arp7y","score":1,"author_fullname":"t2_c4pwgz16","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Domain then?","edited":false,"author_flair_css_class":null,"name":"t1_n0coc6b","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Domain then?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lmk2dj","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0coc6b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751168148,"author_flair_text":null,"collapsed":false,"created_utc":1751168148,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0arp7y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ah7zt","score":1,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Client project in the EU.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0arp7y","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Client project in the EU.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0arp7y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751142539,"author_flair_text":null,"treatment_tags":[],"created_utc":1751142539,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ah7zt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"JustImmunity","can_mod_post":false,"send_replies":true,"parent_id":"t1_n084qyu","score":1,"author_fullname":"t2_c4pwgz16","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Which benchmarks?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0ah7zt","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Which benchmarks?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0ah7zt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751139128,"author_flair_text":null,"treatment_tags":[],"created_utc":1751139128,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n084qyu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"entsnack","can_mod_post":false,"created_utc":1751110486,"send_replies":true,"parent_id":"t1_n084h9h","score":-47,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Wow it feels like ages. I also don't get the negativity here for Llama 4 when it's pretty much tied with DeepSeek and Qwen in each size class. I think Llama 4s \\"marketing\\" mistake was not releasing a smaller model. I recently ran a benchmark with Qwen3 vs. Llama 3.1 / 3.2 and both Llama 3.2-3B and Llama-3.1-8B outperformed Qwen3 4B and 8B significantly.","edited":1751117049,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n084qyu","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Wow it feels like ages. I also don&amp;#39;t get the negativity here for Llama 4 when it&amp;#39;s pretty much tied with DeepSeek and Qwen in each size class. I think Llama 4s &amp;quot;marketing&amp;quot; mistake was not releasing a smaller model. I recently ran a benchmark with Qwen3 vs. Llama 3.1 / 3.2 and both Llama 3.2-3B and Llama-3.1-8B outperformed Qwen3 4B and 8B significantly.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n084qyu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751110486,"author_flair_text":null,"treatment_tags":[],"collapsed":true,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-47}}],"before":null}},"user_reports":[],"saved":false,"id":"n084h9h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Brilliant-Weekend-68","can_mod_post":false,"created_utc":1751110354,"send_replies":true,"parent_id":"t3_1lmk2dj","score":246,"author_fullname":"t2_4gnfcfjr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Uh, is it not a bit early to call progress stalled when the top 5 models are about 2-3 months old?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n084h9h","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Uh, is it not a bit early to call progress stalled when the top 5 models are about 2-3 months old?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n084h9h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751110354,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmk2dj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":246}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n08u3y8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AuspiciousApple","can_mod_post":false,"send_replies":true,"parent_id":"t1_n08kp0d","score":10,"author_fullname":"t2_bndbg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What do you do concretely?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n08u3y8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What do you do concretely?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n08u3y8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751120505,"author_flair_text":null,"treatment_tags":[],"created_utc":1751120505,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":0,"name":"t1__","id":"_","parent_id":"t1_n093fzn","depth":10,"children":[]}}],"before":null}},"user_reports":[],"saved":false,"id":"n093fzn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"silenceimpaired","can_mod_post":false,"created_utc":1751123513,"send_replies":true,"parent_id":"t1_n091hn1","score":5,"author_fullname":"t2_dissgzyl","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I appreciate this. I haven’t yet, but I have two 24 gb cards so I should be able to train a reasonable sized model.\\n\\nI’ll have to think on this more.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n093fzn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I appreciate this. I haven’t yet, but I have two 24 gb cards so I should be able to train a reasonable sized model.&lt;/p&gt;\\n\\n&lt;p&gt;I’ll have to think on this more.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n093fzn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751123513,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmk2dj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":9,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n091hn1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"entsnack","can_mod_post":false,"created_utc":1751122891,"send_replies":true,"parent_id":"t1_n08zz6s","score":9,"author_fullname":"t2_1a48h7vf","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Can you fine tune an LLM? It just a matter of prompting and fine tuning.\\n\\nFor example:\\n\\nThis is a transaction and some user information. Will this user initiate a chargeback in the next week? Respond with one word, yes or no:\\n\\nFind some data or generate synthetic data. Train and test. The challenging part is data collection and data augmentation, finding unexplored forecasting problems, and finding clients.\\n\\nFor the client building problem, check out the blog by Kalzumeus.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n091hn1","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Can you fine tune an LLM? It just a matter of prompting and fine tuning.&lt;/p&gt;\\n\\n&lt;p&gt;For example:&lt;/p&gt;\\n\\n&lt;p&gt;This is a transaction and some user information. Will this user initiate a chargeback in the next week? Respond with one word, yes or no:&lt;/p&gt;\\n\\n&lt;p&gt;Find some data or generate synthetic data. Train and test. The challenging part is data collection and data augmentation, finding unexplored forecasting problems, and finding clients.&lt;/p&gt;\\n\\n&lt;p&gt;For the client building problem, check out the blog by Kalzumeus.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n091hn1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751122891,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmk2dj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":8,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}}],"before":null}},"user_reports":[],"saved":false,"id":"n08zz6s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"silenceimpaired","can_mod_post":false,"created_utc":1751122407,"send_replies":true,"parent_id":"t1_n08zos6","score":1,"author_fullname":"t2_dissgzyl","approved_by":null,"mod_note":null,"all_awardings":[],"body":"That’s super cool. Congrats! I definitely don’t have the know how to do that. Any articles to recommend? I am in a field where forecasting could have some value.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n08zz6s","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That’s super cool. Congrats! I definitely don’t have the know how to do that. Any articles to recommend? I am in a field where forecasting could have some value.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lmk2dj","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n08zz6s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751122407,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n08zos6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"entsnack","can_mod_post":false,"send_replies":true,"parent_id":"t1_n08zcju","score":5,"author_fullname":"t2_1a48h7vf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Before LLMs a lot of my forecasts were too inaccurate to monetize. Ever since Llama2 that changed.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n08zos6","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Before LLMs a lot of my forecasts were too inaccurate to monetize. Ever since Llama2 that changed.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lmk2dj","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n08zos6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751122315,"author_flair_text":null,"treatment_tags":[],"created_utc":1751122315,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n08zcju","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"silenceimpaired","can_mod_post":false,"send_replies":true,"parent_id":"t1_n08z6sz","score":3,"author_fullname":"t2_dissgzyl","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Interesting. And a regular LLM does this fairly well for you huh?","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n08zcju","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Interesting. And a regular LLM does this fairly well for you huh?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lmk2dj","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n08zcju/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751122207,"author_flair_text":null,"treatment_tags":[],"created_utc":1751122207,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n08z6sz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"entsnack","can_mod_post":false,"send_replies":true,"parent_id":"t1_n08uex7","score":8,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Forecast something people will pay to know in advance. Prices, supply, demand, machine failures, ...","edited":false,"author_flair_css_class":null,"name":"t1_n08z6sz","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Forecast something people will pay to know in advance. Prices, supply, demand, machine failures, ...&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lmk2dj","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n08z6sz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751122156,"author_flair_text":null,"collapsed":false,"created_utc":1751122156,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}}],"before":null}},"user_reports":[],"saved":false,"id":"n08uex7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"silenceimpaired","can_mod_post":false,"send_replies":true,"parent_id":"t1_n08kp0d","score":3,"author_fullname":"t2_dissgzyl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Tell me how to make this money oh wise one.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n08uex7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Tell me how to make this money oh wise one.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n08uex7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751120606,"author_flair_text":null,"treatment_tags":[],"created_utc":1751120606,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n08kp0d","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"entsnack","can_mod_post":false,"send_replies":true,"parent_id":"t1_n08hi0p","score":-7,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"For me, the model's performance after fine-tuning literally decides my paycheck. When my ROC-AUC jumps from 0.75-0.85 because of a new model release, my paycheck doubles. The smaller models are great but still not competitive for anything I can make money from.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n08kp0d","is_submitter":true,"collapsed":true,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For me, the model&amp;#39;s performance after fine-tuning literally decides my paycheck. When my ROC-AUC jumps from 0.75-0.85 because of a new model release, my paycheck doubles. The smaller models are great but still not competitive for anything I can make money from.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n08kp0d/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751117168,"author_flair_text":null,"treatment_tags":[],"created_utc":1751117168,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n08ld2r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ArcaneThoughts","can_mod_post":false,"send_replies":true,"parent_id":"t1_n08hi0p","score":-1,"author_fullname":"t2_hgivzvub","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"100% agree","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n08ld2r","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;100% agree&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n08ld2r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751117417,"author_flair_text":null,"treatment_tags":[],"created_utc":1751117417,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n08hi0p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AuspiciousApple","can_mod_post":false,"created_utc":1751115945,"send_replies":true,"parent_id":"t1_n083etv","score":25,"author_fullname":"t2_bndbg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Progress on all fronts is welcome, but to me 4-14B models matter most as that's what I can run quickly locally. For very high performance stuff, I'm happy with Claude/ChatGPT for now.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n08hi0p","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Progress on all fronts is welcome, but to me 4-14B models matter most as that&amp;#39;s what I can run quickly locally. For very high performance stuff, I&amp;#39;m happy with Claude/ChatGPT for now.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n08hi0p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751115945,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":25}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n084k4o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"created_utc":1751110394,"send_replies":true,"parent_id":"t1_n083etv","score":3,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Good insight, I wasn't looking at improvements in the right side of this plot (which is cropped, where the small models are).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n084k4o","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Good insight, I wasn&amp;#39;t looking at improvements in the right side of this plot (which is cropped, where the small models are).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n084k4o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751110394,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0agc4g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"chisleu","can_mod_post":false,"send_replies":true,"parent_id":"t1_n09fxy8","score":3,"author_fullname":"t2_cbxyn","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"IDK who lied to you. None of the AI giants are worried about copyright when it comes to training LLMs.\\n\\nGoogle already demonstrated they could train models to be more accurate than it's input data. ~7 years ago.\\n\\nSynthetic data isn't the enemy.\\n\\nIs it possible the way you are using the models is changing instead of the models regressing? You are giving them harder and harder tasks as you grow in skill?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0agc4g","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;IDK who lied to you. None of the AI giants are worried about copyright when it comes to training LLMs.&lt;/p&gt;\\n\\n&lt;p&gt;Google already demonstrated they could train models to be more accurate than it&amp;#39;s input data. ~7 years ago.&lt;/p&gt;\\n\\n&lt;p&gt;Synthetic data isn&amp;#39;t the enemy.&lt;/p&gt;\\n\\n&lt;p&gt;Is it possible the way you are using the models is changing instead of the models regressing? You are giving them harder and harder tasks as you grow in skill?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0agc4g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751138838,"author_flair_text":null,"treatment_tags":[],"created_utc":1751138838,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n09fxy8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MoffKalast","can_mod_post":false,"created_utc":1751127459,"send_replies":true,"parent_id":"t1_n083etv","score":2,"author_fullname":"t2_d2nyh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I think non-reasoning models are actually slowly regressing if you ignore benchmark numbers since they are contaminated with all of them anyway. Each new release has less world knowledge than the previous one, repetitions seem to be getting worse, there's more synthetic data and less copyrighted material in the datasets which makes the model makers feel more comfortable with their legal stance, but the end result feels noticeably cut down.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n09fxy8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think non-reasoning models are actually slowly regressing if you ignore benchmark numbers since they are contaminated with all of them anyway. Each new release has less world knowledge than the previous one, repetitions seem to be getting worse, there&amp;#39;s more synthetic data and less copyrighted material in the datasets which makes the model makers feel more comfortable with their legal stance, but the end result feels noticeably cut down.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n09fxy8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751127459,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0eorjd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ArcaneThoughts","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0e9436","score":1,"author_fullname":"t2_hgivzvub","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"For my use cases it's Gemma 2 (yes, 2) 9b. The latest surprise though was Qwen3 0.6b, that one is incredibly intelligent for its size.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0eorjd","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For my use cases it&amp;#39;s Gemma 2 (yes, 2) 9b. The latest surprise though was Qwen3 0.6b, that one is incredibly intelligent for its size.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0eorjd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751205181,"author_flair_text":null,"treatment_tags":[],"created_utc":1751205181,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0e9436","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Expensive-Apricot-25","can_mod_post":false,"created_utc":1751198982,"send_replies":true,"parent_id":"t1_n083etv","score":1,"author_fullname":"t2_idqkwio0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What is your favorite small model? (8b-ish)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0e9436","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What is your favorite small model? (8b-ish)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0e9436/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751198982,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n083etv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ArcaneThoughts","can_mod_post":false,"created_utc":1751109822,"send_replies":true,"parent_id":"t3_1lmk2dj","score":89,"author_fullname":"t2_hgivzvub","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yes I think so. For my use cases I don't care about reasoning and I noticed that they haven't improved for a while. That being said small models ARE improving, which is pretty good for running them locally.","edited":1751111157,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n083etv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes I think so. For my use cases I don&amp;#39;t care about reasoning and I noticed that they haven&amp;#39;t improved for a while. That being said small models ARE improving, which is pretty good for running them locally.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n083etv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751109822,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmk2dj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":89}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n086txb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"rerri","can_mod_post":false,"created_utc":1751111458,"send_replies":true,"parent_id":"t1_n085731","score":22,"author_fullname":"t2_12aeph","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes, why wouldn't it? The Qwen3 models in this graph are all run without reasoning enabled. Artificial Analysis has separate tests for them with reasoning enabled.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n086txb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, why wouldn&amp;#39;t it? The Qwen3 models in this graph are all run without reasoning enabled. Artificial Analysis has separate tests for them with reasoning enabled.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n086txb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751111458,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":22}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0868xi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"created_utc":1751111191,"send_replies":true,"parent_id":"t1_n085731","score":2,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes it does. Qwen3 is a bit confusing because their non-reasoning benchmark numbers are only in the tech report, not on the website.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0868xi","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes it does. Qwen3 is a bit confusing because their non-reasoning benchmark numbers are only in the tech report, not on the website.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0868xi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751111191,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"d2642412-d9ce-11ed-ae30-32b11309f5bd","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n09r2wf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Everlier","can_mod_post":false,"created_utc":1751130958,"send_replies":true,"parent_id":"t1_n085731","score":2,"author_fullname":"t2_o7p5m","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm not sure why, but I really can't make it work like Llama, it's definitely OK for Math and a bit of programming, but for normal usage it's just slop, emojis and lists all over the place. It's also not trained (or distillation erased that) on a few interesting tasks (scrambled inputs, unfinished assistant turns) that significantly degrade its usability for my usecases.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n09r2wf","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Alpaca"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m not sure why, but I really can&amp;#39;t make it work like Llama, it&amp;#39;s definitely OK for Math and a bit of programming, but for normal usage it&amp;#39;s just slop, emojis and lists all over the place. It&amp;#39;s also not trained (or distillation erased that) on a few interesting tasks (scrambled inputs, unfinished assistant turns) that significantly degrade its usability for my usecases.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n09r2wf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751130958,"author_flair_text":"Alpaca","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bd9e9e","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n085731","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MokoshHydro","can_mod_post":false,"created_utc":1751110699,"send_replies":true,"parent_id":"t3_1lmk2dj","score":21,"author_fullname":"t2_vtgj4az4a","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Does \\"Qwen3 /no\\\\_think\\" count as non-reasoning?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n085731","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Does &amp;quot;Qwen3 /no_think&amp;quot; count as non-reasoning?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n085731/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751110699,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmk2dj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":21}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0a98q1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"created_utc":1751136575,"send_replies":true,"parent_id":"t1_n0a4305","score":3,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes this is correct, another commenter pointed out the same.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0a98q1","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes this is correct, another commenter pointed out the same.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0a98q1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751136575,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0cskfg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Kooky-Net784","can_mod_post":false,"created_utc":1751170018,"send_replies":true,"parent_id":"t1_n0a4305","score":2,"author_fullname":"t2_1c7x793800","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I agree with this. The recently released Gemma 3n is on par with the best-in-class proprietary model from just six months ago (1.5 Pro). \\n\\nI expect lots more progress to come at the lower end of model sizes. Modular multimodality, more intricate matryoshka structures, per-layer embeddings, etc, to name a few. \\n\\nExciting times ahead.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0cskfg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I agree with this. The recently released Gemma 3n is on par with the best-in-class proprietary model from just six months ago (1.5 Pro). &lt;/p&gt;\\n\\n&lt;p&gt;I expect lots more progress to come at the lower end of model sizes. Modular multimodality, more intricate matryoshka structures, per-layer embeddings, etc, to name a few. &lt;/p&gt;\\n\\n&lt;p&gt;Exciting times ahead.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0cskfg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751170018,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0a4305","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ArsNeph","can_mod_post":false,"created_utc":1751134935,"send_replies":true,"parent_id":"t3_1lmk2dj","score":14,"author_fullname":"t2_vt0xkv60d","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Not at all, look at the parameter counts of these models. We are getting performance above the 110B Command A from Mistral Small 3.2 24B and Qwen 3 32B. There's definitely stagnation on the high-end, but we're able to accomplish with the high-end models do with increasingly less and less parameters","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0a4305","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not at all, look at the parameter counts of these models. We are getting performance above the 110B Command A from Mistral Small 3.2 24B and Qwen 3 32B. There&amp;#39;s definitely stagnation on the high-end, but we&amp;#39;re able to accomplish with the high-end models do with increasingly less and less parameters&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0a4305/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751134935,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmk2dj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":14}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0eevma","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Western_Objective209","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0e1vfl","score":1,"author_fullname":"t2_laeja0564","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yeah 4.5 is a massive model, trillions of params. In the chatGPT interface they hide it under \\"other models\\" as a research preview, and you don't get many requests for it","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0eevma","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah 4.5 is a massive model, trillions of params. In the chatGPT interface they hide it under &amp;quot;other models&amp;quot; as a research preview, and you don&amp;#39;t get many requests for it&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0eevma/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751201461,"author_flair_text":null,"treatment_tags":[],"created_utc":1751201461,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0e1vfl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ProfessionalJackals","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0a6o5x","score":1,"author_fullname":"t2_1lqtnyul8t","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Cost so much to run, that Microsoft is pulling it from CoPilot (where they charged 50 premium requests per request). \\n\\nClaude 3.5 &gt; 4 are at one premium request and are external. So if your model that you can run internally, on your own hardware, has a 50x costs factor, ...","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0e1vfl","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Cost so much to run, that Microsoft is pulling it from CoPilot (where they charged 50 premium requests per request). &lt;/p&gt;\\n\\n&lt;p&gt;Claude 3.5 &amp;gt; 4 are at one premium request and are external. So if your model that you can run internally, on your own hardware, has a 50x costs factor, ...&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0e1vfl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751195414,"author_flair_text":null,"treatment_tags":[],"created_utc":1751195414,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0gcpfk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"4sater","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0a6o5x","score":1,"author_fullname":"t2_ca23hfn1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Not really, for coding, analysis, etc. it was still behind most reasoning models.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0gcpfk","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not really, for coding, analysis, etc. it was still behind most reasoning models.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0gcpfk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751224263,"author_flair_text":null,"treatment_tags":[],"created_utc":1751224263,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0a6o5x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Western_Objective209","can_mod_post":false,"created_utc":1751135761,"send_replies":true,"parent_id":"t1_n083x4t","score":1,"author_fullname":"t2_laeja0564","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I thought gpt4.5 was getting near reasoning model performance","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0a6o5x","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I thought gpt4.5 was getting near reasoning model performance&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0a6o5x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751135761,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n09h8c5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Chemical_Mode2736","can_mod_post":false,"send_replies":true,"parent_id":"t1_n095hhm","score":2,"author_fullname":"t2_1dw3eipoio","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"local people aren't gonna like this but while current trend is smaller models getting more capable, I think with memory wall softening given Blackwell and rubin have so much more memory and the entrance of nvl72 and more, rack-based inference will strictly dominate home servers. basically barbell effect, with either edge computing models or seriously capable agentic models on hyperscaler servers. the order of priority for hbm goes from hyperscaler &gt; auto (bc reliability needs) &gt; consumer and without hbm memory wall for consumer will never go away","edited":false,"author_flair_css_class":null,"name":"t1_n09h8c5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;local people aren&amp;#39;t gonna like this but while current trend is smaller models getting more capable, I think with memory wall softening given Blackwell and rubin have so much more memory and the entrance of nvl72 and more, rack-based inference will strictly dominate home servers. basically barbell effect, with either edge computing models or seriously capable agentic models on hyperscaler servers. the order of priority for hbm goes from hyperscaler &amp;gt; auto (bc reliability needs) &amp;gt; consumer and without hbm memory wall for consumer will never go away&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lmk2dj","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n09h8c5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751127870,"author_flair_text":null,"collapsed":false,"created_utc":1751127870,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n095hhm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"send_replies":true,"parent_id":"t1_n094ij7","score":1,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yeah I agree with all points, but we need much faster inference. Reasoning now feels like browsing the internet at 56kbps.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n095hhm","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah I agree with all points, but we need much faster inference. Reasoning now feels like browsing the internet at 56kbps.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n095hhm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751124150,"author_flair_text":null,"treatment_tags":[],"created_utc":1751124150,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n094ij7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Chemical_Mode2736","can_mod_post":false,"send_replies":true,"parent_id":"t1_n084wdy","score":2,"author_fullname":"t2_1dw3eipoio","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"test time scaling is just a much more efficient scaling mechanism. it would be much harder to compute purely off non-reasoning. also reasoning is strictly better at coding and coding is the most financially viable use case right now. we're also earlier on the scaling curve for test-time vs non-reasoning, so more bang for your buck.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n094ij7","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;test time scaling is just a much more efficient scaling mechanism. it would be much harder to compute purely off non-reasoning. also reasoning is strictly better at coding and coding is the most financially viable use case right now. we&amp;#39;re also earlier on the scaling curve for test-time vs non-reasoning, so more bang for your buck.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n094ij7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751123847,"author_flair_text":null,"treatment_tags":[],"created_utc":1751123847,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n084wdy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"created_utc":1751110558,"send_replies":true,"parent_id":"t1_n083x4t","score":-1,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah I guess, GPT 4.1 was the last big performance boost for me.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n084wdy","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah I guess, GPT 4.1 was the last big performance boost for me.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n084wdy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751110558,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n083x4t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"pip25hu","can_mod_post":false,"created_utc":1751110069,"send_replies":true,"parent_id":"t3_1lmk2dj","score":14,"author_fullname":"t2_9u8ghp9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"More like progress stalled with non-reasoning models in general.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n083x4t","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;More like progress stalled with non-reasoning models in general.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n083x4t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751110069,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmk2dj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":14}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n09ghsb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Caffdy","can_mod_post":false,"send_replies":true,"parent_id":"t1_n08yjwq","score":1,"author_fullname":"t2_ql2vu0wz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"if you can just switch on and off reasoning, then it's a reasoning model (some people call them hybrids, but reasoning non the less)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n09ghsb","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;if you can just switch on and off reasoning, then it&amp;#39;s a reasoning model (some people call them hybrids, but reasoning non the less)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n09ghsb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751127635,"author_flair_text":null,"treatment_tags":[],"created_utc":1751127635,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n09d3pu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"amranu","can_mod_post":false,"send_replies":true,"parent_id":"t1_n09ck2n","score":6,"author_fullname":"t2_3yvyd","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Not in my experience. But I'm starting to judge models on their ability to find context in a codebase to solve problems themselves, and Claude is way better at that","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n09d3pu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not in my experience. But I&amp;#39;m starting to judge models on their ability to find context in a codebase to solve problems themselves, and Claude is way better at that&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n09d3pu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751126552,"author_flair_text":null,"treatment_tags":[],"created_utc":1751126552,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n09ck2n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"send_replies":true,"parent_id":"t1_n08yjwq","score":0,"author_fullname":"t2_h5utwre7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Like opus? Because sonnet 4 was pretty comparable.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n09ck2n","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Like opus? Because sonnet 4 was pretty comparable.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n09ck2n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751126379,"author_flair_text":null,"treatment_tags":[],"created_utc":1751126379,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n08yjwq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"amranu","can_mod_post":false,"created_utc":1751121951,"send_replies":true,"parent_id":"t1_n08qzpg","score":9,"author_fullname":"t2_3yvyd","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Claude 4 is so far beyond Deepseek V3 it's not even funny - and it's non-reasoning unless you enable reasoning.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n08yjwq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Claude 4 is so far beyond Deepseek V3 it&amp;#39;s not even funny - and it&amp;#39;s non-reasoning unless you enable reasoning.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n08yjwq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751121951,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}}],"before":null}},"user_reports":[],"saved":false,"id":"n08qzpg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MKU64","can_mod_post":false,"created_utc":1751119453,"send_replies":true,"parent_id":"t3_1lmk2dj","score":14,"author_fullname":"t2_wn7it","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Progress is stalled in non-reasoning models in general. If you focus in the Artificial Analysis Intelligence Index then DeepSeek V3 is the best non-reasoning model in both closed and open source.\\n\\nI think it’s just difficult to keep making non-reasoning smarter without going bigger. I think the only non-reasoning models I like more than V3 is GPT 4.1 and Sonnet 4, both are more than 8x more expensive so likely way bigger. Regardless they aren’t exactly smarter than V3 they just are better for some of my use cases.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n08qzpg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Progress is stalled in non-reasoning models in general. If you focus in the Artificial Analysis Intelligence Index then DeepSeek V3 is the best non-reasoning model in both closed and open source.&lt;/p&gt;\\n\\n&lt;p&gt;I think it’s just difficult to keep making non-reasoning smarter without going bigger. I think the only non-reasoning models I like more than V3 is GPT 4.1 and Sonnet 4, both are more than 8x more expensive so likely way bigger. Regardless they aren’t exactly smarter than V3 they just are better for some of my use cases.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n08qzpg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751119453,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmk2dj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":14}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0b8t9j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"created_utc":1751148259,"send_replies":true,"parent_id":"t1_n0b3y84","score":1,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Very cool.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0b8t9j","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Very cool.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0b8t9j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751148259,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0b3y84","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"BidWestern1056","can_mod_post":false,"created_utc":1751146581,"send_replies":true,"parent_id":"t3_1lmk2dj","score":3,"author_fullname":"t2_uzxql7po","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"these results and others are showing that we are approaching a fundamental efficacy limit for models that work primarily through a layer of natural language\\n\\n\\nhttps://arxiv.org/abs/2506.10077","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0b3y84","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;these results and others are showing that we are approaching a fundamental efficacy limit for models that work primarily through a layer of natural language&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://arxiv.org/abs/2506.10077\\"&gt;https://arxiv.org/abs/2506.10077&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0b3y84/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751146581,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmk2dj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n08awf6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"created_utc":1751113267,"send_replies":true,"parent_id":"t1_n08a3sv","score":1,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah I lost track of time, dsv3.1 is in the screenshot along with Llama 4. These are both 2025 models.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n08awf6","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah I lost track of time, dsv3.1 is in the screenshot along with Llama 4. These are both 2025 models.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n08awf6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751113267,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n08a3sv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"myvirtualrealitymask","can_mod_post":false,"created_utc":1751112920,"send_replies":true,"parent_id":"t3_1lmk2dj","score":7,"author_fullname":"t2_tfqbspezw","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"definitely not stalled, compare dsv3.1 to even closed sourced non reasoning models,  it is highly competitive and this was only a few months ago, look at mistral small 3.2 and compare it to mistral small 3.1's scores, it is way smarter","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n08a3sv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;definitely not stalled, compare dsv3.1 to even closed sourced non reasoning models,  it is highly competitive and this was only a few months ago, look at mistral small 3.2 and compare it to mistral small 3.1&amp;#39;s scores, it is way smarter&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n08a3sv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751112920,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmk2dj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0cutvk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0bze4a","score":1,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'm sold.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0cutvk","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m sold.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0cutvk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751171083,"author_flair_text":null,"treatment_tags":[],"created_utc":1751171083,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0bze4a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FPham","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0bxs7y","score":3,"author_fullname":"t2_7f6bw7v0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Well, what would break other models will not break Gemma-3. I did some pedal to the metal training on Gemma-3 and it is still not a blabbing baboon. Like the EP3 and EP4 should be by all means just reciting Dr. Seuss.   \\nGeema-3 Is the best finetuned model I've seen in a long time.\\n\\nhttps://preview.redd.it/dcbpdt60jr9f1.png?width=1200&amp;format=png&amp;auto=webp&amp;s=d8a0e1d812134b6f1ec2a23148fab4eb985a8443\\n\\nThis is actually Sydney-4 (As used at EP3)  \\n[https://huggingface.co/FPHam/Clever\\\\_Sydney-4\\\\_12b\\\\_GGUF](https://huggingface.co/FPHam/Clever_Sydney-4_12b_GGUF)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0bze4a","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Well, what would break other models will not break Gemma-3. I did some pedal to the metal training on Gemma-3 and it is still not a blabbing baboon. Like the EP3 and EP4 should be by all means just reciting Dr. Seuss.&lt;br/&gt;\\nGeema-3 Is the best finetuned model I&amp;#39;ve seen in a long time.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/dcbpdt60jr9f1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d8a0e1d812134b6f1ec2a23148fab4eb985a8443\\"&gt;https://preview.redd.it/dcbpdt60jr9f1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d8a0e1d812134b6f1ec2a23148fab4eb985a8443&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;This is actually Sydney-4 (As used at EP3)&lt;br/&gt;\\n&lt;a href=\\"https://huggingface.co/FPHam/Clever_Sydney-4_12b_GGUF\\"&gt;https://huggingface.co/FPHam/Clever_Sydney-4_12b_GGUF&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0bze4a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751157981,"media_metadata":{"dcbpdt60jr9f1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":63,"x":108,"u":"https://preview.redd.it/dcbpdt60jr9f1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=014dec01c73cf597ded1e2a84ec2590e585f284b"},{"y":126,"x":216,"u":"https://preview.redd.it/dcbpdt60jr9f1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9704447979e24dc92a25fbb4cbfa1e5d031e4803"},{"y":186,"x":320,"u":"https://preview.redd.it/dcbpdt60jr9f1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=121a488fe89f16ff80345648ffbd20a8a1769c7b"},{"y":373,"x":640,"u":"https://preview.redd.it/dcbpdt60jr9f1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9e82d43f3035aa19813cc6762f66022984aedd07"},{"y":560,"x":960,"u":"https://preview.redd.it/dcbpdt60jr9f1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=881ef88a9d85d72cab2660d41c347588b17aeb17"},{"y":630,"x":1080,"u":"https://preview.redd.it/dcbpdt60jr9f1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=322c2c72827b5556a7e05f3dfe84a757defa1893"}],"s":{"y":700,"x":1200,"u":"https://preview.redd.it/dcbpdt60jr9f1.png?width=1200&amp;format=png&amp;auto=webp&amp;s=d8a0e1d812134b6f1ec2a23148fab4eb985a8443"},"id":"dcbpdt60jr9f1"}},"author_flair_text":null,"treatment_tags":[],"created_utc":1751157981,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0bxs7y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"created_utc":1751157362,"send_replies":true,"parent_id":"t1_n0b9v4u","score":1,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Did you say fine-tune? Now I need to try this. I just realized post-finetuning performance is not very correlated with \\"intelligence\\" on this plot. It's more correlated with the number of pretraining tokens, and the model size because that determines the model's capacity to memorize and uncover patterns in the pretraining tokens.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0bxs7y","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Did you say fine-tune? Now I need to try this. I just realized post-finetuning performance is not very correlated with &amp;quot;intelligence&amp;quot; on this plot. It&amp;#39;s more correlated with the number of pretraining tokens, and the model size because that determines the model&amp;#39;s capacity to memorize and uncover patterns in the pretraining tokens.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0bxs7y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751157362,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0b9v4u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FPham","can_mod_post":false,"created_utc":1751148631,"send_replies":true,"parent_id":"t3_1lmk2dj","score":3,"author_fullname":"t2_7f6bw7v0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Where is my girl Gemma-3?   \\nSeriously, I've been dragging her through mud and she is something else.  In my opinion (which as we know is worth nothing) it is the best model that appeared in a long time. 130k context! Vision included! Finetunes like a butter. (Yeah, I know, I'm strong in analogies)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0b9v4u","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Where is my girl Gemma-3?&lt;br/&gt;\\nSeriously, I&amp;#39;ve been dragging her through mud and she is something else.  In my opinion (which as we know is worth nothing) it is the best model that appeared in a long time. 130k context! Vision included! Finetunes like a butter. (Yeah, I know, I&amp;#39;m strong in analogies)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0b9v4u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751148631,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmk2dj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n08k3ma","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"masc98","can_mod_post":false,"created_utc":1751116944,"send_replies":true,"parent_id":"t3_1lmk2dj","score":6,"author_fullname":"t2_12nt66","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"they are focusing on \\"reasoning\\" language models and image gen ones to produce much more high quality data which will then be fed to classic LM/VLMs.\\n\\nin the next months we're gonna see new releases for sure.\\n\\nclassic models is what 99% of people need to build applications. so dont worry","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n08k3ma","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;they are focusing on &amp;quot;reasoning&amp;quot; language models and image gen ones to produce much more high quality data which will then be fed to classic LM/VLMs.&lt;/p&gt;\\n\\n&lt;p&gt;in the next months we&amp;#39;re gonna see new releases for sure.&lt;/p&gt;\\n\\n&lt;p&gt;classic models is what 99% of people need to build applications. so dont worry&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n08k3ma/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751116944,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmk2dj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n09xeib","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"send_replies":true,"parent_id":"t1_n09vdwh","score":1,"author_fullname":"t2_h5utwre7","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Large is good, as was pixtral-large. I didn't try much serious work with them. If you swing those, you can likely do the 235b. I like it, but it's hard to trust it's answers because it hallucinates a lot. Didn't bother with dots due to how the root mean law paints it capability.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n09xeib","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Large is good, as was pixtral-large. I didn&amp;#39;t try much serious work with them. If you swing those, you can likely do the 235b. I like it, but it&amp;#39;s hard to trust it&amp;#39;s answers because it hallucinates a lot. Didn&amp;#39;t bother with dots due to how the root mean law paints it capability.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lmk2dj","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n09xeib/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751132874,"author_flair_text":null,"treatment_tags":[],"created_utc":1751132874,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n09vdwh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"custodiam99","can_mod_post":false,"send_replies":true,"parent_id":"t1_n09uns2","score":2,"author_fullname":"t2_nqnhgqqf5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Well for me dots.llm1 and Mistral Large are the largest ones I can run on my hardware.","edited":false,"author_flair_css_class":null,"name":"t1_n09vdwh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Well for me dots.llm1 and Mistral Large are the largest ones I can run on my hardware.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lmk2dj","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n09vdwh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751132269,"author_flair_text":null,"collapsed":false,"created_utc":1751132269,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n09uns2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"send_replies":true,"parent_id":"t1_n09l65p","score":2,"author_fullname":"t2_h5utwre7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Qwen got a lot more math/stem than L3.3 so there is that too. Papers are it's jam. \\n\\nIn fictional scenarios, the 32b will dumb harder than the 70b and that's where it's most visible for me. It also knows way less real world stuff, but imo more qwen than the size. When you give it rag, it will use it superficially, copy it's writing style, and take up context (which seems only effective up to 32k for both models anyway).\\n\\nWhen I've tried to use these small models for code or sysadmin things, even with websearch, I find myself going back to deepseek v3 (large non reasoning model, whoops). For what I ask, none of the small models seem to ever get me good outputs, 70b included.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n09uns2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen got a lot more math/stem than L3.3 so there is that too. Papers are it&amp;#39;s jam. &lt;/p&gt;\\n\\n&lt;p&gt;In fictional scenarios, the 32b will dumb harder than the 70b and that&amp;#39;s where it&amp;#39;s most visible for me. It also knows way less real world stuff, but imo more qwen than the size. When you give it rag, it will use it superficially, copy it&amp;#39;s writing style, and take up context (which seems only effective up to 32k for both models anyway).&lt;/p&gt;\\n\\n&lt;p&gt;When I&amp;#39;ve tried to use these small models for code or sysadmin things, even with websearch, I find myself going back to deepseek v3 (large non reasoning model, whoops). For what I ask, none of the small models seem to ever get me good outputs, 70b included.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n09uns2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751132050,"author_flair_text":null,"treatment_tags":[],"created_utc":1751132050,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n09l65p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"custodiam99","can_mod_post":false,"send_replies":true,"parent_id":"t1_n09dooa","score":2,"author_fullname":"t2_nqnhgqqf5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Not if the data is very abstract (like arXiv PDFs). Also I use Llama 70b 3.3 a lot, but I honestly don't see that it is *really* better than Qwen3 32b.","edited":1751129378,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n09l65p","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not if the data is very abstract (like arXiv PDFs). Also I use Llama 70b 3.3 a lot, but I honestly don&amp;#39;t see that it is &lt;em&gt;really&lt;/em&gt; better than Qwen3 32b.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n09l65p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751129137,"author_flair_text":null,"treatment_tags":[],"created_utc":1751129137,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n09dooa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1751126736,"send_replies":true,"parent_id":"t1_n085c7p","score":6,"author_fullname":"t2_h5utwre7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Large model still \\"understands\\" more. Spamming COT tokens can't really fix that. If you're just doing data processing, it's probably overkill.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n09dooa","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Large model still &amp;quot;understands&amp;quot; more. Spamming COT tokens can&amp;#39;t really fix that. If you&amp;#39;re just doing data processing, it&amp;#39;s probably overkill.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n09dooa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751126736,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n092bbk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"send_replies":true,"parent_id":"t1_n08s2i2","score":1,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"If it exists on Runpod I'd try it.","edited":false,"author_flair_css_class":null,"name":"t1_n092bbk","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If it exists on Runpod I&amp;#39;d try it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lmk2dj","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n092bbk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751123153,"author_flair_text":null,"collapsed":false,"created_utc":1751123153,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n08s2i2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Amazing_Athlete_2265","can_mod_post":false,"send_replies":true,"parent_id":"t1_n08l9fv","score":2,"author_fullname":"t2_1nw9fzb7dt","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Get a powerful rig,  and reason at 1000t/s","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n08s2i2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Get a powerful rig,  and reason at 1000t/s&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n08s2i2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751119822,"author_flair_text":null,"treatment_tags":[],"created_utc":1751119822,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0aaf6r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"send_replies":true,"parent_id":"t1_n09hpup","score":1,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"In my case (which is very-specific), the customer-facing agents take actions like pulling up related information, looking up products, etc. while the human customer service agent talks to the customer. This information is visible to both the customer and the agent. Think of it as a second pair of hands for the customer service agent.\\n\\nI don't think there is a good learning resource for this specific problem, I am learning through trial and error. I am also old and have a lot of experience fine-tuning BERT models before LLMs became a thing, so I just repurposed my old code.","edited":false,"author_flair_css_class":null,"name":"t1_n0aaf6r","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;In my case (which is very-specific), the customer-facing agents take actions like pulling up related information, looking up products, etc. while the human customer service agent talks to the customer. This information is visible to both the customer and the agent. Think of it as a second pair of hands for the customer service agent.&lt;/p&gt;\\n\\n&lt;p&gt;I don&amp;#39;t think there is a good learning resource for this specific problem, I am learning through trial and error. I am also old and have a lot of experience fine-tuning BERT models before LLMs became a thing, so I just repurposed my old code.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lmk2dj","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0aaf6r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751136938,"author_flair_text":null,"collapsed":false,"created_utc":1751136938,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n09hpup","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Caffdy","can_mod_post":false,"send_replies":true,"parent_id":"t1_n08l9fv","score":1,"author_fullname":"t2_ql2vu0wz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"what do you mean by customer facing agents? I'm interested in such development, where could I start learning about them?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n09hpup","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;what do you mean by customer facing agents? I&amp;#39;m interested in such development, where could I start learning about them?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n09hpup/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751128028,"author_flair_text":null,"treatment_tags":[],"created_utc":1751128028,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n08l9fv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"send_replies":true,"parent_id":"t1_n08c8uk","score":1,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I build realtime customer facing agents for a living.\\n\\nYou can't do realtime with reasoning right now.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n08l9fv","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I build realtime customer facing agents for a living.&lt;/p&gt;\\n\\n&lt;p&gt;You can&amp;#39;t do realtime with reasoning right now.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n08l9fv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751117380,"author_flair_text":null,"treatment_tags":[],"created_utc":1751117380,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n08teqz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"silenceimpaired","can_mod_post":false,"send_replies":true,"parent_id":"t1_n08djcp","score":1,"author_fullname":"t2_dissgzyl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I spit digitally on them and their model license… no model that allows for absolutely no commercial use is worth anything other than casual entertainment.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n08teqz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I spit digitally on them and their model license… no model that allows for absolutely no commercial use is worth anything other than casual entertainment.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n08teqz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751120273,"author_flair_text":null,"treatment_tags":[],"created_utc":1751120273,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n08djcp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"myvirtualrealitymask","can_mod_post":false,"send_replies":true,"parent_id":"t1_n08c8uk","score":1,"author_fullname":"t2_tfqbspezw","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes cohere's command A is a stellar corporate model. Good for chatting too","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n08djcp","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes cohere&amp;#39;s command A is a stellar corporate model. Good for chatting too&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n08djcp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751114375,"author_flair_text":null,"treatment_tags":[],"created_utc":1751114375,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n08c8uk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"vacationcelebration","can_mod_post":false,"created_utc":1751113838,"send_replies":true,"parent_id":"t1_n085c7p","score":3,"author_fullname":"t2_106kea","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Take a realtime customer facing agent that needs to intelligently communicate, take customer requests and act upon them with function calls, feedback and recommendations, consistently and at low latency.\\n\\nRegarding open weights, only qwen2.5 72b instruct and Cohere's latest command model have been able to (just barely) meet my standards; not deepseek, not even any of the qwen3 models.\\n\\nSo personally, I really hope we haven't reached a plateau.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n08c8uk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Take a realtime customer facing agent that needs to intelligently communicate, take customer requests and act upon them with function calls, feedback and recommendations, consistently and at low latency.&lt;/p&gt;\\n\\n&lt;p&gt;Regarding open weights, only qwen2.5 72b instruct and Cohere&amp;#39;s latest command model have been able to (just barely) meet my standards; not deepseek, not even any of the qwen3 models.&lt;/p&gt;\\n\\n&lt;p&gt;So personally, I really hope we haven&amp;#39;t reached a plateau.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n08c8uk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751113838,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n08bt13","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"custodiam99","can_mod_post":false,"send_replies":true,"parent_id":"t1_n08a8ut","score":3,"author_fullname":"t2_nqnhgqqf5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"They can write very consistent and structured large texts. In my experience they are much better for summarizing and data mining, because they can find hidden meaning too, not just verbal and syntactic similarity.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n08bt13","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They can write very consistent and structured large texts. In my experience they are much better for summarizing and data mining, because they can find hidden meaning too, not just verbal and syntactic similarity.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n08bt13/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751113654,"author_flair_text":null,"treatment_tags":[],"created_utc":1751113654,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n08a8ut","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"myvirtualrealitymask","can_mod_post":false,"created_utc":1751112981,"send_replies":true,"parent_id":"t1_n085c7p","score":7,"author_fullname":"t2_tfqbspezw","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"reasoning models are trash for writing and anything except math and coding","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n08a8ut","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;reasoning models are trash for writing and anything except math and coding&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n08a8ut/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751112981,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0e9qpt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"custodiam99","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0e8tmy","score":1,"author_fullname":"t2_nqnhgqqf5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I tested it out and - from my experience - it is not wrong.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0e9qpt","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I tested it out and - from my experience - it is not wrong.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0e9qpt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751199266,"author_flair_text":null,"treatment_tags":[],"created_utc":1751199266,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0e8tmy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"woahdudee2a","can_mod_post":false,"created_utc":1751198846,"send_replies":true,"parent_id":"t1_n085c7p","score":1,"author_fullname":"t2_o6qm5t0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I don't really get why people keep making this argument when you can just test it out and see it's wrong","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0e8tmy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t really get why people keep making this argument when you can just test it out and see it&amp;#39;s wrong&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0e8tmy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751198846,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n091xwo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"created_utc":1751123034,"send_replies":true,"parent_id":"t1_n08oj96","score":1,"author_fullname":"t2_1a48h7vf","approved_by":null,"mod_note":null,"all_awardings":[],"body":"It's a tradeoff. The average consumer loses attention in 5 seconds. My main project right now is a realtime voice application, 6-20 seconds is too long. And Qwen reasons that long for just a one word response to a 50-100 word prompt.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n091xwo","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s a tradeoff. The average consumer loses attention in 5 seconds. My main project right now is a realtime voice application, 6-20 seconds is too long. And Qwen reasons that long for just a one word response to a 50-100 word prompt.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n091xwo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751123034,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmk2dj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":8,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n08oj96","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"custodiam99","can_mod_post":false,"created_utc":1751118590,"send_replies":true,"parent_id":"t1_n08g69w","score":1,"author_fullname":"t2_nqnhgqqf5","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I use Qwen3 14b for summarizing and it takes 6-20 seconds to summarize 10 sentences. But the quality of reasoning models is much-much better.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n08oj96","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I use Qwen3 14b for summarizing and it takes 6-20 seconds to summarize 10 sentences. But the quality of reasoning models is much-much better.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lmk2dj","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n08oj96/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751118590,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n08g69w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"entsnack","can_mod_post":false,"send_replies":true,"parent_id":"t1_n08ceaw","score":5,"author_fullname":"t2_1a48h7vf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thank you, I though low-latency was a clear enough term. I work a lot with real-time voice calls and I can't have a model thinking for 1-2 minutes before providing concise advice.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n08g69w","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you, I though low-latency was a clear enough term. I work a lot with real-time voice calls and I can&amp;#39;t have a model thinking for 1-2 minutes before providing concise advice.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lmk2dj","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n08g69w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751115429,"author_flair_text":null,"treatment_tags":[],"created_utc":1751115429,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n08ceaw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"JaffyCaledonia","can_mod_post":false,"send_replies":true,"parent_id":"t1_n08be9u","score":9,"author_fullname":"t2_frpbdj6","approved_by":null,"mod_note":null,"all_awardings":[],"body":"In terms of tokens per second, sure. But a reasoning model might generate 2000 tokens of reasoning before giving a 1 word answer.\\n\\nUnless the small model is literally 2000x faster at generation, a large non-reasoning wins out!","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n08ceaw","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;In terms of tokens per second, sure. But a reasoning model might generate 2000 tokens of reasoning before giving a 1 word answer.&lt;/p&gt;\\n\\n&lt;p&gt;Unless the small model is literally 2000x faster at generation, a large non-reasoning wins out!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lmk2dj","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n08ceaw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751113903,"author_flair_text":null,"treatment_tags":[],"created_utc":1751113903,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}}],"before":null}},"user_reports":[],"saved":false,"id":"n08be9u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"custodiam99","can_mod_post":false,"send_replies":true,"parent_id":"t1_n088p99","score":-2,"author_fullname":"t2_nqnhgqqf5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I though smaller reasoning models are low-latency.","edited":false,"author_flair_css_class":null,"name":"t1_n08be9u","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I though smaller reasoning models are low-latency.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lmk2dj","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n08be9u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751113479,"author_flair_text":null,"collapsed":false,"created_utc":1751113479,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-2}}],"before":null}},"user_reports":[],"saved":false,"id":"n088p99","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"send_replies":true,"parent_id":"t1_n086n1u","score":0,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Do you not understand the phrase \\"low-latency\\"?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n088p99","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Do you not understand the phrase &amp;quot;low-latency&amp;quot;?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n088p99/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751112303,"author_flair_text":null,"treatment_tags":[],"created_utc":1751112303,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n086n1u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"custodiam99","can_mod_post":false,"send_replies":true,"parent_id":"t1_n08620r","score":1,"author_fullname":"t2_nqnhgqqf5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"A very clever small model can identify any information connected to quantum collapse but *it can't identify fraud* (if it has the training data)? That's kind of strange.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n086n1u","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;A very clever small model can identify any information connected to quantum collapse but &lt;em&gt;it can&amp;#39;t identify fraud&lt;/em&gt; (if it has the training data)? That&amp;#39;s kind of strange.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n086n1u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751111372,"author_flair_text":null,"treatment_tags":[],"created_utc":1751111372,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n08620r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"created_utc":1751111104,"send_replies":true,"parent_id":"t1_n085c7p","score":1,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Low-latency applications, like classifying fraud.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n08620r","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Low-latency applications, like classifying fraud.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n08620r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751111104,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n085c7p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"custodiam99","can_mod_post":false,"created_utc":1751110766,"send_replies":true,"parent_id":"t3_1lmk2dj","score":5,"author_fullname":"t2_nqnhgqqf5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I don't really get large non-reasoning models anymore. If I have a large database and a small, very clever reasoning model, why do I need a large model? I mean what for? The small model can use the database and it can mine VERY niche knowledge. It can use that mined knowledge and develop it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n085c7p","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t really get large non-reasoning models anymore. If I have a large database and a small, very clever reasoning model, why do I need a large model? I mean what for? The small model can use the database and it can mine VERY niche knowledge. It can use that mined knowledge and develop it.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n085c7p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751110766,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmk2dj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"fe89e94a-13f2-11f0-a9de-6262c74956cf","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n08fvxm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"created_utc":1751115316,"send_replies":true,"parent_id":"t1_n08f2ju","score":1,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes my screenshot is non-reasoning models only, it says so at the top left.\\n\\nEdit: I'm actually trying Mistral Small 3.2 right now!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n08fvxm","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes my screenshot is non-reasoning models only, it says so at the top left.&lt;/p&gt;\\n\\n&lt;p&gt;Edit: I&amp;#39;m actually trying Mistral Small 3.2 right now!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n08fvxm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751115316,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n08f2ju","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Asleep-Ratio7535","can_mod_post":false,"created_utc":1751114993,"send_replies":true,"parent_id":"t3_1lmk2dj","score":2,"author_fullname":"t2_1lfyddwf0c","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"sorry, but in your benchmark, QWEN3 has no think mode, and Mistral small has gained a lot, look at that Mistral large 2 published just half year ago.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n08f2ju","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 4"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;sorry, but in your benchmark, QWEN3 has no think mode, and Mistral small has gained a lot, look at that Mistral large 2 published just half year ago.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n08f2ju/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751114993,"author_flair_text":"Llama 4","treatment_tags":[],"link_id":"t3_1lmk2dj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#b0ae9b","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n09ghd3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RobotRobotWhatDoUSee","can_mod_post":false,"created_utc":1751127631,"send_replies":true,"parent_id":"t3_1lmk2dj","score":2,"author_fullname":"t2_m78cdz1nv","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"In my experience, only the most recently released non-reasoning models have been both smart enough and fast enough to be helpful with eg. statitical programming tasks, vs just being so incorrect or taking so long that it wasn't worth it. I felt like only very very recently have there been \\"good enough\\" local models for my use cases.\\n\\nBut as they say, YMMV!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n09ghd3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;In my experience, only the most recently released non-reasoning models have been both smart enough and fast enough to be helpful with eg. statitical programming tasks, vs just being so incorrect or taking so long that it wasn&amp;#39;t worth it. I felt like only very very recently have there been &amp;quot;good enough&amp;quot; local models for my use cases.&lt;/p&gt;\\n\\n&lt;p&gt;But as they say, YMMV!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n09ghd3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751127631,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmk2dj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"e75d59de-c910-11ed-9258-8e03d7ee129a","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0b797o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DigThatData","can_mod_post":false,"created_utc":1751147718,"send_replies":true,"parent_id":"t3_1lmk2dj","score":2,"author_fullname":"t2_jozy285b","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"y'all. it's 2025. this shit is still brand new, and it's peak vacation season. chill.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0b797o","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 7B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;y&amp;#39;all. it&amp;#39;s 2025. this shit is still brand new, and it&amp;#39;s peak vacation season. chill.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0b797o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751147718,"author_flair_text":"Llama 7B","treatment_tags":[],"link_id":"t3_1lmk2dj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n08z96d","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Hoodfu","can_mod_post":false,"created_utc":1751122176,"send_replies":true,"parent_id":"t3_1lmk2dj","score":3,"author_fullname":"t2_dnq0h","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I switched back from deepseek r1 0528 to deepseek v3 because I didn't feel like waiting for all the reasoning tokens and v3 is very close to r1 anyway for most stuff that I need it for. It seriously feels like a cheat code though. It's at the top because it truly feels like having Claude at home.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n08z96d","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I switched back from deepseek r1 0528 to deepseek v3 because I didn&amp;#39;t feel like waiting for all the reasoning tokens and v3 is very close to r1 anyway for most stuff that I need it for. It seriously feels like a cheat code though. It&amp;#39;s at the top because it truly feels like having Claude at home.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n08z96d/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751122176,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmk2dj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n09c9bq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1751126283,"send_replies":true,"parent_id":"t3_1lmk2dj","score":2,"author_fullname":"t2_h5utwre7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Stuff has been incremental for ages. Not just open source.\\n\\nPeople often say \\"nuh-uh\\" because it improved on their particular application or they still buy into benchmarks.\\n\\nThe focus has shifted to getting small models better and math/stem maxxing at the expense of everything else. Probably next thing will be pushing agents, which has already started.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n09c9bq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Stuff has been incremental for ages. Not just open source.&lt;/p&gt;\\n\\n&lt;p&gt;People often say &amp;quot;nuh-uh&amp;quot; because it improved on their particular application or they still buy into benchmarks.&lt;/p&gt;\\n\\n&lt;p&gt;The focus has shifted to getting small models better and math/stem maxxing at the expense of everything else. Probably next thing will be pushing agents, which has already started.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n09c9bq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751126283,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmk2dj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n08jzt7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"yaosio","can_mod_post":false,"created_utc":1751116903,"send_replies":true,"parent_id":"t3_1lmk2dj","score":1,"author_fullname":"t2_3z3zm","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"When did each of those models release?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n08jzt7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;When did each of those models release?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n08jzt7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751116903,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmk2dj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0959ne","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"created_utc":1751124082,"send_replies":true,"parent_id":"t1_n092psq","score":1,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"read post title and subreddit name","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0959ne","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;read post title and subreddit name&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0959ne/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751124082,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n092psq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kaleNhearty","can_mod_post":false,"created_utc":1751123282,"send_replies":true,"parent_id":"t3_1lmk2dj","score":1,"author_fullname":"t2_6wb1e","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"These are just the Open Source models, which is excluding a lot of the top models.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n092psq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;These are just the Open Source models, which is excluding a lot of the top models.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n092psq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751123282,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmk2dj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0apvwc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mythicinfinity","can_mod_post":false,"created_utc":1751141959,"send_replies":true,"parent_id":"t3_1lmk2dj","score":1,"author_fullname":"t2_s29nlx61","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Closed source seems to be improving.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0apvwc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Closed source seems to be improving.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0apvwc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751141959,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmk2dj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"d2642412-d9ce-11ed-ae30-32b11309f5bd","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0c6t7x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ortegaalfredo","can_mod_post":false,"created_utc":1751160884,"send_replies":true,"parent_id":"t3_1lmk2dj","score":1,"author_fullname":"t2_g177e","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"So for all the trashing and the firing of people and researchers quitting that Llama4 got, its there at second place? With a model half the size of Deepseek?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0c6t7x","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Alpaca"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So for all the trashing and the firing of people and researchers quitting that Llama4 got, its there at second place? With a model half the size of Deepseek?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0c6t7x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751160884,"author_flair_text":"Alpaca","treatment_tags":[],"link_id":"t3_1lmk2dj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bd9e9e","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0cmwj3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"306d316b72306e","can_mod_post":false,"created_utc":1751167514,"send_replies":true,"parent_id":"t3_1lmk2dj","score":1,"author_fullname":"t2_qrgr2","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"How are reasoning models difference? HLE and MMLU charting matches stagnation..","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0cmwj3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How are reasoning models difference? HLE and MMLU charting matches stagnation..&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0cmwj3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751167514,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmk2dj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jxxw5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Teetota","can_mod_post":false,"created_utc":1751278898,"send_replies":true,"parent_id":"t3_1lmk2dj","score":2,"author_fullname":"t2_8bu5nb0n","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Most gains right now are in efficiency area, not quality. It's actually good. Making research affordable to groups with tight budgets will eventually bring performance increases as well. What happens is China tryin to break big capital's monopoly in AI which I believe is the most important focus area for humanity right now.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jxxw5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Most gains right now are in efficiency area, not quality. It&amp;#39;s actually good. Making research affordable to groups with tight budgets will eventually bring performance increases as well. What happens is China tryin to break big capital&amp;#39;s monopoly in AI which I believe is the most important focus area for humanity right now.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0jxxw5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751278898,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmk2dj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ae6zn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AdventurousSwim1312","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0a9hlm","score":2,"author_fullname":"t2_nqj4dsg7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Have you tried exl3 and Awq? The Q4 quants almost don't affect performance.\\n\\nYeah, I downloaded the gptq version(tencent did one directly) but looks like inferences engines are not ready yet (I even tried to install vllm from the pr version of tencent team, but no luck, I'll wait for a few more days)\\n\\nFor policy optimization, you might want to take a look at Qwen embeddings models or modernbert tho, it seem more indicated than générative modeling to me.","edited":false,"author_flair_css_class":null,"name":"t1_n0ae6zn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Have you tried exl3 and Awq? The Q4 quants almost don&amp;#39;t affect performance.&lt;/p&gt;\\n\\n&lt;p&gt;Yeah, I downloaded the gptq version(tencent did one directly) but looks like inferences engines are not ready yet (I even tried to install vllm from the pr version of tencent team, but no luck, I&amp;#39;ll wait for a few more days)&lt;/p&gt;\\n\\n&lt;p&gt;For policy optimization, you might want to take a look at Qwen embeddings models or modernbert tho, it seem more indicated than générative modeling to me.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lmk2dj","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0ae6zn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751138145,"author_flair_text":null,"collapsed":false,"created_utc":1751138145,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0a9hlm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0a0sxi","score":1,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I assume you'll have to quantize it. I can't quantize my models because I also use them as reinforcement learning policies, which doesn't do well with quantization right now.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0a9hlm","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I assume you&amp;#39;ll have to quantize it. I can&amp;#39;t quantize my models because I also use them as reinforcement learning policies, which doesn&amp;#39;t do well with quantization right now.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0a9hlm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751136651,"author_flair_text":null,"treatment_tags":[],"created_utc":1751136651,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0a0sxi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AdventurousSwim1312","can_mod_post":false,"send_replies":true,"parent_id":"t1_n09dv6a","score":1,"author_fullname":"t2_nqj4dsg7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Tencent also just dropped a 80B13A model a few hours ago, didn't test yet (still downloading) but they announce similar bench as Qwen3 235B, but you can run it with only 48gb vram (so 2x3090) instead of 8 for qwen3","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0a0sxi","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Tencent also just dropped a 80B13A model a few hours ago, didn&amp;#39;t test yet (still downloading) but they announce similar bench as Qwen3 235B, but you can run it with only 48gb vram (so 2x3090) instead of 8 for qwen3&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0a0sxi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751133906,"author_flair_text":null,"treatment_tags":[],"created_utc":1751133906,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n09dv6a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"created_utc":1751126794,"send_replies":true,"parent_id":"t1_n09ahym","score":1,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Trying Mistral Small 3.2 now!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n09dv6a","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Trying Mistral Small 3.2 now!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n09dv6a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751126794,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n09ahym","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AdventurousSwim1312","can_mod_post":false,"created_utc":1751125726,"send_replies":true,"parent_id":"t3_1lmk2dj","score":1,"author_fullname":"t2_nqj4dsg7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Check new hunyuan model :)\\n\\nPlus given the power of mistral small and medium, upcoming large should re balance the cards ;)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n09ahym","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Check new hunyuan model :)&lt;/p&gt;\\n\\n&lt;p&gt;Plus given the power of mistral small and medium, upcoming large should re balance the cards ;)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n09ahym/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751125726,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmk2dj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ayzau","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"spawncampinitiated","can_mod_post":false,"created_utc":1751144906,"send_replies":true,"parent_id":"t3_1lmk2dj","score":1,"author_fullname":"t2_7u9m0dfr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Synthetic data causes this. We need better and evolving datasets (by evolving I mean daily snapshots of internet where actual and rich discussions are being held, not Reddit LULE)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ayzau","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Synthetic data causes this. We need better and evolving datasets (by evolving I mean daily snapshots of internet where actual and rich discussions are being held, not Reddit LULE)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0ayzau/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751144906,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmk2dj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0bd7ez","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No-Flight-2821","can_mod_post":false,"send_replies":true,"parent_id":"t1_n091m5a","score":2,"author_fullname":"t2_1gg4msmone","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It's a facade. Can't do anything at production level still without a human in the loop","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0bd7ez","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s a facade. Can&amp;#39;t do anything at production level still without a human in the loop&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0bd7ez/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751149832,"author_flair_text":null,"treatment_tags":[],"created_utc":1751149832,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n091m5a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"created_utc":1751122931,"send_replies":true,"parent_id":"t1_n08tuqh","score":1,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Test-time scaling is real though.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n091m5a","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Test-time scaling is real though.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n091m5a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751122931,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n08tuqh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Alkeryn","can_mod_post":false,"created_utc":1751120419,"send_replies":true,"parent_id":"t3_1lmk2dj","score":1,"author_fullname":"t2_xbcpo","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Reasoning is a meme and i wish it to be abandoned.","edited":1751172880,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n08tuqh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Reasoning is a meme and i wish it to be abandoned.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n08tuqh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751120419,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmk2dj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n09fyuk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"created_utc":1751127467,"send_replies":true,"parent_id":"t1_n09fslc","score":0,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Clearly you don't fine-tune.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n09fyuk","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Clearly you don&amp;#39;t fine-tune.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n09fyuk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751127467,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n09fslc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"michaelmalak","can_mod_post":false,"created_utc":1751127411,"send_replies":true,"parent_id":"t3_1lmk2dj","score":0,"author_fullname":"t2_9j2ab","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Eschew reasoning? Blurting out the first thing that comes to its mind like a middle-schooler can only take an LLM so far.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n09fslc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Eschew reasoning? Blurting out the first thing that comes to its mind like a middle-schooler can only take an LLM so far.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n09fslc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751127411,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmk2dj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n08aevj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0896u2","score":2,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Qwen is doing a good job for sure. Llama would be better off in public perception if they'd released smaller models with the Llama 4 suite.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n08aevj","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen is doing a good job for sure. Llama would be better off in public perception if they&amp;#39;d released smaller models with the Llama 4 suite.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n08aevj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751113054,"author_flair_text":null,"treatment_tags":[],"created_utc":1751113054,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n08sxyo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"silenceimpaired","can_mod_post":false,"send_replies":true,"parent_id":"t1_n08nmv2","score":1,"author_fullname":"t2_dissgzyl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What models do you like for writing? What type of writing?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n08sxyo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What models do you like for writing? What type of writing?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n08sxyo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751120116,"author_flair_text":null,"treatment_tags":[],"created_utc":1751120116,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n08nmv2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Super_Sierra","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0896u2","score":2,"author_fullname":"t2_9757bxah","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It writes like dog shit.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n08nmv2","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It writes like dog shit.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n08nmv2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751118262,"author_flair_text":null,"treatment_tags":[],"created_utc":1751118262,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n08tzmj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"dobomex761604","can_mod_post":false,"send_replies":true,"parent_id":"t1_n08ssmo","score":2,"author_fullname":"t2_o0bqaw8dp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"In my experience, Qwen models wrote very generic results for any creative tasks. Maybe they can be dragged out of it with careful prompting, but again - it goes towards my point that they are not general-purpose. Yes, mainline Mistral models, starting back from 7b, are better in creative writing than Qwen models.","edited":false,"author_flair_css_class":null,"name":"t1_n08tzmj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;In my experience, Qwen models wrote very generic results for any creative tasks. Maybe they can be dragged out of it with careful prompting, but again - it goes towards my point that they are not general-purpose. Yes, mainline Mistral models, starting back from 7b, are better in creative writing than Qwen models.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lmk2dj","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n08tzmj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751120465,"author_flair_text":null,"collapsed":false,"created_utc":1751120465,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n08ssmo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"silenceimpaired","can_mod_post":false,"send_replies":true,"parent_id":"t1_n08bhv3","score":1,"author_fullname":"t2_dissgzyl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I’m not sure … are you saying Mistral is better than Qwen at creative writing? Which is better for instruct following in adjusting existing text in your mind?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n08ssmo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I’m not sure … are you saying Mistral is better than Qwen at creative writing? Which is better for instruct following in adjusting existing text in your mind?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n08ssmo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751120067,"author_flair_text":null,"treatment_tags":[],"created_utc":1751120067,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0bl6s0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"-dysangel-","can_mod_post":false,"send_replies":true,"parent_id":"t1_n08bhv3","score":1,"author_fullname":"t2_12ggykute6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"oh for sure not finished. But the smaller sized models feel SOTA compared to everything else I've tried. The only ones I've liked better have been fine tunes of Qwen 3. For the largest open source models, Deepseek are still my favourite.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0bl6s0","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;oh for sure not finished. But the smaller sized models feel SOTA compared to everything else I&amp;#39;ve tried. The only ones I&amp;#39;ve liked better have been fine tunes of Qwen 3. For the largest open source models, Deepseek are still my favourite.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0bl6s0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751152735,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1751152735,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n08bhv3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"dobomex761604","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0896u2","score":1,"author_fullname":"t2_o0bqaw8dp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"They are not as great to be called finished, though. On the level of Mistral's models, better at coding, worse at following complex prompts, worse at creative writing  - still not a stable general-purpose model.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n08bhv3","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They are not as great to be called finished, though. On the level of Mistral&amp;#39;s models, better at coding, worse at following complex prompts, worse at creative writing  - still not a stable general-purpose model.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n08bhv3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751113523,"author_flair_text":null,"treatment_tags":[],"created_utc":1751113523,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0896u2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"-dysangel-","can_mod_post":false,"created_utc":1751112519,"send_replies":true,"parent_id":"t1_n0875tt","score":3,"author_fullname":"t2_12ggykute6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"the mid sized Qwen 3 models are in that range, and they're great","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0896u2","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;the mid sized Qwen 3 models are in that range, and they&amp;#39;re great&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0896u2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751112519,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n08ulb9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"dobomex761604","can_mod_post":false,"send_replies":true,"parent_id":"t1_n08mc4u","score":-1,"author_fullname":"t2_o0bqaw8dp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"\\\\&gt; No need to use a single model for everything.\\n\\nI disagree. I believe LLMs are mature enough as a technology to provide models that are good for most usecases. It's a shame that compute is wasted on models that can do only a very limited range of text tasks.","edited":false,"author_flair_css_class":null,"name":"t1_n08ulb9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&amp;gt; No need to use a single model for everything.&lt;/p&gt;\\n\\n&lt;p&gt;I disagree. I believe LLMs are mature enough as a technology to provide models that are good for most usecases. It&amp;#39;s a shame that compute is wasted on models that can do only a very limited range of text tasks.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lmk2dj","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n08ulb9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751120663,"author_flair_text":null,"collapsed":false,"created_utc":1751120663,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n08mc4u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"EasternBeyond","can_mod_post":false,"send_replies":true,"parent_id":"t1_n08kepl","score":6,"author_fullname":"t2_1a6diqhz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"To each his own. I find Gemma 3 to be better for a lot of things compared with others. No need to use a single model for everything.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n08mc4u","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;To each his own. I find Gemma 3 to be better for a lot of things compared with others. No need to use a single model for everything.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n08mc4u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751117781,"author_flair_text":null,"treatment_tags":[],"created_utc":1751117781,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n08kepl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"dobomex761604","can_mod_post":false,"send_replies":true,"parent_id":"t1_n08cek4","score":-1,"author_fullname":"t2_o0bqaw8dp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes, and? It's an overfitted nightmare that repeats a few structures over and over. It's not good at coding, it's censored as hell, and it has such a strong baked-in \\"personality\\" that trying to give it another one is a challenge. It's not a good model, and far from being general-purpose.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n08kepl","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, and? It&amp;#39;s an overfitted nightmare that repeats a few structures over and over. It&amp;#39;s not good at coding, it&amp;#39;s censored as hell, and it has such a strong baked-in &amp;quot;personality&amp;quot; that trying to give it another one is a challenge. It&amp;#39;s not a good model, and far from being general-purpose.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n08kepl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751117060,"author_flair_text":null,"treatment_tags":[],"created_utc":1751117060,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n08cek4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"EasternBeyond","can_mod_post":false,"created_utc":1751113906,"send_replies":true,"parent_id":"t1_n0875tt","score":2,"author_fullname":"t2_1a6diqhz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Gemma 27b is from Google","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n08cek4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Gemma 27b is from Google&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n08cek4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751113906,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n088mst","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"created_utc":1751112272,"send_replies":true,"parent_id":"t1_n0875tt","score":1,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I was thinking the same, there is indeed a rush to put something out on the leaderboard, and not enough emphasis on understanding what worked and what didn't work.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n088mst","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I was thinking the same, there is indeed a rush to put something out on the leaderboard, and not enough emphasis on understanding what worked and what didn&amp;#39;t work.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n088mst/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751112272,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0875tt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"dobomex761604","can_mod_post":false,"created_utc":1751111608,"send_replies":true,"parent_id":"t3_1lmk2dj","score":-1,"author_fullname":"t2_o0bqaw8dp","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yeah, maybe if companies weren't chasing fresh trends just to show-off, and finished at least one general-purpose model as a solid product, this wouldn't happen. Instead, we have reasoning models that are wasteful and aren't as useful as they are advertised.\\n\\nLlama series has no model in sizes from 14b to 35b at all, Mistral and Google failed to train at least one stably-performing model in that size, others don't seem to care about anything of average size - it's either 4b and lower, or 70+b.\\n\\nConsidering improvements to architectures, even training an old-size (7b, 14b, 22b?) model would give a better result, you just need to focus on finishing at least one model instead of experimenting on every new hot idea. Without it, all these new cool architectures and improvements will never be fully explored and will never become effective.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0875tt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, maybe if companies weren&amp;#39;t chasing fresh trends just to show-off, and finished at least one general-purpose model as a solid product, this wouldn&amp;#39;t happen. Instead, we have reasoning models that are wasteful and aren&amp;#39;t as useful as they are advertised.&lt;/p&gt;\\n\\n&lt;p&gt;Llama series has no model in sizes from 14b to 35b at all, Mistral and Google failed to train at least one stably-performing model in that size, others don&amp;#39;t seem to care about anything of average size - it&amp;#39;s either 4b and lower, or 70+b.&lt;/p&gt;\\n\\n&lt;p&gt;Considering improvements to architectures, even training an old-size (7b, 14b, 22b?) model would give a better result, you just need to focus on finishing at least one model instead of experimenting on every new hot idea. Without it, all these new cool architectures and improvements will never be fully explored and will never become effective.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0875tt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751111608,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmk2dj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}},{"kind":"t1","data":{"total_awards_received":0,"approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"ups":0,"removal_reason":null,"link_id":"t3_1lmk2dj","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n093ez5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"created_utc":1751123504,"send_replies":true,"parent_id":"t1_n091yit","score":1,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"We call it test-time compute in academia and it doesn indeed improve performance.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n093ez5","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;We call it test-time compute in academia and it doesn indeed improve performance.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n093ez5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751123504,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n091yit","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"DELETED","no_follow":true,"author":"[deleted]","can_mod_post":false,"send_replies":true,"parent_id":"t3_1lmk2dj","score":0,"approved_by":null,"report_reasons":null,"all_awardings":[],"subreddit_id":"t5_81eyvm","body":"[deleted]","edited":false,"downs":0,"author_flair_css_class":null,"collapsed":true,"is_submitter":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;[deleted]&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"associated_award":null,"stickied":false,"subreddit_type":"public","can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n091yit/","num_reports":null,"locked":false,"name":"t1_n091yit","created":1751123040,"subreddit":"LocalLLaMA","author_flair_text":null,"treatment_tags":[],"created_utc":1751123040,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"","collapsed_because_crowd_control":null,"mod_reports":[],"mod_note":null,"distinguished":null}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n08rfin","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"silenceimpaired","can_mod_post":false,"created_utc":1751119603,"send_replies":true,"parent_id":"t1_n08j4p6","score":3,"author_fullname":"t2_dissgzyl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I think that depends on what you’re doing. When it comes to creative writing 8b is no where close to 70b.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n08rfin","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think that depends on what you’re doing. When it comes to creative writing 8b is no where close to 70b.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n08rfin/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751119603,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n08j4p6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DataCraftsman","can_mod_post":false,"created_utc":1751116573,"send_replies":true,"parent_id":"t3_1lmk2dj","score":-3,"author_fullname":"t2_1awad2722o","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Gemma 3n, mistral small 3.2, qwen 3 are all incredible and new. The models are just getting denser. A year ago you would use llama 3.1 70b for the same results you'd get from an 8b model now. Most people are using llms on single gpus, or just paying for an online service, so it makes sense to lower the size of the open source models. Gemma 3n is equivalent to llama 3 70b, but has vision, 4x the context length, and runs on a phone cpu.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n08j4p6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Gemma 3n, mistral small 3.2, qwen 3 are all incredible and new. The models are just getting denser. A year ago you would use llama 3.1 70b for the same results you&amp;#39;d get from an 8b model now. Most people are using llms on single gpus, or just paying for an online service, so it makes sense to lower the size of the open source models. Gemma 3n is equivalent to llama 3 70b, but has vision, 4x the context length, and runs on a phone cpu.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n08j4p6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751116573,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmk2dj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-3}},{"kind":"t1","data":{"total_awards_received":0,"approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"ups":-1,"removal_reason":null,"link_id":"t3_1lmk2dj","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"total_awards_received":0,"approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"ups":0,"removal_reason":null,"link_id":"t3_1lmk2dj","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ba1y8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0b9ovs","score":1,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The post is not about proprietary SOTA models. The post is about open-source non-reasoning models. You're not \\"highlighting\\" anything lmao. Shill.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ba1y8","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The post is not about proprietary SOTA models. The post is about open-source non-reasoning models. You&amp;#39;re not &amp;quot;highlighting&amp;quot; anything lmao. Shill.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0ba1y8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751148697,"author_flair_text":null,"treatment_tags":[],"created_utc":1751148697,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0b9ovs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"[deleted]","can_mod_post":false,"created_utc":1751148570,"send_replies":true,"parent_id":"t1_n0b8ypg","score":0,"approved_by":null,"report_reasons":null,"all_awardings":[],"subreddit_id":"t5_81eyvm","body":"How is it completely unrelated? I just highlighted how DeepSeek isn’t much of a downgrade compared to proprietary SOTA models lol","edited":false,"author_flair_css_class":null,"downs":0,"is_submitter":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How is it completely unrelated? I just highlighted how DeepSeek isn’t much of a downgrade compared to proprietary SOTA models lol&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"associated_award":null,"stickied":false,"subreddit_type":"public","can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0b9ovs/","num_reports":null,"locked":false,"name":"t1_n0b9ovs","created":1751148570,"subreddit":"LocalLLaMA","author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"","collapsed_because_crowd_control":null,"mod_reports":[],"mod_note":null,"distinguished":null}}],"before":null}},"user_reports":[],"saved":false,"id":"n0b8ypg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"created_utc":1751148312,"send_replies":true,"parent_id":"t1_n0b5hc4","score":2,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"of course someone comes in to shill DeepSeek on a completely unrelated note","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0b8ypg","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;of course someone comes in to shill DeepSeek on a completely unrelated note&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0b8ypg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751148312,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0b5hc4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"[deleted]","can_mod_post":false,"send_replies":true,"parent_id":"t3_1lmk2dj","score":-1,"approved_by":null,"report_reasons":null,"all_awardings":[],"subreddit_id":"t5_81eyvm","body":"Just be patient. These models aren’t usually developed by the local trillion dollar corporation. I use DeepSeek regularly without it feeling like it’s a major downgrade vs Gemini or ChatGPT","edited":false,"downs":0,"author_flair_css_class":null,"collapsed":false,"is_submitter":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just be patient. These models aren’t usually developed by the local trillion dollar corporation. I use DeepSeek regularly without it feeling like it’s a major downgrade vs Gemini or ChatGPT&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"associated_award":null,"stickied":false,"subreddit_type":"public","can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0b5hc4/","num_reports":null,"locked":false,"name":"t1_n0b5hc4","created":1751147107,"subreddit":"LocalLLaMA","author_flair_text":null,"treatment_tags":[],"created_utc":1751147107,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"","collapsed_because_crowd_control":null,"mod_reports":[],"mod_note":null,"distinguished":null}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0a9caj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"created_utc":1751136606,"send_replies":true,"parent_id":"t1_n0a1jxo","score":3,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Have you considered thinking before writing?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0a9caj","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Have you considered thinking before writing?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmk2dj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0a9caj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751136606,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0a1jxo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"owenwp","can_mod_post":false,"created_utc":1751134141,"send_replies":true,"parent_id":"t3_1lmk2dj","score":-4,"author_fullname":"t2_95880","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"A non reasoning model is akin to a person who is only allowed to give instantaneous intuitive answers to questions, with no opportunity to take multiple steps to find an answer. So their natural theoretical limit is the limit of intuition.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0a1jxo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;A non reasoning model is akin to a person who is only allowed to give instantaneous intuitive answers to questions, with no opportunity to take multiple steps to find an answer. So their natural theoretical limit is the limit of intuition.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmk2dj/progress_stalled_in_nonreasoning_opensource_models/n0a1jxo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751134141,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmk2dj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-4}}],"before":null}}]`),o=()=>e.jsx(l,{data:a});export{o as default};
