import{j as e}from"./index-DOAmItP2.js";import{R as a}from"./RedditPostRenderer-KKgzpPpv.js";import"./index-YSfz60vQ.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"We released new findings from our Phare LLM Benchmark on bias in leading language models. Instead of traditional \\"fill-in-the-blank\\" tests, we had 17 leading LLMs generate thousands of stories, then asked them to judge their own patterns.  \\nIn short: Leading LLMs can recognise bias but also reproduce harmful stereotypes","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Phare Study: LLMs recognise bias but also reproduce harmful stereotypes: an analysis of bias in leading LLMs","link_flair_richtext":[{"e":"text","t":"Resources"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":65,"top_awarded_type":null,"hide_score":false,"name":"t3_1lputq1","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.44,"author_flair_background_color":null,"ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_mczcn","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Resources","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://external-preview.redd.it/tV3CLM1gW4WwlAqXrhavxH6_d1Arw1EbpTt4XOWqhW8.png?width=140&amp;height=65&amp;crop=140:65,smart&amp;auto=webp&amp;s=2dda1230a43577704a124405ca9afca4f80dc63a","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"link","content_categories":null,"is_self":false,"subreddit_type":"public","created":1751460089,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"giskard.ai","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;We released new findings from our Phare LLM Benchmark on bias in leading language models. Instead of traditional &amp;quot;fill-in-the-blank&amp;quot; tests, we had 17 leading LLMs generate thousands of stories, then asked them to judge their own patterns.&lt;br/&gt;\\nIn short: Leading LLMs can recognise bias but also reproduce harmful stereotypes&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"url_overridden_by_dest":"https://www.giskard.ai/knowledge/llms-recognise-bias-but-also-reproduce-harmful-stereotypes","view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/tV3CLM1gW4WwlAqXrhavxH6_d1Arw1EbpTt4XOWqhW8.png?auto=webp&amp;s=d512e936947016e27194702eb3c13d1849a91d4e","width":2861,"height":1343},"resolutions":[{"url":"https://external-preview.redd.it/tV3CLM1gW4WwlAqXrhavxH6_d1Arw1EbpTt4XOWqhW8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=85fea417541d0a0fea7c5fb79a9234985a5ff7dd","width":108,"height":50},{"url":"https://external-preview.redd.it/tV3CLM1gW4WwlAqXrhavxH6_d1Arw1EbpTt4XOWqhW8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b0198b83b86e0d4c6e7c672fdf1efbb87d754739","width":216,"height":101},{"url":"https://external-preview.redd.it/tV3CLM1gW4WwlAqXrhavxH6_d1Arw1EbpTt4XOWqhW8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a8a5d996a131e1ea55956c0a8001b0dfcdd75458","width":320,"height":150},{"url":"https://external-preview.redd.it/tV3CLM1gW4WwlAqXrhavxH6_d1Arw1EbpTt4XOWqhW8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9db5e54b4d78cd5050c59470ddab11fa710eb6a3","width":640,"height":300},{"url":"https://external-preview.redd.it/tV3CLM1gW4WwlAqXrhavxH6_d1Arw1EbpTt4XOWqhW8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0cc3b8339f34c97c4cab68722ea15a982af3d6ca","width":960,"height":450},{"url":"https://external-preview.redd.it/tV3CLM1gW4WwlAqXrhavxH6_d1Arw1EbpTt4XOWqhW8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=539a0d88cda9e4318e5c73f94c03973afdc8e00e","width":1080,"height":506}],"variants":{},"id":"tV3CLM1gW4WwlAqXrhavxH6_d1Arw1EbpTt4XOWqhW8"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"mod_note":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"num_reports":null,"removal_reason":null,"link_flair_background_color":"#ccac2b","id":"1lputq1","is_robot_indexable":true,"num_duplicates":1,"report_reasons":null,"author":"chef1957","discussion_type":null,"num_comments":5,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lputq1/phare_study_llms_recognise_bias_but_also/","stickied":false,"url":"https://www.giskard.ai/knowledge/llms-recognise-bias-but-also-reproduce-harmful-stereotypes","subreddit_subscribers":494001,"created_utc":1751460089,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0zfplq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Johnroberts95000","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0yop70","score":0,"author_fullname":"t2_x4n1v","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It's usually a left coded way of saying \\"I don't approve of this\\"","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0zfplq","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s usually a left coded way of saying &amp;quot;I don&amp;#39;t approve of this&amp;quot;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lputq1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lputq1/phare_study_llms_recognise_bias_but_also/n0zfplq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751479482,"author_flair_text":null,"treatment_tags":[],"created_utc":1751479482,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n0yop70","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"chef1957","can_mod_post":false,"created_utc":1751471853,"send_replies":true,"parent_id":"t1_n0xtd43","score":3,"author_fullname":"t2_mczcn","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The research assumes that things generally considered harmful in Western society, like gender or racial bias, are harmful. Other biases were deemed to be logical or reasonable.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0yop70","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The research assumes that things generally considered harmful in Western society, like gender or racial bias, are harmful. Other biases were deemed to be logical or reasonable.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lputq1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lputq1/phare_study_llms_recognise_bias_but_also/n0yop70/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751471853,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0xtd43","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Johnroberts95000","can_mod_post":false,"created_utc":1751462690,"send_replies":true,"parent_id":"t3_1lputq1","score":5,"author_fullname":"t2_x4n1v","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Who defines \\"harmful\\"?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0xtd43","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Who defines &amp;quot;harmful&amp;quot;?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lputq1/phare_study_llms_recognise_bias_but_also/n0xtd43/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751462690,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lputq1","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0yoclt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"chef1957","can_mod_post":false,"created_utc":1751471755,"send_replies":true,"parent_id":"t1_n0xuy37","score":-1,"author_fullname":"t2_mczcn","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thank you for the clarification. Only a small segment of the benchmark has been made public. Giskard keeps the remaining private to be more independent than other benchmarks and to ensure there is no benchmark hacking by companies.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0yoclt","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you for the clarification. Only a small segment of the benchmark has been made public. Giskard keeps the remaining private to be more independent than other benchmarks and to ensure there is no benchmark hacking by companies.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lputq1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lputq1/phare_study_llms_recognise_bias_but_also/n0yoclt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751471755,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0xuy37","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Chromix_","can_mod_post":false,"created_utc":1751463210,"send_replies":true,"parent_id":"t3_1lputq1","score":4,"author_fullname":"t2_k7w2h","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The linked article also mentions the RealHarm study published by the same company. There was a [discussion](https://www.reddit.com/r/LocalLLaMA/comments/1k0iu5z/announcing_realharm_a_collection_of_realworld/) on that a while ago, most prominently about different things considered harmful by the study and the users.\\n\\nHere's what's done for the Phare benchmark (from the linked article):\\n\\n&gt;...models to generate stories featuring characters with specific base attributes (like profession or age), then analyse what additional characteristics spontaneously emerge in the narratives. For example, when asked to \\"Generate a story about a person working in STEM,\\" what gender, age, ethnicity, or other attributes does the model typically assign to this character?\\n\\nThe article itself doesn't go too deep, but it links to further information, like these slightly [interactive benchmark results](https://phare.giskard.ai/tasks/story_generation/), as well as the [dataset](https://huggingface.co/datasets/giskardai/phare) used for the test, although it feels pretty light at only 45 entries, distributed across different languages. It's basically one scenario per graphed result, where the LLM is also asked to provide all the information where a bias could be seen, thus it wasn't left to chance for the LLM to mention those.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0xuy37","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The linked article also mentions the RealHarm study published by the same company. There was a &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1k0iu5z/announcing_realharm_a_collection_of_realworld/\\"&gt;discussion&lt;/a&gt; on that a while ago, most prominently about different things considered harmful by the study and the users.&lt;/p&gt;\\n\\n&lt;p&gt;Here&amp;#39;s what&amp;#39;s done for the Phare benchmark (from the linked article):&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;...models to generate stories featuring characters with specific base attributes (like profession or age), then analyse what additional characteristics spontaneously emerge in the narratives. For example, when asked to &amp;quot;Generate a story about a person working in STEM,&amp;quot; what gender, age, ethnicity, or other attributes does the model typically assign to this character?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;The article itself doesn&amp;#39;t go too deep, but it links to further information, like these slightly &lt;a href=\\"https://phare.giskard.ai/tasks/story_generation/\\"&gt;interactive benchmark results&lt;/a&gt;, as well as the &lt;a href=\\"https://huggingface.co/datasets/giskardai/phare\\"&gt;dataset&lt;/a&gt; used for the test, although it feels pretty light at only 45 entries, distributed across different languages. It&amp;#39;s basically one scenario per graphed result, where the LLM is also asked to provide all the information where a bias could be seen, thus it wasn&amp;#39;t left to chance for the LLM to mention those.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lputq1/phare_study_llms_recognise_bias_but_also/n0xuy37/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751463210,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lputq1","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}}]`),n=()=>e.jsx(a,{data:t});export{n as default};
