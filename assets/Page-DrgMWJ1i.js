import{j as e}from"./index-M4edQi1P.js";import{R as l}from"./RedditPostRenderer-CESBGIIy.js";import"./index-DFpL1mt4.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Huawei releases an open weight model Pangu Pro 72B A16B. Weights are on HF. It should be competitive with Qwen3 32B and it was trained entirely on Huawei Ascend NPUs. (2505.21411)","link_flair_richtext":[{"e":"text","t":"New Model"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":75,"top_awarded_type":null,"hide_score":false,"name":"t3_1lp9gh2","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.97,"author_flair_background_color":null,"ups":518,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_9s7pmakgx","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"New Model","can_mod_post":false,"score":518,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://external-preview.redd.it/KKUaRRu1NZXsmquOk2Id9DRnEhBD6P6w5Y5xZQur5Yc.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=cdd7cf8a2002d72c1a2a37a8f23acfa4d1952c22","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"link","content_categories":null,"is_self":false,"subreddit_type":"public","created":1751394651,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"huggingface.co","allow_live_comments":false,"selftext_html":null,"likes":null,"suggested_sort":null,"banned_at_utc":null,"url_overridden_by_dest":"https://huggingface.co/IntervitensInc/pangu-pro-moe-model","view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/KKUaRRu1NZXsmquOk2Id9DRnEhBD6P6w5Y5xZQur5Yc.png?auto=webp&amp;s=192b3d3e9c02a82a06d21d0bae530698ffef8dc3","width":1200,"height":648},"resolutions":[{"url":"https://external-preview.redd.it/KKUaRRu1NZXsmquOk2Id9DRnEhBD6P6w5Y5xZQur5Yc.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5fa50f130c1d12794fe17be8766a2c4749d61f5a","width":108,"height":58},{"url":"https://external-preview.redd.it/KKUaRRu1NZXsmquOk2Id9DRnEhBD6P6w5Y5xZQur5Yc.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fc158227d51a8b988d7232029edea6b3b7fbf734","width":216,"height":116},{"url":"https://external-preview.redd.it/KKUaRRu1NZXsmquOk2Id9DRnEhBD6P6w5Y5xZQur5Yc.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a5ff1813a9a4dc8452b94c5ff74ce5bebf716297","width":320,"height":172},{"url":"https://external-preview.redd.it/KKUaRRu1NZXsmquOk2Id9DRnEhBD6P6w5Y5xZQur5Yc.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=dd39a4d6488e7f71969bdc8665d7c2dbe902c2b5","width":640,"height":345},{"url":"https://external-preview.redd.it/KKUaRRu1NZXsmquOk2Id9DRnEhBD6P6w5Y5xZQur5Yc.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=65e8e5e6138c705563eb6eca5921c617f5071ab4","width":960,"height":518},{"url":"https://external-preview.redd.it/KKUaRRu1NZXsmquOk2Id9DRnEhBD6P6w5Y5xZQur5Yc.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c0c728888e14c64c37ad356f82ee2cc33223ddf6","width":1080,"height":583}],"variants":{},"id":"KKUaRRu1NZXsmquOk2Id9DRnEhBD6P6w5Y5xZQur5Yc"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ced98442-f5d3-11ed-b657-66d3b15490c6","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"mod_note":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"num_reports":null,"removal_reason":null,"link_flair_background_color":"#ffb000","id":"1lp9gh2","is_robot_indexable":true,"num_duplicates":1,"report_reasons":null,"author":"FullOf_Bad_Ideas","discussion_type":null,"num_comments":79,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/","stickied":false,"url":"https://huggingface.co/IntervitensInc/pangu-pro-moe-model","subreddit_subscribers":494001,"created_utc":1751394651,"num_crossposts":1,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0uwrtn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Severin_Suveren","can_mod_post":false,"created_utc":1751416022,"send_replies":true,"parent_id":"t1_n0um97t","score":1,"author_fullname":"t2_vfpsd1c8","approved_by":null,"mod_note":null,"all_awardings":[],"body":"\\\\**random pop corn chillin gif*\\\\*","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0uwrtn","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;*&lt;em&gt;random pop corn chillin gif&lt;/em&gt;*&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0uwrtn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751416022,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp9gh2","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":8,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0vn62w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"Caffdy","can_mod_post":false,"created_utc":1751425414,"send_replies":true,"parent_id":"t1_n0um97t","score":-6,"author_fullname":"t2_ql2vu0wz","approved_by":null,"mod_note":null,"all_awardings":[],"body":"are the downvotes in the room with us right now?","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0vn62w","is_submitter":false,"collapsed":true,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;are the downvotes in the room with us right now?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0vn62w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751425414,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp9gh2","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":8,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-6}}],"before":null}},"user_reports":[],"saved":false,"id":"n0um97t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Orolol","can_mod_post":false,"created_utc":1751412393,"send_replies":true,"parent_id":"t1_n0ujt6o","score":9,"author_fullname":"t2_fbzx9","approved_by":null,"mod_note":null,"all_awardings":[],"body":"You got triggered by downvotes.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n0um97t","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You got triggered by downvotes.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lp9gh2","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0um97t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751412393,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ujt6o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Caffdy","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0u2bk4","score":1,"author_fullname":"t2_ql2vu0wz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"caps are for emphasis in this case. People on the internet are a bunch of snowflakes that gets triggered by anything","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n0ujt6o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;caps are for emphasis in this case. People on the internet are a bunch of snowflakes that gets triggered by anything&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lp9gh2","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0ujt6o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751411589,"author_flair_text":null,"treatment_tags":[],"created_utc":1751411589,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0u2bk4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Orolol","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0tnvgs","score":23,"author_fullname":"t2_fbzx9","approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; respectfully   \\n  \\nAll caps suggested otherwise.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0u2bk4","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;respectfully   &lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;All caps suggested otherwise.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lp9gh2","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0u2bk4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751405973,"author_flair_text":null,"treatment_tags":[],"created_utc":1751405973,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":23}}],"before":null}},"user_reports":[],"saved":false,"id":"n0tnvgs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"dugavo","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0tnofs","score":22,"author_fullname":"t2_1nge67um4h","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You're welcome.\\n\\nI don't get all the people downvoting me, how can we improve ourselves without respectfully correcting each other?","edited":false,"author_flair_css_class":null,"name":"t1_n0tnvgs","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You&amp;#39;re welcome.&lt;/p&gt;\\n\\n&lt;p&gt;I don&amp;#39;t get all the people downvoting me, how can we improve ourselves without respectfully correcting each other?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lp9gh2","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0tnvgs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751401773,"author_flair_text":null,"collapsed":false,"created_utc":1751401773,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":22}}],"before":null}},"user_reports":[],"saved":false,"id":"n0tnofs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FullOf_Bad_Ideas","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0tlb4k","score":28,"author_fullname":"t2_9s7pmakgx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yeah you're right, I corrected it. Thanks.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0tnofs","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah you&amp;#39;re right, I corrected it. Thanks.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0tnofs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751401718,"author_flair_text":null,"treatment_tags":[],"created_utc":1751401718,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":28}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ucwfs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jbutlerdev","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0tlb4k","score":0,"author_fullname":"t2_azse6ibv","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It is weights though...","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ucwfs","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It is weights though...&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0ucwfs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751409306,"author_flair_text":null,"treatment_tags":[],"created_utc":1751409306,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n0tlb4k","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"dugavo","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0t2uws","score":18,"author_fullname":"t2_1nge67um4h","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"IT'S means IT IS\\n\\nYou probably mean: \\"its weights\\"","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0tlb4k","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;IT&amp;#39;S means IT IS&lt;/p&gt;\\n\\n&lt;p&gt;You probably mean: &amp;quot;its weights&amp;quot;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0tlb4k/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751401056,"author_flair_text":null,"treatment_tags":[],"created_utc":1751401056,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":18}}],"before":null}},"user_reports":[],"saved":false,"id":"n0t2uws","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FullOf_Bad_Ideas","can_mod_post":false,"created_utc":1751395741,"send_replies":true,"parent_id":"t1_n0t0na8","score":76,"author_fullname":"t2_9s7pmakgx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"They also trained Pangu Ultra 718B A39B on their own chips, here's a paper - https://huggingface.co/papers/2505.04519\\n\\nIt's close to DeepSeek R1 on evals. \\n\\nI don't think its weights were released, but maybe I just don't know the right places to look for it.","edited":1751401697,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0t2uws","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They also trained Pangu Ultra 718B A39B on their own chips, here&amp;#39;s a paper - &lt;a href=\\"https://huggingface.co/papers/2505.04519\\"&gt;https://huggingface.co/papers/2505.04519&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s close to DeepSeek R1 on evals. &lt;/p&gt;\\n\\n&lt;p&gt;I don&amp;#39;t think its weights were released, but maybe I just don&amp;#39;t know the right places to look for it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0t2uws/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751395741,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":76}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0u1apd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"mission_tiefsee","can_mod_post":false,"created_utc":1751405665,"send_replies":true,"parent_id":"t1_n0t0na8","score":10,"author_fullname":"t2_1l0xi85fdm","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"i wonder if it is all china from here. Best models (or close eg deepseek), best open source(qwen) and now also chips? Thumbs up! Hope to see more hardware and more competition to nvidia!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0u1apd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;i wonder if it is all china from here. Best models (or close eg deepseek), best open source(qwen) and now also chips? Thumbs up! Hope to see more hardware and more competition to nvidia!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0u1apd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751405665,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}}],"before":null}},"user_reports":[],"saved":false,"id":"n0t0na8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"atape_1","can_mod_post":false,"created_utc":1751395118,"send_replies":true,"parent_id":"t3_1lp9gh2","score":218,"author_fullname":"t2_k35bw","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"First models trained on Huawei chips, nice. Can't wait to see more. We need more competition in the hardware space.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0t0na8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;First models trained on Huawei chips, nice. Can&amp;#39;t wait to see more. We need more competition in the hardware space.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0t0na8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751395118,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp9gh2","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":218}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0upc2b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullOf_Bad_Ideas","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ugkva","score":2,"author_fullname":"t2_9s7pmakgx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I agree in principle.\\n\\nWe get many different models in various sizes, and everyone is free to pick the model that works for their usecase. If you have a task that requires heavy parallelization, you might like MoEs since less activated parameters means less compute needed per each forward pass, which means that you can squeeze in more throughput, if you have the VRAM for it. There are hundreds of usecases for LLMs and hundreds of different hardware configurations, more choice is good. 32B dense is nice, but I don't want all models to be 32B dense.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0upc2b","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I agree in principle.&lt;/p&gt;\\n\\n&lt;p&gt;We get many different models in various sizes, and everyone is free to pick the model that works for their usecase. If you have a task that requires heavy parallelization, you might like MoEs since less activated parameters means less compute needed per each forward pass, which means that you can squeeze in more throughput, if you have the VRAM for it. There are hundreds of usecases for LLMs and hundreds of different hardware configurations, more choice is good. 32B dense is nice, but I don&amp;#39;t want all models to be 32B dense.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0upc2b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751413416,"author_flair_text":null,"treatment_tags":[],"created_utc":1751413416,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ugkva","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No-Refrigerator-1672","can_mod_post":false,"created_utc":1751410519,"send_replies":true,"parent_id":"t1_n0t13nf","score":22,"author_fullname":"t2_baavelp5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt;smaller 32B dense models leave some VRAM unused\\n\\nThere's no such thing as useless VRAM; each GB that is not filled by weights can be filled by activations and KV cache to either handle long contextes or multiple requests in parallel, or it can be allocated for embedding model, draft model, tts/stt models, etc. So trading off 2x larger weight memory for up to 2x performance uplift is kinda too niche usecase; especially given that with speculative decoding you get more favourable memory/speed uplift ratio. A good 70B MoE needs either less active parameters or significantly better task performance to be a true substitution for 32B dense model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ugkva","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;smaller 32B dense models leave some VRAM unused&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;There&amp;#39;s no such thing as useless VRAM; each GB that is not filled by weights can be filled by activations and KV cache to either handle long contextes or multiple requests in parallel, or it can be allocated for embedding model, draft model, tts/stt models, etc. So trading off 2x larger weight memory for up to 2x performance uplift is kinda too niche usecase; especially given that with speculative decoding you get more favourable memory/speed uplift ratio. A good 70B MoE needs either less active parameters or significantly better task performance to be a true substitution for 32B dense model.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0ugkva/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751410519,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":22}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0xbyk6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullOf_Bad_Ideas","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0w14xk","score":2,"author_fullname":"t2_9s7pmakgx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It's a very custom architecture and you can't run the model even on enterprise-grade Nvidia GPUs right now. I think it's unlikely that it will be supported by llama.cpp, there's probably not enough interest in open source community in making it compatible with llama.cpp, but we'll see.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0xbyk6","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s a very custom architecture and you can&amp;#39;t run the model even on enterprise-grade Nvidia GPUs right now. I think it&amp;#39;s unlikely that it will be supported by llama.cpp, there&amp;#39;s probably not enough interest in open source community in making it compatible with llama.cpp, but we&amp;#39;ll see.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0xbyk6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751456275,"author_flair_text":null,"treatment_tags":[],"created_utc":1751456275,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0w14xk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ttkciar","can_mod_post":false,"created_utc":1751431266,"send_replies":true,"parent_id":"t1_n0t13nf","score":1,"author_fullname":"t2_cpegz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Groovy.  Looking forward to GGUFs so I can evaluate it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0w14xk","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Groovy.  Looking forward to GGUFs so I can evaluate it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0w14xk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751431266,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0t13nf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FullOf_Bad_Ideas","can_mod_post":false,"created_utc":1751395245,"send_replies":true,"parent_id":"t3_1lp9gh2","score":64,"author_fullname":"t2_9s7pmakgx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"link to paper: https://arxiv.org/abs/2505.21411\\n\\nIt's MoE architecture with special focus on expert grouping for increased enterprise-grade inference throughput on multi-accelerator deployment. No GGUF, support in vLLM and SGLang is uncertain - both vLLM and SGLang have transformers inference compatibility layer by now, but I would expect to run into some issues when trying to use it with this model.\\n\\nI think it's close to perfect size for enthusiast-grade local reasoning LLMs. 70B dense models are often too slow during reasoning to be useful, and smaller 32B dense models leave some VRAM unused when you're using a quant that's close to 4-bits and you have 48GB VRAM budget. I hope to see more open weight models trained on non-Nvidia accelerators - as they get more competitive, hopefully we'll see A100/H100 prices crash to the point of becoming affordable for enthusiasts.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0t13nf","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;link to paper: &lt;a href=\\"https://arxiv.org/abs/2505.21411\\"&gt;https://arxiv.org/abs/2505.21411&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s MoE architecture with special focus on expert grouping for increased enterprise-grade inference throughput on multi-accelerator deployment. No GGUF, support in vLLM and SGLang is uncertain - both vLLM and SGLang have transformers inference compatibility layer by now, but I would expect to run into some issues when trying to use it with this model.&lt;/p&gt;\\n\\n&lt;p&gt;I think it&amp;#39;s close to perfect size for enthusiast-grade local reasoning LLMs. 70B dense models are often too slow during reasoning to be useful, and smaller 32B dense models leave some VRAM unused when you&amp;#39;re using a quant that&amp;#39;s close to 4-bits and you have 48GB VRAM budget. I hope to see more open weight models trained on non-Nvidia accelerators - as they get more competitive, hopefully we&amp;#39;ll see A100/H100 prices crash to the point of becoming affordable for enthusiasts.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0t13nf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751395245,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp9gh2","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":64}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0u9129","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Iory1998","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0tqn5n","score":8,"author_fullname":"t2_byt5wa14","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Let's wait until we tried the models and see.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0u9129","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Let&amp;#39;s wait until we tried the models and see.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0u9129/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751408048,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1751408048,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0yc4ci","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mrjackspade","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ybaaq","score":3,"author_fullname":"t2_5ow51","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I run 100B+ dense on my CPU, I know how it goes","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0yc4ci","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I run 100B+ dense on my CPU, I know how it goes&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0yc4ci/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751468343,"author_flair_text":null,"treatment_tags":[],"created_utc":1751468343,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ybaaq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Freonr2","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0tqn5n","score":2,"author_fullname":"t2_8xi6x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Running even A16B on cpu is pretty painful unless you're spending RTX Pro 6000 Blackwell type money on the box (ex. Epyc).","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0ybaaq","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Running even A16B on cpu is pretty painful unless you&amp;#39;re spending RTX Pro 6000 Blackwell type money on the box (ex. Epyc).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0ybaaq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751468105,"author_flair_text":null,"treatment_tags":[],"created_utc":1751468105,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ymmub","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Zestyclose-Shift710","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0xvdzc","score":1,"author_fullname":"t2_zq2u99w69","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"interesting, thanks","edited":false,"author_flair_css_class":null,"name":"t1_n0ymmub","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;interesting, thanks&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lp9gh2","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0ymmub/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751471285,"author_flair_text":null,"collapsed":false,"created_utc":1751471285,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0xvdzc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Competitive_Ideal866","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0wuudp","score":2,"author_fullname":"t2_1d13xm6n7f","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt; Also doesn't a 72B know more than a 32B?\\n\\nIME that rule of thumb only works for dense models, e.g. Llama 3.3 70B certainly knows more general knowledge than Qwen2.5/3 32b.\\n\\nHowever, for MoE models I've found the knowledge has more to do with the number of active parameters and, in practice, I've never been impressed with experts under 24B.\\n\\nFor example, I can run Qwen3 235B A22B q3 but I've found it to be stupider than Qwen3 32B q4 (but I do get 30tps vs 26tps). Also, Qwen3 30B has only 3B active parameters and is really stupid compared to the dense 32B (but I do get 124tps).\\n\\nLlama4 is a notoriously stupid 109b model that disappointed many when it was released. I think that's because it has only 17B active parameters which is too small to be competitively intelligent.\\n\\nIn contrast, Deepseek 671B has 37B active parameters which is enough to be competitively clever.\\n\\nSimilarly for mixtral 8x22b.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0xvdzc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Also doesn&amp;#39;t a 72B know more than a 32B?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;IME that rule of thumb only works for dense models, e.g. Llama 3.3 70B certainly knows more general knowledge than Qwen2.5/3 32b.&lt;/p&gt;\\n\\n&lt;p&gt;However, for MoE models I&amp;#39;ve found the knowledge has more to do with the number of active parameters and, in practice, I&amp;#39;ve never been impressed with experts under 24B.&lt;/p&gt;\\n\\n&lt;p&gt;For example, I can run Qwen3 235B A22B q3 but I&amp;#39;ve found it to be stupider than Qwen3 32B q4 (but I do get 30tps vs 26tps). Also, Qwen3 30B has only 3B active parameters and is really stupid compared to the dense 32B (but I do get 124tps).&lt;/p&gt;\\n\\n&lt;p&gt;Llama4 is a notoriously stupid 109b model that disappointed many when it was released. I think that&amp;#39;s because it has only 17B active parameters which is too small to be competitively intelligent.&lt;/p&gt;\\n\\n&lt;p&gt;In contrast, Deepseek 671B has 37B active parameters which is enough to be competitively clever.&lt;/p&gt;\\n\\n&lt;p&gt;Similarly for mixtral 8x22b.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0xvdzc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751463353,"author_flair_text":null,"treatment_tags":[],"created_utc":1751463353,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0wuudp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Zestyclose-Shift710","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0tqn5n","score":1,"author_fullname":"t2_zq2u99w69","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Also doesn't a 72B know more than a 32B?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0wuudp","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Also doesn&amp;#39;t a 72B know more than a 32B?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0wuudp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751447600,"author_flair_text":null,"treatment_tags":[],"created_utc":1751447600,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0tqn5n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"mrjackspade","can_mod_post":false,"created_utc":1751402554,"send_replies":true,"parent_id":"t1_n0tjca9","score":28,"author_fullname":"t2_5ow51","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Its pretty good if you're running on full CPU, because you'll get more speed for the same scores.\\n\\nAll things being equal I'd rather use the 72B with 16B active, than the 32B","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0tqn5n","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Its pretty good if you&amp;#39;re running on full CPU, because you&amp;#39;ll get more speed for the same scores.&lt;/p&gt;\\n\\n&lt;p&gt;All things being equal I&amp;#39;d rather use the 72B with 16B active, than the 32B&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0tqn5n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751402554,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":28}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"d2642412-d9ce-11ed-ae30-32b11309f5bd","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"d2642412-d9ce-11ed-ae30-32b11309f5bd","distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0x538w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ortegaalfredo","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0wlvf4","score":1,"author_fullname":"t2_g177e","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I have about 300 GB of vram and I need it mostly for speed and quality, I can run deepseek or qwen-235B but it's too slow, Qwen3-32B is still too slow, so I run multiple instances of it, but I think this model would be much faster.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0x538w","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"Alpaca"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have about 300 GB of vram and I need it mostly for speed and quality, I can run deepseek or qwen-235B but it&amp;#39;s too slow, Qwen3-32B is still too slow, so I run multiple instances of it, but I think this model would be much faster.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lp9gh2","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0x538w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751453186,"author_flair_text":"Alpaca","treatment_tags":[],"created_utc":1751453186,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":"#bd9e9e","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0wlvf4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No-Refrigerator-1672","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0utc7p","score":1,"author_fullname":"t2_baavelp5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Wut? Vram-rich people use AI for either doing some complex tasks or serving a lot of clients (or both), they are even more sensitive to available kv cache space than average Joes.","edited":false,"author_flair_css_class":null,"name":"t1_n0wlvf4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Wut? Vram-rich people use AI for either doing some complex tasks or serving a lot of clients (or both), they are even more sensitive to available kv cache space than average Joes.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lp9gh2","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0wlvf4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751442265,"author_flair_text":null,"collapsed":false,"created_utc":1751442265,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0utc7p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"pseudonerv","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ugzq5","score":12,"author_fullname":"t2_eerln","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yeah because they are ram rich. You are ram poor.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0utc7p","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah because they are ram rich. You are ram poor.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0utc7p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751414794,"author_flair_text":null,"treatment_tags":[],"created_utc":1751414794,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0xp5be","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Baldur-Norddahl","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ugzq5","score":3,"author_fullname":"t2_bvqb8ng0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I have the M4 Max MacBook Pro with 128 GB of ram. MoE is made for a computer like this. Even if you only had 64 GB it would still be enough for long context and twice as fast.\\n\\nIt is not just the Macs. DGX Spark and AMD AI 395 are two new PCs with 128 GB of ram and unified memory.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0xp5be","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have the M4 Max MacBook Pro with 128 GB of ram. MoE is made for a computer like this. Even if you only had 64 GB it would still be enough for long context and twice as fast.&lt;/p&gt;\\n\\n&lt;p&gt;It is not just the Macs. DGX Spark and AMD AI 395 are two new PCs with 128 GB of ram and unified memory.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0xp5be/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751461273,"author_flair_text":null,"treatment_tags":[],"created_utc":1751461273,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ugzq5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No-Refrigerator-1672","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0tus5z","score":3,"author_fullname":"t2_baavelp5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"But the performance is not the same, cause given the same amount of system memory, this MoE eats up a lot more space and thus is heavily slashing down effective context length. You aren't running a 70B model to process a tiny 4k long chat, are you?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0ugzq5","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;But the performance is not the same, cause given the same amount of system memory, this MoE eats up a lot more space and thus is heavily slashing down effective context length. You aren&amp;#39;t running a 70B model to process a tiny 4k long chat, are you?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0ugzq5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751410655,"author_flair_text":null,"treatment_tags":[],"created_utc":1751410655,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0tus5z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ortegaalfredo","can_mod_post":false,"created_utc":1751403725,"send_replies":true,"parent_id":"t1_n0tjca9","score":14,"author_fullname":"t2_g177e","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You are trading more memory usage for much faster model, and 32B is quite slow already so this is arguably a better model, if the performance is the same.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0tus5z","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Alpaca"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You are trading more memory usage for much faster model, and 32B is quite slow already so this is arguably a better model, if the performance is the same.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0tus5z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751403725,"author_flair_text":"Alpaca","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bd9e9e","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":14}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0vcbqo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Iory1998","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0v7706","score":6,"author_fullname":"t2_byt5wa14","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Don't forget, it's the world's largest hardware manufacturer.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0vcbqo","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Don&amp;#39;t forget, it&amp;#39;s the world&amp;#39;s largest hardware manufacturer.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0vcbqo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751421529,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1751421529,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n0v7706","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"jonas-reddit","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0uk0pn","score":5,"author_fullname":"t2_vfaq1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Uhm. It’s China, we’d expected nothing less from the world’s second largest economy.  We’re not talking about Luxembourg.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0v7706","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Uhm. It’s China, we’d expected nothing less from the world’s second largest economy.  We’re not talking about Luxembourg.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0v7706/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751419740,"author_flair_text":null,"treatment_tags":[],"created_utc":1751419740,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n0uk0pn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Caffdy","can_mod_post":false,"created_utc":1751411657,"send_replies":true,"parent_id":"t1_n0tjca9","score":5,"author_fullname":"t2_ql2vu0wz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; but the fact that it was trained on a home grown GPU, that is huge!\\n\\nyep, how many countries can boast of home-grown delevoped AI chips *and* robust models trained in such chips?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0uk0pn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;but the fact that it was trained on a home grown GPU, that is huge!&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;yep, how many countries can boast of home-grown delevoped AI chips &lt;em&gt;and&lt;/em&gt; robust models trained in such chips?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0uk0pn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751411657,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0twmsn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Zc5Gwu","can_mod_post":false,"created_utc":1751404266,"send_replies":true,"parent_id":"t1_n0tjca9","score":4,"author_fullname":"t2_67qrvlir","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It might be much faster depending on how long its reasoning traces are.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0twmsn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It might be much faster depending on how long its reasoning traces are.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0twmsn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751404266,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n0tjca9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Iory1998","can_mod_post":false,"created_utc":1751400490,"send_replies":true,"parent_id":"t3_1lp9gh2","score":63,"author_fullname":"t2_byt5wa14","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You see, a model that's 72B on par with a 32B model is not really stimulating even if it's an MoE one, but the fact that it was trained on a home grown GPU, that is huge!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0tjca9","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You see, a model that&amp;#39;s 72B on par with a 32B model is not really stimulating even if it&amp;#39;s an MoE one, but the fact that it was trained on a home grown GPU, that is huge!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0tjca9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751400490,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lp9gh2","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":63}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0t3eet","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"noage","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0t1tvc","score":7,"author_fullname":"t2_5ao30","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks! That is cool to see. The paper definitely suggest they are trying to cement their technology and hardware, and it definitely seems reasonable for them to be focusing on that audience. It seems like they used a different architecture so I'll probably have to wait for some llama.cpp compatibility update.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0t3eet","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks! That is cool to see. The paper definitely suggest they are trying to cement their technology and hardware, and it definitely seems reasonable for them to be focusing on that audience. It seems like they used a different architecture so I&amp;#39;ll probably have to wait for some llama.cpp compatibility update.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0t3eet/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751395892,"author_flair_text":null,"treatment_tags":[],"created_utc":1751395892,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}}],"before":null}},"user_reports":[],"saved":false,"id":"n0t1tvc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FullOf_Bad_Ideas","can_mod_post":false,"created_utc":1751395449,"send_replies":true,"parent_id":"t1_n0szo8v","score":21,"author_fullname":"t2_9s7pmakgx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"here's a paper - https://arxiv.org/abs/2505.21411\\n\\nI wasn't sure whether it's better to link to paper or model weights, but I figured the community would be more interested in using the model than reading a research paper. It's trained on English and performs better on English-oriented benchmarks than Llama 4 Scout.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0t1tvc","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;here&amp;#39;s a paper - &lt;a href=\\"https://arxiv.org/abs/2505.21411\\"&gt;https://arxiv.org/abs/2505.21411&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;I wasn&amp;#39;t sure whether it&amp;#39;s better to link to paper or model weights, but I figured the community would be more interested in using the model than reading a research paper. It&amp;#39;s trained on English and performs better on English-oriented benchmarks than Llama 4 Scout.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0t1tvc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751395449,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":21}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0tkg48","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"plankalkul-z1","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0t1kut","score":3,"author_fullname":"t2_w73n3yrsx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; https://huggingface-co.translate.goog/IntervitensInc/pangu-pro-moe-model?_x_tr_sl=zh-CN&amp;_x_tr_tl=en&amp;_x_tr_hl=en-US&amp;_x_tr_pto=wapp\\n\\n\\nThanks for the translation link, appreciated.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0tkg48","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;&lt;a href=\\"https://huggingface-co.translate.goog/IntervitensInc/pangu-pro-moe-model?_x_tr_sl=zh-CN&amp;amp;_x_tr_tl=en&amp;amp;_x_tr_hl=en-US&amp;amp;_x_tr_pto=wapp\\"&gt;https://huggingface-co.translate.goog/IntervitensInc/pangu-pro-moe-model?_x_tr_sl=zh-CN&amp;amp;_x_tr_tl=en&amp;amp;_x_tr_hl=en-US&amp;amp;_x_tr_pto=wapp&lt;/a&gt;&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Thanks for the translation link, appreciated.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0tkg48/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751400811,"author_flair_text":null,"treatment_tags":[],"created_utc":1751400811,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0t1kut","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Entubulated","can_mod_post":false,"created_utc":1751395379,"send_replies":true,"parent_id":"t1_n0szo8v","score":11,"author_fullname":"t2_1opxde6hyq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Model technical report in English: [https://arxiv.org/abs/2505.21411](https://arxiv.org/abs/2505.21411)\\n\\nFound by feeding the HF page to google translate.  \\n[https://huggingface-co.translate.goog/IntervitensInc/pangu-pro-moe-model?\\\\_x\\\\_tr\\\\_sl=zh-CN&amp;\\\\_x\\\\_tr\\\\_tl=en&amp;\\\\_x\\\\_tr\\\\_hl=en-US&amp;\\\\_x\\\\_tr\\\\_pto=wapp](https://huggingface-co.translate.goog/IntervitensInc/pangu-pro-moe-model?_x_tr_sl=zh-CN&amp;_x_tr_tl=en&amp;_x_tr_hl=en-US&amp;_x_tr_pto=wapp)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0t1kut","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Model technical report in English: &lt;a href=\\"https://arxiv.org/abs/2505.21411\\"&gt;https://arxiv.org/abs/2505.21411&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Found by feeding the HF page to google translate.&lt;br/&gt;\\n&lt;a href=\\"https://huggingface-co.translate.goog/IntervitensInc/pangu-pro-moe-model?_x_tr_sl=zh-CN&amp;amp;_x_tr_tl=en&amp;amp;_x_tr_hl=en-US&amp;amp;_x_tr_pto=wapp\\"&gt;https://huggingface-co.translate.goog/IntervitensInc/pangu-pro-moe-model?_x_tr_sl=zh-CN&amp;amp;_x_tr_tl=en&amp;amp;_x_tr_hl=en-US&amp;amp;_x_tr_pto=wapp&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0t1kut/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751395379,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}}],"before":null}},"user_reports":[],"saved":false,"id":"n0szo8v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"noage","can_mod_post":false,"created_utc":1751394842,"send_replies":true,"parent_id":"t3_1lp9gh2","score":18,"author_fullname":"t2_5ao30","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Any English post about this? Is a model trained in English? This is the first post that I can recall for a big Chinese group that didn't have a concurrent English facing post as well.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0szo8v","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Any English post about this? Is a model trained in English? This is the first post that I can recall for a big Chinese group that didn&amp;#39;t have a concurrent English facing post as well.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0szo8v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751394842,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp9gh2","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":18}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0yq897","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ttkciar","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0tejs8","score":2,"author_fullname":"t2_cpegz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yep.  It's one of the reasons I use Phi-4 for Evol-Instruct.  It uses the MIT license.\\n\\nQwen3 uses Apache 2.0 which is very nearly as painless.  Good enough for synthetic data generation, though I wish it were better at self-critique.\\n\\n**Edited to add:** Writing this comment made me think to try OLMo-2-32B for self-critique, and it blew me away.  I need to do paid-work now, but will evaluate *Qwen3 answers --&gt; OLMo2 critiques --&gt; Qwen3 rewrites* and *Qwen3 answers --&gt; OLMo2 critiques --&gt; OLMo2 rewrites* pipelines later.","edited":1751476793,"author_flair_css_class":null,"name":"t1_n0yq897","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yep.  It&amp;#39;s one of the reasons I use Phi-4 for Evol-Instruct.  It uses the MIT license.&lt;/p&gt;\\n\\n&lt;p&gt;Qwen3 uses Apache 2.0 which is very nearly as painless.  Good enough for synthetic data generation, though I wish it were better at self-critique.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Edited to add:&lt;/strong&gt; Writing this comment made me think to try OLMo-2-32B for self-critique, and it blew me away.  I need to do paid-work now, but will evaluate &lt;em&gt;Qwen3 answers --&amp;gt; OLMo2 critiques --&amp;gt; Qwen3 rewrites&lt;/em&gt; and &lt;em&gt;Qwen3 answers --&amp;gt; OLMo2 critiques --&amp;gt; OLMo2 rewrites&lt;/em&gt; pipelines later.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lp9gh2","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0yq897/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751472285,"author_flair_text":"llama.cpp","collapsed":false,"created_utc":1751472285,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0yfj7z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MMAgeezer","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ycikr","score":1,"author_fullname":"t2_34hhuqbx","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I mostly agree, but the specific details about patent grants &amp; retaliation, attribution and NOTICE files make Apache 2.0 quite a bit more complex than just \\"do whatever you want, at your own risk\\" - which is what I was responding to.\\n\\nAs you noted, MIT licenses are 3 lines, where Apache 2.0 is multiple pages of quite detailed legalese.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0yfj7z","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I mostly agree, but the specific details about patent grants &amp;amp; retaliation, attribution and NOTICE files make Apache 2.0 quite a bit more complex than just &amp;quot;do whatever you want, at your own risk&amp;quot; - which is what I was responding to.&lt;/p&gt;\\n\\n&lt;p&gt;As you noted, MIT licenses are 3 lines, where Apache 2.0 is multiple pages of quite detailed legalese.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lp9gh2","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0yfj7z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751469314,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1751469314,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ycikr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Freonr2","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0tejs8","score":1,"author_fullname":"t2_8xi6x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Pretty much all the \\"permissive license\\" open source licenses are this.  \\n\\nEven Apache is mostly this but adds a patent grant so contributors can't patent troll. It looks a lot longer with more legalese, but that's the gist.\\n\\nThings only really get more complicated for copyleft licenses.","edited":false,"author_flair_css_class":null,"name":"t1_n0ycikr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Pretty much all the &amp;quot;permissive license&amp;quot; open source licenses are this.  &lt;/p&gt;\\n\\n&lt;p&gt;Even Apache is mostly this but adds a patent grant so contributors can&amp;#39;t patent troll. It looks a lot longer with more legalese, but that&amp;#39;s the gist.&lt;/p&gt;\\n\\n&lt;p&gt;Things only really get more complicated for copyleft licenses.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lp9gh2","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0ycikr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751468455,"author_flair_text":null,"collapsed":false,"created_utc":1751468455,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0tejs8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MMAgeezer","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0t4q7w","score":8,"author_fullname":"t2_34hhuqbx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"That's just an MIT license. Which is very common. \\n\\nIt lets anyone use, copy, modify, merge, publish, distribute, sublicense and sell the software with almost no restrictions.\\n\\nIts warranty disclaimer says the software is provided:\\n\\n&gt; as is, without warranty of any kind… In no event shall the authors or copyright holders be liable for any claim, damages or other liability.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0tejs8","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s just an MIT license. Which is very common. &lt;/p&gt;\\n\\n&lt;p&gt;It lets anyone use, copy, modify, merge, publish, distribute, sublicense and sell the software with almost no restrictions.&lt;/p&gt;\\n\\n&lt;p&gt;Its warranty disclaimer says the software is provided:&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;as is, without warranty of any kind… In no event shall the authors or copyright holders be liable for any claim, damages or other liability.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0tejs8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751399110,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1751399110,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}}],"before":null}},"user_reports":[],"saved":false,"id":"n0t4q7w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"silenceimpaired","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0t3vnj","score":2,"author_fullname":"t2_dissgzyl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah, I beat you with my comment :) \\n\\nI don’t get why this isn’t a more typical license: feel free to do what you want with this model as long as you recognize you’re responsible and you can’t take us to court.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0t4q7w","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, I beat you with my comment :) &lt;/p&gt;\\n\\n&lt;p&gt;I don’t get why this isn’t a more typical license: feel free to do what you want with this model as long as you recognize you’re responsible and you can’t take us to court.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0t4q7w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751396267,"author_flair_text":null,"treatment_tags":[],"created_utc":1751396267,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0t3vnj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Alternative_Quote246","can_mod_post":false,"created_utc":1751396025,"send_replies":true,"parent_id":"t1_n0t0yfe","score":32,"author_fullname":"t2_4sfm1yax","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It’s pretty free except one can’t use it in EU. Maybe to avoid trouble for the EU AI act.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0t3vnj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It’s pretty free except one can’t use it in EU. Maybe to avoid trouble for the EU AI act.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0t3vnj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751396025,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":32}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0t5sk7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DeltaSqueezer","can_mod_post":false,"created_utc":1751396573,"send_replies":true,"parent_id":"t1_n0t0yfe","score":7,"author_fullname":"t2_8jqx3m14","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It's not great, but could be worse. It's a bit like the 4-clause BSD licnse with an EU ban and an indemnity clause.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0t5sk7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s not great, but could be worse. It&amp;#39;s a bit like the 4-clause BSD licnse with an EU ban and an indemnity clause.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0t5sk7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751396573,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0t3lqu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"silenceimpaired","can_mod_post":false,"created_utc":1751395949,"send_replies":true,"parent_id":"t1_n0t0yfe","score":12,"author_fullname":"t2_dissgzyl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That said… it looks like it just has the new Europe Dunce Hat license… where it basically says you can use this model without restriction unless you are Europe, in which case you have to sit in a corner and think about what you’ve done. (That said I’m no lawyer and I was trying to read the license on my phone.)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0t3lqu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That said… it looks like it just has the new Europe Dunce Hat license… where it basically says you can use this model without restriction unless you are Europe, in which case you have to sit in a corner and think about what you’ve done. (That said I’m no lawyer and I was trying to read the license on my phone.)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0t3lqu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751395949,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0yel2k","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Freonr2","can_mod_post":false,"created_utc":1751469045,"send_replies":true,"parent_id":"t1_n0t0yfe","score":2,"author_fullname":"t2_8xi6x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The \\"EU-can-fuck-off\\" clauses we sometimes see tacked on might be a bit vindictive but not utterly without merit.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0yel2k","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The &amp;quot;EU-can-fuck-off&amp;quot; clauses we sometimes see tacked on might be a bit vindictive but not utterly without merit.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0yel2k/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751469045,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0t0yfe","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"silenceimpaired","can_mod_post":false,"created_utc":1751395204,"send_replies":true,"parent_id":"t3_1lp9gh2","score":14,"author_fullname":"t2_dissgzyl","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Disappointed that it isn’t Apache or MIT licensed.\\n\\nEDIT: it isn’t the worst license if you’re not in Europe.","edited":1751396896,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0t0yfe","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Disappointed that it isn’t Apache or MIT licensed.&lt;/p&gt;\\n\\n&lt;p&gt;EDIT: it isn’t the worst license if you’re not in Europe.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0t0yfe/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751395204,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp9gh2","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":14}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0wd1wq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"-samka","can_mod_post":false,"created_utc":1751437275,"send_replies":true,"parent_id":"t3_1lp9gh2","score":4,"author_fullname":"t2_4w7g74ec","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Huawei, release your cards globally at a good price, good docs, and no stupid restrictions, and I _guarantee_ that your cards will get first-class support on all major software platforms without you spending an additional cent.\\n\\nYou can eviscerate western companies and samsung the AI GPU market. Even if the US doesn't want to play along, pretty much everyone else does. It's up to you.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0wd1wq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Huawei, release your cards globally at a good price, good docs, and no stupid restrictions, and I &lt;em&gt;guarantee&lt;/em&gt; that your cards will get first-class support on all major software platforms without you spending an additional cent.&lt;/p&gt;\\n\\n&lt;p&gt;You can eviscerate western companies and samsung the AI GPU market. Even if the US doesn&amp;#39;t want to play along, pretty much everyone else does. It&amp;#39;s up to you.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0wd1wq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751437275,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp9gh2","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0uomee","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FullOf_Bad_Ideas","can_mod_post":false,"created_utc":1751413179,"send_replies":true,"parent_id":"t1_n0ug0fb","score":10,"author_fullname":"t2_9s7pmakgx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I meant competitive in quality of outputs.\\n\\nDepending on your hardware, it will be easier or harder to run then Qwen3 32B. If you have single 3090/4090, you'll have better time with Qwen3 32B. But, if you have 2 x 3090 setup, which is quite popular here, there might soon be a way of running this model on it and getting 2x faster inference than with Qwen3 32B, since the number of activated parameters is 2x smaller. And in that case, you might get the same quality, but with 2x faster output, which is in my opinion significant. If you have smaller GPU and you're offloading to CPU, there also might be a way to have Pangu Pro 72B run faster than Qwen3 32B.\\n\\nWhat I like is that we get models of various sizes and we can choose which one suits our hardware best, I think that's really good to see.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0uomee","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I meant competitive in quality of outputs.&lt;/p&gt;\\n\\n&lt;p&gt;Depending on your hardware, it will be easier or harder to run then Qwen3 32B. If you have single 3090/4090, you&amp;#39;ll have better time with Qwen3 32B. But, if you have 2 x 3090 setup, which is quite popular here, there might soon be a way of running this model on it and getting 2x faster inference than with Qwen3 32B, since the number of activated parameters is 2x smaller. And in that case, you might get the same quality, but with 2x faster output, which is in my opinion significant. If you have smaller GPU and you&amp;#39;re offloading to CPU, there also might be a way to have Pangu Pro 72B run faster than Qwen3 32B.&lt;/p&gt;\\n\\n&lt;p&gt;What I like is that we get models of various sizes and we can choose which one suits our hardware best, I think that&amp;#39;s really good to see.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0uomee/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751413179,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ug0fb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Cool-Chemical-5629","can_mod_post":false,"created_utc":1751410331,"send_replies":true,"parent_id":"t3_1lp9gh2","score":4,"author_fullname":"t2_qz1qjc86","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Not sure how should I feel about that \\"It should be competitive with Qwen3 32B\\".\\n\\nIn case of my hardware it means that a 72B model which is too big for my hardware to even load let alone run at reasonable speed, is comparable to a model which I can at least load and run slowly.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ug0fb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not sure how should I feel about that &amp;quot;It should be competitive with Qwen3 32B&amp;quot;.&lt;/p&gt;\\n\\n&lt;p&gt;In case of my hardware it means that a 72B model which is too big for my hardware to even load let alone run at reasonable speed, is comparable to a model which I can at least load and run slowly.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0ug0fb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751410331,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp9gh2","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0v1ctb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DataLearnerAI","can_mod_post":false,"created_utc":1751417659,"send_replies":true,"parent_id":"t3_1lp9gh2","score":8,"author_fullname":"t2_ilxa9crpd","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This model appears highly competitive at the 30B parameter scale. In benchmark tests, it achieves a score of 73.70 on the GPQA Diamond dataset, which is comparable to the performance of DeepSeek R1’s older version. The overall benchmark results closely resemble those of Qwen-32B. Notably, this is a Mixture-of-Experts (MoE) model, where only about 16.5B parameters are activated during inference.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0v1ctb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This model appears highly competitive at the 30B parameter scale. In benchmark tests, it achieves a score of 73.70 on the GPQA Diamond dataset, which is comparable to the performance of DeepSeek R1’s older version. The overall benchmark results closely resemble those of Qwen-32B. Notably, this is a Mixture-of-Experts (MoE) model, where only about 16.5B parameters are activated during inference.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0v1ctb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751417659,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp9gh2","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0u0xxt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FullOf_Bad_Ideas","can_mod_post":false,"created_utc":1751405558,"send_replies":true,"parent_id":"t1_n0tx9kx","score":7,"author_fullname":"t2_9s7pmakgx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"They seem to be on the bleeding edge if you trust their benchmarks. Base model appears to be better than Llama 4 Scout and similar to Hunyuan 80B A13B released just a few days ago. Instruct model has reasoning, and again, appears similar to Hunyuan 80B A13B, while Llama 4 Scout has no reasoning support.\\n\\nI think Chinese AI labs will try to use those accelerators if they will find it easy to switch to them. I think it's moreso an ad for their hardware that is meant to show that it's possible to train a useful model on their hardware, and that by itself is really impressive. I don't remember seeing a model of this kind pre-trained on AMD Instruct accelerators, so there's that.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0u0xxt","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They seem to be on the bleeding edge if you trust their benchmarks. Base model appears to be better than Llama 4 Scout and similar to Hunyuan 80B A13B released just a few days ago. Instruct model has reasoning, and again, appears similar to Hunyuan 80B A13B, while Llama 4 Scout has no reasoning support.&lt;/p&gt;\\n\\n&lt;p&gt;I think Chinese AI labs will try to use those accelerators if they will find it easy to switch to them. I think it&amp;#39;s moreso an ad for their hardware that is meant to show that it&amp;#39;s possible to train a useful model on their hardware, and that by itself is really impressive. I don&amp;#39;t remember seeing a model of this kind pre-trained on AMD Instruct accelerators, so there&amp;#39;s that.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0u0xxt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751405558,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}}],"before":null}},"user_reports":[],"saved":false,"id":"n0tx9kx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Rich_Artist_8327","can_mod_post":false,"created_utc":1751404452,"send_replies":true,"parent_id":"t3_1lp9gh2","score":4,"author_fullname":"t2_1jk2ep8a52","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I am sure after 3 years Huawei models are 1 year ahead of everyone else.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0tx9kx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I am sure after 3 years Huawei models are 1 year ahead of everyone else.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0tx9kx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751404452,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp9gh2","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0uzj80","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FullOf_Bad_Ideas","can_mod_post":false,"created_utc":1751417015,"send_replies":true,"parent_id":"t1_n0tu62d","score":8,"author_fullname":"t2_9s7pmakgx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"2 months ago Huawei released a paper where they described training 718B Pangu Ultra on their NPUs - https://arxiv.org/abs/2505.04519\\n\\nIf Nvidia stock were to crash because of losing dominance on training in the future, it would be May 7th when this paper came out. It didn't crash on that day.\\n\\nWe may very well be looking at this before analysts sweep in - DeepSeek showed me how people/bots who make those investment decisions are driven by word on the street moreso than actual information that could predict future. So, stock price doesn't seem as much driven by actual circumstances, it's driven by reporting on those circumstances.\\n\\nDeepSeek showed the world that you can train a great model on Nvidia GPUs for cheap.\\n\\nPangu Ultra showed that you can train a great model on non-Nvidia NPUs for even cheaper.\\n\\nNow that word is out in the technical science circles, people will start showing this to their managers, managers might start buying more Huawei Ascend NPUs, and then Nvidia forecasts for sales to China might start looking a tad bleak and then word on Wall Street will be negative on Nvidia. Just sharing my thoughts on the topic, if you disagree or agree here I am happy to continue discussion about it.","edited":1751458196,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0uzj80","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;2 months ago Huawei released a paper where they described training 718B Pangu Ultra on their NPUs - &lt;a href=\\"https://arxiv.org/abs/2505.04519\\"&gt;https://arxiv.org/abs/2505.04519&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;If Nvidia stock were to crash because of losing dominance on training in the future, it would be May 7th when this paper came out. It didn&amp;#39;t crash on that day.&lt;/p&gt;\\n\\n&lt;p&gt;We may very well be looking at this before analysts sweep in - DeepSeek showed me how people/bots who make those investment decisions are driven by word on the street moreso than actual information that could predict future. So, stock price doesn&amp;#39;t seem as much driven by actual circumstances, it&amp;#39;s driven by reporting on those circumstances.&lt;/p&gt;\\n\\n&lt;p&gt;DeepSeek showed the world that you can train a great model on Nvidia GPUs for cheap.&lt;/p&gt;\\n\\n&lt;p&gt;Pangu Ultra showed that you can train a great model on non-Nvidia NPUs for even cheaper.&lt;/p&gt;\\n\\n&lt;p&gt;Now that word is out in the technical science circles, people will start showing this to their managers, managers might start buying more Huawei Ascend NPUs, and then Nvidia forecasts for sales to China might start looking a tad bleak and then word on Wall Street will be negative on Nvidia. Just sharing my thoughts on the topic, if you disagree or agree here I am happy to continue discussion about it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0uzj80/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751417015,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ua9v7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ForsookComparison","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0u6k23","score":12,"author_fullname":"t2_on5es7pe3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Nobody is surprised. Hell, I have a China-phone and run Qwen locally. The China pill tastes damn good.\\n\\nIt's still quite the story that a model like this came from China sourced hardware, it's a milestone, the start of the end for one of the USA's final monopolies that matter.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0ua9v7","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nobody is surprised. Hell, I have a China-phone and run Qwen locally. The China pill tastes damn good.&lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s still quite the story that a model like this came from China sourced hardware, it&amp;#39;s a milestone, the start of the end for one of the USA&amp;#39;s final monopolies that matter.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0ua9v7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751408449,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1751408449,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}}],"before":null}},"user_reports":[],"saved":false,"id":"n0u6k23","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"emprahsFury","can_mod_post":false,"created_utc":1751407261,"send_replies":false,"parent_id":"t1_n0tu62d","score":2,"author_fullname":"t2_177r8n","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"100% not a huge story. If you are still surprised that China is doing things in China then that's on you. Not only is it literally the second largest economy in the world (and the largest if you let them game the score w pop numbers)- the Chinese govt has been specifically pursuing \\"Made in China 2025\\" since 2015. Has designated AI a national endeavor [since 2017](https://digichina.stanford.edu/work/full-translation-chinas-new-generation-artificial-intelligence-development-plan-2017/). You guys are simply not allowed to be surprised at this stuff. Pay better attention to the world around you.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0u6k23","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;100% not a huge story. If you are still surprised that China is doing things in China then that&amp;#39;s on you. Not only is it literally the second largest economy in the world (and the largest if you let them game the score w pop numbers)- the Chinese govt has been specifically pursuing &amp;quot;Made in China 2025&amp;quot; since 2015. Has designated AI a national endeavor &lt;a href=\\"https://digichina.stanford.edu/work/full-translation-chinas-new-generation-artificial-intelligence-development-plan-2017/\\"&gt;since 2017&lt;/a&gt;. You guys are simply not allowed to be surprised at this stuff. Pay better attention to the world around you.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0u6k23/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751407261,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0tu62d","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ForsookComparison","can_mod_post":false,"created_utc":1751403549,"send_replies":true,"parent_id":"t3_1lp9gh2","score":8,"author_fullname":"t2_on5es7pe3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This feels like a huge story even outside of this community. Why are none of the big business channels discussing this?\\n\\nIsn't a big chunk of the US economy propped up by monopoly on training?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0tu62d","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This feels like a huge story even outside of this community. Why are none of the big business channels discussing this?&lt;/p&gt;\\n\\n&lt;p&gt;Isn&amp;#39;t a big chunk of the US economy propped up by monopoly on training?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0tu62d/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751403549,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lp9gh2","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0w5qos","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0t7w2a","score":0,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Famous Mistral Nemo is largely an nvidia product; this is why it is very different from all other LLMs made by Mistral.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0w5qos","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Famous Mistral Nemo is largely an nvidia product; this is why it is very different from all other LLMs made by Mistral.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0w5qos/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751433474,"author_flair_text":null,"treatment_tags":[],"created_utc":1751433474,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0tubab","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"eloquentemu","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0tjxcl","score":9,"author_fullname":"t2_lpdsy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Haha, that's kind of an ironic comment to make on a model released by Huawei that was designed rather specifically for a Huawei product :).  Which is, to be clear, completely reasonable and is literately stated in the paper: \\"The configuration of Pangu Pro MoE is optimized for Ascend 300I Duo and 800I A2\\".\\n\\nWhile much like the Nivida models they aren't tied to their arch, the goals of the model seem to be to balance the pros and cons of their platform.  What's the point of a 70B MoE that would give similar functional performance to a 32B dense model?  Ah, their product is a 48GB / 400GBps processor so it makes sense to trade size for bandwidth requirements vs say a ~3090 which has 24GB / 1000GBps.  It also has a similar interest in balancing MoE activation to not overload bandwidth on distributed inference.\\n\\nSo it's a cool model and would be great for the B60 (if those are ever affordable) since those are lower bandwidth cards that seem to target distributed inference too, but it's definitely designed with their own product in mind.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0tubab","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Haha, that&amp;#39;s kind of an ironic comment to make on a model released by Huawei that was designed rather specifically for a Huawei product :).  Which is, to be clear, completely reasonable and is literately stated in the paper: &amp;quot;The configuration of Pangu Pro MoE is optimized for Ascend 300I Duo and 800I A2&amp;quot;.&lt;/p&gt;\\n\\n&lt;p&gt;While much like the Nivida models they aren&amp;#39;t tied to their arch, the goals of the model seem to be to balance the pros and cons of their platform.  What&amp;#39;s the point of a 70B MoE that would give similar functional performance to a 32B dense model?  Ah, their product is a 48GB / 400GBps processor so it makes sense to trade size for bandwidth requirements vs say a ~3090 which has 24GB / 1000GBps.  It also has a similar interest in balancing MoE activation to not overload bandwidth on distributed inference.&lt;/p&gt;\\n\\n&lt;p&gt;So it&amp;#39;s a cool model and would be great for the B60 (if those are ever affordable) since those are lower bandwidth cards that seem to target distributed inference too, but it&amp;#39;s definitely designed with their own product in mind.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0tubab/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751403590,"author_flair_text":null,"treatment_tags":[],"created_utc":1751403590,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}}],"before":null}},"user_reports":[],"saved":false,"id":"n0tjxcl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"secopsml","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0t7w2a","score":-4,"author_fullname":"t2_pmniwf57y","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It just feels natural for NVIDIA to just use their own products better than anyone else?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0tjxcl","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It just feels natural for NVIDIA to just use their own products better than anyone else?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0tjxcl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751400659,"author_flair_text":null,"treatment_tags":[],"created_utc":1751400659,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-4}}],"before":null}},"user_reports":[],"saved":false,"id":"n0t7w2a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"eloquentemu","can_mod_post":false,"created_utc":1751397176,"send_replies":true,"parent_id":"t1_n0t06zq","score":17,"author_fullname":"t2_lpdsy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What do you mean? [Nvidia has released quite a few LLMs](https://huggingface.co/nvidia/collections).  They're kind of done as a tech demo I guess (like this AFAICT) though are apparently quite usable.  I've heard good things about \`Llama-3_3-Nemotron-Super-49B-v1\` in particular.","edited":1751398232,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0t7w2a","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What do you mean? &lt;a href=\\"https://huggingface.co/nvidia/collections\\"&gt;Nvidia has released quite a few LLMs&lt;/a&gt;.  They&amp;#39;re kind of done as a tech demo I guess (like this AFAICT) though are apparently quite usable.  I&amp;#39;ve heard good things about &lt;code&gt;Llama-3_3-Nemotron-Super-49B-v1&lt;/code&gt; in particular.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0t7w2a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751397176,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":17}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0u1nqg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mission_tiefsee","can_mod_post":false,"created_utc":1751405775,"send_replies":true,"parent_id":"t1_n0t06zq","score":1,"author_fullname":"t2_1l0xi85fdm","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"they are busy ripping us of with their hardware.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0u1nqg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;they are busy ripping us of with their hardware.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0u1nqg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751405775,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0t6wz0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"Accomplished_Mode170","can_mod_post":false,"created_utc":1751396895,"send_replies":true,"parent_id":"t1_n0t06zq","score":-6,"author_fullname":"t2_4hfmiefj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yep 👍 either own ‘compute as a service’ or get Snowflake’d ❄️ 📊","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0t6wz0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yep 👍 either own ‘compute as a service’ or get Snowflake’d ❄️ 📊&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0t6wz0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751396895,"author_flair_text":null,"treatment_tags":[],"collapsed":true,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-6}}],"before":null}},"user_reports":[],"saved":false,"id":"n0t06zq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"secopsml","can_mod_post":false,"created_utc":1751394991,"send_replies":true,"parent_id":"t3_1lp9gh2","score":0,"author_fullname":"t2_pmniwf57y","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This is what Nvidia should do","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0t06zq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is what Nvidia should do&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0t06zq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751394991,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp9gh2","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0xdn2f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullOf_Bad_Ideas","can_mod_post":false,"created_utc":1751456971,"send_replies":true,"parent_id":"t1_n0w50hj","score":1,"author_fullname":"t2_9s7pmakgx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Here's a good video on this model - https://www.youtube.com/watch?v=Norj1fb6zEI\\n\\nI haven't used it yet (I don't have compatible hardware) but I imagine it would be close to Qwen3 32B on most metrics, meaning that it would be reasonably good at coding and would be rather smart. I don't think it has toggle for thinking though and it will do a reasoning chain on each question. It's pretty exotic when it comes to architecture - right now inference works only on Huawei Ascend NPUs but Nvidia GPUs can't run it, forget about llama.cpp support.\\n\\nThe biggest achievement here is that it's trained on Huawei's hardware, and Nvidia had a big moat there until now.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0xdn2f","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Here&amp;#39;s a good video on this model - &lt;a href=\\"https://www.youtube.com/watch?v=Norj1fb6zEI\\"&gt;https://www.youtube.com/watch?v=Norj1fb6zEI&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;I haven&amp;#39;t used it yet (I don&amp;#39;t have compatible hardware) but I imagine it would be close to Qwen3 32B on most metrics, meaning that it would be reasonably good at coding and would be rather smart. I don&amp;#39;t think it has toggle for thinking though and it will do a reasoning chain on each question. It&amp;#39;s pretty exotic when it comes to architecture - right now inference works only on Huawei Ascend NPUs but Nvidia GPUs can&amp;#39;t run it, forget about llama.cpp support.&lt;/p&gt;\\n\\n&lt;p&gt;The biggest achievement here is that it&amp;#39;s trained on Huawei&amp;#39;s hardware, and Nvidia had a big moat there until now.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0xdn2f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751456971,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0w50hj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Subject-Giraffe-3879","can_mod_post":false,"created_utc":1751433114,"send_replies":true,"parent_id":"t3_1lp9gh2","score":1,"author_fullname":"t2_1ctk9dia0f","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"There are a lot of chinese characters that I can't read. What are the pros and cons of this model? Like what is it good at?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0w50hj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There are a lot of chinese characters that I can&amp;#39;t read. What are the pros and cons of this model? Like what is it good at?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0w50hj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751433114,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp9gh2","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0xc95z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullOf_Bad_Ideas","can_mod_post":false,"created_utc":1751456398,"send_replies":true,"parent_id":"t1_n0wjirn","score":1,"author_fullname":"t2_9s7pmakgx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah, it's optimized for their hardware. As of now, it doesn't run on Nvidia GPUs at all. I think it could be ported if you had a small team of engineers though, it's not that custom.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0xc95z","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, it&amp;#39;s optimized for their hardware. As of now, it doesn&amp;#39;t run on Nvidia GPUs at all. I think it could be ported if you had a small team of engineers though, it&amp;#39;s not that custom.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0xc95z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751456398,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0wjirn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"bfroemel","can_mod_post":false,"created_utc":1751440881,"send_replies":true,"parent_id":"t3_1lp9gh2","score":1,"author_fullname":"t2_3s5b7ra0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"So.. is this model just optimized for Ascend hardware, or wouldn't it run (well, or even at all) on other hardware?\\n\\nCould be an interesting geopolitical counter-move regarding that US ban on Ascend chips: come up with an incredibly useful model, but make it depend on banned chips. Everyone respecting the ban would miss out. (ofc, even if this MOE model isn't truly amazing yet, it clearly demonstrates their potential capabilities. Could be that we will see less and less models from China that we can run well or even at all on our hardware.)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0wjirn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So.. is this model just optimized for Ascend hardware, or wouldn&amp;#39;t it run (well, or even at all) on other hardware?&lt;/p&gt;\\n\\n&lt;p&gt;Could be an interesting geopolitical counter-move regarding that US ban on Ascend chips: come up with an incredibly useful model, but make it depend on banned chips. Everyone respecting the ban would miss out. (ofc, even if this MOE model isn&amp;#39;t truly amazing yet, it clearly demonstrates their potential capabilities. Could be that we will see less and less models from China that we can run well or even at all on our hardware.)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0wjirn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751440881,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp9gh2","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0xrro9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Psychological_Bell48","can_mod_post":false,"created_utc":1751462164,"send_replies":true,"parent_id":"t3_1lp9gh2","score":1,"author_fullname":"t2_8hfqrgc7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Pangu ai studio, huawei cloud, youtube etc... please make it globally to compete 🙏 ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0xrro9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Pangu ai studio, huawei cloud, youtube etc... please make it globally to compete 🙏 &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0xrro9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751462164,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp9gh2","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n13jq6h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Bharat01123","can_mod_post":false,"created_utc":1751535225,"send_replies":true,"parent_id":"t3_1lp9gh2","score":1,"author_fullname":"t2_tsigokdk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"So there are into hardware game , thats why they are constantly releasing cool models.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n13jq6h","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So there are into hardware game , thats why they are constantly releasing cool models.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n13jq6h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751535225,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp9gh2","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0u9ri8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"digitaltransmutation","can_mod_post":false,"created_utc":1751408284,"send_replies":true,"parent_id":"t1_n0u2u1n","score":12,"author_fullname":"t2_490mm32t","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Weights are the result of the training. \\n\\nImagine you have a handful of 6-sided dice. When you throw them, you a bunch of random numbers every time, right? But if you pop them in the microwave for a bit, they will become weighted towards a desired result. \\n\\nNow, make a computer file that describes the changes you've made to the dice. Other people can apply the file to their own dice and enjoy the results. This is the 'weights' and why we like them.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0u9ri8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Weights are the result of the training. &lt;/p&gt;\\n\\n&lt;p&gt;Imagine you have a handful of 6-sided dice. When you throw them, you a bunch of random numbers every time, right? But if you pop them in the microwave for a bit, they will become weighted towards a desired result. &lt;/p&gt;\\n\\n&lt;p&gt;Now, make a computer file that describes the changes you&amp;#39;ve made to the dice. Other people can apply the file to their own dice and enjoy the results. This is the &amp;#39;weights&amp;#39; and why we like them.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0u9ri8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751408284,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0y7rvc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Freonr2","can_mod_post":false,"created_utc":1751467103,"send_replies":true,"parent_id":"t1_n0u2u1n","score":2,"author_fullname":"t2_8xi6x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The many gigabytes of data you download to run the model.\\n\\nThe stuff inside the .safetensors or .gguf files.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0y7rvc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The many gigabytes of data you download to run the model.&lt;/p&gt;\\n\\n&lt;p&gt;The stuff inside the .safetensors or .gguf files.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp9gh2","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0y7rvc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751467103,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0u2u1n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"lyth","can_mod_post":false,"created_utc":1751406129,"send_replies":true,"parent_id":"t3_1lp9gh2","score":1,"author_fullname":"t2_4gtur","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What are \\"weights\\"?  Is it the relative importance of individual training data sets?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0u2u1n","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What are &amp;quot;weights&amp;quot;?  Is it the relative importance of individual training data sets?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp9gh2/huawei_releases_an_open_weight_model_pangu_pro/n0u2u1n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751406129,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp9gh2","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(l,{data:a});export{n as default};
