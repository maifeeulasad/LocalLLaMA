import{j as e}from"./index-CeRg6Q3f.js";import{R as l}from"./RedditPostRenderer-D7n1g-D8.js";import"./index-DPToWe3n.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Anyone found any issues with Exaone 4.0 1.2b yet? the bf16 version i've tried does 11tok/s on my amd 5600G using cpu only inference and it doesnt seemed to repeat itself (the kind that goes on and on and on). It does repeat itself but it will end and that's occasional. I'm very impressed with it.\\n\\nWhat are your thoughts about this? It's kind of usable to me for filtering spam or vulgar words etc.\\n\\n[https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B](https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B)","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Just tried out the Exaone 4.0 1.2b bf16 and i'm extremely suprised at how good a 1.2b can be!","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m0pxot","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.88,"author_flair_background_color":null,"subreddit_type":"public","ups":42,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_5wbwvkoiw","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":42,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1752606357,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1752604773,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Anyone found any issues with Exaone 4.0 1.2b yet? the bf16 version i&amp;#39;ve tried does 11tok/s on my amd 5600G using cpu only inference and it doesnt seemed to repeat itself (the kind that goes on and on and on). It does repeat itself but it will end and that&amp;#39;s occasional. I&amp;#39;m very impressed with it.&lt;/p&gt;\\n\\n&lt;p&gt;What are your thoughts about this? It&amp;#39;s kind of usable to me for filtering spam or vulgar words etc.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B\\"&gt;https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/k3SjItvmyZuy4vYHT60cBUW4iHcoxLCjXlycy2nzStU.png?auto=webp&amp;s=fb8de62d0f61fbc6b07c1d3377c15fedd3b626a8","width":1200,"height":648},"resolutions":[{"url":"https://external-preview.redd.it/k3SjItvmyZuy4vYHT60cBUW4iHcoxLCjXlycy2nzStU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9ea586f84848afa9cf43c38a00d3481e13ba17ab","width":108,"height":58},{"url":"https://external-preview.redd.it/k3SjItvmyZuy4vYHT60cBUW4iHcoxLCjXlycy2nzStU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=dca90bd760b8a9b92ad1a938ad4f7af91f33c789","width":216,"height":116},{"url":"https://external-preview.redd.it/k3SjItvmyZuy4vYHT60cBUW4iHcoxLCjXlycy2nzStU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7495543d8d70892821308abc910a06cf54457fc7","width":320,"height":172},{"url":"https://external-preview.redd.it/k3SjItvmyZuy4vYHT60cBUW4iHcoxLCjXlycy2nzStU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6663014ebf8319cb8cf6f0178a8859c3f05ae157","width":640,"height":345},{"url":"https://external-preview.redd.it/k3SjItvmyZuy4vYHT60cBUW4iHcoxLCjXlycy2nzStU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bcf9aaca4e2279c0fc00fd47b47cb4f4eae94d30","width":960,"height":518},{"url":"https://external-preview.redd.it/k3SjItvmyZuy4vYHT60cBUW4iHcoxLCjXlycy2nzStU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0c3126bd581fb98f2fb0d3a7f83446bf5b2f69f8","width":1080,"height":583}],"variants":{},"id":"k3SjItvmyZuy4vYHT60cBUW4iHcoxLCjXlycy2nzStU"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1m0pxot","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"cloudxaas","discussion_type":null,"num_comments":21,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m0pxot/just_tried_out_the_exaone_40_12b_bf16_and_im/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m0pxot/just_tried_out_the_exaone_40_12b_bf16_and_im/","subreddit_subscribers":499773,"created_utc":1752604773,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3cxz0f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Annual_Role_5066","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3cxfnk","score":2,"author_fullname":"t2_7lqud3g7","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I’m building a portable offline rag application right now and using Phi-mini, definitely gonna try that out and see how it works. Thanks !","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3cxz0f","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I’m building a portable offline rag application right now and using Phi-mini, definitely gonna try that out and see how it works. Thanks !&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m0pxot","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0pxot/just_tried_out_the_exaone_40_12b_bf16_and_im/n3cxz0f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752624437,"author_flair_text":null,"treatment_tags":[],"created_utc":1752624437,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3cxfnk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"smayonak","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3be06t","score":2,"author_fullname":"t2_3xyf2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I like some of the quants of 3B DeepCogito. It seems significantly better than anything else of a similar size.","edited":false,"author_flair_css_class":null,"name":"t1_n3cxfnk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I like some of the quants of 3B DeepCogito. It seems significantly better than anything else of a similar size.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m0pxot","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0pxot/just_tried_out_the_exaone_40_12b_bf16_and_im/n3cxfnk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752624250,"author_flair_text":null,"collapsed":false,"created_utc":1752624250,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3be06t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Annual_Role_5066","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3bb6ye","score":3,"author_fullname":"t2_7lqud3g7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"BitNet 0.4mb for 2b is insane but yeah, unusable with the repetition issues. If they fix that it'll be game-changing.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3be06t","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;BitNet 0.4mb for 2b is insane but yeah, unusable with the repetition issues. If they fix that it&amp;#39;ll be game-changing.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0pxot","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0pxot/just_tried_out_the_exaone_40_12b_bf16_and_im/n3be06t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752607346,"author_flair_text":null,"treatment_tags":[],"created_utc":1752607346,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n3bb6ye","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cloudxaas","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3bacjn","score":3,"author_fullname":"t2_5wbwvkoiw","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"the only llm model that's also good but not usable coz of repeating is the bitnet 2b 1T. i really hope for bitnet more coz it's good but it repeats. it only uses 0.4mb ram for 2b model so that's really impressive and it does inference speedily too. hoping to see a 7b or 8b bitnet or a4.8 bitnet stuff.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3bb6ye","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;the only llm model that&amp;#39;s also good but not usable coz of repeating is the bitnet 2b 1T. i really hope for bitnet more coz it&amp;#39;s good but it repeats. it only uses 0.4mb ram for 2b model so that&amp;#39;s really impressive and it does inference speedily too. hoping to see a 7b or 8b bitnet or a4.8 bitnet stuff.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0pxot","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0pxot/just_tried_out_the_exaone_40_12b_bf16_and_im/n3bb6ye/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752606542,"author_flair_text":null,"treatment_tags":[],"created_utc":1752606542,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n3bacjn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cloudxaas","can_mod_post":false,"created_utc":1752606303,"send_replies":true,"parent_id":"t1_n3b6lsz","score":4,"author_fullname":"t2_5wbwvkoiw","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"you can chk the model card vs qwen 3 1.7b. i need something small yet usable for cpu inference. 1.2b seemed like a sweet spot for me. bf16 uses 2.4gb ram for inference. that's very cheap for cloud / vps hosting. as long as it doesnt repeat itself without end i'm happy with it. i wont try anything lower than 1b coz of bad experiences with never ending repeating themselves\\n\\n[https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B](https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3bacjn","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;you can chk the model card vs qwen 3 1.7b. i need something small yet usable for cpu inference. 1.2b seemed like a sweet spot for me. bf16 uses 2.4gb ram for inference. that&amp;#39;s very cheap for cloud / vps hosting. as long as it doesnt repeat itself without end i&amp;#39;m happy with it. i wont try anything lower than 1b coz of bad experiences with never ending repeating themselves&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B\\"&gt;https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0pxot","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0pxot/just_tried_out_the_exaone_40_12b_bf16_and_im/n3bacjn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752606303,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3c9z5j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"claythearc","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3b9xys","score":2,"author_fullname":"t2_65rk0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Not super surprising. These micro models can have effective contexts of like… &lt;1k tokens and then get effectively brain dead. Very niche uses for them but kinda powerful when you have one","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3c9z5j","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not super surprising. These micro models can have effective contexts of like… &amp;lt;1k tokens and then get effectively brain dead. Very niche uses for them but kinda powerful when you have one&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0pxot","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0pxot/just_tried_out_the_exaone_40_12b_bf16_and_im/n3c9z5j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752616476,"author_flair_text":null,"treatment_tags":[],"created_utc":1752616476,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3b9xys","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ArchdukeofHyperbole","can_mod_post":false,"created_utc":1752606188,"send_replies":true,"parent_id":"t1_n3b6lsz","score":4,"author_fullname":"t2_1p41v97q5d","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I've tried using qwen 0.6B in a pipeline where it's role was paraphrase something like 250-1200 words and it rarely worked right, like either wouldn't follow the prompt exactly (no preludes, don't address to user, just paraphrase type of prompt) and would sometimes think despite the /no_think tag. \\n\\nI'll try out this new model eventually. Im really impressed with qwen for its size, just couldn't use it how I wanted.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3b9xys","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve tried using qwen 0.6B in a pipeline where it&amp;#39;s role was paraphrase something like 250-1200 words and it rarely worked right, like either wouldn&amp;#39;t follow the prompt exactly (no preludes, don&amp;#39;t address to user, just paraphrase type of prompt) and would sometimes think despite the /no_think tag. &lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ll try out this new model eventually. Im really impressed with qwen for its size, just couldn&amp;#39;t use it how I wanted.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0pxot","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0pxot/just_tried_out_the_exaone_40_12b_bf16_and_im/n3b9xys/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752606188,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n3b6lsz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MKU64","can_mod_post":false,"created_utc":1752605256,"send_replies":true,"parent_id":"t3_1m0pxot","score":15,"author_fullname":"t2_wn7it","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Have you tried Qwen 3 0.6B and Qwen 3 1.7B? Do you know how does it compare? I think they are the only usable models of that size too (There’s also ERNIE 0.3B which was good but that came out like 2 weeks ago)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3b6lsz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Have you tried Qwen 3 0.6B and Qwen 3 1.7B? Do you know how does it compare? I think they are the only usable models of that size too (There’s also ERNIE 0.3B which was good but that came out like 2 weeks ago)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0pxot/just_tried_out_the_exaone_40_12b_bf16_and_im/n3b6lsz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752605256,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0pxot","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":15}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3fboyq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"thirteen-bit","can_mod_post":false,"created_utc":1752663693,"send_replies":true,"parent_id":"t1_n3dvoph","score":1,"author_fullname":"t2_9l12dgc5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; certify destruction in writing\\n\\nYou're right, won't touch this, looks too bothersome even for home use.\\n\\nFirst paragraph of the README:\\n\\n&gt; 🎉 License Updated! We are pleased to announce our more flexible licensing terms 🤗\\n\\nDo I even want to know what was there before?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3fboyq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;certify destruction in writing&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;You&amp;#39;re right, won&amp;#39;t touch this, looks too bothersome even for home use.&lt;/p&gt;\\n\\n&lt;p&gt;First paragraph of the README:&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;🎉 License Updated! We are pleased to announce our more flexible licensing terms 🤗&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Do I even want to know what was there before?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0pxot","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0pxot/just_tried_out_the_exaone_40_12b_bf16_and_im/n3fboyq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752663693,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3dvoph","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"HealthCorrect","can_mod_post":false,"created_utc":1752636547,"send_replies":true,"parent_id":"t3_1m0pxot","score":5,"author_fullname":"t2_7w7ujxhh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The license feels a little limiting for local LLMs. Look at these provisions in their Agreement:\\n\\n1. Anti‑Competitive Clause (Bad for OSS community)\\n   * *Section 3.1* forbids using the Model, any Derivative, or even Output “to develop or improve any models that compete with the Licensor’s models.”\\n   * *Implication*: You can’t use fine‑tuning or prompt‑engineering insights to build a new open‑source alternative, effectively stifling downstream innovation.\\n2. Termination Terms\\n   * *Section 7.1–7.2*: Licensor can terminate without cause, then you must immediately destroy *all* copies (even backups) and certify destruction in writing.\\n3. Ambiguous “Research‑Only” Clauses\\n   * *Section 2.1.a* allows “research and educational” use, but Section 3.1 then broadly bans any “commercial” application, and even non‑monetary deployments might be deemed commercial.\\n   * *Implications*: Unclear boundary between “educational demo” and “service”\\n4. Vague “Ethical Use” Clauses &amp; Reverse Engineering Prohibition\\n   * *Section 3.4* lists broad, subjective prohibitions (“harm,” “offensive,” “misinformation”) without clear definition or dispute‑resolution process.\\n   * *Section 3.2* bans decompilation or **bypassing protections** “except as expressly permitted by law,” but the license claims broad research rights.\\n   * *Implication*: Makes the model less useful for some folks (jailbreakers)\\n\\n**tl;dr** : Useful for tinkering, but shouldn't touch the model for anything else (esp. jailbreaking and fine-tuning)\\n\\nAlso, these folks created a PR asking llama.cpp to just look at their transformers implementation and port it over. LG AI should at least help llama.cpp with some work, llama.cpp devs aren't some free labor.\\n\\nI'm not an expert in law, the above conclusions are just my understandings.\\n\\nEdit: Grammar","edited":1752637629,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dvoph","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The license feels a little limiting for local LLMs. Look at these provisions in their Agreement:&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;Anti‑Competitive Clause (Bad for OSS community)\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;em&gt;Section 3.1&lt;/em&gt; forbids using the Model, any Derivative, or even Output “to develop or improve any models that compete with the Licensor’s models.”&lt;/li&gt;\\n&lt;li&gt;&lt;em&gt;Implication&lt;/em&gt;: You can’t use fine‑tuning or prompt‑engineering insights to build a new open‑source alternative, effectively stifling downstream innovation.&lt;/li&gt;\\n&lt;/ul&gt;&lt;/li&gt;\\n&lt;li&gt;Termination Terms\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;em&gt;Section 7.1–7.2&lt;/em&gt;: Licensor can terminate without cause, then you must immediately destroy &lt;em&gt;all&lt;/em&gt; copies (even backups) and certify destruction in writing.&lt;/li&gt;\\n&lt;/ul&gt;&lt;/li&gt;\\n&lt;li&gt;Ambiguous “Research‑Only” Clauses\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;em&gt;Section 2.1.a&lt;/em&gt; allows “research and educational” use, but Section 3.1 then broadly bans any “commercial” application, and even non‑monetary deployments might be deemed commercial.&lt;/li&gt;\\n&lt;li&gt;&lt;em&gt;Implications&lt;/em&gt;: Unclear boundary between “educational demo” and “service”&lt;/li&gt;\\n&lt;/ul&gt;&lt;/li&gt;\\n&lt;li&gt;Vague “Ethical Use” Clauses &amp;amp; Reverse Engineering Prohibition\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;em&gt;Section 3.4&lt;/em&gt; lists broad, subjective prohibitions (“harm,” “offensive,” “misinformation”) without clear definition or dispute‑resolution process.&lt;/li&gt;\\n&lt;li&gt;&lt;em&gt;Section 3.2&lt;/em&gt; bans decompilation or &lt;strong&gt;bypassing protections&lt;/strong&gt; “except as expressly permitted by law,” but the license claims broad research rights.&lt;/li&gt;\\n&lt;li&gt;&lt;em&gt;Implication&lt;/em&gt;: Makes the model less useful for some folks (jailbreakers)&lt;/li&gt;\\n&lt;/ul&gt;&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt; : Useful for tinkering, but shouldn&amp;#39;t touch the model for anything else (esp. jailbreaking and fine-tuning)&lt;/p&gt;\\n\\n&lt;p&gt;Also, these folks created a PR asking llama.cpp to just look at their transformers implementation and port it over. LG AI should at least help llama.cpp with some work, llama.cpp devs aren&amp;#39;t some free labor.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m not an expert in law, the above conclusions are just my understandings.&lt;/p&gt;\\n\\n&lt;p&gt;Edit: Grammar&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0pxot/just_tried_out_the_exaone_40_12b_bf16_and_im/n3dvoph/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752636547,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0pxot","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3blis9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Annual_Role_5066","can_mod_post":false,"created_utc":1752609484,"send_replies":true,"parent_id":"t1_n3bf752","score":2,"author_fullname":"t2_7lqud3g7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I’ve used phi mini and have gotten great results but takes a lot of prompt engineering.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3blis9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I’ve used phi mini and have gotten great results but takes a lot of prompt engineering.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0pxot","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0pxot/just_tried_out_the_exaone_40_12b_bf16_and_im/n3blis9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752609484,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3bf752","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DeltaSqueezer","can_mod_post":false,"created_utc":1752607686,"send_replies":true,"parent_id":"t3_1m0pxot","score":3,"author_fullname":"t2_8jqx3m14","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I haven't tried that, but what about the smaller Gemmas?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3bf752","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I haven&amp;#39;t tried that, but what about the smaller Gemmas?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0pxot/just_tried_out_the_exaone_40_12b_bf16_and_im/n3bf752/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752607686,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0pxot","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3cpdod","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cms2307","can_mod_post":false,"created_utc":1752621555,"send_replies":true,"parent_id":"t3_1m0pxot","score":2,"author_fullname":"t2_1otju2ya","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Why use bf16 on cpu? You could get like 4-5x faster speed using gpu with a quantized model","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3cpdod","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why use bf16 on cpu? You could get like 4-5x faster speed using gpu with a quantized model&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0pxot/just_tried_out_the_exaone_40_12b_bf16_and_im/n3cpdod/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752621555,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0pxot","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3cvb0u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"One_5549","can_mod_post":false,"created_utc":1752623514,"send_replies":true,"parent_id":"t3_1m0pxot","score":2,"author_fullname":"t2_hpmepbjgt","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"how does it compare with something like Gemma 3n E2b?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3cvb0u","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;how does it compare with something like Gemma 3n E2b?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0pxot/just_tried_out_the_exaone_40_12b_bf16_and_im/n3cvb0u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752623514,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0pxot","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ehoxs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"stoppableDissolution","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3dp9sl","score":1,"author_fullname":"t2_1n0su21k4z","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It doesnt, but I'm looking for a base model for the tune that I'm going to publish. Not like it was a big deal anyway, just a little annoyance on top of the main issue, but still","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3ehoxs","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It doesnt, but I&amp;#39;m looking for a base model for the tune that I&amp;#39;m going to publish. Not like it was a big deal anyway, just a little annoyance on top of the main issue, but still&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0pxot","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0pxot/just_tried_out_the_exaone_40_12b_bf16_and_im/n3ehoxs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752647089,"author_flair_text":null,"treatment_tags":[],"created_utc":1752647089,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3dp9sl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cloudxaas","can_mod_post":false,"created_utc":1752634075,"send_replies":true,"parent_id":"t1_n3blgjy","score":1,"author_fullname":"t2_5wbwvkoiw","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"how does licensing limit us from abusing it offline anyway? just curious.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dp9sl","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;how does licensing limit us from abusing it offline anyway? just curious.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0pxot","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0pxot/just_tried_out_the_exaone_40_12b_bf16_and_im/n3dp9sl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752634075,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3blgjy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"stoppableDissolution","can_mod_post":false,"created_utc":1752609467,"send_replies":true,"parent_id":"t3_1m0pxot","score":1,"author_fullname":"t2_1n0su21k4z","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I almost got excited (32 heads/8kv in small footprint is exactly what I want), but no base model and crappy license :c","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3blgjy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I almost got excited (32 heads/8kv in small footprint is exactly what I want), but no base model and crappy license :c&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0pxot/just_tried_out_the_exaone_40_12b_bf16_and_im/n3blgjy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752609467,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0pxot","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3dskka","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HealthCorrect","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3dp5fe","score":2,"author_fullname":"t2_7w7ujxhh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The LLM used is important as well, the DB stores the info and with the help of an embedding model it will search relevant snippets and pass them to the LLM. Understanding and interpreting the passed data solely depends on the LLM used","edited":1752636789,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3dskka","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The LLM used is important as well, the DB stores the info and with the help of an embedding model it will search relevant snippets and pass them to the LLM. Understanding and interpreting the passed data solely depends on the LLM used&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0pxot","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0pxot/just_tried_out_the_exaone_40_12b_bf16_and_im/n3dskka/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752635333,"author_flair_text":null,"treatment_tags":[],"created_utc":1752635333,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3dp5fe","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cloudxaas","can_mod_post":false,"created_utc":1752634029,"send_replies":true,"parent_id":"t1_n3dnph0","score":1,"author_fullname":"t2_5wbwvkoiw","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"what RAG do you mean? isnt RAG means db storage for llm?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dp5fe","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;what RAG do you mean? isnt RAG means db storage for llm?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0pxot","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0pxot/just_tried_out_the_exaone_40_12b_bf16_and_im/n3dp5fe/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752634029,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3dnph0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HealthCorrect","can_mod_post":false,"created_utc":1752633493,"send_replies":true,"parent_id":"t3_1m0pxot","score":1,"author_fullname":"t2_7w7ujxhh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The benchmark scores are really good for its size. I’ll try it today. Might be useful in RAG etc","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dnph0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The benchmark scores are really good for its size. I’ll try it today. Might be useful in RAG etc&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0pxot/just_tried_out_the_exaone_40_12b_bf16_and_im/n3dnph0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752633493,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0pxot","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(l,{data:t});export{r as default};
