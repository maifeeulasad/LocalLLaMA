import{j as e}from"./index-DQXiEb7D.js";import{R as t}from"./RedditPostRenderer-BjndLgq8.js";import"./index-B-ILyjT1.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Abstract\\n\\nWe propose a novel inference-time optimization method for resource-constrained deployment of large language models (LLMs), enabling high-quality output from models too large to fit into a single consumer-grade GPU. This technique—Prompt-Guided Dynamic Slicing and Insertion Point Resolution (PG-DSIR)—leverages coarse inference from a quantized or low-parameter proxy model to define a hyperspherical subregion of the solution space. This subregion is then mapped onto the full-precision model, from which only the minimal required model weights are dynamically loaded and computed.\\n\\nTo reduce redundant early-layer computation and maximize efficiency, PG-DSIR determines a static insertion point within the full model’s architecture where the low-precision representation aligns most closely with a hidden state of the larger model. This alignment is determined through cosine similarity across the hidden states of the full model, enabling direct embedding injection midstream. A lightweight corrective module (e.g., a LoRA or learned delta predictor) bridges the precision gap between the coarse embedding and the ground truth embedding, improving alignment and preserving output quality.\\n\\nOur technique draws conceptual inspiration from hybrid latent-space workflows in image generation (e.g., LCM + SD schedulers) and departs from traditional quantization, distillation, or Mixture-of-Experts (MoE) routing by enabling continuous, per-inference specialization of the model footprint. The resulting system provides a principled and geometry-driven pathway to real-time inference using ultra-large models on modest hardware, dramatically reducing both VRAM usage and computational overhead.\\n\\n\\n---\\n\\nIntroduction: Toward Focused Large-Model Inference via Geometric Slicing and Targeted Insertion\\n\\nLarge Language Models have achieved unprecedented capability at the cost of significant resource demands. Models such as Meta’s LLaMA 3 70B, for example, require over 140 GB of VRAM to run in full precision—placing them well beyond reach for consumer or even prosumer-grade hardware. Traditional strategies to reduce inference cost include quantization, distillation, parameter pruning, and expert routing. These techniques, while effective, trade off flexibility, output fidelity, or require extensive fine-tuning.\\n\\nWe introduce Prompt-Guided Dynamic Slicing and Insertion Point Resolution (PG-DSIR) as an alternative strategy, rooted in a geometric understanding of latent representations. Our method operates under the key insight that inference can be reconceived not as a global pass through the entire parameter space of a model, but as a locally focused traversal through a high-dimensional latent graph—constrained by an informed estimate of the solution space.\\n\\n1. Conceptual Overview\\n\\nThe method begins with a prompt passed to a smaller, quantized proxy model (e.g., 7B), which produces a low-precision embedding in a shared latent space. Although this representation lacks the full nuance of a high-precision model, it defines a directional “search vector” within the solution space. By interpreting this vector as a hypersphere in the full model’s higher-precision latent space, we delimit the relevant solution subregion for the current prompt.\\n\\nRather than processing the entire 70B model, we instead extract and load only the parameter subset required to refine that coarse embedding within its solution subregion. This step parallels the logic of MoE routing but bypasses the need for discrete experts or static routing logic, instead constructing a dynamically sliced micro-expert composed only of what the full model knows about the specific problem space defined by the prompt.\\n\\n2. Insertion Point Resolution\\n\\nTo avoid recomputation of early transformer blocks—often the most computationally expensive—we perform embedding handoff into the full model at an internal layer corresponding to the hidden state most similar to the coarse embedding. This “insertion point” is found by analyzing cosine similarity between the proxy embedding and the hidden states of the full model when run on the same prompt. Importantly, this mapping is prompt-agnostic and only needs to be computed once per proxy/full model pair.\\n\\n3. Precision Bridging and Correction\\n\\nEven with accurate slicing and entry point resolution, discrepancies will remain between the proxy embedding and the full model’s expected hidden state. We address this through a corrective module—either a learned LoRA, linear mapping, or shallow neural delta predictor—trained on embedding pairs generated via dual model evaluation on a large prompt corpus. This allows us to cleanly bridge the two latent spaces with negligible overhead.\\n\\n4. Implications\\n\\nThe proposed PG-DSIR pipeline enables inference from large-scale models like LLaMA 3 70B or Mixtral on consumer GPUs (e.g., RTX 3060–4090), significantly reducing required VRAM and compute without necessitating global model transformation. Moreover, this method maintains the full model’s capabilities and expressiveness, differing from quantization approaches that often suffer from irrecoverable degradation.\\n\\n\\n---\\n\\nConclusion\\n\\nBy treating inference as a navigational process through high-dimensional geometry, PG-DSIR transforms the challenge of large model execution into a targeted optimization problem. We believe this technique can unlock a new era of ultra-large model accessibility, enabling research, development, and deployment of frontier models on commodity hardware—without compromising capability.\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"PGDS approach to full model inference on consumer grade GPUs","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lto3t9","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.46,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_m2bkgz51p","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751874048,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Abstract&lt;/p&gt;\\n\\n&lt;p&gt;We propose a novel inference-time optimization method for resource-constrained deployment of large language models (LLMs), enabling high-quality output from models too large to fit into a single consumer-grade GPU. This technique—Prompt-Guided Dynamic Slicing and Insertion Point Resolution (PG-DSIR)—leverages coarse inference from a quantized or low-parameter proxy model to define a hyperspherical subregion of the solution space. This subregion is then mapped onto the full-precision model, from which only the minimal required model weights are dynamically loaded and computed.&lt;/p&gt;\\n\\n&lt;p&gt;To reduce redundant early-layer computation and maximize efficiency, PG-DSIR determines a static insertion point within the full model’s architecture where the low-precision representation aligns most closely with a hidden state of the larger model. This alignment is determined through cosine similarity across the hidden states of the full model, enabling direct embedding injection midstream. A lightweight corrective module (e.g., a LoRA or learned delta predictor) bridges the precision gap between the coarse embedding and the ground truth embedding, improving alignment and preserving output quality.&lt;/p&gt;\\n\\n&lt;p&gt;Our technique draws conceptual inspiration from hybrid latent-space workflows in image generation (e.g., LCM + SD schedulers) and departs from traditional quantization, distillation, or Mixture-of-Experts (MoE) routing by enabling continuous, per-inference specialization of the model footprint. The resulting system provides a principled and geometry-driven pathway to real-time inference using ultra-large models on modest hardware, dramatically reducing both VRAM usage and computational overhead.&lt;/p&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;p&gt;Introduction: Toward Focused Large-Model Inference via Geometric Slicing and Targeted Insertion&lt;/p&gt;\\n\\n&lt;p&gt;Large Language Models have achieved unprecedented capability at the cost of significant resource demands. Models such as Meta’s LLaMA 3 70B, for example, require over 140 GB of VRAM to run in full precision—placing them well beyond reach for consumer or even prosumer-grade hardware. Traditional strategies to reduce inference cost include quantization, distillation, parameter pruning, and expert routing. These techniques, while effective, trade off flexibility, output fidelity, or require extensive fine-tuning.&lt;/p&gt;\\n\\n&lt;p&gt;We introduce Prompt-Guided Dynamic Slicing and Insertion Point Resolution (PG-DSIR) as an alternative strategy, rooted in a geometric understanding of latent representations. Our method operates under the key insight that inference can be reconceived not as a global pass through the entire parameter space of a model, but as a locally focused traversal through a high-dimensional latent graph—constrained by an informed estimate of the solution space.&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;Conceptual Overview&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;The method begins with a prompt passed to a smaller, quantized proxy model (e.g., 7B), which produces a low-precision embedding in a shared latent space. Although this representation lacks the full nuance of a high-precision model, it defines a directional “search vector” within the solution space. By interpreting this vector as a hypersphere in the full model’s higher-precision latent space, we delimit the relevant solution subregion for the current prompt.&lt;/p&gt;\\n\\n&lt;p&gt;Rather than processing the entire 70B model, we instead extract and load only the parameter subset required to refine that coarse embedding within its solution subregion. This step parallels the logic of MoE routing but bypasses the need for discrete experts or static routing logic, instead constructing a dynamically sliced micro-expert composed only of what the full model knows about the specific problem space defined by the prompt.&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;Insertion Point Resolution&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;To avoid recomputation of early transformer blocks—often the most computationally expensive—we perform embedding handoff into the full model at an internal layer corresponding to the hidden state most similar to the coarse embedding. This “insertion point” is found by analyzing cosine similarity between the proxy embedding and the hidden states of the full model when run on the same prompt. Importantly, this mapping is prompt-agnostic and only needs to be computed once per proxy/full model pair.&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;Precision Bridging and Correction&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;Even with accurate slicing and entry point resolution, discrepancies will remain between the proxy embedding and the full model’s expected hidden state. We address this through a corrective module—either a learned LoRA, linear mapping, or shallow neural delta predictor—trained on embedding pairs generated via dual model evaluation on a large prompt corpus. This allows us to cleanly bridge the two latent spaces with negligible overhead.&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;Implications&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;The proposed PG-DSIR pipeline enables inference from large-scale models like LLaMA 3 70B or Mixtral on consumer GPUs (e.g., RTX 3060–4090), significantly reducing required VRAM and compute without necessitating global model transformation. Moreover, this method maintains the full model’s capabilities and expressiveness, differing from quantization approaches that often suffer from irrecoverable degradation.&lt;/p&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;p&gt;Conclusion&lt;/p&gt;\\n\\n&lt;p&gt;By treating inference as a navigational process through high-dimensional geometry, PG-DSIR transforms the challenge of large model execution into a targeted optimization problem. We believe this technique can unlock a new era of ultra-large model accessibility, enabling research, development, and deployment of frontier models on commodity hardware—without compromising capability.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lto3t9","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"finnabrahamson","discussion_type":null,"num_comments":9,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lto3t9/pgds_approach_to_full_model_inference_on_consumer/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lto3t9/pgds_approach_to_full_model_inference_on_consumer/","subreddit_subscribers":496034,"created_utc":1751874048,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1t86zq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"finnabrahamson","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1sgxr7","score":-1,"author_fullname":"t2_m2bkgz51p","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I've looked over the paper you referenced in your post. I'm not sure your methodology of approach for your attempted implementation, but I can assure you that your intuition that Olamma4 Mavrick might share some resemblance structurally or otherwise could not be farther from accurate.  Mavrick 4 produces embeddins with  25% the dimensionality of Ollama 3.1 70b.  In short, these 2 models don't share the same representational system and neither could be used to orient each other on the basis of the others output.  Even is both models produced the same responce to a given prompt, their output tokens are not capable of equality and because they lack both a common latent space or common representational framework, cosign similarity is a meaningless metric with regards trans model vector comparison.\\nAdditionally, Ollame 4 Maverick is a MoE model, which means its internal geometry is artificially segmented along Perceived domain boundries, and in this case modalities.  This model ALREADY uses inference time prompt dependant specialty selection.  At 400b parameters, only 17b actually see any inference activation.  Unfortunately, all of the unused parameters still end up in VRAM.  The problem extends even further then that as only one model is multimodal , only one model is trained using Matryoshka Representational Learning.\\nOllama 3.1 is a viable candidate for the kind of geometricaly informed subgraph selection outlined in my paper, ituses  standard decoder-only Transformer architecture,.  Mavrick, or any artificially arranged for routing schema for MoE style static distillation based models introduce to many problems to be considered for this kind of pre-election process. and layer insertion simply can not be made to work in a routed inference environment.\\nConceptually, your attempted more closely resembles the research found here: https://huggingface.co/erax-ai/EraX-LLaMA3.1-8B-DeepSeekR1-MLA-MoE-Raw#:~:text=Citation%20%F0%9F%93%9D-,EraX:%20Reimagine%20LLaMA%203.1%20with%20DeepSeek's%20Innovation!,performance%20changes%20to%20specific%20modifications\\nthan it does my geometricly informed process.\\n\\nIf you are interested in manipulating or exploring the inner structure of models, there is little need to make assumptions concerning their layout or architecture.  \\nCheck the model’s metadata using Ollama’s /api/show endpoint with the key bert.embedding_length for the specific model (e.g., ollama run llama4:maverick).Refer to the official model card on Meta’s Llama website (llama.com) or Hugging Face under the meta-llama/Llama-4-Maverick-17B-128E repository for detailed specifications.\\n\\nInforming one model's opperations, and particularly its structure with inference from another model requires near perfect alignment to yield usable results.  I hope you keep imagining ways to advance this field. The world needs curious minds willing to explore the boundries of what is thought to be possjble.\\n\\nI wish I could tell you rhat you were on the right track with the work you attempted. But your model selection made your effort dead on arrival. Thank you for reading and considering my original post.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1t86zq","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve looked over the paper you referenced in your post. I&amp;#39;m not sure your methodology of approach for your attempted implementation, but I can assure you that your intuition that Olamma4 Mavrick might share some resemblance structurally or otherwise could not be farther from accurate.  Mavrick 4 produces embeddins with  25% the dimensionality of Ollama 3.1 70b.  In short, these 2 models don&amp;#39;t share the same representational system and neither could be used to orient each other on the basis of the others output.  Even is both models produced the same responce to a given prompt, their output tokens are not capable of equality and because they lack both a common latent space or common representational framework, cosign similarity is a meaningless metric with regards trans model vector comparison.\\nAdditionally, Ollame 4 Maverick is a MoE model, which means its internal geometry is artificially segmented along Perceived domain boundries, and in this case modalities.  This model ALREADY uses inference time prompt dependant specialty selection.  At 400b parameters, only 17b actually see any inference activation.  Unfortunately, all of the unused parameters still end up in VRAM.  The problem extends even further then that as only one model is multimodal , only one model is trained using Matryoshka Representational Learning.\\nOllama 3.1 is a viable candidate for the kind of geometricaly informed subgraph selection outlined in my paper, ituses  standard decoder-only Transformer architecture,.  Mavrick, or any artificially arranged for routing schema for MoE style static distillation based models introduce to many problems to be considered for this kind of pre-election process. and layer insertion simply can not be made to work in a routed inference environment.\\nConceptually, your attempted more closely resembles the research found here: &lt;a href=\\"https://huggingface.co/erax-ai/EraX-LLaMA3.1-8B-DeepSeekR1-MLA-MoE-Raw#:%7E:text=Citation%20%F0%9F%93%9D-,EraX:%20Reimagine%20LLaMA%203.1%20with%20DeepSeek&amp;#x27;s%20Innovation!,performance%20changes%20to%20specific%20modifications\\"&gt;https://huggingface.co/erax-ai/EraX-LLaMA3.1-8B-DeepSeekR1-MLA-MoE-Raw#:~:text=Citation%20%F0%9F%93%9D-,EraX:%20Reimagine%20LLaMA%203.1%20with%20DeepSeek&amp;#39;s%20Innovation!,performance%20changes%20to%20specific%20modifications&lt;/a&gt;\\nthan it does my geometricly informed process.&lt;/p&gt;\\n\\n&lt;p&gt;If you are interested in manipulating or exploring the inner structure of models, there is little need to make assumptions concerning their layout or architecture.&lt;br/&gt;\\nCheck the model’s metadata using Ollama’s /api/show endpoint with the key bert.embedding_length for the specific model (e.g., ollama run llama4:maverick).Refer to the official model card on Meta’s Llama website (llama.com) or Hugging Face under the meta-llama/Llama-4-Maverick-17B-128E repository for detailed specifications.&lt;/p&gt;\\n\\n&lt;p&gt;Informing one model&amp;#39;s opperations, and particularly its structure with inference from another model requires near perfect alignment to yield usable results.  I hope you keep imagining ways to advance this field. The world needs curious minds willing to explore the boundries of what is thought to be possjble.&lt;/p&gt;\\n\\n&lt;p&gt;I wish I could tell you rhat you were on the right track with the work you attempted. But your model selection made your effort dead on arrival. Thank you for reading and considering my original post.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lto3t9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lto3t9/pgds_approach_to_full_model_inference_on_consumer/n1t86zq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751897974,"author_flair_text":null,"treatment_tags":[],"created_utc":1751897974,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1sgxr7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"silenceimpaired","can_mod_post":false,"created_utc":1751887900,"send_replies":true,"parent_id":"t1_n1rueji","score":2,"author_fullname":"t2_dissgzyl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Sounds like OP read my off the wall post that got no traction and ran with it in AI: https://www.reddit.com/r/LocalLLaMA/s/W0Hx7sKVNA\\n\\nLove to see the code and know I was on the right track, but just back 100 miles. Doubt it though based on the comments for my post.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1sgxr7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sounds like OP read my off the wall post that got no traction and ran with it in AI: &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/s/W0Hx7sKVNA\\"&gt;https://www.reddit.com/r/LocalLLaMA/s/W0Hx7sKVNA&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Love to see the code and know I was on the right track, but just back 100 miles. Doubt it though based on the comments for my post.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lto3t9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lto3t9/pgds_approach_to_full_model_inference_on_consumer/n1sgxr7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751887900,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1rueji","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"arekku255","can_mod_post":false,"created_utc":1751875352,"send_replies":true,"parent_id":"t3_1lto3t9","score":5,"author_fullname":"t2_15rugs","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"By looking at the amount of mdashes, technobabble and tall claims, (royal) we believe this post to be AI generated slop.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1rueji","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;By looking at the amount of mdashes, technobabble and tall claims, (royal) we believe this post to be AI generated slop.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lto3t9/pgds_approach_to_full_model_inference_on_consumer/n1rueji/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751875352,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lto3t9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1rszj9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cybran3","can_mod_post":false,"created_utc":1751874517,"send_replies":true,"parent_id":"t3_1lto3t9","score":3,"author_fullname":"t2_41gmkw5z","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"So, how to run it? Did you implement this anywhere or is this just theoretical? If so then how do you know it works?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1rszj9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So, how to run it? Did you implement this anywhere or is this just theoretical? If so then how do you know it works?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lto3t9/pgds_approach_to_full_model_inference_on_consumer/n1rszj9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751874517,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lto3t9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1rwk1v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Chromix_","can_mod_post":false,"created_utc":1751876649,"send_replies":true,"parent_id":"t3_1lto3t9","score":2,"author_fullname":"t2_k7w2h","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This is either ingenious - or doesn't work at all. The basic idea here is:\\n\\n* Take for example a 7B version of a model and a 70B\\n* \\"Pair\\" them with a *ton* of pre-processing\\n* During inference the 7B does most of the work, and barely anything of the 70B needs to be loaded to VRAM, vaguely like with a MoE.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1rwk1v","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is either ingenious - or doesn&amp;#39;t work at all. The basic idea here is:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Take for example a 7B version of a model and a 70B&lt;/li&gt;\\n&lt;li&gt;&amp;quot;Pair&amp;quot; them with a &lt;em&gt;ton&lt;/em&gt; of pre-processing&lt;/li&gt;\\n&lt;li&gt;During inference the 7B does most of the work, and barely anything of the 70B needs to be loaded to VRAM, vaguely like with a MoE.&lt;/li&gt;\\n&lt;/ul&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lto3t9/pgds_approach_to_full_model_inference_on_consumer/n1rwk1v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751876649,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lto3t9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1sbjwg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1751885318,"send_replies":true,"parent_id":"t3_1lto3t9","score":1,"author_fullname":"t2_uz37qfx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Sounds interesting, but too academically sounding.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1sbjwg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sounds interesting, but too academically sounding.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lto3t9/pgds_approach_to_full_model_inference_on_consumer/n1sbjwg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751885318,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lto3t9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1scndu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"finnabrahamson","can_mod_post":false,"created_utc":1751885857,"send_replies":true,"parent_id":"t3_1lto3t9","score":0,"author_fullname":"t2_m2bkgz51p","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Another problem with this method that makes it unsuitable for all use cases is a latency issue born from the fact that with each prompt, the model slice for responce must be relocated and loaded into VRAM across the PCIe bus.  I'm working out a way to mitigate this issue with a rolling context that maintains the previous prompts solution space in VRAM with the assumption that subsequent prompts will be on topic. When the next prompt yields the subgraph geometry the system should only excize the delta of that space and append it to the VRAM.  This will still limit subsequent prompts to symanticly related prompt sequences, but it should greatly reduce the latency created by heavy PCIe traffic on resource constrained systems.  A delta size ratio threshold that can identify when to clear the VRAM where followups lack the requisite symantic similarity, and a way to avoid OOM conditions resulting from long conversations and delta appending still need to be worked out before the latency issue can be addressed in this way.  Work still needs to be done in terms of identifying the optimal percision disparity between the target model and its proxy.  The optimal insertion point is clearly a function of this disparity, where the greater the gap in percision, the further from the output layer the insertion point will be found.  computation is more costly at distance from the output layer due to an increase in effective surface area and a higher activation density. The ability for the coarse embedding corrective adapter or LoRA to align the proxy models work with the expected hidden state at insertion will likely be the most determination factor in the maximum viable precision reduction.  The questions involved with this reality involve a catch-22, in that adapter training data is insertion layer dependent, and the optimal insertion layer for the target model depends on the activity of the LoRA.  Attempts producing a multilayer LoRA that can be utilized in answering questions about loss related to layer selection and percision truncation may be attempted at a later date but it represents a anon-trivial computational and implementation effort.  But now I feel like I'm spouting g techno cable again.\\nIn laymens terms: Chat GPT can probably write you a Swedish poem in iambic pentameter about the Irish potatoes famine, hut unless you ask it to  that capacity is just taking up space.  Traditionally this is addressed through MoE or model distillation. Distillation works well enough, but you do loosse the ability to request edge responces like custom sweedish poetry.  Users are required to keep and manually route to what they assume is the correct model distillation for their use case.  MoE reduces computational load, but actually increases memory requirements, and an agents expertise is staticly pre designed within the model.  This process works on the exact same principles as MoE, but instead of routing to a staticly defined expert (most systems actually route to 2 experts because of the limitations involved in staticly defigned solution space boundaries) who specializes in an entire field of inquiry, we atrempt to carve out an agent who has expertise in only one thing. That thing is a simple question: what did the first agent get wrong when he produced this responce?\\"  Its all he needs to know, and on the basis of a staticly quantized model's inherent alignment with its foundation, the embedding produced by the proxy model tells us where inside the larger model dhat expertise is located.\\nIf this is hard to imagine in a 4096 Dimensional environment, let's reduce the problem to 2 dimensions:\\n\\nIf we imagime a 10x10 grid or graph and provide for  4 places of decimal precision our graph will have 1000000 degrees of potential freedom. Now let's imagine that somewhere on that grid is the  the correct answer to a problem, our graph now becomes our \\"solutio  space\\". Let's pretend that our solution resides at [4.2387, 7.9642] , but that we can't look for it because it is computationaly too much work to consider 100000 degrees of freedom.  The usual answer ot to quantize the model, leaving the same dimensionality, but producing an imperfect answer.  If we reduce our precision back to integers and run inference on our solution space, we get a [4,7] vector.  If we are playing horseshoes That'll do.  This is what quantitzation is all about.  Getting a good enough answer when computational loads are prohibitive.\\nbut,  If I can be confident about my [4,7] responce as being close to my more desired responce, I can go back to my 10x10 graph and reduce it  to the single space at 4x7 by this focusing of the graph my total solution space is reduced to 1% its former area. (it no longer writes poetry in sweedish) but it still contains the answer to the question we asked the first agent.  This is as non-technical way I can describe the mechanics of the described process.  LLMs produce Vector responces, Vectors are literally points on a graph, and governed by geometric laws.  This is why geometric concepts like cosign similarity are so useful in AI, its all just geometry.  We can see that when an aligned low pecision vector is mapped to a higher order of precision, it ceases to be a point, this because points are indivisable, and by adding additional precision to a point it becomes divisible and as such now represents a graph eather then a point.  From. there the target model's job no longer includes the reduction of its entire space to a point most closely relevant to the provided prompt, but to navigate the tiny space once represented as a point i  the lower precision space.  It becomes a refinement task informed by only the knowledge base directly reliavent to the 1 specific question asked and aided by a headscarf estimate from a lighter more manageable model.\\nIf you've got AI, feed this to it and ask it if its bullying.  Over simplified in this case:Absoluty. Grounded in the reality of AI inference: Absolutely.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1scndu","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Another problem with this method that makes it unsuitable for all use cases is a latency issue born from the fact that with each prompt, the model slice for responce must be relocated and loaded into VRAM across the PCIe bus.  I&amp;#39;m working out a way to mitigate this issue with a rolling context that maintains the previous prompts solution space in VRAM with the assumption that subsequent prompts will be on topic. When the next prompt yields the subgraph geometry the system should only excize the delta of that space and append it to the VRAM.  This will still limit subsequent prompts to symanticly related prompt sequences, but it should greatly reduce the latency created by heavy PCIe traffic on resource constrained systems.  A delta size ratio threshold that can identify when to clear the VRAM where followups lack the requisite symantic similarity, and a way to avoid OOM conditions resulting from long conversations and delta appending still need to be worked out before the latency issue can be addressed in this way.  Work still needs to be done in terms of identifying the optimal percision disparity between the target model and its proxy.  The optimal insertion point is clearly a function of this disparity, where the greater the gap in percision, the further from the output layer the insertion point will be found.  computation is more costly at distance from the output layer due to an increase in effective surface area and a higher activation density. The ability for the coarse embedding corrective adapter or LoRA to align the proxy models work with the expected hidden state at insertion will likely be the most determination factor in the maximum viable precision reduction.  The questions involved with this reality involve a catch-22, in that adapter training data is insertion layer dependent, and the optimal insertion layer for the target model depends on the activity of the LoRA.  Attempts producing a multilayer LoRA that can be utilized in answering questions about loss related to layer selection and percision truncation may be attempted at a later date but it represents a anon-trivial computational and implementation effort.  But now I feel like I&amp;#39;m spouting g techno cable again.\\nIn laymens terms: Chat GPT can probably write you a Swedish poem in iambic pentameter about the Irish potatoes famine, hut unless you ask it to  that capacity is just taking up space.  Traditionally this is addressed through MoE or model distillation. Distillation works well enough, but you do loosse the ability to request edge responces like custom sweedish poetry.  Users are required to keep and manually route to what they assume is the correct model distillation for their use case.  MoE reduces computational load, but actually increases memory requirements, and an agents expertise is staticly pre designed within the model.  This process works on the exact same principles as MoE, but instead of routing to a staticly defined expert (most systems actually route to 2 experts because of the limitations involved in staticly defigned solution space boundaries) who specializes in an entire field of inquiry, we atrempt to carve out an agent who has expertise in only one thing. That thing is a simple question: what did the first agent get wrong when he produced this responce?&amp;quot;  Its all he needs to know, and on the basis of a staticly quantized model&amp;#39;s inherent alignment with its foundation, the embedding produced by the proxy model tells us where inside the larger model dhat expertise is located.\\nIf this is hard to imagine in a 4096 Dimensional environment, let&amp;#39;s reduce the problem to 2 dimensions:&lt;/p&gt;\\n\\n&lt;p&gt;If we imagime a 10x10 grid or graph and provide for  4 places of decimal precision our graph will have 1000000 degrees of potential freedom. Now let&amp;#39;s imagine that somewhere on that grid is the  the correct answer to a problem, our graph now becomes our &amp;quot;solutio  space&amp;quot;. Let&amp;#39;s pretend that our solution resides at [4.2387, 7.9642] , but that we can&amp;#39;t look for it because it is computationaly too much work to consider 100000 degrees of freedom.  The usual answer ot to quantize the model, leaving the same dimensionality, but producing an imperfect answer.  If we reduce our precision back to integers and run inference on our solution space, we get a [4,7] vector.  If we are playing horseshoes That&amp;#39;ll do.  This is what quantitzation is all about.  Getting a good enough answer when computational loads are prohibitive.\\nbut,  If I can be confident about my [4,7] responce as being close to my more desired responce, I can go back to my 10x10 graph and reduce it  to the single space at 4x7 by this focusing of the graph my total solution space is reduced to 1% its former area. (it no longer writes poetry in sweedish) but it still contains the answer to the question we asked the first agent.  This is as non-technical way I can describe the mechanics of the described process.  LLMs produce Vector responces, Vectors are literally points on a graph, and governed by geometric laws.  This is why geometric concepts like cosign similarity are so useful in AI, its all just geometry.  We can see that when an aligned low pecision vector is mapped to a higher order of precision, it ceases to be a point, this because points are indivisable, and by adding additional precision to a point it becomes divisible and as such now represents a graph eather then a point.  From. there the target model&amp;#39;s job no longer includes the reduction of its entire space to a point most closely relevant to the provided prompt, but to navigate the tiny space once represented as a point i  the lower precision space.  It becomes a refinement task informed by only the knowledge base directly reliavent to the 1 specific question asked and aided by a headscarf estimate from a lighter more manageable model.\\nIf you&amp;#39;ve got AI, feed this to it and ask it if its bullying.  Over simplified in this case:Absoluty. Grounded in the reality of AI inference: Absolutely.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lto3t9/pgds_approach_to_full_model_inference_on_consumer/n1scndu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751885857,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lto3t9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1t0zu3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"__JockY__","can_mod_post":false,"created_utc":1751895686,"send_replies":true,"parent_id":"t3_1lto3t9","score":2,"author_fullname":"t2_qf8h7ka8","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Code? \\n\\nPaper?\\n\\nNope. Just some AI slop.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1t0zu3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Code? &lt;/p&gt;\\n\\n&lt;p&gt;Paper?&lt;/p&gt;\\n\\n&lt;p&gt;Nope. Just some AI slop.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lto3t9/pgds_approach_to_full_model_inference_on_consumer/n1t0zu3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751895686,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lto3t9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),l=()=>e.jsx(t,{data:a});export{l as default};
