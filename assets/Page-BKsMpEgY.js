import{j as e}from"./index-F0NXdzZX.js";import{R as t}from"./RedditPostRenderer-CoEZB1dt.js";import"./index-DrtravXm.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I have some problems on applying local LLMs to structured workflows.\\n\\nI use 8b to 24b models on my 16GB 4070 Super TI\\n\\nI have no problems in chatting or doing web rag with my models, either using open webui or AnythingLLM or custom solutions in python or nodejs. What I am unable to do is doing some more structured work. \\n\\nSpecifically, but this is just an example, I am trying to have my models output a specific JSON format. \\n\\nI am trying almost everything in the system prompt and even in forcing json responses from ollama, but 70% of the times the models just produce wrong outputs. \\n\\nNow, my question is more generic than having this specific json so I am not sure about posting the prompt etc. \\n\\nMy question is: are there models that are more suited to follow instructions than others? \\n\\nMistral 3.2 is almost always a failure in producing a decent json, so is Gemma 12b\\n\\nAny specific tips and tricks or models to test? ","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Local models not following instructions","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lox1f7","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.67,"author_flair_background_color":null,"subreddit_type":"public","ups":2,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_pvo138ggw","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":2,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751361365,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I have some problems on applying local LLMs to structured workflows.&lt;/p&gt;\\n\\n&lt;p&gt;I use 8b to 24b models on my 16GB 4070 Super TI&lt;/p&gt;\\n\\n&lt;p&gt;I have no problems in chatting or doing web rag with my models, either using open webui or AnythingLLM or custom solutions in python or nodejs. What I am unable to do is doing some more structured work. &lt;/p&gt;\\n\\n&lt;p&gt;Specifically, but this is just an example, I am trying to have my models output a specific JSON format. &lt;/p&gt;\\n\\n&lt;p&gt;I am trying almost everything in the system prompt and even in forcing json responses from ollama, but 70% of the times the models just produce wrong outputs. &lt;/p&gt;\\n\\n&lt;p&gt;Now, my question is more generic than having this specific json so I am not sure about posting the prompt etc. &lt;/p&gt;\\n\\n&lt;p&gt;My question is: are there models that are more suited to follow instructions than others? &lt;/p&gt;\\n\\n&lt;p&gt;Mistral 3.2 is almost always a failure in producing a decent json, so is Gemma 12b&lt;/p&gt;\\n\\n&lt;p&gt;Any specific tips and tricks or models to test? &lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lox1f7","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"thecookingsenpai","discussion_type":null,"num_comments":4,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lox1f7/local_models_not_following_instructions/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lox1f7/local_models_not_following_instructions/","subreddit_subscribers":493458,"created_utc":1751361365,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0qlaof","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Black-Mack","can_mod_post":false,"created_utc":1751367962,"send_replies":true,"parent_id":"t3_1lox1f7","score":2,"author_fullname":"t2_1s8rdwqx9a","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Qwen 3 follows instructions better than Gemma 3.\\n\\nAlso, make sure you turn off all samplers (Top-K, Min-P, Mirostat, etc.) because they interfere with what the model has been trained to know (This goes for coding, knowledge retrieval and data processing).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0qlaof","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen 3 follows instructions better than Gemma 3.&lt;/p&gt;\\n\\n&lt;p&gt;Also, make sure you turn off all samplers (Top-K, Min-P, Mirostat, etc.) because they interfere with what the model has been trained to know (This goes for coding, knowledge retrieval and data processing).&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lox1f7/local_models_not_following_instructions/n0qlaof/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751367962,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lox1f7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0qri2g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"EmberGlitch","can_mod_post":false,"created_utc":1751370709,"send_replies":true,"parent_id":"t3_1lox1f7","score":3,"author_fullname":"t2_h6ljsiqs","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Take a look at [structured outputs](https://ollama.com/blog/structured-outputs) if you're using ollama. If you're using something else to run the models, they might have it implemented a bit differently. It's in the OpenAI API spec, though.: https://openai.com/index/introducing-structured-outputs-in-the-api/\\n\\nNever had an issue with using structured outputs with Gemma3. using low temperatures around 0.2 might help.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0qri2g","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Take a look at &lt;a href=\\"https://ollama.com/blog/structured-outputs\\"&gt;structured outputs&lt;/a&gt; if you&amp;#39;re using ollama. If you&amp;#39;re using something else to run the models, they might have it implemented a bit differently. It&amp;#39;s in the OpenAI API spec, though.: &lt;a href=\\"https://openai.com/index/introducing-structured-outputs-in-the-api/\\"&gt;https://openai.com/index/introducing-structured-outputs-in-the-api/&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Never had an issue with using structured outputs with Gemma3. using low temperatures around 0.2 might help.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lox1f7/local_models_not_following_instructions/n0qri2g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751370709,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lox1f7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0rczkt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DinoAmino","can_mod_post":false,"created_utc":1751378373,"send_replies":true,"parent_id":"t3_1lox1f7","score":1,"author_fullname":"t2_j1v7f","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Hmm. The new Mistral is great at instruction following. It must be sensitive to that much quantization then. Models fine-tuned as agents and for function calling should do well. Like Devstral.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0rczkt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hmm. The new Mistral is great at instruction following. It must be sensitive to that much quantization then. Models fine-tuned as agents and for function calling should do well. Like Devstral.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lox1f7/local_models_not_following_instructions/n0rczkt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751378373,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lox1f7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0rnrsm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"synw_","can_mod_post":false,"created_utc":1751381595,"send_replies":true,"parent_id":"t3_1lox1f7","score":2,"author_fullname":"t2_ecqgod","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Use deterministic sampling parameters and provide an example shot with a prompt and the desired output. For json even small models have shown to be efficient in my experience using example shots","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0rnrsm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Use deterministic sampling parameters and provide an example shot with a prompt and the desired output. For json even small models have shown to be efficient in my experience using example shots&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lox1f7/local_models_not_following_instructions/n0rnrsm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751381595,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lox1f7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),n=()=>e.jsx(t,{data:l});export{n as default};
