import{j as e}from"./index-BlGsFJYy.js";import{R as t}from"./RedditPostRenderer-B6uvq_Zl.js";import"./index-DDvVtNwD.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Over the past 2 months, I’ve been testing various combinations of models and front ends for a local LLM. I have a windows computer with a 3090 (24gb VRAM), 32gb motherboard ram, and a 2tb ssd. I’m running ollama on the backend and openwebui and anythingllm for front ends. I’m successful with direct connections to ollama as well as basic chat in oui and aLLM.\\n\\nThe problems start as soon as I try to invoke web search, call any tool, or use oui’s or allm’s built-in RAG tools. I have yet to find a single model that fits on my 3090 that can reliably use these functions. I’ve tried a *lot* of different models of different sizes, optimized and trained for tool-use and not. I simply cannot get reliable functionality from any model.\\n\\nCan anyone share their working setup? Is  my hardware not capable enough for some reason? Or is this whole home LLM thing just wishful thinking and one of those hobbies where the joy is in the fiddling because it’s not possible to use this for actual work?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Need help with basic functionality","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lu8x2i","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":1,"author_flair_background_color":null,"subreddit_type":"public","ups":3,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_gr2fr79s1","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":3,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751929493,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Over the past 2 months, I’ve been testing various combinations of models and front ends for a local LLM. I have a windows computer with a 3090 (24gb VRAM), 32gb motherboard ram, and a 2tb ssd. I’m running ollama on the backend and openwebui and anythingllm for front ends. I’m successful with direct connections to ollama as well as basic chat in oui and aLLM.&lt;/p&gt;\\n\\n&lt;p&gt;The problems start as soon as I try to invoke web search, call any tool, or use oui’s or allm’s built-in RAG tools. I have yet to find a single model that fits on my 3090 that can reliably use these functions. I’ve tried a &lt;em&gt;lot&lt;/em&gt; of different models of different sizes, optimized and trained for tool-use and not. I simply cannot get reliable functionality from any model.&lt;/p&gt;\\n\\n&lt;p&gt;Can anyone share their working setup? Is  my hardware not capable enough for some reason? Or is this whole home LLM thing just wishful thinking and one of those hobbies where the joy is in the fiddling because it’s not possible to use this for actual work?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lu8x2i","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"evilbarron2","discussion_type":null,"num_comments":5,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lu8x2i/need_help_with_basic_functionality/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lu8x2i/need_help_with_basic_functionality/","subreddit_subscribers":496034,"created_utc":1751929493,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1xsjm9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Marksta","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1xj7a1","score":1,"author_fullname":"t2_559a1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Setting context from frontend is proprietary black magic. I'm fairly sure open webui actually supports it because it does a lot of proprietary ollama junk but really doubt aLLM plays that game. It's probably just telling it what context window it can expect and thus will go insane quickly once Ollama default context strikes.\\n\\nMy biggest advice would be to do inference from aLLM or check out LM Studio. It's closed source but it's the best training wheels experience to llama.cpp if you're not up for just running llama.cpp directly. Then at least you'd know what your context window actually is set at.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1xsjm9","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Setting context from frontend is proprietary black magic. I&amp;#39;m fairly sure open webui actually supports it because it does a lot of proprietary ollama junk but really doubt aLLM plays that game. It&amp;#39;s probably just telling it what context window it can expect and thus will go insane quickly once Ollama default context strikes.&lt;/p&gt;\\n\\n&lt;p&gt;My biggest advice would be to do inference from aLLM or check out LM Studio. It&amp;#39;s closed source but it&amp;#39;s the best training wheels experience to llama.cpp if you&amp;#39;re not up for just running llama.cpp directly. Then at least you&amp;#39;d know what your context window actually is set at.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lu8x2i","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lu8x2i/need_help_with_basic_functionality/n1xsjm9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751953143,"author_flair_text":null,"treatment_tags":[],"created_utc":1751953143,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1xj7a1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"evilbarron2","can_mod_post":false,"created_utc":1751948709,"send_replies":true,"parent_id":"t1_n1wsig4","score":1,"author_fullname":"t2_gr2fr79s1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I’ve tried, but I’ve found that past about 20k token context window almost all these models lose their minds. At 10k most models can act functionally. Note that I am setting this in oui and aLLM as I assume they set context window parameters with each ollama request. That said, the results I see are suggestive of a mismatch in context window sizes somewhere.\\n\\nAnd I am relieved to hear my hardware is enough. I’m confident I can figure out everything except how to justify spending any more money on this.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1xj7a1","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I’ve tried, but I’ve found that past about 20k token context window almost all these models lose their minds. At 10k most models can act functionally. Note that I am setting this in oui and aLLM as I assume they set context window parameters with each ollama request. That said, the results I see are suggestive of a mismatch in context window sizes somewhere.&lt;/p&gt;\\n\\n&lt;p&gt;And I am relieved to hear my hardware is enough. I’m confident I can figure out everything except how to justify spending any more money on this.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lu8x2i","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lu8x2i/need_help_with_basic_functionality/n1xj7a1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751948709,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1wsig4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Key-Software3774","can_mod_post":false,"created_utc":1751938590,"send_replies":true,"parent_id":"t3_1lu8x2i","score":1,"author_fullname":"t2_16mzyqohf2","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Have you configured ollama with bigger context window than the default 2k tokens?\\nhttps://news.ycombinator.com/item?id=42833427\\n\\nYour HW is more than enough ;)","edited":1751938816,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1wsig4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Have you configured ollama with bigger context window than the default 2k tokens?\\n&lt;a href=\\"https://news.ycombinator.com/item?id=42833427\\"&gt;https://news.ycombinator.com/item?id=42833427&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Your HW is more than enough ;)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lu8x2i/need_help_with_basic_functionality/n1wsig4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751938590,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lu8x2i","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1xk4v9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"evilbarron2","can_mod_post":false,"created_utc":1751949124,"send_replies":true,"parent_id":"t1_n1x0w0p","score":1,"author_fullname":"t2_gr2fr79s1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I assumed the context window parameters were controlled by oui and aLLM, but a mismatch in context window sizes makes sense.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1xk4v9","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I assumed the context window parameters were controlled by oui and aLLM, but a mismatch in context window sizes makes sense.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lu8x2i","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lu8x2i/need_help_with_basic_functionality/n1xk4v9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751949124,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1x0w0p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"triynizzles1","can_mod_post":false,"created_utc":1751941485,"send_replies":true,"parent_id":"t3_1lu8x2i","score":1,"author_fullname":"t2_zr0g49ixt","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It could be a ollama‘s default context window as others have said or perhaps the type of files are not being read properly or RAG pipeline simply isn’t very good. 3090 and phi4 q4 or granite 3.3 should be able to handle RAG just fine.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1x0w0p","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It could be a ollama‘s default context window as others have said or perhaps the type of files are not being read properly or RAG pipeline simply isn’t very good. 3090 and phi4 q4 or granite 3.3 should be able to handle RAG just fine.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lu8x2i/need_help_with_basic_functionality/n1x0w0p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751941485,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lu8x2i","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),s=()=>e.jsx(t,{data:l});export{s as default};
