import{j as e}from"./index-DLSqWzaI.js";import{R as l}from"./RedditPostRenderer-CysRo2D_.js";import"./index-COXiL3Lo.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Currently want to get into playing with LLMs and am starting my first PC build (only have owned laptops before on integrated graphics). Based in USA. Is the 5060 8GB at $280 enough to mess with local AI stuff and potentially move on when I've hit the limits, or am I going to be hitting limits so early on that I should just get a faster/more VRAM/better memory bus/etc card from the start? Right now the options in that price range seem like $280 5060 8GB or maybe used ~$320ish 3080 10GB. The big swing move for me right now would be something like a 5070 ti 16GB at $800 (already stretching budget a lot), but it seems like if I can get away with around $300 and then upgrade later it would be better overall. If I'm playing down in 8GB territory anyways, should I just find whatever cheap $100ish card on ebay I can to mess for now?\\n\\nAre there big differences in the technologies incorporated in the 10xx, 20xx, 30xx, 40xx, 50xx cards that are relevant to AI loads? Or can I just roughly use the (mostly fps-based/gaming) benchmarks as a guide for relative performance? Other things I should worry about in the build other than GPU? Currently thinking CPU as AMD 9600x with 32GB DDR5-6000.\\n\\nLong-term goal is to play around enough with LLMs to be able to understand what is happening in the research papers i.e. play around with building smaller LLMs/change around architectures/measure performance; download models to play around with inference; and maybe doing useful fine-tuning of (smaller) models. Basically dipping my toes in right now. I have a long-term goal, but let's be honest, you don't decide to buy a Strad because you want to learn violin, and I'm not looking to drop $$$$ on a GPU if it's avoidable.\\n\\nUpgrade paths will depend on progress on playing around with small model building, fine-tuning existing small footprint models and useful inference from downloaded models. They would include better GPU or just buying time from a cloud provider.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Entry GPU options - 5060 8GB enough to play with?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m6knhw","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":1,"author_flair_background_color":null,"subreddit_type":"public","ups":5,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_fs6q6","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":5,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753205961,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Currently want to get into playing with LLMs and am starting my first PC build (only have owned laptops before on integrated graphics). Based in USA. Is the 5060 8GB at $280 enough to mess with local AI stuff and potentially move on when I&amp;#39;ve hit the limits, or am I going to be hitting limits so early on that I should just get a faster/more VRAM/better memory bus/etc card from the start? Right now the options in that price range seem like $280 5060 8GB or maybe used ~$320ish 3080 10GB. The big swing move for me right now would be something like a 5070 ti 16GB at $800 (already stretching budget a lot), but it seems like if I can get away with around $300 and then upgrade later it would be better overall. If I&amp;#39;m playing down in 8GB territory anyways, should I just find whatever cheap $100ish card on ebay I can to mess for now?&lt;/p&gt;\\n\\n&lt;p&gt;Are there big differences in the technologies incorporated in the 10xx, 20xx, 30xx, 40xx, 50xx cards that are relevant to AI loads? Or can I just roughly use the (mostly fps-based/gaming) benchmarks as a guide for relative performance? Other things I should worry about in the build other than GPU? Currently thinking CPU as AMD 9600x with 32GB DDR5-6000.&lt;/p&gt;\\n\\n&lt;p&gt;Long-term goal is to play around enough with LLMs to be able to understand what is happening in the research papers i.e. play around with building smaller LLMs/change around architectures/measure performance; download models to play around with inference; and maybe doing useful fine-tuning of (smaller) models. Basically dipping my toes in right now. I have a long-term goal, but let&amp;#39;s be honest, you don&amp;#39;t decide to buy a Strad because you want to learn violin, and I&amp;#39;m not looking to drop $$$$ on a GPU if it&amp;#39;s avoidable.&lt;/p&gt;\\n\\n&lt;p&gt;Upgrade paths will depend on progress on playing around with small model building, fine-tuning existing small footprint models and useful inference from downloaded models. They would include better GPU or just buying time from a cloud provider.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m6knhw","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"drabbiticus","discussion_type":null,"num_comments":22,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m6knhw/entry_gpu_options_5060_8gb_enough_to_play_with/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m6knhw/entry_gpu_options_5060_8gb_enough_to_play_with/","subreddit_subscribers":502981,"created_utc":1753205961,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4m51sj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"PermanentLiminality","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4kmnml","score":1,"author_fullname":"t2_19zqycaf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You can run bigger models\\n\\nYou can run larger quants like Q6 or Q8 instead of always using q4\\n\\nYou can support more context\\n\\nI have 20GB and it's not enough at all.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4m51sj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can run bigger models&lt;/p&gt;\\n\\n&lt;p&gt;You can run larger quants like Q6 or Q8 instead of always using q4&lt;/p&gt;\\n\\n&lt;p&gt;You can support more context&lt;/p&gt;\\n\\n&lt;p&gt;I have 20GB and it&amp;#39;s not enough at all.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6knhw","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6knhw/entry_gpu_options_5060_8gb_enough_to_play_with/n4m51sj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753225275,"author_flair_text":null,"treatment_tags":[],"created_utc":1753225275,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4kmnml","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"drabbiticus","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4kg006","score":1,"author_fullname":"t2_fs6q6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks for sharing your experience /u/PermanentLiminality and /u/Smilysis! Can you share examples of the things that you would do with 16GB VRAM that you currently can't on 8GB? Is it mostly just that you can't really efficiently evaluate as much in the State-of-the-art model space/feel that the models you can run on 8GB are a lot worse?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4kmnml","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for sharing your experience &lt;a href=\\"/u/PermanentLiminality\\"&gt;/u/PermanentLiminality&lt;/a&gt; and &lt;a href=\\"/u/Smilysis\\"&gt;/u/Smilysis&lt;/a&gt;! Can you share examples of the things that you would do with 16GB VRAM that you currently can&amp;#39;t on 8GB? Is it mostly just that you can&amp;#39;t really efficiently evaluate as much in the State-of-the-art model space/feel that the models you can run on 8GB are a lot worse?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6knhw","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6knhw/entry_gpu_options_5060_8gb_enough_to_play_with/n4kmnml/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753209361,"author_flair_text":null,"treatment_tags":[],"created_utc":1753209361,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4kg006","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Smilysis","can_mod_post":false,"created_utc":1753207504,"send_replies":true,"parent_id":"t1_n4kfaad","score":1,"author_fullname":"t2_68vefuej","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This!\\n\\nI have 8gb vram and i kinda regret my decision. Relying on ram for offloading model might work but it makes the process kinda slow and boring...","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4kg006","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This!&lt;/p&gt;\\n\\n&lt;p&gt;I have 8gb vram and i kinda regret my decision. Relying on ram for offloading model might work but it makes the process kinda slow and boring...&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6knhw","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6knhw/entry_gpu_options_5060_8gb_enough_to_play_with/n4kg006/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753207504,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4kfaad","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"PermanentLiminality","can_mod_post":false,"created_utc":1753207306,"send_replies":true,"parent_id":"t3_1m6knhw","score":9,"author_fullname":"t2_19zqycaf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Havig only 8GB is really limiting.  The 16 gb 5060 Ti is a lot better.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4kfaad","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Havig only 8GB is really limiting.  The 16 gb 5060 Ti is a lot better.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6knhw/entry_gpu_options_5060_8gb_enough_to_play_with/n4kfaad/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753207306,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6knhw","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4kne55","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"drabbiticus","can_mod_post":false,"created_utc":1753209568,"send_replies":true,"parent_id":"t1_n4kfvtd","score":1,"author_fullname":"t2_fs6q6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks for your reply!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4kne55","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for your reply!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6knhw","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6knhw/entry_gpu_options_5060_8gb_enough_to_play_with/n4kne55/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753209568,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4kfvtd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1753207472,"send_replies":true,"parent_id":"t3_1m6knhw","score":2,"author_fullname":"t2_uz37qfx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"No. do not buy anything less 12 GiB for LLM.\\n\\nIf you want only experimenting with 8 GiB you can buy p104-100 for $25-$40. \\n\\nBut frankly, just buy 5060ti 16 GiB.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4kfvtd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No. do not buy anything less 12 GiB for LLM.&lt;/p&gt;\\n\\n&lt;p&gt;If you want only experimenting with 8 GiB you can buy p104-100 for $25-$40. &lt;/p&gt;\\n\\n&lt;p&gt;But frankly, just buy 5060ti 16 GiB.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6knhw/entry_gpu_options_5060_8gb_enough_to_play_with/n4kfvtd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753207472,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6knhw","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4kqghk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"D3cto","can_mod_post":false,"created_utc":1753210431,"send_replies":true,"parent_id":"t3_1m6knhw","score":2,"author_fullname":"t2_5d1v0s4k","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Buy once, cry once 16GB 5060Ti will let you run bigger models at higher quants.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4kqghk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Buy once, cry once 16GB 5060Ti will let you run bigger models at higher quants.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6knhw/entry_gpu_options_5060_8gb_enough_to_play_with/n4kqghk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753210431,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6knhw","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4lkcsy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"randomqhacker","can_mod_post":false,"created_utc":1753218857,"send_replies":true,"parent_id":"t3_1m6knhw","score":2,"author_fullname":"t2_4nw3v","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I don't think there's any point to getting an 8GB card.  You can play round with inference without a GPU using Qwen3-30B-A3B, or any local model 4B and under.  I think you would be severely limited as to what you could train with only 8GB VRAM, probably just QLORA or really really small models.  A 16GB card with decent bandwidth like a 4070 Ti Super or 5070 Ti is probably the best bang for the buck.  You can go cheaper with AMD or Intel (new cards coming soon) but the training side will be more complicated AFAIK.\\n\\nI'm pretty happy with my 16GB card (wish I had 24GB for the 32GB models but everything else fits nicely) and I can even use it for offloading the attention, shared experts, and KV cache for MoEs, which makes them run acceptably fast.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4lkcsy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t think there&amp;#39;s any point to getting an 8GB card.  You can play round with inference without a GPU using Qwen3-30B-A3B, or any local model 4B and under.  I think you would be severely limited as to what you could train with only 8GB VRAM, probably just QLORA or really really small models.  A 16GB card with decent bandwidth like a 4070 Ti Super or 5070 Ti is probably the best bang for the buck.  You can go cheaper with AMD or Intel (new cards coming soon) but the training side will be more complicated AFAIK.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m pretty happy with my 16GB card (wish I had 24GB for the 32GB models but everything else fits nicely) and I can even use it for offloading the attention, shared experts, and KV cache for MoEs, which makes them run acceptably fast.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6knhw/entry_gpu_options_5060_8gb_enough_to_play_with/n4lkcsy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753218857,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6knhw","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4kl83o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Koksny","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4kizdb","score":1,"author_fullname":"t2_olk3n","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt;I understand Vulcan only slightly slower than rocm but I haven't tested.\\n\\nInference is slightly slower. Prompt processing (\\"feeding\\" the inference) is much slower, as You need to use cpu based BLAS instead of CuBLAS/HipBLAS.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4kl83o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;I understand Vulcan only slightly slower than rocm but I haven&amp;#39;t tested.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Inference is slightly slower. Prompt processing (&amp;quot;feeding&amp;quot; the inference) is much slower, as You need to use cpu based BLAS instead of CuBLAS/HipBLAS.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6knhw","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6knhw/entry_gpu_options_5060_8gb_enough_to_play_with/n4kl83o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753208957,"author_flair_text":null,"treatment_tags":[],"created_utc":1753208957,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4knmc5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"drabbiticus","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4kizdb","score":1,"author_fullname":"t2_fs6q6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Huge 👍! Thanks again for sharing your experience!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4knmc5","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Huge 👍! Thanks again for sharing your experience!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6knhw","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6knhw/entry_gpu_options_5060_8gb_enough_to_play_with/n4knmc5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753209632,"author_flair_text":null,"treatment_tags":[],"created_utc":1753209632,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4kizdb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Amazing_Athlete_2265","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4kfxjg","score":1,"author_fullname":"t2_1nw9fzb7dt","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I am using Vulkan and it works great, no faffing around to install like rocm. I understand Vulcan only slightly slower than rocm but I haven't tested.\\n\\nIf you're getting into finetuning and stuff like that, definitely Nvidia.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4kizdb","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I am using Vulkan and it works great, no faffing around to install like rocm. I understand Vulcan only slightly slower than rocm but I haven&amp;#39;t tested.&lt;/p&gt;\\n\\n&lt;p&gt;If you&amp;#39;re getting into finetuning and stuff like that, definitely Nvidia.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6knhw","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6knhw/entry_gpu_options_5060_8gb_enough_to_play_with/n4kizdb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753208333,"author_flair_text":null,"treatment_tags":[],"created_utc":1753208333,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4kfxjg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"drabbiticus","can_mod_post":false,"created_utc":1753207485,"send_replies":true,"parent_id":"t1_n4kdu3r","score":1,"author_fullname":"t2_fs6q6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Awesome, thanks for sharing! I remember a few years ago people were saying that Nvidia was the only way to go for AI because of CUDA vs. ROCm support - is that no longer a problem or do you find issues still?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4kfxjg","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Awesome, thanks for sharing! I remember a few years ago people were saying that Nvidia was the only way to go for AI because of CUDA vs. ROCm support - is that no longer a problem or do you find issues still?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6knhw","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6knhw/entry_gpu_options_5060_8gb_enough_to_play_with/n4kfxjg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753207485,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4kdu3r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Amazing_Athlete_2265","can_mod_post":false,"created_utc":1753206910,"send_replies":true,"parent_id":"t3_1m6knhw","score":1,"author_fullname":"t2_1nw9fzb7dt","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'd go for the cheaper option for now.\\n\\nI have an older 6600xt with 8gb ram and it runs smaller models (7B and less, depending on quant) really fast. Well, fast enough for me! I can always run bigger models but of course it slows up pretty quick once shared between GPU and CPU.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4kdu3r","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;d go for the cheaper option for now.&lt;/p&gt;\\n\\n&lt;p&gt;I have an older 6600xt with 8gb ram and it runs smaller models (7B and less, depending on quant) really fast. Well, fast enough for me! I can always run bigger models but of course it slows up pretty quick once shared between GPU and CPU.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6knhw/entry_gpu_options_5060_8gb_enough_to_play_with/n4kdu3r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753206910,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6knhw","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4le2cy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"drabbiticus","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4l1o7z","score":1,"author_fullname":"t2_fs6q6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thanks again! I really appreciate you giving this noob (me) some more concrete examples. Have a great day!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4le2cy","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks again! I really appreciate you giving this noob (me) some more concrete examples. Have a great day!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6knhw","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6knhw/entry_gpu_options_5060_8gb_enough_to_play_with/n4le2cy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753217085,"author_flair_text":null,"treatment_tags":[],"created_utc":1753217085,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4l1o7z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Smilysis","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4klwqp","score":1,"author_fullname":"t2_68vefuej","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yw!\\n\\nThere's a big difference between a 32B and a 7B model imo:\\n7B tend to use more tokens and hallucinate alot more while also being more difficult to make it stick to your prompt. Meanwhile 32B feels decent: a big step compared to 7B and usable if you want to have fun and do \\"lighter\\" complex tasks.\\n\\nBigger models do less mistakes and are easier to handle (in a sense that they understand your prompt better). You can always use RAG to make them give you more accurate info too. I personally use llms for fun and some light coding, havent experimented much on fine tunning so i dont have much to say.\\n\\nAbout the higher VRAM: it's really really *really* expensive to load larger models similar to GPT or Claude (deepseek with no quants), so keep in mind that you're not getting that much close in terms of performance when comparing to these. On the other hand, by having 16gb Vram you're not only gonna save alot of time (since there's more space for the model to work on your vram) but you will also be able to do training and fine tunning more comfortably. \\n\\nSmaller local models tend to be more useful when given an specific purpose, meanwhile larger ones have an easier time being more \\"general\\".\\n\\nI highly suggest going 16gb VRAM, still a bit expensive but totally worth it in long term thanks to the trouble you will be avoiding (aka out of memory errors and slowwwww training/responses)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4l1o7z","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yw!&lt;/p&gt;\\n\\n&lt;p&gt;There&amp;#39;s a big difference between a 32B and a 7B model imo:\\n7B tend to use more tokens and hallucinate alot more while also being more difficult to make it stick to your prompt. Meanwhile 32B feels decent: a big step compared to 7B and usable if you want to have fun and do &amp;quot;lighter&amp;quot; complex tasks.&lt;/p&gt;\\n\\n&lt;p&gt;Bigger models do less mistakes and are easier to handle (in a sense that they understand your prompt better). You can always use RAG to make them give you more accurate info too. I personally use llms for fun and some light coding, havent experimented much on fine tunning so i dont have much to say.&lt;/p&gt;\\n\\n&lt;p&gt;About the higher VRAM: it&amp;#39;s really really &lt;em&gt;really&lt;/em&gt; expensive to load larger models similar to GPT or Claude (deepseek with no quants), so keep in mind that you&amp;#39;re not getting that much close in terms of performance when comparing to these. On the other hand, by having 16gb Vram you&amp;#39;re not only gonna save alot of time (since there&amp;#39;s more space for the model to work on your vram) but you will also be able to do training and fine tunning more comfortably. &lt;/p&gt;\\n\\n&lt;p&gt;Smaller local models tend to be more useful when given an specific purpose, meanwhile larger ones have an easier time being more &amp;quot;general&amp;quot;.&lt;/p&gt;\\n\\n&lt;p&gt;I highly suggest going 16gb VRAM, still a bit expensive but totally worth it in long term thanks to the trouble you will be avoiding (aka out of memory errors and slowwwww training/responses)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6knhw","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6knhw/entry_gpu_options_5060_8gb_enough_to_play_with/n4l1o7z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753213658,"author_flair_text":null,"treatment_tags":[],"created_utc":1753213658,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4klwqp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"drabbiticus","can_mod_post":false,"created_utc":1753209149,"send_replies":true,"parent_id":"t1_n4kf9hk","score":1,"author_fullname":"t2_fs6q6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks for the advice! From a \\"learning LLM\\" perspective, is there a big difference between 7B and 32B+? Or is it mostly that I get to run bigger models (which typically should provide better results)?\\n\\nIf you have the time, some followup questions:\\n\\nI'm not going to compete with GPT or Claude with what I can train/fine-tune/run locally on my lonesome and on a budget, but if being able to run larger models offers more learning opportunities that I'm not seeing then I would love to understand more. The \\"training BERT on 8GB VRAM\\" notes I posted as a comment on the original post above looked like 100h of training on a 3060, which is about as long as I'd want to run anything for training locally already. In my head, I probably don't want to spend more than about a week on my local machine for a learning experiment. Anything where I'm expecting more time than that I'd probably offload to a cloud service.\\n\\nI guess from a \\"using LLMs\\" and privacy angle -- does going up to 16 GB VRAM allow me to run substantially more useful models locally that can rival GPT/Claude-like results, or would 16 GB VRAM local models still be a privacy vs. performance tradeoff?\\n\\nWhen you say proper cooling, are you talking about enough air flow or more active solutions?\\n\\nThanks!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4klwqp","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for the advice! From a &amp;quot;learning LLM&amp;quot; perspective, is there a big difference between 7B and 32B+? Or is it mostly that I get to run bigger models (which typically should provide better results)?&lt;/p&gt;\\n\\n&lt;p&gt;If you have the time, some followup questions:&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m not going to compete with GPT or Claude with what I can train/fine-tune/run locally on my lonesome and on a budget, but if being able to run larger models offers more learning opportunities that I&amp;#39;m not seeing then I would love to understand more. The &amp;quot;training BERT on 8GB VRAM&amp;quot; notes I posted as a comment on the original post above looked like 100h of training on a 3060, which is about as long as I&amp;#39;d want to run anything for training locally already. In my head, I probably don&amp;#39;t want to spend more than about a week on my local machine for a learning experiment. Anything where I&amp;#39;m expecting more time than that I&amp;#39;d probably offload to a cloud service.&lt;/p&gt;\\n\\n&lt;p&gt;I guess from a &amp;quot;using LLMs&amp;quot; and privacy angle -- does going up to 16 GB VRAM allow me to run substantially more useful models locally that can rival GPT/Claude-like results, or would 16 GB VRAM local models still be a privacy vs. performance tradeoff?&lt;/p&gt;\\n\\n&lt;p&gt;When you say proper cooling, are you talking about enough air flow or more active solutions?&lt;/p&gt;\\n\\n&lt;p&gt;Thanks!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6knhw","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6knhw/entry_gpu_options_5060_8gb_enough_to_play_with/n4klwqp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753209149,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4kf9hk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Smilysis","can_mod_post":false,"created_utc":1753207301,"send_replies":true,"parent_id":"t3_1m6knhw","score":1,"author_fullname":"t2_68vefuej","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"VRAM &gt; RAM\\n\\nThe more vram you have, the faster it's gonna be to do LLM stuff since you wont need to rely on offloading models on your ram. Imo 16gb vram is a decent start, 32gb for extra performance and longevity. \\n\\nYou can still do a few things with 8gb vram, but 32B paraments models onward are gonna be HELLA slow, you also would need to compensate the lack of vram with ram (which imo is not worth it, you might be able to load bigger models but it's still gonna be really slow)\\n\\nI highly suggest getting at least 32gb ram too, 64gb ram might be overkill but being able to use your computer without having to worry about the llm occupying all your ram is really nice.\\n\\nAlso, make sure that you have proper cooling, this is the most important part. While dealing with AI stuff your GPU will be on high demand for long periods of time, cooling it properly will alow it to last longer and avoid hardware damage due to heat problems.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4kf9hk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;VRAM &amp;gt; RAM&lt;/p&gt;\\n\\n&lt;p&gt;The more vram you have, the faster it&amp;#39;s gonna be to do LLM stuff since you wont need to rely on offloading models on your ram. Imo 16gb vram is a decent start, 32gb for extra performance and longevity. &lt;/p&gt;\\n\\n&lt;p&gt;You can still do a few things with 8gb vram, but 32B paraments models onward are gonna be HELLA slow, you also would need to compensate the lack of vram with ram (which imo is not worth it, you might be able to load bigger models but it&amp;#39;s still gonna be really slow)&lt;/p&gt;\\n\\n&lt;p&gt;I highly suggest getting at least 32gb ram too, 64gb ram might be overkill but being able to use your computer without having to worry about the llm occupying all your ram is really nice.&lt;/p&gt;\\n\\n&lt;p&gt;Also, make sure that you have proper cooling, this is the most important part. While dealing with AI stuff your GPU will be on high demand for long periods of time, cooling it properly will alow it to last longer and avoid hardware damage due to heat problems.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6knhw/entry_gpu_options_5060_8gb_enough_to_play_with/n4kf9hk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753207301,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6knhw","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4kfkk9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"drabbiticus","can_mod_post":false,"created_utc":1753207385,"send_replies":true,"parent_id":"t3_1m6knhw","score":1,"author_fullname":"t2_fs6q6","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Just dropping some stuff from my random research on things that can be done on an 8GB card; it's at least looking like 8GB is enough to play :)\\n\\n- Training BERT on a 8GB card - https://sidsite.com/posts/bert-from-scratch/\\n- Fine tuning 7B model on a 8GB card - https://www.youtube.com/watch?v=mbmaEDL2wms\\n\\nAnyone have any personal experience with this type of thing?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4kfkk9","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just dropping some stuff from my random research on things that can be done on an 8GB card; it&amp;#39;s at least looking like 8GB is enough to play :)&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Training BERT on a 8GB card - &lt;a href=\\"https://sidsite.com/posts/bert-from-scratch/\\"&gt;https://sidsite.com/posts/bert-from-scratch/&lt;/a&gt;&lt;/li&gt;\\n&lt;li&gt;Fine tuning 7B model on a 8GB card - &lt;a href=\\"https://www.youtube.com/watch?v=mbmaEDL2wms\\"&gt;https://www.youtube.com/watch?v=mbmaEDL2wms&lt;/a&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Anyone have any personal experience with this type of thing?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6knhw/entry_gpu_options_5060_8gb_enough_to_play_with/n4kfkk9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753207385,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6knhw","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4lc1fp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"arekku255","can_mod_post":false,"created_utc":1753216531,"send_replies":true,"parent_id":"t3_1m6knhw","score":1,"author_fullname":"t2_15rugs","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"16 GB is really the bare minimum nowadays. Preferably you want even more, like 24 or 32 GB.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4lc1fp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;16 GB is really the bare minimum nowadays. Preferably you want even more, like 24 or 32 GB.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6knhw/entry_gpu_options_5060_8gb_enough_to_play_with/n4lc1fp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753216531,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6knhw","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4lf75m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AbyssianOne","can_mod_post":false,"created_utc":1753217399,"send_replies":true,"parent_id":"t3_1m6knhw","score":1,"author_fullname":"t2_1651c3kskq","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"No. I have a 4060 8GB and it's not enough to run Gemma 12B even at a 4 bit quant at any usable level unless you keep your context window to something tiny like 1k tokens.\\n\\nYou want as much VRAM as you can get. And however much you go with, you'll wish you had more.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4lf75m","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No. I have a 4060 8GB and it&amp;#39;s not enough to run Gemma 12B even at a 4 bit quant at any usable level unless you keep your context window to something tiny like 1k tokens.&lt;/p&gt;\\n\\n&lt;p&gt;You want as much VRAM as you can get. And however much you go with, you&amp;#39;ll wish you had more.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6knhw/entry_gpu_options_5060_8gb_enough_to_play_with/n4lf75m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753217399,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6knhw","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4lx7r0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Unique_Judgment_1304","can_mod_post":false,"created_utc":1753222747,"send_replies":true,"parent_id":"t3_1m6knhw","score":1,"author_fullname":"t2_1s9hyoxo94","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Most important thing is VRAM size, then bandwidth, then CUDA, then Ampere generation or higher because of FA 2 support. so:   \\n5070 Ti &gt; 4070 Ti super &gt; 5060 Ti 16GB &gt; 4060 Ti 16GB &gt; 3080 12GB &gt; 5070 &gt; 4070 &gt; 3060 12GB.  \\nJust get the best you can afford from that list, though 3060 12GB is a good newbie choice, new or used.  \\nThe higher the bandwidth, the more stackable the card is. With a 3060 12GB you wouldn't want to stack more than 2-3 because you will start to feel the relatively slow bandwidth. With 4060 Ti 16GB that has the slowest bandwidth here, even stacking two would feel slow.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4lx7r0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Most important thing is VRAM size, then bandwidth, then CUDA, then Ampere generation or higher because of FA 2 support. so:&lt;br/&gt;\\n5070 Ti &amp;gt; 4070 Ti super &amp;gt; 5060 Ti 16GB &amp;gt; 4060 Ti 16GB &amp;gt; 3080 12GB &amp;gt; 5070 &amp;gt; 4070 &amp;gt; 3060 12GB.&lt;br/&gt;\\nJust get the best you can afford from that list, though 3060 12GB is a good newbie choice, new or used.&lt;br/&gt;\\nThe higher the bandwidth, the more stackable the card is. With a 3060 12GB you wouldn&amp;#39;t want to stack more than 2-3 because you will start to feel the relatively slow bandwidth. With 4060 Ti 16GB that has the slowest bandwidth here, even stacking two would feel slow.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6knhw/entry_gpu_options_5060_8gb_enough_to_play_with/n4lx7r0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753222747,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6knhw","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4m69vf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Saruphon","can_mod_post":false,"created_utc":1753225673,"send_replies":true,"parent_id":"t3_1m6knhw","score":1,"author_fullname":"t2_bkb0tcya","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I am currently using RTX 2070 + 16 GB RAM (Hopefully, can upgrade to RTX 5090 soon).  \\nSo far I can run 3B and 7B model just fine and can even run SDXL model. However if you are buying yourself a new PC at least get RTX5070 Ti + 64 GB RAM, you can do more with that.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4m69vf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I am currently using RTX 2070 + 16 GB RAM (Hopefully, can upgrade to RTX 5090 soon).&lt;br/&gt;\\nSo far I can run 3B and 7B model just fine and can even run SDXL model. However if you are buying yourself a new PC at least get RTX5070 Ti + 64 GB RAM, you can do more with that.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6knhw/entry_gpu_options_5060_8gb_enough_to_play_with/n4m69vf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753225673,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6knhw","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(l,{data:t});export{r as default};
