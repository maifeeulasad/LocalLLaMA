import{j as e}from"./index-BlGsFJYy.js";import{R as t}from"./RedditPostRenderer-B6uvq_Zl.js";import"./index-DDvVtNwD.js";const l=JSON.parse('[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hi everyone,\\n\\nIâ€™ve developed a tool that calculates the *optimal quantisation mix* tailored to your VRAM and RAM specifications specifically for the DeepSeek-R1-0528 model. If youâ€™d like to try it out, you can find it here:  \\nðŸ”— [GGUF Tool Suite on GitHub](https://github.com/Thireus/GGUF-Tool-Suite/)\\n\\nYou can also create custom quantisation recipes using this Colab notebook:  \\nðŸ”— [Quant Recipe Pipeline](https://colab.research.google.com/github/Thireus/GGUF-Tool-Suite/blob/main/quant_recipe_pipeline.ipynb)\\n\\nOnce you have a recipe, use the [quant_downloader.sh](https://github.com/Thireus/GGUF-Tool-Suite/blob/main/quant_downloader.sh) script to download the model shards using the `.recipe` file. Please note that the scripts have mainly been tested in a Linux environment; support for macOS is planned. For best results, run the downloader on Linux. After downloading, load the model with `ik_llama` using [this patch](https://github.com/Thireus/ik_llama.cpp/commit/a66490410a366a9605234b94d67f3d9b7b389140) (also donâ€™t forget to run `ulimit -n 99999` first).\\n\\nYou can find examples of recipes (including perplexity scores and other metrics) available here:  \\nðŸ”— [Recipe Examples](https://github.com/Thireus/GGUF-Tool-Suite/tree/main/recipe_examples)\\n\\nI\'ve tried to produce examples to benchmark against GGUF quants from other reputable creators such as unsloth, ubergarm, bartowski.\\n\\nFor full details and setup instructions, please refer to the repoâ€™s README:  \\nðŸ”— [GGUF Tool Suite README](https://github.com/Thireus/GGUF-Tool-Suite/)\\n\\nIâ€™m also planning to publish an article soon that will explore the capabilities of the GGUF Tool Suite and demonstrate how it can be used to produce an optimised mixture of quants for other LLM models.\\n\\nIâ€™d love to hear your feedback or answer any questions you may have!\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Introducing GGUF Tool Suite - Create and Optimise Quantisation Mix for DeepSeek-R1-0528 for Your Own Specs","link_flair_richtext":[{"e":"text","t":"Resources"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1ly84xd","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.9,"author_flair_background_color":null,"subreddit_type":"public","ups":17,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_8u7n5","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Resources","can_mod_post":false,"score":17,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1752395931,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1752346612,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\\n\\n&lt;p&gt;Iâ€™ve developed a tool that calculates the &lt;em&gt;optimal quantisation mix&lt;/em&gt; tailored to your VRAM and RAM specifications specifically for the DeepSeek-R1-0528 model. If youâ€™d like to try it out, you can find it here:&lt;br/&gt;\\nðŸ”— &lt;a href=\\"https://github.com/Thireus/GGUF-Tool-Suite/\\"&gt;GGUF Tool Suite on GitHub&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;You can also create custom quantisation recipes using this Colab notebook:&lt;br/&gt;\\nðŸ”— &lt;a href=\\"https://colab.research.google.com/github/Thireus/GGUF-Tool-Suite/blob/main/quant_recipe_pipeline.ipynb\\"&gt;Quant Recipe Pipeline&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Once you have a recipe, use the &lt;a href=\\"https://github.com/Thireus/GGUF-Tool-Suite/blob/main/quant_downloader.sh\\"&gt;quant_downloader.sh&lt;/a&gt; script to download the model shards using the &lt;code&gt;.recipe&lt;/code&gt; file. Please note that the scripts have mainly been tested in a Linux environment; support for macOS is planned. For best results, run the downloader on Linux. After downloading, load the model with &lt;code&gt;ik_llama&lt;/code&gt; using &lt;a href=\\"https://github.com/Thireus/ik_llama.cpp/commit/a66490410a366a9605234b94d67f3d9b7b389140\\"&gt;this patch&lt;/a&gt; (also donâ€™t forget to run &lt;code&gt;ulimit -n 99999&lt;/code&gt; first).&lt;/p&gt;\\n\\n&lt;p&gt;You can find examples of recipes (including perplexity scores and other metrics) available here:&lt;br/&gt;\\nðŸ”— &lt;a href=\\"https://github.com/Thireus/GGUF-Tool-Suite/tree/main/recipe_examples\\"&gt;Recipe Examples&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ve tried to produce examples to benchmark against GGUF quants from other reputable creators such as unsloth, ubergarm, bartowski.&lt;/p&gt;\\n\\n&lt;p&gt;For full details and setup instructions, please refer to the repoâ€™s README:&lt;br/&gt;\\nðŸ”— &lt;a href=\\"https://github.com/Thireus/GGUF-Tool-Suite/\\"&gt;GGUF Tool Suite README&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Iâ€™m also planning to publish an article soon that will explore the capabilities of the GGUF Tool Suite and demonstrate how it can be used to produce an optimised mixture of quants for other LLM models.&lt;/p&gt;\\n\\n&lt;p&gt;Iâ€™d love to hear your feedback or answer any questions you may have!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/mj_iC-YyHvPh4mHsOjFii7sajPyqVswMWvlO_8Xqgks.png?auto=webp&amp;s=7243f31103c2d94003681d0b82858d4108e3b776","width":1200,"height":600},"resolutions":[{"url":"https://external-preview.redd.it/mj_iC-YyHvPh4mHsOjFii7sajPyqVswMWvlO_8Xqgks.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=791d882324dd951016663a2ce43b55901557ce50","width":108,"height":54},{"url":"https://external-preview.redd.it/mj_iC-YyHvPh4mHsOjFii7sajPyqVswMWvlO_8Xqgks.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1e062fe80c217f3a281559e624465b7c16b3f417","width":216,"height":108},{"url":"https://external-preview.redd.it/mj_iC-YyHvPh4mHsOjFii7sajPyqVswMWvlO_8Xqgks.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=33b7b94f116c5275ef617d0c15bc5d6d7c2bc55a","width":320,"height":160},{"url":"https://external-preview.redd.it/mj_iC-YyHvPh4mHsOjFii7sajPyqVswMWvlO_8Xqgks.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=86b150bbdb03fe31c0459d1545d66ed999bc7931","width":640,"height":320},{"url":"https://external-preview.redd.it/mj_iC-YyHvPh4mHsOjFii7sajPyqVswMWvlO_8Xqgks.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e887dcf77db029bbc01d10f6307b6ac00aed69b4","width":960,"height":480},{"url":"https://external-preview.redd.it/mj_iC-YyHvPh4mHsOjFii7sajPyqVswMWvlO_8Xqgks.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b15c0446137fce63ca36558e3c438df37406c0c1","width":1080,"height":540}],"variants":{},"id":"mj_iC-YyHvPh4mHsOjFii7sajPyqVswMWvlO_8Xqgks"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#ccac2b","id":"1ly84xd","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Thireus","discussion_type":null,"num_comments":2,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1ly84xd/introducing_gguf_tool_suite_create_and_optimise/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1ly84xd/introducing_gguf_tool_suite_create_and_optimise/","subreddit_subscribers":498345,"created_utc":1752346612,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2rzr98","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Thireus","can_mod_post":false,"created_utc":1752348515,"send_replies":true,"parent_id":"t3_1ly84xd","score":3,"author_fullname":"t2_8u7n5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Hereâ€™s an example based on my own setup and goals:\\n\\nIâ€™m using this recipe, which I generated to fully utilise my available VRAM and RAM while running DeepSeek-R1-0528 at a **110k context size**:  \\nðŸ”— [DeepSeek-R1-0528.THIREUS-3.1027bpw-3.3372ppl.242GB](https://github.com/Thireus/GGUF-Tool-Suite/blob/main/recipe_examples/DeepSeek-R1-0528.THIREUS-3.1027bpw-3.3372ppl.242GB-GGUF_11GB-GPU_231GB-CPU.3c88ec6_adc8101.recipe)\\n\\n**My system specs:**\\n- 1Ã— RTX 5090 + 2Ã— RTX 3090\\n- 256â€¯GB DDR4 RAM\\n- Intel i9-9980XE CPU\\n\\n**Recipe performance:**\\n```\\n3.3372 Â± 0.01781 ppl  \\n242â€¯GB total model size  \\n11â€¯GB VRAM used  \\n231â€¯GB RAM used  \\n113.10 tokens/s (prompt processing)  \\n5.70 tokens/s (eval)  \\n```\\n\\nIf I ever need better perplexity and can trade off some context size, I can switch recipes *without redownloading the entire model*. The `quant_downloader.sh` script will automatically detect and fetch only the changed tensors, as long as I run it in the same model directory.\\n\\nFor example, this alternate recipe provides **lower perplexity (3.2734)** but takes up **22â€¯GB more memory**:  \\nðŸ”— [`DeepSeek-R1-0528.THIREUS-3.5652bpw-3.2734ppl.278GB`](https://github.com/Thireus/GGUF-Tool-Suite/blob/main/recipe_examples/DeepSeek-R1-0528.THIREUS-3.5652bpw-3.2734ppl.278GB-GGUF_14GB-GPU_264GB-CPU.3c88ec6_9b5660b.recipe)\\n\\nHope this helps anyone trying to balance performance, perplexity, and memory usage!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2rzr98","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hereâ€™s an example based on my own setup and goals:&lt;/p&gt;\\n\\n&lt;p&gt;Iâ€™m using this recipe, which I generated to fully utilise my available VRAM and RAM while running DeepSeek-R1-0528 at a &lt;strong&gt;110k context size&lt;/strong&gt;:&lt;br/&gt;\\nðŸ”— &lt;a href=\\"https://github.com/Thireus/GGUF-Tool-Suite/blob/main/recipe_examples/DeepSeek-R1-0528.THIREUS-3.1027bpw-3.3372ppl.242GB-GGUF_11GB-GPU_231GB-CPU.3c88ec6_adc8101.recipe\\"&gt;DeepSeek-R1-0528.THIREUS-3.1027bpw-3.3372ppl.242GB&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;My system specs:&lt;/strong&gt;\\n- 1Ã— RTX 5090 + 2Ã— RTX 3090\\n- 256â€¯GB DDR4 RAM\\n- Intel i9-9980XE CPU&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Recipe performance:&lt;/strong&gt;\\n&lt;code&gt;\\n3.3372 Â± 0.01781 ppl  \\n242â€¯GB total model size  \\n11â€¯GB VRAM used  \\n231â€¯GB RAM used  \\n113.10 tokens/s (prompt processing)  \\n5.70 tokens/s (eval)  \\n&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;If I ever need better perplexity and can trade off some context size, I can switch recipes &lt;em&gt;without redownloading the entire model&lt;/em&gt;. The &lt;code&gt;quant_downloader.sh&lt;/code&gt; script will automatically detect and fetch only the changed tensors, as long as I run it in the same model directory.&lt;/p&gt;\\n\\n&lt;p&gt;For example, this alternate recipe provides &lt;strong&gt;lower perplexity (3.2734)&lt;/strong&gt; but takes up &lt;strong&gt;22â€¯GB more memory&lt;/strong&gt;:&lt;br/&gt;\\nðŸ”— &lt;a href=\\"https://github.com/Thireus/GGUF-Tool-Suite/blob/main/recipe_examples/DeepSeek-R1-0528.THIREUS-3.5652bpw-3.2734ppl.278GB-GGUF_14GB-GPU_264GB-CPU.3c88ec6_9b5660b.recipe\\"&gt;&lt;code&gt;DeepSeek-R1-0528.THIREUS-3.5652bpw-3.2734ppl.278GB&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Hope this helps anyone trying to balance performance, perplexity, and memory usage!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ly84xd/introducing_gguf_tool_suite_create_and_optimise/n2rzr98/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752348515,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ly84xd","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2v0olu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Corporate_Drone31","can_mod_post":false,"created_utc":1752391199,"send_replies":true,"parent_id":"t3_1ly84xd","score":2,"author_fullname":"t2_32o8hu91","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"That looks very interesting! I\'ll have to check it out properly later, but your tool certainly looks unique.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2v0olu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That looks very interesting! I&amp;#39;ll have to check it out properly later, but your tool certainly looks unique.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ly84xd/introducing_gguf_tool_suite_create_and_optimise/n2v0olu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752391199,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ly84xd","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]'),n=()=>e.jsx(t,{data:l});export{n as default};
