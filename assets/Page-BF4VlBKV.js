import{j as t}from"./index-BUtHYhT3.js";import{R as e}from"./RedditPostRenderer-BaN1Fn7z.js";import"./index-Cli9kp5v.js";const a=[{kind:"Listing",data:{after:null,dist:1,modhash:"",geo_filter:"",children:[{kind:"t3",data:{approved_at_utc:null,subreddit:"LocalLLaMA",selftext:`I've been working on a lightweight macOS desktop chat application that runs entirely offline and communicates with local LLMs through Ollama. No internet required once set up!

  Key features:

  \\- ðŸ§  Local LLM integration via Ollama

  \\- ðŸ’¬ Clean, modern chat interface with real-time streaming

  \\- ðŸ“ Full markdown support with syntax highlighting

  \\- ðŸ•˜ Persistent chat history

  \\- ðŸ”„ Easy model switching

  \\- ðŸŽ¨ Auto dark/light theme

  \\- ðŸ“¦ Under 20MB final app size

Built with Tauri, React, and Rust for optimal performance. The app automatically detects available Ollama models and provides a native macOS experience.

Perfect for anyone who wants to chat with AI models privately without sending data to external servers. Works great with llama3, codellama, and other Ollama models.

Available on GitHub with releases for macOS. Would love feedback from the community!

[https://github.com/abhijeetlokhande1996/local-chat-releases/releases/download/v0.1.0/Local.Chat\\_0.1.0\\_aarch64.dmg](https://github.com/abhijeetlokhande1996/local-chat-releases/releases/download/v0.1.0/Local.Chat_0.1.0_aarch64.dmg)`,user_reports:[],saved:!1,mod_reason_title:null,gilded:0,clicked:!1,title:"Built an offline AI chat app for macOS that works with local LLMs via Ollama",link_flair_richtext:[{e:"text",t:"News"}],subreddit_name_prefixed:"r/LocalLLaMA",hidden:!1,pwls:6,link_flair_css_class:"",downs:0,thumbnail_height:null,top_awarded_type:null,hide_score:!1,name:"t3_1lrqtzj",quarantine:!1,link_flair_text_color:"light",upvote_ratio:.27,author_flair_background_color:null,subreddit_type:"public",ups:0,total_awards_received:0,media_embed:{},thumbnail_width:null,author_flair_template_id:null,is_original_content:!1,author_fullname:"t2_9i10z0gx",secure_media:null,is_reddit_media_domain:!1,is_meta:!1,category:null,secure_media_embed:{},link_flair_text:"News",can_mod_post:!1,score:0,approved_by:null,is_created_from_ads_ui:!1,author_premium:!1,thumbnail:"self",edited:!1,author_flair_css_class:null,author_flair_richtext:[],gildings:{},content_categories:null,is_self:!0,mod_note:null,created:1751656221,link_flair_type:"richtext",wls:6,removed_by_category:null,banned_by:null,author_flair_type:"text",domain:"self.LocalLLaMA",allow_live_comments:!1,selftext_html:`&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I&amp;#39;ve been working on a lightweight macOS desktop chat application that runs entirely offline and communicates with local LLMs through Ollama. No internet required once set up!&lt;/p&gt;

&lt;p&gt;Key features:&lt;/p&gt;

&lt;p&gt;- ðŸ§  Local LLM integration via Ollama&lt;/p&gt;

&lt;p&gt;- ðŸ’¬ Clean, modern chat interface with real-time streaming&lt;/p&gt;

&lt;p&gt;- ðŸ“ Full markdown support with syntax highlighting&lt;/p&gt;

&lt;p&gt;- ðŸ•˜ Persistent chat history&lt;/p&gt;

&lt;p&gt;- ðŸ”„ Easy model switching&lt;/p&gt;

&lt;p&gt;- ðŸŽ¨ Auto dark/light theme&lt;/p&gt;

&lt;p&gt;- ðŸ“¦ Under 20MB final app size&lt;/p&gt;

&lt;p&gt;Built with Tauri, React, and Rust for optimal performance. The app automatically detects available Ollama models and provides a native macOS experience.&lt;/p&gt;

&lt;p&gt;Perfect for anyone who wants to chat with AI models privately without sending data to external servers. Works great with llama3, codellama, and other Ollama models.&lt;/p&gt;

&lt;p&gt;Available on GitHub with releases for macOS. Would love feedback from the community!&lt;/p&gt;

&lt;p&gt;&lt;a href="https://github.com/abhijeetlokhande1996/local-chat-releases/releases/download/v0.1.0/Local.Chat_0.1.0_aarch64.dmg"&gt;https://github.com/abhijeetlokhande1996/local-chat-releases/releases/download/v0.1.0/Local.Chat_0.1.0_aarch64.dmg&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;`,likes:null,suggested_sort:null,banned_at_utc:null,view_count:null,archived:!1,no_follow:!0,is_crosspostable:!1,pinned:!1,over_18:!1,all_awardings:[],awarders:[],media_only:!1,link_flair_template_id:"f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",can_gild:!1,spoiler:!1,locked:!1,author_flair_text:null,treatment_tags:[],visited:!1,removed_by:null,num_reports:null,distinguished:null,subreddit_id:"t5_81eyvm",author_is_blocked:!1,mod_reason_by:null,removal_reason:null,link_flair_background_color:"#cc3600",id:"1lrqtzj",is_robot_indexable:!0,num_duplicates:0,report_reasons:null,author:"Disastrous-Parsnip93",discussion_type:null,num_comments:0,send_replies:!0,media:null,contest_mode:!1,author_patreon_flair:!1,author_flair_text_color:null,permalink:"/r/LocalLLaMA/comments/1lrqtzj/built_an_offline_ai_chat_app_for_macos_that_works/",stickied:!1,url:"https://www.reddit.com/r/LocalLLaMA/comments/1lrqtzj/built_an_offline_ai_chat_app_for_macos_that_works/",subreddit_subscribers:494898,created_utc:1751656221,num_crossposts:0,mod_reports:[],is_video:!1}}],before:null}},{kind:"Listing",data:{after:null,dist:null,modhash:"",geo_filter:"",children:[],before:null}}],o=()=>t.jsx(e,{data:a});export{o as default};
