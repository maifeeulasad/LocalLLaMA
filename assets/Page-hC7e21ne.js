import{j as e}from"./index-DQXiEb7D.js";import{R as l}from"./RedditPostRenderer-BjndLgq8.js";import"./index-B-ILyjT1.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Using Llama.cpp post migration from Ollama for a few weeks, and my workflow is better than ever. I know we are mostly limited by Hardware, but seeing how far the project have come along in the past few months from Multi-Modalities support, to pure performance is mind blowing. How much improvement is there still..? My only concern is stagnation, as I've seen that happen with some of my favorite repos over the years.\\n\\n  \\nTo all the awesome community of developers behind the project, my humble PC and I thank you!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Llama.cpp - Any room for further Significant Improvement?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lr1i84","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.73,"author_flair_background_color":null,"subreddit_type":"public","ups":8,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_vbzgnic","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":8,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751578321,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Using Llama.cpp post migration from Ollama for a few weeks, and my workflow is better than ever. I know we are mostly limited by Hardware, but seeing how far the project have come along in the past few months from Multi-Modalities support, to pure performance is mind blowing. How much improvement is there still..? My only concern is stagnation, as I&amp;#39;ve seen that happen with some of my favorite repos over the years.&lt;/p&gt;\\n\\n&lt;p&gt;To all the awesome community of developers behind the project, my humble PC and I thank you!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lr1i84","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"simracerman","discussion_type":null,"num_comments":6,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lr1i84/llamacpp_any_room_for_further_significant/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lr1i84/llamacpp_any_room_for_further_significant/","subreddit_subscribers":494198,"created_utc":1751578321,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n17hmg1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"panchovix","can_mod_post":false,"created_utc":1751581466,"send_replies":true,"parent_id":"t3_1lr1i84","score":7,"author_fullname":"t2_j1kqr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you have multiple GPUs there is still work to do on llamacpp, as backends like exllama, SGLagng or vLLM outperform it by quite a lot.\\n\\nI think TP is just really hard to implement on llamacpp, it may not be possible even.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n17hmg1","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you have multiple GPUs there is still work to do on llamacpp, as backends like exllama, SGLagng or vLLM outperform it by quite a lot.&lt;/p&gt;\\n\\n&lt;p&gt;I think TP is just really hard to implement on llamacpp, it may not be possible even.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lr1i84/llamacpp_any_room_for_further_significant/n17hmg1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751581466,"author_flair_text":"Llama 405B","treatment_tags":[],"link_id":"t3_1lr1i84","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n18ismr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Few-Yam9901","can_mod_post":false,"created_utc":1751594846,"send_replies":true,"parent_id":"t1_n18b4r8","score":1,"author_fullname":"t2_1rhlf3bcfk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Is it possible to use KTransformers with 4x Xeon v4 cpu Systems? E.g. hpe dl 580 gen 9?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n18ismr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Is it possible to use KTransformers with 4x Xeon v4 cpu Systems? E.g. hpe dl 580 gen 9?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lr1i84","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lr1i84/llamacpp_any_room_for_further_significant/n18ismr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751594846,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n18b4r8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Marksta","can_mod_post":false,"created_utc":1751591978,"send_replies":true,"parent_id":"t3_1lr1i84","score":4,"author_fullname":"t2_559a1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"There's a whole lot that could be worked on, given time and maybe some funding too. There's a new, non breaking, rarely regression causing push to main like daily so I don't think stagnation is a worry for a long while. And the motivation and want for more is definitely there.\\n\\nThe short list of fantasy additions they could do:\\n\\nReplicating or merging CPU speed ups found in Ik_llama.cpp. As well as the cutting edge quantization support.\\n\\nLike in vLLM: TP, DP, and concurrency optimizations\\n\\nRPC client could use a lot of improvement, it has a lot of baked in overhead even running within local machine.\\n\\nNUMA optimizations: Similar to TP/RPC, with better architecture design dual CPU builds should be seeing +100% performance but that doesn't fit the design currently. I heard KTransformers demoed/has some data parallel NUMA mirroring to sidestep NUMA link speed issues and actually hit the +100% performance. I'd really love that in llama.cpp","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n18b4r8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There&amp;#39;s a whole lot that could be worked on, given time and maybe some funding too. There&amp;#39;s a new, non breaking, rarely regression causing push to main like daily so I don&amp;#39;t think stagnation is a worry for a long while. And the motivation and want for more is definitely there.&lt;/p&gt;\\n\\n&lt;p&gt;The short list of fantasy additions they could do:&lt;/p&gt;\\n\\n&lt;p&gt;Replicating or merging CPU speed ups found in Ik_llama.cpp. As well as the cutting edge quantization support.&lt;/p&gt;\\n\\n&lt;p&gt;Like in vLLM: TP, DP, and concurrency optimizations&lt;/p&gt;\\n\\n&lt;p&gt;RPC client could use a lot of improvement, it has a lot of baked in overhead even running within local machine.&lt;/p&gt;\\n\\n&lt;p&gt;NUMA optimizations: Similar to TP/RPC, with better architecture design dual CPU builds should be seeing +100% performance but that doesn&amp;#39;t fit the design currently. I heard KTransformers demoed/has some data parallel NUMA mirroring to sidestep NUMA link speed issues and actually hit the +100% performance. I&amp;#39;d really love that in llama.cpp&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lr1i84/llamacpp_any_room_for_further_significant/n18b4r8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751591978,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lr1i84","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n18uvp3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"OutrageousMinimum191","can_mod_post":false,"created_utc":1751599553,"send_replies":true,"parent_id":"t3_1lr1i84","score":3,"author_fullname":"t2_13ejj8rbqm","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"MCP servers support in llama-server web interface is needed desperately. I saw that pull request was submitted. I hope that it'll be implemented one day.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n18uvp3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;MCP servers support in llama-server web interface is needed desperately. I saw that pull request was submitted. I hope that it&amp;#39;ll be implemented one day.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lr1i84/llamacpp_any_room_for_further_significant/n18uvp3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751599553,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lr1i84","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n192zlw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"PaceZealousideal6091","can_mod_post":false,"created_utc":1751603013,"send_replies":true,"parent_id":"t3_1lr1i84","score":1,"author_fullname":"t2_bpkdm1tk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Support for diffusion model is another missing link.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n192zlw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Support for diffusion model is another missing link.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lr1i84/llamacpp_any_room_for_further_significant/n192zlw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751603013,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lr1i84","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n199mun","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ttkciar","can_mod_post":false,"created_utc":1751606082,"send_replies":true,"parent_id":"t3_1lr1i84","score":1,"author_fullname":"t2_cpegz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'm (re)working on a self-mixing feature (inferring on layers multiple times).  Someone else is working on re-implementing the training and finetune examples so that they use any supported back-end (CUDA, Vulkan, SYCL, CPU).\\n\\nVision support is there, but is also very much a work in progress.  Other places to improve which come to mind are a more generalized Guided Generation interface (like grammars, but with arbitrary external logic), or support for more architectures like PHATGOOSE (old) and Pangu Pro (new).\\n\\nThere's no shortage of things to keep devs busy, I think.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n199mun","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m (re)working on a self-mixing feature (inferring on layers multiple times).  Someone else is working on re-implementing the training and finetune examples so that they use any supported back-end (CUDA, Vulkan, SYCL, CPU).&lt;/p&gt;\\n\\n&lt;p&gt;Vision support is there, but is also very much a work in progress.  Other places to improve which come to mind are a more generalized Guided Generation interface (like grammars, but with arbitrary external logic), or support for more architectures like PHATGOOSE (old) and Pangu Pro (new).&lt;/p&gt;\\n\\n&lt;p&gt;There&amp;#39;s no shortage of things to keep devs busy, I think.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lr1i84/llamacpp_any_room_for_further_significant/n199mun/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751606082,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lr1i84","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),o=()=>e.jsx(l,{data:a});export{o as default};
