import{j as e}from"./index-BOnf-UhU.js";import{R as t}from"./RedditPostRenderer-Ce39qICS.js";import"./index-CDase6VD.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Long story short I've got a system with 16GB RAM and a 6750XT GPU with 12GB VRAM, I'm happy with it for my daily usage but for AI stuff (coding/roleplay using koboldcpp) it's quite limiting. \\n\\nFor a cheapskate upgrade, do you think it'd be worth it to buy 2 RAM sticks of 16GB for ~40$ each (bringing me to 48GB total) in order to run MOE models like Qwen 30B.A3B / bigger ? Or should I stick with my current setup instead and keep running quantized models like mistrall 24B ?\\n\\nIdeally I just want to avoid buying a new GPU while also being able to use better models and have bigger context. I'm quite a noob and I don't know what I should really do, so any help/suggestion is more than welcomed.\\n\\nThanks in advance :)","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Is it worth getting 48GB of RAM alongside my 12GB VRAM GPU ? (cheapskate upgrade)","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m3nb1q","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.7,"author_flair_background_color":null,"subreddit_type":"public","ups":4,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_ux1pavfwr","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":4,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1752900827,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752899555,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Long story short I&amp;#39;ve got a system with 16GB RAM and a 6750XT GPU with 12GB VRAM, I&amp;#39;m happy with it for my daily usage but for AI stuff (coding/roleplay using koboldcpp) it&amp;#39;s quite limiting. &lt;/p&gt;\\n\\n&lt;p&gt;For a cheapskate upgrade, do you think it&amp;#39;d be worth it to buy 2 RAM sticks of 16GB for ~40$ each (bringing me to 48GB total) in order to run MOE models like Qwen 30B.A3B / bigger ? Or should I stick with my current setup instead and keep running quantized models like mistrall 24B ?&lt;/p&gt;\\n\\n&lt;p&gt;Ideally I just want to avoid buying a new GPU while also being able to use better models and have bigger context. I&amp;#39;m quite a noob and I don&amp;#39;t know what I should really do, so any help/suggestion is more than welcomed.&lt;/p&gt;\\n\\n&lt;p&gt;Thanks in advance :)&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m3nb1q","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"QuackMania","discussion_type":null,"num_comments":10,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m3nb1q/is_it_worth_getting_48gb_of_ram_alongside_my_12gb/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m3nb1q/is_it_worth_getting_48gb_of_ram_alongside_my_12gb/","subreddit_subscribers":501752,"created_utc":1752899555,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3yy7b5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kironlau","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3yjaz4","score":1,"author_fullname":"t2_tb0dz2ds","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"frankly speaking，ik\\\\_lamma.cpp just started supporting vulkan，it supports MOE well，to me enchance 30\\\\~50% in speed (I just OC my ddr4 ram to 3733mhz，coz I use 4 ram slot，the timing is not good.)\\n\\nYou could tried to setup one，to see if ur card work for it.(mainline llamacpp do support it，but the enhancement for MOE is not significant.)\\n\\nMy config: Qwn3-30b-A3B，IQ4\\\\_KS，with 32k context size. 30 token/s.\\n\\n(I am using 4070 12gb in windows.)\\n\\nIt depends on how many layer you offload to gpu (using overtensor parameter)\\n\\nFor my personal experience，if you can use mistral small 24B Q3，the performance is more or less close to qwen3-30b-a3b. (for Chinese，the latter is better，therefore I stayed to qwen3 for daily use.)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3yy7b5","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;frankly speaking，ik_lamma.cpp just started supporting vulkan，it supports MOE well，to me enchance 30~50% in speed (I just OC my ddr4 ram to 3733mhz，coz I use 4 ram slot，the timing is not good.)&lt;/p&gt;\\n\\n&lt;p&gt;You could tried to setup one，to see if ur card work for it.(mainline llamacpp do support it，but the enhancement for MOE is not significant.)&lt;/p&gt;\\n\\n&lt;p&gt;My config: Qwn3-30b-A3B，IQ4_KS，with 32k context size. 30 token/s.&lt;/p&gt;\\n\\n&lt;p&gt;(I am using 4070 12gb in windows.)&lt;/p&gt;\\n\\n&lt;p&gt;It depends on how many layer you offload to gpu (using overtensor parameter)&lt;/p&gt;\\n\\n&lt;p&gt;For my personal experience，if you can use mistral small 24B Q3，the performance is more or less close to qwen3-30b-a3b. (for Chinese，the latter is better，therefore I stayed to qwen3 for daily use.)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3nb1q","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nb1q/is_it_worth_getting_48gb_of_ram_alongside_my_12gb/n3yy7b5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752919073,"author_flair_text":null,"treatment_tags":[],"created_utc":1752919073,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3yjaz4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"QuackMania","can_mod_post":false,"created_utc":1752910616,"send_replies":true,"parent_id":"t1_n3y032x","score":2,"author_fullname":"t2_ux1pavfwr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I currently run mistral 24B in VRAM using a Q3_K_M model ! But it's a bit frustrating to be limited by 12GB of VRAM, that's why I was considering buying a nice amount of RAM and switch to MOE models instead. But others mentioned the speed wouldn't be great so it might a better idea to buy a cheap GPU to increase VRAM instead (and stick to dense models).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3yjaz4","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I currently run mistral 24B in VRAM using a Q3_K_M model ! But it&amp;#39;s a bit frustrating to be limited by 12GB of VRAM, that&amp;#39;s why I was considering buying a nice amount of RAM and switch to MOE models instead. But others mentioned the speed wouldn&amp;#39;t be great so it might a better idea to buy a cheap GPU to increase VRAM instead (and stick to dense models).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3nb1q","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nb1q/is_it_worth_getting_48gb_of_ram_alongside_my_12gb/n3yjaz4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752910616,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3y032x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kironlau","can_mod_post":false,"created_utc":1752900498,"send_replies":true,"parent_id":"t3_1m3nb1q","score":2,"author_fullname":"t2_tb0dz2ds","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yes, for MOE Qwen-30B-A3B. (buy a pair of lower latency ram, oc to 3600+ mhz for ddr4, if you using a CPU/motherboard doesn't support XMP/ram OC, forget about MOE. Qwen 14B is much faster.)\\n\\nNot much gain in mistral 24B (any layers offloading to the ram...will get you a huge drop of speed)....  \\nI am using 4070 12GB VRAM, I suggest you could use a smaller size of quant of mistral 24b. (offloading the vision part and context to cpu, you could host a Q3 model in VRAM.)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3y032x","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, for MOE Qwen-30B-A3B. (buy a pair of lower latency ram, oc to 3600+ mhz for ddr4, if you using a CPU/motherboard doesn&amp;#39;t support XMP/ram OC, forget about MOE. Qwen 14B is much faster.)&lt;/p&gt;\\n\\n&lt;p&gt;Not much gain in mistral 24B (any layers offloading to the ram...will get you a huge drop of speed)....&lt;br/&gt;\\nI am using 4070 12GB VRAM, I suggest you could use a smaller size of quant of mistral 24b. (offloading the vision part and context to cpu, you could host a Q3 model in VRAM.)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nb1q/is_it_worth_getting_48gb_of_ram_alongside_my_12gb/n3y032x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752900498,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3nb1q","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ymcqu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FieldProgrammable","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3yket4","score":2,"author_fullname":"t2_moet0t","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I would say 16GB VRAM should be the minimum for any GPU for LLM. Maybe if you are on a gaming laptop, 8GB is excusable, but for desktop there are 16GB choices 5060 Ti 16GB being the obvious one.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3ymcqu","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I would say 16GB VRAM should be the minimum for any GPU for LLM. Maybe if you are on a gaming laptop, 8GB is excusable, but for desktop there are 16GB choices 5060 Ti 16GB being the obvious one.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3nb1q","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nb1q/is_it_worth_getting_48gb_of_ram_alongside_my_12gb/n3ymcqu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752912316,"author_flair_text":null,"treatment_tags":[],"created_utc":1752912316,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3yket4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"QuackMania","can_mod_post":false,"created_utc":1752911240,"send_replies":true,"parent_id":"t1_n3ybhmv","score":1,"author_fullname":"t2_ux1pavfwr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes I'll upgrade my ram a tad bit for sure ! But now I'm also tempted in grabbing a cheap 8GB GPU after redditors suggesting that option :p\\n\\nAlso yes I could definitely run it and offload the rest on my RAM/CPU but the token/s is at minimum halved, it's not that usable anymore for me sadly :/","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3yket4","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes I&amp;#39;ll upgrade my ram a tad bit for sure ! But now I&amp;#39;m also tempted in grabbing a cheap 8GB GPU after redditors suggesting that option :p&lt;/p&gt;\\n\\n&lt;p&gt;Also yes I could definitely run it and offload the rest on my RAM/CPU but the token/s is at minimum halved, it&amp;#39;s not that usable anymore for me sadly :/&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3nb1q","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nb1q/is_it_worth_getting_48gb_of_ram_alongside_my_12gb/n3yket4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752911240,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ybhmv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ArchdukeofHyperbole","can_mod_post":false,"created_utc":1752906294,"send_replies":true,"parent_id":"t3_1m3nb1q","score":2,"author_fullname":"t2_1p41v97q5d","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You can already run qwen 3AB. Q4 is 16GB or so file size. offload like 11.5 to vram and do the rest on regular. But I'd say upgrade it anyway if you can, and go for 64GB ram 😏","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ybhmv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can already run qwen 3AB. Q4 is 16GB or so file size. offload like 11.5 to vram and do the rest on regular. But I&amp;#39;d say upgrade it anyway if you can, and go for 64GB ram 😏&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nb1q/is_it_worth_getting_48gb_of_ram_alongside_my_12gb/n3ybhmv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752906294,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3nb1q","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3yfuzr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FunnyAsparagus1253","can_mod_post":false,"created_utc":1752908675,"send_replies":true,"parent_id":"t1_n3y5jxm","score":1,"author_fullname":"t2_i6c8tay3w","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I agree with this. It’ll be too slow to be any fun. I know because I tried it! (It’s still nice having so much RAM in my main PC though, but I gave up using it for inference)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3yfuzr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I agree with this. It’ll be too slow to be any fun. I know because I tried it! (It’s still nice having so much RAM in my main PC though, but I gave up using it for inference)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3nb1q","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nb1q/is_it_worth_getting_48gb_of_ram_alongside_my_12gb/n3yfuzr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752908675,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3yiwga","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"QuackMania","can_mod_post":false,"created_utc":1752910381,"send_replies":true,"parent_id":"t1_n3y5jxm","score":1,"author_fullname":"t2_ux1pavfwr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Your comment reminded me that I had a somewhat broken 2080 laying around ! \\n(TLDR it used to crash ingame and cause artifacts so I bought my current 6750XT to replace it).\\n\\nSo I tried it a few minutes ago (now have 2GPUs in my PC, yay) and for some reasons when koboldcpp uses CUDA as its backend the card works perfectly fine, but if I dare try to use Vulkan it spits out gibberish and within seconds turns the screen black before crashing 💀\\n\\nDo you think it would be possible to use it somehow ? A mix of CUDA and Vulkan/ROCM, where one deals with the context itself and the rest deals with image generation or something like that. Else I'll do what you recommended and find a cheap 8GB card 👍","edited":1752911378,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3yiwga","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Your comment reminded me that I had a somewhat broken 2080 laying around ! \\n(TLDR it used to crash ingame and cause artifacts so I bought my current 6750XT to replace it).&lt;/p&gt;\\n\\n&lt;p&gt;So I tried it a few minutes ago (now have 2GPUs in my PC, yay) and for some reasons when koboldcpp uses CUDA as its backend the card works perfectly fine, but if I dare try to use Vulkan it spits out gibberish and within seconds turns the screen black before crashing 💀&lt;/p&gt;\\n\\n&lt;p&gt;Do you think it would be possible to use it somehow ? A mix of CUDA and Vulkan/ROCM, where one deals with the context itself and the rest deals with image generation or something like that. Else I&amp;#39;ll do what you recommended and find a cheap 8GB card 👍&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3nb1q","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nb1q/is_it_worth_getting_48gb_of_ram_alongside_my_12gb/n3yiwga/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752910381,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3y5jxm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ironcodegaming","can_mod_post":false,"created_utc":1752903181,"send_replies":true,"parent_id":"t3_1m3nb1q","score":1,"author_fullname":"t2_eeaio","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Adding RAM is generally useful. But unless you have a reasonably fast system, offloading to CPU will be a big hit to speed.\\n\\nIf possible, and if it can be installed in your PC, buy a cheap 8GB Card!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3y5jxm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Adding RAM is generally useful. But unless you have a reasonably fast system, offloading to CPU will be a big hit to speed.&lt;/p&gt;\\n\\n&lt;p&gt;If possible, and if it can be installed in your PC, buy a cheap 8GB Card!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nb1q/is_it_worth_getting_48gb_of_ram_alongside_my_12gb/n3y5jxm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752903181,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3nb1q","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ylv9q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FieldProgrammable","can_mod_post":false,"created_utc":1752912047,"send_replies":true,"parent_id":"t3_1m3nb1q","score":1,"author_fullname":"t2_moet0t","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I was in a similar position on my old rig. The only reason I upgraded the RAM was because certain model loaders first wanted to move the entire model to RAM from disk before passing it to the GPU. Diffusion models for txt2img were particularly bad for this to the point they ran into the disk swap file. Once it got to the GPU it was fine but up to that point was excruciating. At no point did I assume this would make a useful impact on inference speed.\\n\\nGenerally if you are expecting to be stuck with an old rig for some time with bottlenecks in PCIE, CPU and PSU, then it's better to just replace the GPU with something that has enough VRAM for your application, then running all inference in that larger VRAM.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ylv9q","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I was in a similar position on my old rig. The only reason I upgraded the RAM was because certain model loaders first wanted to move the entire model to RAM from disk before passing it to the GPU. Diffusion models for txt2img were particularly bad for this to the point they ran into the disk swap file. Once it got to the GPU it was fine but up to that point was excruciating. At no point did I assume this would make a useful impact on inference speed.&lt;/p&gt;\\n\\n&lt;p&gt;Generally if you are expecting to be stuck with an old rig for some time with bottlenecks in PCIE, CPU and PSU, then it&amp;#39;s better to just replace the GPU with something that has enough VRAM for your application, then running all inference in that larger VRAM.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nb1q/is_it_worth_getting_48gb_of_ram_alongside_my_12gb/n3ylv9q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752912047,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3nb1q","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(t,{data:l});export{r as default};
