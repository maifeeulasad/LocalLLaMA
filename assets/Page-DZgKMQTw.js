import{j as e}from"./index-BOnf-UhU.js";import{R as t}from"./RedditPostRenderer-Ce39qICS.js";import"./index-CDase6VD.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Long story short I've got a system with 16GB RAM and a 6750XT GPU with 12GB VRAM, I'm happy with it for my daily usage but for AI stuff (coding/roleplay using koboldcpp) it's quite limiting. \\n\\nFor a cheapskate upgrade, do you think it'd be worth it to buy 2 RAM sticks of 16GB for ~40$ each (bringing me to 48GB total) in order to run MOE models like Qwen 30B.A3B / bigger ? Or should I stick with my current setup instead and keep running quantized models like mistrall 24B ?\\n\\nIdeally I just want to avoid buying a new GPU while also being able to use better models and have bigger context. I'm quite a noob and I don't know what I should really do, so any help/suggestion is more than welcomed.\\n\\nThanks in advance :)","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Is it worth getting 48GB of RAM alongside my 12GB VRAM GPU ? (cheapskate upgrade)","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m3nb1q","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.7,"author_flair_background_color":null,"subreddit_type":"public","ups":4,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_ux1pavfwr","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":4,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1752900827,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752899555,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Long story short I&amp;#39;ve got a system with 16GB RAM and a 6750XT GPU with 12GB VRAM, I&amp;#39;m happy with it for my daily usage but for AI stuff (coding/roleplay using koboldcpp) it&amp;#39;s quite limiting. &lt;/p&gt;\\n\\n&lt;p&gt;For a cheapskate upgrade, do you think it&amp;#39;d be worth it to buy 2 RAM sticks of 16GB for ~40$ each (bringing me to 48GB total) in order to run MOE models like Qwen 30B.A3B / bigger ? Or should I stick with my current setup instead and keep running quantized models like mistrall 24B ?&lt;/p&gt;\\n\\n&lt;p&gt;Ideally I just want to avoid buying a new GPU while also being able to use better models and have bigger context. I&amp;#39;m quite a noob and I don&amp;#39;t know what I should really do, so any help/suggestion is more than welcomed.&lt;/p&gt;\\n\\n&lt;p&gt;Thanks in advance :)&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m3nb1q","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"QuackMania","discussion_type":null,"num_comments":10,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m3nb1q/is_it_worth_getting_48gb_of_ram_alongside_my_12gb/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m3nb1q/is_it_worth_getting_48gb_of_ram_alongside_my_12gb/","subreddit_subscribers":501752,"created_utc":1752899555,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3yy7b5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kironlau","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3yjaz4","score":1,"author_fullname":"t2_tb0dz2ds","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"frankly speakingï¼Œik\\\\_lamma.cpp just started supporting vulkanï¼Œit supports MOE wellï¼Œto me enchance 30\\\\~50% in speed (I just OC my ddr4 ram to 3733mhzï¼Œcoz I use 4 ram slotï¼Œthe timing is not good.)\\n\\nYou could tried to setup oneï¼Œto see if ur card work for it.(mainline llamacpp do support itï¼Œbut the enhancement for MOE is not significant.)\\n\\nMy config: Qwn3-30b-A3Bï¼ŒIQ4\\\\_KSï¼Œwith 32k context size. 30 token/s.\\n\\n(I am using 4070 12gb in windows.)\\n\\nIt depends on how many layer you offload to gpu (using overtensor parameter)\\n\\nFor my personal experienceï¼Œif you can use mistral small 24B Q3ï¼Œthe performance is more or less close to qwen3-30b-a3b. (for Chineseï¼Œthe latter is betterï¼Œtherefore I stayed to qwen3 for daily use.)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3yy7b5","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;frankly speakingï¼Œik_lamma.cpp just started supporting vulkanï¼Œit supports MOE wellï¼Œto me enchance 30~50% in speed (I just OC my ddr4 ram to 3733mhzï¼Œcoz I use 4 ram slotï¼Œthe timing is not good.)&lt;/p&gt;\\n\\n&lt;p&gt;You could tried to setup oneï¼Œto see if ur card work for it.(mainline llamacpp do support itï¼Œbut the enhancement for MOE is not significant.)&lt;/p&gt;\\n\\n&lt;p&gt;My config: Qwn3-30b-A3Bï¼ŒIQ4_KSï¼Œwith 32k context size. 30 token/s.&lt;/p&gt;\\n\\n&lt;p&gt;(I am using 4070 12gb in windows.)&lt;/p&gt;\\n\\n&lt;p&gt;It depends on how many layer you offload to gpu (using overtensor parameter)&lt;/p&gt;\\n\\n&lt;p&gt;For my personal experienceï¼Œif you can use mistral small 24B Q3ï¼Œthe performance is more or less close to qwen3-30b-a3b. (for Chineseï¼Œthe latter is betterï¼Œtherefore I stayed to qwen3 for daily use.)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3nb1q","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nb1q/is_it_worth_getting_48gb_of_ram_alongside_my_12gb/n3yy7b5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752919073,"author_flair_text":null,"treatment_tags":[],"created_utc":1752919073,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3yjaz4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"QuackMania","can_mod_post":false,"created_utc":1752910616,"send_replies":true,"parent_id":"t1_n3y032x","score":2,"author_fullname":"t2_ux1pavfwr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I currently run mistral 24B in VRAM using a Q3_K_M model ! But it's a bit frustrating to be limited by 12GB of VRAM, that's why I was considering buying a nice amount of RAM and switch to MOE models instead. But others mentioned the speed wouldn't be great so it might a better idea to buy a cheap GPU to increase VRAM instead (and stick to dense models).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3yjaz4","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I currently run mistral 24B in VRAM using a Q3_K_M model ! But it&amp;#39;s a bit frustrating to be limited by 12GB of VRAM, that&amp;#39;s why I was considering buying a nice amount of RAM and switch to MOE models instead. But others mentioned the speed wouldn&amp;#39;t be great so it might a better idea to buy a cheap GPU to increase VRAM instead (and stick to dense models).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3nb1q","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nb1q/is_it_worth_getting_48gb_of_ram_alongside_my_12gb/n3yjaz4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752910616,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3y032x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kironlau","can_mod_post":false,"created_utc":1752900498,"send_replies":true,"parent_id":"t3_1m3nb1q","score":2,"author_fullname":"t2_tb0dz2ds","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yes, for MOE Qwen-30B-A3B. (buy a pair of lower latency ram, oc to 3600+ mhz for ddr4, if you using a CPU/motherboard doesn't support XMP/ram OC, forget about MOE. Qwen 14B is much faster.)\\n\\nNot much gain in mistral 24B (any layers offloading to the ram...will get you a huge drop of speed)....  \\nI am using 4070 12GB VRAM, I suggest you could use a smaller size of quant of mistral 24b. (offloading the vision part and context to cpu, you could host a Q3 model in VRAM.)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3y032x","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, for MOE Qwen-30B-A3B. (buy a pair of lower latency ram, oc to 3600+ mhz for ddr4, if you using a CPU/motherboard doesn&amp;#39;t support XMP/ram OC, forget about MOE. Qwen 14B is much faster.)&lt;/p&gt;\\n\\n&lt;p&gt;Not much gain in mistral 24B (any layers offloading to the ram...will get you a huge drop of speed)....&lt;br/&gt;\\nI am using 4070 12GB VRAM, I suggest you could use a smaller size of quant of mistral 24b. (offloading the vision part and context to cpu, you could host a Q3 model in VRAM.)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nb1q/is_it_worth_getting_48gb_of_ram_alongside_my_12gb/n3y032x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752900498,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3nb1q","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ymcqu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FieldProgrammable","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3yket4","score":2,"author_fullname":"t2_moet0t","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I would say 16GB VRAM should be the minimum for any GPU for LLM. Maybe if you are on a gaming laptop, 8GB is excusable, but for desktop there are 16GB choices 5060 Ti 16GB being the obvious one.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3ymcqu","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I would say 16GB VRAM should be the minimum for any GPU for LLM. Maybe if you are on a gaming laptop, 8GB is excusable, but for desktop there are 16GB choices 5060 Ti 16GB being the obvious one.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3nb1q","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nb1q/is_it_worth_getting_48gb_of_ram_alongside_my_12gb/n3ymcqu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752912316,"author_flair_text":null,"treatment_tags":[],"created_utc":1752912316,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3yket4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"QuackMania","can_mod_post":false,"created_utc":1752911240,"send_replies":true,"parent_id":"t1_n3ybhmv","score":1,"author_fullname":"t2_ux1pavfwr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes I'll upgrade my ram a tad bit for sure ! But now I'm also tempted in grabbing a cheap 8GB GPU after redditors suggesting that option :p\\n\\nAlso yes I could definitely run it and offload the rest on my RAM/CPU but the token/s is at minimum halved, it's not that usable anymore for me sadly :/","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3yket4","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes I&amp;#39;ll upgrade my ram a tad bit for sure ! But now I&amp;#39;m also tempted in grabbing a cheap 8GB GPU after redditors suggesting that option :p&lt;/p&gt;\\n\\n&lt;p&gt;Also yes I could definitely run it and offload the rest on my RAM/CPU but the token/s is at minimum halved, it&amp;#39;s not that usable anymore for me sadly :/&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3nb1q","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nb1q/is_it_worth_getting_48gb_of_ram_alongside_my_12gb/n3yket4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752911240,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ybhmv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ArchdukeofHyperbole","can_mod_post":false,"created_utc":1752906294,"send_replies":true,"parent_id":"t3_1m3nb1q","score":2,"author_fullname":"t2_1p41v97q5d","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You can already run qwen 3AB. Q4 is 16GB or so file size. offload like 11.5 to vram and do the rest on regular. But I'd say upgrade it anyway if you can, and go for 64GB ram ðŸ˜","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ybhmv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can already run qwen 3AB. Q4 is 16GB or so file size. offload like 11.5 to vram and do the rest on regular. But I&amp;#39;d say upgrade it anyway if you can, and go for 64GB ram ðŸ˜&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nb1q/is_it_worth_getting_48gb_of_ram_alongside_my_12gb/n3ybhmv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752906294,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3nb1q","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3yfuzr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FunnyAsparagus1253","can_mod_post":false,"created_utc":1752908675,"send_replies":true,"parent_id":"t1_n3y5jxm","score":1,"author_fullname":"t2_i6c8tay3w","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I agree with this. Itâ€™ll be too slow to be any fun. I know because I tried it! (Itâ€™s still nice having so much RAM in my main PC though, but I gave up using it for inference)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3yfuzr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I agree with this. Itâ€™ll be too slow to be any fun. I know because I tried it! (Itâ€™s still nice having so much RAM in my main PC though, but I gave up using it for inference)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3nb1q","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nb1q/is_it_worth_getting_48gb_of_ram_alongside_my_12gb/n3yfuzr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752908675,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3yiwga","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"QuackMania","can_mod_post":false,"created_utc":1752910381,"send_replies":true,"parent_id":"t1_n3y5jxm","score":1,"author_fullname":"t2_ux1pavfwr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Your comment reminded me that I had a somewhat broken 2080 laying around ! \\n(TLDR it used to crash ingame and cause artifacts so I bought my current 6750XT to replace it).\\n\\nSo I tried it a few minutes ago (now have 2GPUs in my PC, yay) and for some reasons when koboldcpp uses CUDA as its backend the card works perfectly fine, but if I dare try to use Vulkan it spits out gibberish and within seconds turns the screen black before crashing ðŸ’€\\n\\nDo you think it would be possible to use it somehow ? A mix of CUDA and Vulkan/ROCM, where one deals with the context itself and the rest deals with image generation or something like that. Else I'll do what you recommended and find a cheap 8GB card ðŸ‘","edited":1752911378,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3yiwga","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Your comment reminded me that I had a somewhat broken 2080 laying around ! \\n(TLDR it used to crash ingame and cause artifacts so I bought my current 6750XT to replace it).&lt;/p&gt;\\n\\n&lt;p&gt;So I tried it a few minutes ago (now have 2GPUs in my PC, yay) and for some reasons when koboldcpp uses CUDA as its backend the card works perfectly fine, but if I dare try to use Vulkan it spits out gibberish and within seconds turns the screen black before crashing ðŸ’€&lt;/p&gt;\\n\\n&lt;p&gt;Do you think it would be possible to use it somehow ? A mix of CUDA and Vulkan/ROCM, where one deals with the context itself and the rest deals with image generation or something like that. Else I&amp;#39;ll do what you recommended and find a cheap 8GB card ðŸ‘&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3nb1q","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nb1q/is_it_worth_getting_48gb_of_ram_alongside_my_12gb/n3yiwga/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752910381,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3y5jxm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ironcodegaming","can_mod_post":false,"created_utc":1752903181,"send_replies":true,"parent_id":"t3_1m3nb1q","score":1,"author_fullname":"t2_eeaio","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Adding RAM is generally useful. But unless you have a reasonably fast system, offloading to CPU will be a big hit to speed.\\n\\nIf possible, and if it can be installed in your PC, buy a cheap 8GB Card!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3y5jxm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Adding RAM is generally useful. But unless you have a reasonably fast system, offloading to CPU will be a big hit to speed.&lt;/p&gt;\\n\\n&lt;p&gt;If possible, and if it can be installed in your PC, buy a cheap 8GB Card!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nb1q/is_it_worth_getting_48gb_of_ram_alongside_my_12gb/n3y5jxm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752903181,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3nb1q","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ylv9q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FieldProgrammable","can_mod_post":false,"created_utc":1752912047,"send_replies":true,"parent_id":"t3_1m3nb1q","score":1,"author_fullname":"t2_moet0t","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I was in a similar position on my old rig. The only reason I upgraded the RAM was because certain model loaders first wanted to move the entire model to RAM from disk before passing it to the GPU. Diffusion models for txt2img were particularly bad for this to the point they ran into the disk swap file. Once it got to the GPU it was fine but up to that point was excruciating. At no point did I assume this would make a useful impact on inference speed.\\n\\nGenerally if you are expecting to be stuck with an old rig for some time with bottlenecks in PCIE, CPU and PSU, then it's better to just replace the GPU with something that has enough VRAM for your application, then running all inference in that larger VRAM.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ylv9q","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I was in a similar position on my old rig. The only reason I upgraded the RAM was because certain model loaders first wanted to move the entire model to RAM from disk before passing it to the GPU. Diffusion models for txt2img were particularly bad for this to the point they ran into the disk swap file. Once it got to the GPU it was fine but up to that point was excruciating. At no point did I assume this would make a useful impact on inference speed.&lt;/p&gt;\\n\\n&lt;p&gt;Generally if you are expecting to be stuck with an old rig for some time with bottlenecks in PCIE, CPU and PSU, then it&amp;#39;s better to just replace the GPU with something that has enough VRAM for your application, then running all inference in that larger VRAM.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nb1q/is_it_worth_getting_48gb_of_ram_alongside_my_12gb/n3ylv9q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752912047,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3nb1q","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(t,{data:l});export{r as default};
