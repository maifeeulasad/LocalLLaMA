import{j as t}from"./index-CqAPCjw5.js";import{R as e}from"./RedditPostRenderer-4oBDAtGr.js";import"./index-D3Sdy_Op.js";const n=JSON.parse('[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"*On Day 11, I gave you a brief introduction to the attention mechanism. Today, we’re going to implement it from scratch in Python. But before we dive into the code, let’s quickly revisit what attention is all about.*\\n\\n# What Is Attention? \\n\\n*Imagine you’re in a room with five people, and you’re trying to understand what’s going on. You don’t pay equal attention to all five people, you naturally focus more on the person who’s talking about something relevant.*\\n\\n*That’s exactly what attention does for LLMs. When reading a sentence, the model “pays more attention” to the words that are important for understanding the context.*\\n\\n*Let’s break it down with a simple example and real code!*\\n\\n# Our Example: “Cats love cozy windows”\\n\\n*Each word will be turned into a vector , just a bunch of numbers that represent the meaning of the word. Here’s what our made-up word vectors look like:*\\n\\n    import torch\\n    \\n    inputs = torch.tensor([\\n        [0.10, 0.20, 0.30],  # Cats     (x¹)\\n        [0.40, 0.50, 0.60],  # love     (x²)\\n        [0.70, 0.80, 0.10],  # cozy     (x³)\\n        [0.90, 0.10, 0.20]   # windows  (x⁴)\\n    ])\\n\\n*Each row is an embedding for a word, just another way of saying, “this is how the model understands the meaning of the word in numbers.”*\\n\\n# 1: Calculating Attention Scores (How Similar Are These Words?)\\n\\n*Let’s say we want to find out how much attention the word* ***“****love****”*** *(second word) should pay to all the others.*\\n\\n*We do that by computing the dot product between the vector for “love” and the others. The higher the score, the more related they are.*\\n\\n    query = inputs[1]  # Embedding for \\"love\\"\\n    \\n    attn_scores = torch.empty(inputs.shape[0])\\n    for i, x_i in enumerate(inputs):\\n        attn_scores[i] = torch.dot(query, x_i)\\n    \\n    print(attn_scores)\\n\\n*Or, even faster, do it for all words at once using matrix multiplication:*\\n\\n    attn_scores_all = inputs @ inputs.T\\n    print(attn_scores_all)\\n\\n*This gives us a matrix of similarities, each number tells how strongly one word is related to another.*\\n\\n# 2: Turning Scores into Meaningful Weights (Using Softmax)\\n\\n*Raw scores are hard to interpret. We want to turn them into weights between 0 and 1 that add up to 1 for each word. This tells us the percentage of focus each word should get.*\\n\\n*We use the softmax function to do this:*\\n\\n    attn_weights = torch.softmax(attn_scores_all, dim=-1)\\n    print(attn_weights)\\n\\n*Now every row in this matrix shows how much attention one word gives to all the others. For instance, row 2 tells us how much “love” attends to “Cats,” “cozy,” and “windows.”*\\n\\n# 3: Creating a Context Vector (The Final Mix)\\n\\n*Here’s the cool part.*\\n\\n*Each word’s final understanding (called a context vector) is calculated by mixing all word vectors together, based on the attention weights.*\\n\\n*If “love” pays 70% attention to “Cats” and 30% to “cozy,” the context vector will be a blend of those two word vectors.*\\n\\n*Let’s do it manually for “love” (row 2):*\\n\\n    attn_weights_love = attn_weights[1]\\n    \\n    context_vec_love = torch.zeros_like(inputs[0])\\n    for i, x_i in enumerate(inputs):\\n        context_vec_love += attn_weights_love[i] * x_i\\n    \\n    print(context_vec_love)\\n\\n*Or faster, do it for all words at once:*\\n\\n    context_vectors = attn_weights @ inputs\\n    print(context_vectors)\\n\\n*Each row now holds a new version of the word that includes information from the whole sentence.* \\n\\n# Why Does This Matter?\\n\\n*This mechanism helps LLMs:*\\n\\n* ***Understand context:*** *It’s not just “what” a word is but how it fits in the sentence.*\\n* ***Be smarter with predictions:*** *It can now decide that “windows” is important because “cats love cozy windows.”*\\n* ***Handle longer sentences:*** *Attention lets the model scale and stay relevant, even with lots of words.*\\n\\n# TL;DR \\n\\n*The attention mechanism in LLMs:*\\n\\n1. *Calculates how similar each word is to every other word.*\\n2. *Converts those scores into weights (softmax).*\\n3. *Builds a new vector for each word using those weights (context vector).*\\n\\n*This simple trick is the backbone of how modern Transformers work, letting them read, understand, and generate human-like text.*\\n\\n*If this helped clarify things, let me know!*.*Tomorrow we are going to code the self attention mechanism with key, query and value matrices.*","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Day 12/50: Building a Small Language Model from Scratch - Implementing a Simplified Attention Mechanism in Python","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lv85jp","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.91,"author_flair_background_color":null,"subreddit_type":"public","ups":27,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_8ht7a116","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":27,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752030201,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;em&gt;On Day 11, I gave you a brief introduction to the attention mechanism. Today, we’re going to implement it from scratch in Python. But before we dive into the code, let’s quickly revisit what attention is all about.&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;h1&gt;What Is Attention? &lt;/h1&gt;\\n\\n&lt;p&gt;&lt;em&gt;Imagine you’re in a room with five people, and you’re trying to understand what’s going on. You don’t pay equal attention to all five people, you naturally focus more on the person who’s talking about something relevant.&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;em&gt;That’s exactly what attention does for LLMs. When reading a sentence, the model “pays more attention” to the words that are important for understanding the context.&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;em&gt;Let’s break it down with a simple example and real code!&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;h1&gt;Our Example: “Cats love cozy windows”&lt;/h1&gt;\\n\\n&lt;p&gt;&lt;em&gt;Each word will be turned into a vector , just a bunch of numbers that represent the meaning of the word. Here’s what our made-up word vectors look like:&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;import torch\\n\\ninputs = torch.tensor([\\n    [0.10, 0.20, 0.30],  # Cats     (x¹)\\n    [0.40, 0.50, 0.60],  # love     (x²)\\n    [0.70, 0.80, 0.10],  # cozy     (x³)\\n    [0.90, 0.10, 0.20]   # windows  (x⁴)\\n])\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;em&gt;Each row is an embedding for a word, just another way of saying, “this is how the model understands the meaning of the word in numbers.”&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;h1&gt;1: Calculating Attention Scores (How Similar Are These Words?)&lt;/h1&gt;\\n\\n&lt;p&gt;&lt;em&gt;Let’s say we want to find out how much attention the word&lt;/em&gt; &lt;strong&gt;&lt;em&gt;“&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;love&lt;/em&gt;&lt;strong&gt;&lt;em&gt;”&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;(second word) should pay to all the others.&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;em&gt;We do that by computing the dot product between the vector for “love” and the others. The higher the score, the more related they are.&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;query = inputs[1]  # Embedding for &amp;quot;love&amp;quot;\\n\\nattn_scores = torch.empty(inputs.shape[0])\\nfor i, x_i in enumerate(inputs):\\n    attn_scores[i] = torch.dot(query, x_i)\\n\\nprint(attn_scores)\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;em&gt;Or, even faster, do it for all words at once using matrix multiplication:&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;attn_scores_all = inputs @ inputs.T\\nprint(attn_scores_all)\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;em&gt;This gives us a matrix of similarities, each number tells how strongly one word is related to another.&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;h1&gt;2: Turning Scores into Meaningful Weights (Using Softmax)&lt;/h1&gt;\\n\\n&lt;p&gt;&lt;em&gt;Raw scores are hard to interpret. We want to turn them into weights between 0 and 1 that add up to 1 for each word. This tells us the percentage of focus each word should get.&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;em&gt;We use the softmax function to do this:&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;attn_weights = torch.softmax(attn_scores_all, dim=-1)\\nprint(attn_weights)\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;em&gt;Now every row in this matrix shows how much attention one word gives to all the others. For instance, row 2 tells us how much “love” attends to “Cats,” “cozy,” and “windows.”&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;h1&gt;3: Creating a Context Vector (The Final Mix)&lt;/h1&gt;\\n\\n&lt;p&gt;&lt;em&gt;Here’s the cool part.&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;em&gt;Each word’s final understanding (called a context vector) is calculated by mixing all word vectors together, based on the attention weights.&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;em&gt;If “love” pays 70% attention to “Cats” and 30% to “cozy,” the context vector will be a blend of those two word vectors.&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;em&gt;Let’s do it manually for “love” (row 2):&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;attn_weights_love = attn_weights[1]\\n\\ncontext_vec_love = torch.zeros_like(inputs[0])\\nfor i, x_i in enumerate(inputs):\\n    context_vec_love += attn_weights_love[i] * x_i\\n\\nprint(context_vec_love)\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;em&gt;Or faster, do it for all words at once:&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;context_vectors = attn_weights @ inputs\\nprint(context_vectors)\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;em&gt;Each row now holds a new version of the word that includes information from the whole sentence.&lt;/em&gt; &lt;/p&gt;\\n\\n&lt;h1&gt;Why Does This Matter?&lt;/h1&gt;\\n\\n&lt;p&gt;&lt;em&gt;This mechanism helps LLMs:&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;strong&gt;&lt;em&gt;Understand context:&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;It’s not just “what” a word is but how it fits in the sentence.&lt;/em&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;&lt;em&gt;Be smarter with predictions:&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;It can now decide that “windows” is important because “cats love cozy windows.”&lt;/em&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;&lt;em&gt;Handle longer sentences:&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;Attention lets the model scale and stay relevant, even with lots of words.&lt;/em&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;h1&gt;TL;DR &lt;/h1&gt;\\n\\n&lt;p&gt;&lt;em&gt;The attention mechanism in LLMs:&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;&lt;em&gt;Calculates how similar each word is to every other word.&lt;/em&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;em&gt;Converts those scores into weights (softmax).&lt;/em&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;em&gt;Builds a new vector for each word using those weights (context vector).&lt;/em&gt;&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;&lt;em&gt;This simple trick is the backbone of how modern Transformers work, letting them read, understand, and generate human-like text.&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;em&gt;If this helped clarify things, let me know!&lt;/em&gt;.&lt;em&gt;Tomorrow we are going to code the self attention mechanism with key, query and value matrices.&lt;/em&gt;&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lv85jp","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Prashant-Lakhera","discussion_type":null,"num_comments":0,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lv85jp/day_1250_building_a_small_language_model_from/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lv85jp/day_1250_building_a_small_language_model_from/","subreddit_subscribers":497025,"created_utc":1752030201,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[],"before":null}}]'),s=()=>t.jsx(e,{data:n});export{s as default};
