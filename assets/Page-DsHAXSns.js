import{j as e}from"./index-BgwOAK4-.js";import{R as l}from"./RedditPostRenderer-BOBjDTFu.js";import"./index-BL22wVg5.js";const t=JSON.parse('[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"So, I was attempting to run the Gemma3n model with transformer libraries on my MacBook Pro, which has the M2 silicon chip. I managed to download the model and use the transformer library, but the inference time was incredibly slow. If anyone has any experience with the MacBook and Gemma3n, it would be really helpful.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Gemma 3n is not performing well with macOS M2 MacBook Pro","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1ltj8pg","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":1,"author_flair_background_color":null,"subreddit_type":"public","ups":2,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_bo0sjlui","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":2,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751856648,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;So, I was attempting to run the Gemma3n model with transformer libraries on my MacBook Pro, which has the M2 silicon chip. I managed to download the model and use the transformer library, but the inference time was incredibly slow. If anyone has any experience with the MacBook and Gemma3n, it would be really helpful.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1ltj8pg","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Strikingaks","discussion_type":null,"num_comments":9,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1ltj8pg/gemma_3n_is_not_performing_well_with_macos_m2/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1ltj8pg/gemma_3n_is_not_performing_well_with_macos_m2/","subreddit_subscribers":496034,"created_utc":1751856648,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1vjxic","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Strikingaks","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1ujo0z","score":1,"author_fullname":"t2_bo0sjlui","approved_by":null,"mod_note":null,"all_awardings":[],"body":"This is very helpful, and I appreciate the detailed answer. I was wondering where we can learn about the conversion processes. Also, while this conversion is taking place, is there any quality degradation for the model? I hope this doesn’t involve quantizing the model.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n1vjxic","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is very helpful, and I appreciate the detailed answer. I was wondering where we can learn about the conversion processes. Also, while this conversion is taking place, is there any quality degradation for the model? I hope this doesn’t involve quantizing the model.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1ltj8pg","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltj8pg/gemma_3n_is_not_performing_well_with_macos_m2/n1vjxic/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751923922,"author_flair_text":null,"treatment_tags":[],"created_utc":1751923922,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1ujo0z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"this-just_in","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1ugmtz","score":2,"author_fullname":"t2_kdmu4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That’s correct, but you are in luck- they’ve already been converted.  MLX Community org on HuggingFace (https://huggingface.co/organizations/mlx-community) has both text only and text + vision of both E2B and E4B variants at various quants.  To tell the difference, the model card will say either converted by mlx-lm (text only) or by mlx-vlm (text + vision).","edited":false,"author_flair_css_class":null,"name":"t1_n1ujo0z","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That’s correct, but you are in luck- they’ve already been converted.  MLX Community org on HuggingFace (&lt;a href=\\"https://huggingface.co/organizations/mlx-community\\"&gt;https://huggingface.co/organizations/mlx-community&lt;/a&gt;) has both text only and text + vision of both E2B and E4B variants at various quants.  To tell the difference, the model card will say either converted by mlx-lm (text only) or by mlx-vlm (text + vision).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1ltj8pg","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltj8pg/gemma_3n_is_not_performing_well_with_macos_m2/n1ujo0z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751911881,"author_flair_text":null,"collapsed":false,"created_utc":1751911881,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1ugmtz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Strikingaks","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1tca7h","score":1,"author_fullname":"t2_bo0sjlui","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Wow, this is amazing! Great information, thank you. So, the Gemini 3n still requires MLX to run on Mac. The small size doesn’t mean we can simply pull the model and use the transformer to run it on Mac. We still need to convert the weights to MLX format and run it on Mac.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ugmtz","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Wow, this is amazing! Great information, thank you. So, the Gemini 3n still requires MLX to run on Mac. The small size doesn’t mean we can simply pull the model and use the transformer to run it on Mac. We still need to convert the weights to MLX format and run it on Mac.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltj8pg","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltj8pg/gemma_3n_is_not_performing_well_with_macos_m2/n1ugmtz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751910944,"author_flair_text":null,"treatment_tags":[],"created_utc":1751910944,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1tca7h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"this-just_in","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1smt1j","score":4,"author_fullname":"t2_kdmu4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"MLX is the most performant format for running AI on Apple Silicon devices.  On a mobile device you would use Swift MLX: https://github.com/ml-explore/mlx-swift.  There are a number of apps on the store that do this already.  This option is only for Apple devices.\\n\\nAn alternative to MLX is Llama.cpp which uses a different model format.  They have an example of integrating Llama.cpp on iOS here: https://github.com/ggml-org/llama.cpp/tree/master/examples/llama.swiftui.  This option is cross platform.\\n\\nYet another option is MLC and they have an iOS SDK here: https://llm.mlc.ai/docs/deploy/ios.html.  This option is also cross platform.\\n\\nI know there are at least two React Native bindings for Llama.cpp, and I’m sure there are React Native bindings for MLX now somewhere too if that’s your thing.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1tca7h","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;MLX is the most performant format for running AI on Apple Silicon devices.  On a mobile device you would use Swift MLX: &lt;a href=\\"https://github.com/ml-explore/mlx-swift\\"&gt;https://github.com/ml-explore/mlx-swift&lt;/a&gt;.  There are a number of apps on the store that do this already.  This option is only for Apple devices.&lt;/p&gt;\\n\\n&lt;p&gt;An alternative to MLX is Llama.cpp which uses a different model format.  They have an example of integrating Llama.cpp on iOS here: &lt;a href=\\"https://github.com/ggml-org/llama.cpp/tree/master/examples/llama.swiftui\\"&gt;https://github.com/ggml-org/llama.cpp/tree/master/examples/llama.swiftui&lt;/a&gt;.  This option is cross platform.&lt;/p&gt;\\n\\n&lt;p&gt;Yet another option is MLC and they have an iOS SDK here: &lt;a href=\\"https://llm.mlc.ai/docs/deploy/ios.html\\"&gt;https://llm.mlc.ai/docs/deploy/ios.html&lt;/a&gt;.  This option is also cross platform.&lt;/p&gt;\\n\\n&lt;p&gt;I know there are at least two React Native bindings for Llama.cpp, and I’m sure there are React Native bindings for MLX now somewhere too if that’s your thing.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltj8pg","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltj8pg/gemma_3n_is_not_performing_well_with_macos_m2/n1tca7h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751899203,"author_flair_text":null,"treatment_tags":[],"created_utc":1751899203,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n1smt1j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Strikingaks","can_mod_post":false,"created_utc":1751890442,"send_replies":true,"parent_id":"t1_n1r3c5u","score":1,"author_fullname":"t2_bo0sjlui","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks but i am confused, how is this going to perform on a mobile device. How do we run this in a mobile device.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1smt1j","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks but i am confused, how is this going to perform on a mobile device. How do we run this in a mobile device.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltj8pg","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltj8pg/gemma_3n_is_not_performing_well_with_macos_m2/n1smt1j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751890442,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1r3c5u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Accomplished_Ad9530","can_mod_post":false,"created_utc":1751860943,"send_replies":true,"parent_id":"t3_1ltj8pg","score":3,"author_fullname":"t2_88fma001","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"`mlx-vlm` is what you’re looking for","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1r3c5u","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;code&gt;mlx-vlm&lt;/code&gt; is what you’re looking for&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltj8pg/gemma_3n_is_not_performing_well_with_macos_m2/n1r3c5u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751860943,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltj8pg","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1szjp3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Slowhill369","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1sojt5","score":2,"author_fullname":"t2_96zelxcg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"How weird. So maybe it truly is for mobile but I have to wonder what specific inference type that involves. ","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1szjp3","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How weird. So maybe it truly is for mobile but I have to wonder what specific inference type that involves. &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltj8pg","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltj8pg/gemma_3n_is_not_performing_well_with_macos_m2/n1szjp3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751895180,"author_flair_text":null,"treatment_tags":[],"created_utc":1751895180,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1sojt5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AyraWinla","can_mod_post":false,"created_utc":1751891147,"send_replies":true,"parent_id":"t1_n1r3fb6","score":1,"author_fullname":"t2_phwgb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The odd thing is, using their Google AI Edge application on my Android phone, 3N is surprisingly fast. But if I use 3N anywhere else, it\'s a *lot* slower than regular Gemma 3.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1sojt5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The odd thing is, using their Google AI Edge application on my Android phone, 3N is surprisingly fast. But if I use 3N anywhere else, it&amp;#39;s a &lt;em&gt;lot&lt;/em&gt; slower than regular Gemma 3.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltj8pg","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltj8pg/gemma_3n_is_not_performing_well_with_macos_m2/n1sojt5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751891147,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1r3fb6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Slowhill369","can_mod_post":false,"created_utc":1751860984,"send_replies":true,"parent_id":"t3_1ltj8pg","score":1,"author_fullname":"t2_96zelxcg","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Same. I’m quite disappointed. Gemma3 runs way better. Idk how 3n was “made for small devices”","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1r3fb6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Same. I’m quite disappointed. Gemma3 runs way better. Idk how 3n was “made for small devices”&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltj8pg/gemma_3n_is_not_performing_well_with_macos_m2/n1r3fb6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751860984,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltj8pg","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]'),n=()=>e.jsx(l,{data:t});export{n as default};
