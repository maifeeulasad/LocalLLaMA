import{j as e}from"./index-BlGsFJYy.js";import{R as t}from"./RedditPostRenderer-B6uvq_Zl.js";import"./index-DDvVtNwD.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Here is something of a hiccup I find myself running into a lot. I type up a prompt, often very elaborate of course, and RIGHT AFTER sending the prompt I realize that I have one more parting thought that could change everything.\\n\\nIt occurs to me that an LLM just flows all previously generated tokens through as it generates the next tokens. The way that thinking models are able to hack around the inherent inaccuracies at counting or arithmetic (for example) in purely one-shot fashion is (near as i can tell) just having them trained deeply on making a good call on how much to keep going back over the response and re-working it until it's confident it can move forward. Which is to say, that if you ask a modern thinking LLM to do math, it's going to work on it in drafts over and over and eventually decide on its own that it's satisfied before emitting the answer, and it's a LOT more likely to be correct.  \\n\\nThat gives me the idea that we should be able to slap in like a \\"BREAKING NEWS: User has offered up this ADDITIONAL THOUGHT that you should consider: &lt;additional prompt&gt;\\" and the thinking process should definitely be able to integrate the added information. In fact based on how I see it work on problems I expect it to ramble on for \\n\\nI doubt a modern LLM even needs much training on this stuff to respond usefully to it. So it seems like a pure frontend engineering question. The timing of the new input is pretty critical since if it doesnt come in fast enough (e.g. before end of thinking) then we kinda don't want to send it in. I also think it could even be possible to feed in the keystrokes in realtime to the LLM while it is inferencing. Why not? ","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Does LLM architecture allow for injecting some more input tokens in the middle of token generation?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m4hfy0","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.75,"author_flair_background_color":null,"subreddit_type":"public","ups":10,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_iifi6ul2l","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":10,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1752991468,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752990962,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Here is something of a hiccup I find myself running into a lot. I type up a prompt, often very elaborate of course, and RIGHT AFTER sending the prompt I realize that I have one more parting thought that could change everything.&lt;/p&gt;\\n\\n&lt;p&gt;It occurs to me that an LLM just flows all previously generated tokens through as it generates the next tokens. The way that thinking models are able to hack around the inherent inaccuracies at counting or arithmetic (for example) in purely one-shot fashion is (near as i can tell) just having them trained deeply on making a good call on how much to keep going back over the response and re-working it until it&amp;#39;s confident it can move forward. Which is to say, that if you ask a modern thinking LLM to do math, it&amp;#39;s going to work on it in drafts over and over and eventually decide on its own that it&amp;#39;s satisfied before emitting the answer, and it&amp;#39;s a LOT more likely to be correct.  &lt;/p&gt;\\n\\n&lt;p&gt;That gives me the idea that we should be able to slap in like a &amp;quot;BREAKING NEWS: User has offered up this ADDITIONAL THOUGHT that you should consider: &amp;lt;additional prompt&amp;gt;&amp;quot; and the thinking process should definitely be able to integrate the added information. In fact based on how I see it work on problems I expect it to ramble on for &lt;/p&gt;\\n\\n&lt;p&gt;I doubt a modern LLM even needs much training on this stuff to respond usefully to it. So it seems like a pure frontend engineering question. The timing of the new input is pretty critical since if it doesnt come in fast enough (e.g. before end of thinking) then we kinda don&amp;#39;t want to send it in. I also think it could even be possible to feed in the keystrokes in realtime to the LLM while it is inferencing. Why not? &lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1m4hfy0","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"michaelsoft__binbows","discussion_type":null,"num_comments":16,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/","subreddit_subscribers":501752,"created_utc":1752990962,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n44qxja","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FunnyAsparagus1253","can_mod_post":false,"send_replies":true,"parent_id":"t1_n44hz8i","score":5,"author_fullname":"t2_i6c8tay3w","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It’s a good idea. There are a bunch of improvements that can be made on the input side, imo. Like why does it wait until you’ve typed your whole thing and hit ‘send’ before it starts processing? Just because it’s simpler. There’s no technical reason it can’t do the prompt processing in real time and be already ready to generate when you hit ‘send’.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n44qxja","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It’s a good idea. There are a bunch of improvements that can be made on the input side, imo. Like why does it wait until you’ve typed your whole thing and hit ‘send’ before it starts processing? Just because it’s simpler. There’s no technical reason it can’t do the prompt processing in real time and be already ready to generate when you hit ‘send’.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4hfy0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/n44qxja/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752997760,"author_flair_text":null,"treatment_tags":[],"created_utc":1752997760,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n44pcwo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Capable-Ad-7494","can_mod_post":false,"send_replies":true,"parent_id":"t1_n44hz8i","score":4,"author_fullname":"t2_9so78ol2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Why not just use cache tokens? way cheaper for multi turn conversations, less ‘after the fact’ cost punishment as well in this instance also.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n44pcwo","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why not just use cache tokens? way cheaper for multi turn conversations, less ‘after the fact’ cost punishment as well in this instance also.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4hfy0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/n44pcwo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752996864,"author_flair_text":null,"treatment_tags":[],"created_utc":1752996864,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n44vmrk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"berni8k","can_mod_post":false,"send_replies":true,"parent_id":"t1_n44hz8i","score":1,"author_fullname":"t2_hfyjp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Use a local LLM runtime that has an input token cache. You can re-feed it a input context that is just slightly modified and it will reuse most of the cached input tokens, making it start generating near instantly even if the input is 50k tokens long.\\n\\nLM Studio has a cache by default. The built in UI also has options to stop generation at any time, slightly edit the context (be it your input or the LLMs output) and then resume generation. I find that useful if the generation starts off good but then goes wrong, i can just re gerate the last 1/4 of a response by deleting that part and resuming generation","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n44vmrk","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Use a local LLM runtime that has an input token cache. You can re-feed it a input context that is just slightly modified and it will reuse most of the cached input tokens, making it start generating near instantly even if the input is 50k tokens long.&lt;/p&gt;\\n\\n&lt;p&gt;LM Studio has a cache by default. The built in UI also has options to stop generation at any time, slightly edit the context (be it your input or the LLMs output) and then resume generation. I find that useful if the generation starts off good but then goes wrong, i can just re gerate the last 1/4 of a response by deleting that part and resuming generation&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4hfy0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/n44vmrk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753000465,"author_flair_text":null,"treatment_tags":[],"created_utc":1753000465,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n44hz8i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"michaelsoft__binbows","can_mod_post":false,"created_utc":1752992785,"send_replies":true,"parent_id":"t1_n44g2ad","score":-1,"author_fullname":"t2_iifi6ul2l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah i do do that often if i decide that the new information is of critical importance. But the cost of input tokens (tons of code context etc) is already burned and i will need to restart the next prompt by re-sending the whole context again so this burns the whole input token cost which is usually the larger cost.\\n\\nI should have emphasized it in my OP but the \\"innovation\\" here would be that injecting the new straggler prompt in a JIT fashion would allow the original input tokens not to be \\"wasted\\" in order to insert the new information. that's the have your cake and eat it too aspect of this.\\n\\nThen again though maybe in some/many cases token/prompt caching could make this kinda work seamlessly.","edited":1752993045,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44hz8i","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah i do do that often if i decide that the new information is of critical importance. But the cost of input tokens (tons of code context etc) is already burned and i will need to restart the next prompt by re-sending the whole context again so this burns the whole input token cost which is usually the larger cost.&lt;/p&gt;\\n\\n&lt;p&gt;I should have emphasized it in my OP but the &amp;quot;innovation&amp;quot; here would be that injecting the new straggler prompt in a JIT fashion would allow the original input tokens not to be &amp;quot;wasted&amp;quot; in order to insert the new information. that&amp;#39;s the have your cake and eat it too aspect of this.&lt;/p&gt;\\n\\n&lt;p&gt;Then again though maybe in some/many cases token/prompt caching could make this kinda work seamlessly.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4hfy0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/n44hz8i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752992785,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n44g2ad","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"cybran3","can_mod_post":false,"created_utc":1752991755,"send_replies":true,"parent_id":"t3_1m4hfy0","score":9,"author_fullname":"t2_41gmkw5z","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Just interrupt the generation when you want to insert new tokens?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44g2ad","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just interrupt the generation when you want to insert new tokens?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/n44g2ad/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752991755,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4hfy0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n44iykl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"bjodah","can_mod_post":false,"send_replies":true,"parent_id":"t1_n44hqk4","score":4,"author_fullname":"t2_atvy2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"you just cancel the previous request, if whatever frontend you are using does not allow for that: switch.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n44iykl","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;you just cancel the previous request, if whatever frontend you are using does not allow for that: switch.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4hfy0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/n44iykl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752993322,"author_flair_text":null,"treatment_tags":[],"created_utc":1752993322,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n44iv07","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"kneeanderthul","can_mod_post":false,"send_replies":true,"parent_id":"t1_n44hqk4","score":4,"author_fullname":"t2_nz7b4uav","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"My intent wasn’t to dismiss your idea — just to offer a practical workaround based on how I understand current models work.\\nAs far as I know, LLMs must complete their processing before you can introduce new tokens. Pausing and injecting mid-inference isn’t currently how the architecture works — even the idea of a 'pause' is really just canceling and re-prompting.\\n\\nThat said, if you do find a way to inject in real time, you’d be breaking new ground. It would fundamentally change how we think about dynamic interaction with LLMs. I genuinely hope you push it forward — would be amazing to see","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n44iv07","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;My intent wasn’t to dismiss your idea — just to offer a practical workaround based on how I understand current models work.\\nAs far as I know, LLMs must complete their processing before you can introduce new tokens. Pausing and injecting mid-inference isn’t currently how the architecture works — even the idea of a &amp;#39;pause&amp;#39; is really just canceling and re-prompting.&lt;/p&gt;\\n\\n&lt;p&gt;That said, if you do find a way to inject in real time, you’d be breaking new ground. It would fundamentally change how we think about dynamic interaction with LLMs. I genuinely hope you push it forward — would be amazing to see&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4hfy0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/n44iv07/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752993267,"author_flair_text":null,"treatment_tags":[],"created_utc":1752993267,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n44hqk4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":false,"author":"michaelsoft__binbows","can_mod_post":false,"created_utc":1752992654,"send_replies":true,"parent_id":"t1_n44evwh","score":-6,"author_fullname":"t2_iifi6ul2l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"no... you're not getting it. i send the prompt. It's going to crunch for a total of 60 seconds, 45 of which is done in thinking mode. My thought comes in at t=2s and i am able to type it by t=7s. There is still time.\\n\\nYou are suggesting I wait the full 60 seconds and issue a new prompt waiting another (expected to be) 60s. Which amounts to two full prompts and responses worth of consumed tokens. \\n\\nI'm talking about something pretty low level and (if the stars align on timing) making more efficient use of time and resources. You're just dismissing the idea by wilfully not considering what i'm trying to describe.","edited":1752992957,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44hqk4","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;no... you&amp;#39;re not getting it. i send the prompt. It&amp;#39;s going to crunch for a total of 60 seconds, 45 of which is done in thinking mode. My thought comes in at t=2s and i am able to type it by t=7s. There is still time.&lt;/p&gt;\\n\\n&lt;p&gt;You are suggesting I wait the full 60 seconds and issue a new prompt waiting another (expected to be) 60s. Which amounts to two full prompts and responses worth of consumed tokens. &lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m talking about something pretty low level and (if the stars align on timing) making more efficient use of time and resources. You&amp;#39;re just dismissing the idea by wilfully not considering what i&amp;#39;m trying to describe.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4hfy0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/n44hqk4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752992654,"author_flair_text":null,"treatment_tags":[],"collapsed":true,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-6}}],"before":null}},"user_reports":[],"saved":false,"id":"n44evwh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"kneeanderthul","can_mod_post":false,"created_utc":1752991132,"send_replies":true,"parent_id":"t3_1m4hfy0","score":11,"author_fullname":"t2_nz7b4uav","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"A simple \\n\\n“Reconsider previous prompt with this new info:” \\n\\nAnd you’re done.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44evwh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;A simple &lt;/p&gt;\\n\\n&lt;p&gt;“Reconsider previous prompt with this new info:” &lt;/p&gt;\\n\\n&lt;p&gt;And you’re done.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/n44evwh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752991132,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4hfy0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n44ez6y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Equivalent_Cut_5845","can_mod_post":false,"created_utc":1752991180,"send_replies":true,"parent_id":"t3_1m4hfy0","score":3,"author_fullname":"t2_1oy2v7xti6","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Limit max token generation to 1 then add your stuff whenever you want to.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44ez6y","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Limit max token generation to 1 then add your stuff whenever you want to.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/n44ez6y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752991180,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4hfy0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n44n0mw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AFruitShopOwner","can_mod_post":false,"created_utc":1752995540,"send_replies":true,"parent_id":"t3_1m4hfy0","score":0,"author_fullname":"t2_h15f0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Doesn't claude code let you do this already?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44n0mw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Doesn&amp;#39;t claude code let you do this already?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/n44n0mw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752995540,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4hfy0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n455uc2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"blepcoin","can_mod_post":false,"created_utc":1753006508,"send_replies":true,"parent_id":"t3_1m4hfy0","score":2,"author_fullname":"t2_dg5ivdb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Take it one step further and remove the send part completely. As you start typing the LLM starts responding (including predicting your question perhaps) immediately. Completing the question and/or modifying it is incorporated into the current llm thoughts rather than resetting every keystroke. You could then tweak and fix as you watch the llm thought process go awry due to that typo or gotcha you should’ve included.\\n\\nInteresting academic challenge to make the training for this work.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n455uc2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Take it one step further and remove the send part completely. As you start typing the LLM starts responding (including predicting your question perhaps) immediately. Completing the question and/or modifying it is incorporated into the current llm thoughts rather than resetting every keystroke. You could then tweak and fix as you watch the llm thought process go awry due to that typo or gotcha you should’ve included.&lt;/p&gt;\\n\\n&lt;p&gt;Interesting academic challenge to make the training for this work.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/n455uc2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753006508,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4hfy0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n44l81p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"derdigga","can_mod_post":false,"created_utc":1752994552,"send_replies":true,"parent_id":"t3_1m4hfy0","score":1,"author_fullname":"t2_i8i51","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"There is a mcp for that, check on reviewgate","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44l81p","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There is a mcp for that, check on reviewgate&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/n44l81p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752994552,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4hfy0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n44fsqe","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"michaelsoft__binbows","can_mod_post":false,"created_utc":1752991615,"send_replies":true,"parent_id":"t3_1m4hfy0","score":0,"author_fullname":"t2_iifi6ul2l","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"i think this is pretty interesting to think about, there is a parallel here with the nuances of carrying on a spoken conversation. Emphasis on nuance. How to make a judgement call based off of a given burst of sound waves to make the call on whether we should stop talking and listen or carry on. It's wildly difficult.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44fsqe","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;i think this is pretty interesting to think about, there is a parallel here with the nuances of carrying on a spoken conversation. Emphasis on nuance. How to make a judgement call based off of a given burst of sound waves to make the call on whether we should stop talking and listen or carry on. It&amp;#39;s wildly difficult.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/n44fsqe/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752991615,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4hfy0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n44rb84","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"triynizzles1","can_mod_post":false,"created_utc":1752997975,"send_replies":true,"parent_id":"t3_1m4hfy0","score":0,"author_fullname":"t2_zr0g49ixt","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think the answer you are looking for is: No. llm architecture does not allow for real-time input of new data while processing the current prompt. As many others have said the workaround is to stop the response, type out what you want to add and then send it again to obtain the new response.\\n\\nQuite frequently, I find myself pressing send when I’ve meant to add a new row of text. My thought has always been the response will be better if the initial prompt had all of the information rather than having it split into two prompts or some sort of injected halfway through method.\\n\\nYou also mentioned thinking models, which get very confused when there is not an appropriate number of open and closed &lt;think&gt;, &lt;/think&gt; brackets. Just giving you a heads up if you try the above mentioned work around.\\n\\nIf I send a prompt too early, I just highlight everything I typed, put it back into the text box and continue typing. If I am able to cancel the response, I will. If not I will delete it from the conversation history along with my botched prompt and send the corrected prompt.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44rb84","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think the answer you are looking for is: No. llm architecture does not allow for real-time input of new data while processing the current prompt. As many others have said the workaround is to stop the response, type out what you want to add and then send it again to obtain the new response.&lt;/p&gt;\\n\\n&lt;p&gt;Quite frequently, I find myself pressing send when I’ve meant to add a new row of text. My thought has always been the response will be better if the initial prompt had all of the information rather than having it split into two prompts or some sort of injected halfway through method.&lt;/p&gt;\\n\\n&lt;p&gt;You also mentioned thinking models, which get very confused when there is not an appropriate number of open and closed &amp;lt;think&amp;gt;, &amp;lt;/think&amp;gt; brackets. Just giving you a heads up if you try the above mentioned work around.&lt;/p&gt;\\n\\n&lt;p&gt;If I send a prompt too early, I just highlight everything I typed, put it back into the text box and continue typing. If I am able to cancel the response, I will. If not I will delete it from the conversation history along with my botched prompt and send the corrected prompt.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/n44rb84/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752997975,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4hfy0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}}]`),r=()=>e.jsx(t,{data:l});export{r as default};
