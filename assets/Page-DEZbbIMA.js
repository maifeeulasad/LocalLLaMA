import{j as e}from"./index-F0NXdzZX.js";import{R as l}from"./RedditPostRenderer-CoEZB1dt.js";import"./index-DrtravXm.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Im trying to sus out if getting a mid tier cpu and a 5050, or 4060 on a laptop with sodimm memory would be more advantageous than getting a ryzen 9 hx370 with lpddr5x 7500mhz.  Would having 8gb vram from the gpu actually yield noticeable results over the igpu of the hx370 being able to leverage the ram?\\n\\nBoth options would have 64gb of system ram, and I'd want to have the option to run 4bit 70b models.  Im aware the 8gbvram can't do this by itself, so im unclear if it really aids at all vs having much faster system ram.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Will an 8gbvram laptop gpu add any value?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lxqk44","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.6,"author_flair_background_color":null,"subreddit_type":"public","ups":1,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_ovn8y","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":1,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752291211,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Im trying to sus out if getting a mid tier cpu and a 5050, or 4060 on a laptop with sodimm memory would be more advantageous than getting a ryzen 9 hx370 with lpddr5x 7500mhz.  Would having 8gb vram from the gpu actually yield noticeable results over the igpu of the hx370 being able to leverage the ram?&lt;/p&gt;\\n\\n&lt;p&gt;Both options would have 64gb of system ram, and I&amp;#39;d want to have the option to run 4bit 70b models.  Im aware the 8gbvram can&amp;#39;t do this by itself, so im unclear if it really aids at all vs having much faster system ram.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lxqk44","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"plzdonforgetthisname","discussion_type":null,"num_comments":4,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lxqk44/will_an_8gbvram_laptop_gpu_add_any_value/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lxqk44/will_an_8gbvram_laptop_gpu_add_any_value/","subreddit_subscribers":498114,"created_utc":1752291211,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2o8w6a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RhubarbSimilar1683","can_mod_post":false,"created_utc":1752292794,"send_replies":true,"parent_id":"t3_1lxqk44","score":3,"author_fullname":"t2_1k4sjdwzk2","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"No, hybrid inference in llama_cpp is not very fast yet. And won't be for a while. If you ran the model with cuda unified memory enabled in llama_cpp you could benefit but the improvement is negligible. I don't see any other way to use VRAM together with normal system RAM, technically Arch Linux lets you use VRAM as system RAM but it's very hacky so it's not advisable","edited":1752293546,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2o8w6a","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No, hybrid inference in llama_cpp is not very fast yet. And won&amp;#39;t be for a while. If you ran the model with cuda unified memory enabled in llama_cpp you could benefit but the improvement is negligible. I don&amp;#39;t see any other way to use VRAM together with normal system RAM, technically Arch Linux lets you use VRAM as system RAM but it&amp;#39;s very hacky so it&amp;#39;s not advisable&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxqk44/will_an_8gbvram_laptop_gpu_add_any_value/n2o8w6a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752292794,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxqk44","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2p899c","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1752311767,"send_replies":true,"parent_id":"t3_1lxqk44","score":1,"author_fullname":"t2_uz37qfx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Adding GPU massively improves prompt processing speed, even you delegate all actual inference on CPU.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2p899c","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Adding GPU massively improves prompt processing speed, even you delegate all actual inference on CPU.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxqk44/will_an_8gbvram_laptop_gpu_add_any_value/n2p899c/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752311767,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxqk44","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2pblue","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"tat_tvam_asshole","can_mod_post":false,"created_utc":1752313823,"send_replies":true,"parent_id":"t3_1lxqk44","score":2,"author_fullname":"t2_jxuvgdyj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I have a 4070 laptop with 8gb vram, 128gb ram and it runs q4 70B models fine enough. Anything lower like ~30B or lower run great","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2pblue","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have a 4070 laptop with 8gb vram, 128gb ram and it runs q4 70B models fine enough. Anything lower like ~30B or lower run great&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxqk44/will_an_8gbvram_laptop_gpu_add_any_value/n2pblue/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752313823,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxqk44","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2pkfso","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Marksta","can_mod_post":false,"created_utc":1752318734,"send_replies":true,"parent_id":"t3_1lxqk44","score":1,"author_fullname":"t2_559a1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It'll aid a lot in PP, but if it's like a 40gig model, have to do the math and see that's going to be nearly all on your CPU. And just splitting a model any percent to CPU and system memory is a huge performance killer if your system isn't a 8+ memory channels server setup. Expect like, 0.1 t/s on 70b models on a laptop.\\n\\nWhatever the biggest baddest unified CPU+GPU they sell for laptops is going to be the best laptop route, but I think 70B will still be torture assuming you meant a dense model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2pkfso","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;ll aid a lot in PP, but if it&amp;#39;s like a 40gig model, have to do the math and see that&amp;#39;s going to be nearly all on your CPU. And just splitting a model any percent to CPU and system memory is a huge performance killer if your system isn&amp;#39;t a 8+ memory channels server setup. Expect like, 0.1 t/s on 70b models on a laptop.&lt;/p&gt;\\n\\n&lt;p&gt;Whatever the biggest baddest unified CPU+GPU they sell for laptops is going to be the best laptop route, but I think 70B will still be torture assuming you meant a dense model.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxqk44/will_an_8gbvram_laptop_gpu_add_any_value/n2pkfso/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752318734,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxqk44","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(l,{data:a});export{n as default};
