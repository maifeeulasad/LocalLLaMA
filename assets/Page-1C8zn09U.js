import{j as e}from"./index-M4edQi1P.js";import{R as l}from"./RedditPostRenderer-CESBGIIy.js";import"./index-DFpL1mt4.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"So as the title said, i got new drivers, but getting CUDA Fatal error when loading. Tried pip uninstall torch, torchaudio, and torch vision with an fresh install again. \\n\\nTried \\n\\n    pip install --pre --upgrade --no-cache-dir torch --extra-index-url https://download.pytorch.org/whl/nightly/cu128\\n    \\n    Not sure what needs to be uninstalled and reinstalled. Im not interested in a full wipe of c:\\\\ ","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Upgraded from 3090 to 5090. Oobabooga complaints.","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lnqaea","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.36,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_ce52x","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751232910,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;So as the title said, i got new drivers, but getting CUDA Fatal error when loading. Tried pip uninstall torch, torchaudio, and torch vision with an fresh install again. &lt;/p&gt;\\n\\n&lt;p&gt;Tried &lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;pip install --pre --upgrade --no-cache-dir torch --extra-index-url https://download.pytorch.org/whl/nightly/cu128\\n\\nNot sure what needs to be uninstalled and reinstalled. Im not interested in a full wipe of c:\\\\ \\n&lt;/code&gt;&lt;/pre&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lnqaea","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"jebeller","discussion_type":null,"num_comments":10,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lnqaea/upgraded_from_3090_to_5090_oobabooga_complaints/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lnqaea/upgraded_from_3090_to_5090_oobabooga_complaints/","subreddit_subscribers":492929,"created_utc":1751232910,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hvrer","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"celsowm","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0huzuz","score":1,"author_fullname":"t2_dyvrh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Or just use sglang:blackwell docker image","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0hvrer","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Or just use sglang:blackwell docker image&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnqaea","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnqaea/upgraded_from_3090_to_5090_oobabooga_complaints/n0hvrer/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751242839,"author_flair_text":null,"treatment_tags":[],"created_utc":1751242839,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0huzuz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jebeller","can_mod_post":false,"created_utc":1751242550,"send_replies":true,"parent_id":"t1_n0h52qy","score":0,"author_fullname":"t2_ce52x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thats way over my head and knowledge.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0huzuz","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thats way over my head and knowledge.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnqaea","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnqaea/upgraded_from_3090_to_5090_oobabooga_complaints/n0huzuz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751242550,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n0h52qy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"celsowm","can_mod_post":false,"created_utc":1751233376,"send_replies":true,"parent_id":"t3_1lnqaea","score":2,"author_fullname":"t2_dyvrh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I gave up and compiled from scratch","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0h52qy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I gave up and compiled from scratch&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnqaea/upgraded_from_3090_to_5090_oobabooga_complaints/n0h52qy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751233376,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnqaea","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0j366b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jacek2023","can_mod_post":false,"created_utc":1751260854,"send_replies":true,"parent_id":"t3_1lnqaea","score":2,"author_fullname":"t2_vqgbql9w","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"when you change from 30x0 to 50x0 you must recompile llama.cpp because it uses different CUDA, I had this problem after moving 3090 to my second computer and installing 5070 on the desktop","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0j366b","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;when you change from 30x0 to 50x0 you must recompile llama.cpp because it uses different CUDA, I had this problem after moving 3090 to my second computer and installing 5070 on the desktop&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnqaea/upgraded_from_3090_to_5090_oobabooga_complaints/n0j366b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751260854,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lnqaea","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hvs2l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"noage","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0hux8b","score":1,"author_fullname":"t2_5ao30","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"If you want to use your 5090, install the vulcan version of oobabooga if you dont know how to update it manually.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0hvs2l","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you want to use your 5090, install the vulcan version of oobabooga if you dont know how to update it manually.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"link_id":"t3_1lnqaea","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnqaea/upgraded_from_3090_to_5090_oobabooga_complaints/n0hvs2l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751242846,"author_flair_text":null,"treatment_tags":[],"created_utc":1751242846,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hux8b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jebeller","can_mod_post":false,"created_utc":1751242523,"send_replies":true,"parent_id":"t1_n0h56v6","score":-2,"author_fullname":"t2_ce52x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That all text was kinda woosh.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hux8b","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That all text was kinda woosh.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnqaea","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnqaea/upgraded_from_3090_to_5090_oobabooga_complaints/n0hux8b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751242523,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0h56v6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"noage","can_mod_post":false,"created_utc":1751233413,"send_replies":true,"parent_id":"t3_1lnqaea","score":1,"author_fullname":"t2_5ao30","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I haven't been using oobabooga since I did the same. The defaults are all for older versions of torch that do not support the 5090 and they have not made any updates in months to make it available in their quick installer via CUDA (Blackwell needs 12.8 and the highest version in their one click installer is 12.4. The Vulcan version should work (i've used a Vulcan build for llama.cpp server) but it wont have all the specific CUDA benefits.  If you want to build the project yourself with the right torch and cuda versions you probably could.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0h56v6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I haven&amp;#39;t been using oobabooga since I did the same. The defaults are all for older versions of torch that do not support the 5090 and they have not made any updates in months to make it available in their quick installer via CUDA (Blackwell needs 12.8 and the highest version in their one click installer is 12.4. The Vulcan version should work (i&amp;#39;ve used a Vulcan build for llama.cpp server) but it wont have all the specific CUDA benefits.  If you want to build the project yourself with the right torch and cuda versions you probably could.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnqaea/upgraded_from_3090_to_5090_oobabooga_complaints/n0h56v6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751233413,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnqaea","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0iedx3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0hupp6","score":1,"author_fullname":"t2_h5utwre7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"does it have a venv folder inside? or does it use conda? it should say what it's doing when you run it. \\n\\nyou never really said which backend you're using. if it's llama.cpp, the python bindings of that may be the problem instead of torch but the process is the same. \\n\\nChange to environment, venv might have venv\\\\Scripts\\\\activate.bat, conda maybe \\"conda activate whatever\\". Then you download/install the wheels. Just beware of any \\"update\\" or one step bat that might undo your work. The normal version of ooba had that.\\n\\nI ignore everyone's scripts and set stuff up myself for reasons like this. Wouldn't surprise me if there is a step by step instruction somewhere in github issues that beats me giving you generalities.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0iedx3","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;does it have a venv folder inside? or does it use conda? it should say what it&amp;#39;s doing when you run it. &lt;/p&gt;\\n\\n&lt;p&gt;you never really said which backend you&amp;#39;re using. if it&amp;#39;s llama.cpp, the python bindings of that may be the problem instead of torch but the process is the same. &lt;/p&gt;\\n\\n&lt;p&gt;Change to environment, venv might have venv\\\\Scripts\\\\activate.bat, conda maybe &amp;quot;conda activate whatever&amp;quot;. Then you download/install the wheels. Just beware of any &amp;quot;update&amp;quot; or one step bat that might undo your work. The normal version of ooba had that.&lt;/p&gt;\\n\\n&lt;p&gt;I ignore everyone&amp;#39;s scripts and set stuff up myself for reasons like this. Wouldn&amp;#39;t surprise me if there is a step by step instruction somewhere in github issues that beats me giving you generalities.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnqaea","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnqaea/upgraded_from_3090_to_5090_oobabooga_complaints/n0iedx3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751249875,"author_flair_text":null,"treatment_tags":[],"created_utc":1751249875,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hupp6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jebeller","can_mod_post":false,"created_utc":1751242445,"send_replies":true,"parent_id":"t1_n0hkdyl","score":1,"author_fullname":"t2_ce52x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"So i have to install it to the local portable directory. Ok got it... \\n\\nHow? :)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hupp6","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So i have to install it to the local portable directory. Ok got it... &lt;/p&gt;\\n\\n&lt;p&gt;How? :)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnqaea","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnqaea/upgraded_from_3090_to_5090_oobabooga_complaints/n0hupp6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751242445,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hkdyl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1751238693,"send_replies":true,"parent_id":"t3_1lnqaea","score":1,"author_fullname":"t2_h5utwre7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You'll manually have to install correct dependencies. On windows make sure you are actually entering the venv/conda that it creates. Otherwise you install torch on your regular environment.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hkdyl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You&amp;#39;ll manually have to install correct dependencies. On windows make sure you are actually entering the venv/conda that it creates. Otherwise you install torch on your regular environment.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnqaea/upgraded_from_3090_to_5090_oobabooga_complaints/n0hkdyl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751238693,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnqaea","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(l,{data:a});export{n as default};
