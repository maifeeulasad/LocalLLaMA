import{j as e}from"./index-Cd3v0jxz.js";import{R as l}from"./RedditPostRenderer-cI96YLhy.js";import"./index-DGBoKyQm.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hey,\\n\\nI'm interested in running different model like qwen3 coder but those are very large and can't run on a laptop. What are the popular options ? Is it doable to take an aws instance with GPU to run it ? Or maybe it's too expensive or not doable at all","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"How to run large model ?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m80dz3","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.5,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_4i6ba67v","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753351532,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey,&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m interested in running different model like qwen3 coder but those are very large and can&amp;#39;t run on a laptop. What are the popular options ? Is it doable to take an aws instance with GPU to run it ? Or maybe it&amp;#39;s too expensive or not doable at all&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m80dz3","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"NoahZhyte","discussion_type":null,"num_comments":9,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m80dz3/how_to_run_large_model/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m80dz3/how_to_run_large_model/","subreddit_subscribers":504023,"created_utc":1753351532,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4w3u1l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Plastic-Letterhead44","can_mod_post":false,"created_utc":1753361494,"send_replies":true,"parent_id":"t3_1m80dz3","score":2,"author_fullname":"t2_rzop9e71d","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Open router is my go to because it's easy to setup and has a great selection of models when you can't run it locally/want to test.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4w3u1l","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Open router is my go to because it&amp;#39;s easy to setup and has a great selection of models when you can&amp;#39;t run it locally/want to test.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m80dz3/how_to_run_large_model/n4w3u1l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753361494,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m80dz3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4wnpbc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mtmttuan","can_mod_post":false,"created_utc":1753367627,"send_replies":true,"parent_id":"t3_1m80dz3","score":2,"author_fullname":"t2_6mjqz0at","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt; take an aws instance with GPU to run it\\n\\nUnless you're an enterprise, which you aren't as you're asking to run models on a laptop, you'll go bankrupt. AWS is stupidly expensive.\\n\\nAnother solution is runpod or vast.ai. They have GPU for rent for much cheaper than AWS. Or you can just go with LLM via API. Probably cheaper unless you use several millions tokens everyday.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4wnpbc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;take an aws instance with GPU to run it&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Unless you&amp;#39;re an enterprise, which you aren&amp;#39;t as you&amp;#39;re asking to run models on a laptop, you&amp;#39;ll go bankrupt. AWS is stupidly expensive.&lt;/p&gt;\\n\\n&lt;p&gt;Another solution is runpod or vast.ai. They have GPU for rent for much cheaper than AWS. Or you can just go with LLM via API. Probably cheaper unless you use several millions tokens everyday.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m80dz3/how_to_run_large_model/n4wnpbc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753367627,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m80dz3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4wazhm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Toooooool","can_mod_post":false,"created_utc":1753363807,"send_replies":true,"parent_id":"t3_1m80dz3","score":0,"author_fullname":"t2_8llornh4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"running FP16 405B is too much for most people, so what most do is opt for a smaller fork i.e. 70B or 36B etc, and even then FP16 is too big for a single consumer GPU so that's where quantized versions come into play.\\n\\nConsider it the difference between talking with a teacher and their student, in the majority of cases they'll have the same answers, all be it one is more summarized than the other.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4wazhm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;running FP16 405B is too much for most people, so what most do is opt for a smaller fork i.e. 70B or 36B etc, and even then FP16 is too big for a single consumer GPU so that&amp;#39;s where quantized versions come into play.&lt;/p&gt;\\n\\n&lt;p&gt;Consider it the difference between talking with a teacher and their student, in the majority of cases they&amp;#39;ll have the same answers, all be it one is more summarized than the other.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m80dz3/how_to_run_large_model/n4wazhm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753363807,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m80dz3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4yzfyx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GPTrack_ai","can_mod_post":false,"send_replies":false,"parent_id":"t1_n4yyn4f","score":-1,"author_fullname":"t2_1tpuoj72sa","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"you are wrong. s.. t... f... u...","edited":false,"author_flair_css_class":null,"name":"t1_n4yzfyx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;you are wrong. s.. t... f... u...&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m80dz3","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m80dz3/how_to_run_large_model/n4yzfyx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753390824,"author_flair_text":null,"collapsed":false,"created_utc":1753390824,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4yyn4f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"NoahZhyte","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4yy2y0","score":2,"author_fullname":"t2_4i6ba67v","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You literally answer to my post. I don't see a single situation where it could me more my business","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4yyn4f","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You literally answer to my post. I don&amp;#39;t see a single situation where it could me more my business&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m80dz3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m80dz3/how_to_run_large_model/n4yyn4f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753390598,"author_flair_text":null,"treatment_tags":[],"created_utc":1753390598,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4yy2y0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GPTrack_ai","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4xwxfm","score":-1,"author_fullname":"t2_1tpuoj72sa","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"can you not comment on things that are none of your business?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4yy2y0","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;can you not comment on things that are none of your business?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m80dz3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m80dz3/how_to_run_large_model/n4yy2y0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753390438,"author_flair_text":null,"treatment_tags":[],"created_utc":1753390438,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4xwxfm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"NoahZhyte","can_mod_post":false,"created_utc":1753379963,"send_replies":true,"parent_id":"t1_n4xp76w","score":2,"author_fullname":"t2_4i6ba67v","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Can you not make your promotion here ?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4xwxfm","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Can you not make your promotion here ?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m80dz3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m80dz3/how_to_run_large_model/n4xwxfm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753379963,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4xp76w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GPTrack_ai","can_mod_post":false,"created_utc":1753377896,"send_replies":true,"parent_id":"t3_1m80dz3","score":-1,"author_fullname":"t2_1tpuoj72sa","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"[GPTrack.ai](http://GPTrack.ai) and [GPTshop.ai](http://GPTshop.ai)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4xp76w","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"http://GPTrack.ai\\"&gt;GPTrack.ai&lt;/a&gt; and &lt;a href=\\"http://GPTshop.ai\\"&gt;GPTshop.ai&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m80dz3/how_to_run_large_model/n4xp76w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753377896,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m80dz3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}}]`),n=()=>e.jsx(l,{data:a});export{n as default};
