import{j as e}from"./index-cvG704yx.js";import{R as t}from"./RedditPostRenderer-CBthLTAH.js";import"./index-D-GavSZU.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I see that LLM is afar superior technology than Vector database and LLM is trained on Natural Language Processing. So is it not always better to send the query to LLM first which can understand the user intent better than anything?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Is LLM first RAG better than traditional RAG?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lwx77q","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.2,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_adnzl8f8x","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752206450,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I see that LLM is afar superior technology than Vector database and LLM is trained on Natural Language Processing. So is it not always better to send the query to LLM first which can understand the user intent better than anything?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lwx77q","is_robot_indexable":true,"num_duplicates":1,"report_reasons":null,"author":"Maleficent_Mess6445","discussion_type":null,"num_comments":19,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lwx77q/is_llm_first_rag_better_than_traditional_rag/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lwx77q/is_llm_first_rag_better_than_traditional_rag/","subreddit_subscribers":497504,"created_utc":1752206450,"num_crossposts":1,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2j9jgf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Maleficent_Mess6445","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2j7i10","score":1,"author_fullname":"t2_adnzl8f8x","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Looks like a good way of doing it.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n2j9jgf","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Looks like a good way of doing it.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lwx77q","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx77q/is_llm_first_rag_better_than_traditional_rag/n2j9jgf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752235309,"author_flair_text":null,"treatment_tags":[],"created_utc":1752235309,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2j7i10","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rorykoehler","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2is1re","score":1,"author_fullname":"t2_ku4i0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You submit loads of records with reranking and confidence scores and let the llm figure out what to use","edited":false,"author_flair_css_class":null,"name":"t1_n2j7i10","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You submit loads of records with reranking and confidence scores and let the llm figure out what to use&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lwx77q","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx77q/is_llm_first_rag_better_than_traditional_rag/n2j7i10/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752234503,"author_flair_text":null,"collapsed":false,"created_utc":1752234503,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2is1re","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Maleficent_Mess6445","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2iiify","score":1,"author_fullname":"t2_adnzl8f8x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Because the vector db would likely misinterpret the query or at least the process needs to be heavily trained to interpret even simple things. Also because it is not wise to reduce the capability of an LLM by just giving it a small subset of data which the developer assumes the vector db has given it correctly.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2is1re","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Because the vector db would likely misinterpret the query or at least the process needs to be heavily trained to interpret even simple things. Also because it is not wise to reduce the capability of an LLM by just giving it a small subset of data which the developer assumes the vector db has given it correctly.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwx77q","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx77q/is_llm_first_rag_better_than_traditional_rag/n2is1re/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752227165,"author_flair_text":null,"treatment_tags":[],"created_utc":1752227165,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2iiify","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rorykoehler","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2i9x1c","score":1,"author_fullname":"t2_ku4i0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Why wouldn’t you supply the context from the db with that?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2iiify","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why wouldn’t you supply the context from the db with that?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwx77q","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx77q/is_llm_first_rag_better_than_traditional_rag/n2iiify/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752221663,"author_flair_text":null,"treatment_tags":[],"created_utc":1752221663,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2i9x1c","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Maleficent_Mess6445","can_mod_post":false,"created_utc":1752216841,"send_replies":true,"parent_id":"t1_n2hqa2b","score":1,"author_fullname":"t2_adnzl8f8x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It just means that user query goes to LLM first instead of vector database","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2i9x1c","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It just means that user query goes to LLM first instead of vector database&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwx77q","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx77q/is_llm_first_rag_better_than_traditional_rag/n2i9x1c/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752216841,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2hqa2b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BobbyL2k","can_mod_post":false,"created_utc":1752207267,"send_replies":true,"parent_id":"t3_1lwx77q","score":3,"author_fullname":"t2_ghoyg","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I assume by “LLM first RAG”, you mean agentic setup where the LLM behaves like an agent and call search tools.\\n\\nThere’s no true “traditional RAG”. The retrieval in RAG could be a super basic keyword search over documents, to LLM powered query expansion into vector embedded documents with comprehensive results merge and reranking.\\n\\nIt entirely depends on how you architect it. I don’t like agentic setups when I want the system to answer only based on documents. So force feeding the documents into the context is better for my use case.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2hqa2b","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I assume by “LLM first RAG”, you mean agentic setup where the LLM behaves like an agent and call search tools.&lt;/p&gt;\\n\\n&lt;p&gt;There’s no true “traditional RAG”. The retrieval in RAG could be a super basic keyword search over documents, to LLM powered query expansion into vector embedded documents with comprehensive results merge and reranking.&lt;/p&gt;\\n\\n&lt;p&gt;It entirely depends on how you architect it. I don’t like agentic setups when I want the system to answer only based on documents. So force feeding the documents into the context is better for my use case.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx77q/is_llm_first_rag_better_than_traditional_rag/n2hqa2b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752207267,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwx77q","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ic8pe","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok_Needleworker_5247","can_mod_post":false,"created_utc":1752218102,"send_replies":true,"parent_id":"t3_1lwx77q","score":3,"author_fullname":"t2_1gmprv51a1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The choice between sending a query to an LLM or a RAG setup first isn't always clear-cut. Google's \\"Data Gemma,\\" as detailed in [this article](https://pub.towardsai.net/demystifying-googles-data-gemma-f07a470c2a39), offers a novel approach to address hallucinations by integrating with a structured knowledge graph. It uses a combination of question expansion and a reliable NL API, providing a structured path to minimize errors. This setup can be particularly effective for complex queries where precision and reliability are crucial. Worth checking out if you're exploring advanced RAG methods.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ic8pe","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The choice between sending a query to an LLM or a RAG setup first isn&amp;#39;t always clear-cut. Google&amp;#39;s &amp;quot;Data Gemma,&amp;quot; as detailed in &lt;a href=\\"https://pub.towardsai.net/demystifying-googles-data-gemma-f07a470c2a39\\"&gt;this article&lt;/a&gt;, offers a novel approach to address hallucinations by integrating with a structured knowledge graph. It uses a combination of question expansion and a reliable NL API, providing a structured path to minimize errors. This setup can be particularly effective for complex queries where precision and reliability are crucial. Worth checking out if you&amp;#39;re exploring advanced RAG methods.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx77q/is_llm_first_rag_better_than_traditional_rag/n2ic8pe/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752218102,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwx77q","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2il3ra","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok-Pipe-5151","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2i9ujs","score":1,"author_fullname":"t2_uxbdufm8b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This won't make any difference unless the LLM is very specifically fine tuned based on user's preference ","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2il3ra","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This won&amp;#39;t make any difference unless the LLM is very specifically fine tuned based on user&amp;#39;s preference &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwx77q","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx77q/is_llm_first_rag_better_than_traditional_rag/n2il3ra/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752223176,"author_flair_text":null,"treatment_tags":[],"created_utc":1752223176,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2i9ujs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Maleficent_Mess6445","can_mod_post":false,"created_utc":1752216805,"send_replies":true,"parent_id":"t1_n2hvrhg","score":-3,"author_fullname":"t2_adnzl8f8x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"No. Where the user query goes to LLM first instead of Vector database","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2i9ujs","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No. Where the user query goes to LLM first instead of Vector database&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwx77q","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx77q/is_llm_first_rag_better_than_traditional_rag/n2i9ujs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752216805,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2hvrhg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok-Pipe-5151","can_mod_post":false,"created_utc":1752209701,"send_replies":true,"parent_id":"t3_1lwx77q","score":2,"author_fullname":"t2_uxbdufm8b","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What is even \\"LLM first\\" RAG? Using a reranker LLM to rerank the results?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2hvrhg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What is even &amp;quot;LLM first&amp;quot; RAG? Using a reranker LLM to rerank the results?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx77q/is_llm_first_rag_better_than_traditional_rag/n2hvrhg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752209701,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwx77q","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2jz1xm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Maleficent_Mess6445","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2jxeft","score":1,"author_fullname":"t2_adnzl8f8x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yes. Put it simply vector databases are not as good as LLM in NLP, that is the end scenario. For the same reason I find that they are practically unusable and this is what I experienced after many attempts. So much so that I don't want to touch them again unless there is a major advancement in this technology.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2jz1xm","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes. Put it simply vector databases are not as good as LLM in NLP, that is the end scenario. For the same reason I find that they are practically unusable and this is what I experienced after many attempts. So much so that I don&amp;#39;t want to touch them again unless there is a major advancement in this technology.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwx77q","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx77q/is_llm_first_rag_better_than_traditional_rag/n2jz1xm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752243749,"author_flair_text":null,"treatment_tags":[],"created_utc":1752243749,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2jxeft","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Informal_Librarian","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2iard9","score":1,"author_fullname":"t2_obq9bdp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Have you looked into how embeddings are generated and what the vectors represent? Saying they’re “not trained on NLP” is misleading because in fact, they’re explicitly created / trained using NLP models to capture semantic meaning. I think what you're getting at is that vector databases can't reason about the data like LLMs can, which is true.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2jxeft","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Have you looked into how embeddings are generated and what the vectors represent? Saying they’re “not trained on NLP” is misleading because in fact, they’re explicitly created / trained using NLP models to capture semantic meaning. I think what you&amp;#39;re getting at is that vector databases can&amp;#39;t reason about the data like LLMs can, which is true.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwx77q","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx77q/is_llm_first_rag_better_than_traditional_rag/n2jxeft/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752243265,"author_flair_text":null,"treatment_tags":[],"created_utc":1752243265,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2iard9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Maleficent_Mess6445","can_mod_post":false,"created_utc":1752217295,"send_replies":true,"parent_id":"t1_n2i3wnl","score":0,"author_fullname":"t2_adnzl8f8x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I see. You're right, LLM is compute intensive when hosted locally but if privacy is not a major concern, this issue is resolved with API. Hallucination is a problem but I don't think  vector db is a solution to it. Vector DB is far inferior technology wrt LLM and is not trained on NLP.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2iard9","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I see. You&amp;#39;re right, LLM is compute intensive when hosted locally but if privacy is not a major concern, this issue is resolved with API. Hallucination is a problem but I don&amp;#39;t think  vector db is a solution to it. Vector DB is far inferior technology wrt LLM and is not trained on NLP.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwx77q","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx77q/is_llm_first_rag_better_than_traditional_rag/n2iard9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752217295,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n2i3wnl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ttkciar","can_mod_post":false,"created_utc":1752213675,"send_replies":true,"parent_id":"t3_1lwx77q","score":1,"author_fullname":"t2_cpegz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The critical difference is that you can populate your RAG database with true things, relevant to some subject.  This helps ground inference in truth, and (mostly) avoids hallucinations.\\n\\n\\"Thinking\\" inference has the advantage of generating only augmenting information which is relevant to the user's prompt, but it relies on the model's intrinsic world knowledge to do so, and a hallucination early on can throw everything else off.  It is also much, much more compute-intensive than RAG.\\n\\nThere's a place for each, especially when you cannot compile a high-quality RAG database for all of the subjects on which you want to infer.\\n\\nThere's also a place for combining the two approaches, via HyDE -- https://medium.com/prompt-engineering/hyde-revolutionising-search-with-hypothetical-document-embeddings-3474df795af8","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2i3wnl","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The critical difference is that you can populate your RAG database with true things, relevant to some subject.  This helps ground inference in truth, and (mostly) avoids hallucinations.&lt;/p&gt;\\n\\n&lt;p&gt;&amp;quot;Thinking&amp;quot; inference has the advantage of generating only augmenting information which is relevant to the user&amp;#39;s prompt, but it relies on the model&amp;#39;s intrinsic world knowledge to do so, and a hallucination early on can throw everything else off.  It is also much, much more compute-intensive than RAG.&lt;/p&gt;\\n\\n&lt;p&gt;There&amp;#39;s a place for each, especially when you cannot compile a high-quality RAG database for all of the subjects on which you want to infer.&lt;/p&gt;\\n\\n&lt;p&gt;There&amp;#39;s also a place for combining the two approaches, via HyDE -- &lt;a href=\\"https://medium.com/prompt-engineering/hyde-revolutionising-search-with-hypothetical-document-embeddings-3474df795af8\\"&gt;https://medium.com/prompt-engineering/hyde-revolutionising-search-with-hypothetical-document-embeddings-3474df795af8&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx77q/is_llm_first_rag_better_than_traditional_rag/n2i3wnl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752213675,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lwx77q","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2j9buy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Maleficent_Mess6445","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2j5lhz","score":1,"author_fullname":"t2_adnzl8f8x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Appreciate the information.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2j9buy","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Appreciate the information.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwx77q","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx77q/is_llm_first_rag_better_than_traditional_rag/n2j9buy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752235227,"author_flair_text":null,"treatment_tags":[],"created_utc":1752235227,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2j5lhz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MoneroXGC","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2islf7","score":2,"author_fullname":"t2_8jhze9u4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I think you might’ve misunderstood what these different technologies are used for. You can get more accurate results from a vector db based on the embedding models/algorithms you use and for each independent type of data you’re starting to store. \\n\\nSQL databases are for structured data (rows and columns). Vector DBs are for unstructured data, so not queryable by sql, that’s why they were invented. \\n\\nThe type of NLP that is done by LLMs is different from the way VDBs do it. One is for generation, the other is for querying. The accuracy of your results in a vector database has less to do with the embedding models capabilities and more to do with how you chunk it, store it, and how much relevant information is available based on your NL query.\\n\\nIf you’re talking about a chatbot that references information, users will speak to an LLM and then the LLM will break the prompt up into a query and send it to the vector DB. That’s how RAG works. If you’re just sending natural language queries to a vector DB you’re querying a database, not building RAG","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2j5lhz","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think you might’ve misunderstood what these different technologies are used for. You can get more accurate results from a vector db based on the embedding models/algorithms you use and for each independent type of data you’re starting to store. &lt;/p&gt;\\n\\n&lt;p&gt;SQL databases are for structured data (rows and columns). Vector DBs are for unstructured data, so not queryable by sql, that’s why they were invented. &lt;/p&gt;\\n\\n&lt;p&gt;The type of NLP that is done by LLMs is different from the way VDBs do it. One is for generation, the other is for querying. The accuracy of your results in a vector database has less to do with the embedding models capabilities and more to do with how you chunk it, store it, and how much relevant information is available based on your NL query.&lt;/p&gt;\\n\\n&lt;p&gt;If you’re talking about a chatbot that references information, users will speak to an LLM and then the LLM will break the prompt up into a query and send it to the vector DB. That’s how RAG works. If you’re just sending natural language queries to a vector DB you’re querying a database, not building RAG&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwx77q","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx77q/is_llm_first_rag_better_than_traditional_rag/n2j5lhz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752233716,"author_flair_text":null,"treatment_tags":[],"created_utc":1752233716,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2islf7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Maleficent_Mess6445","can_mod_post":false,"created_utc":1752227463,"send_replies":true,"parent_id":"t1_n2ilxxt","score":1,"author_fullname":"t2_adnzl8f8x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I think for NLP the LLM is the most superior technology and so the user query should go to LLM first else it is misinterpreted. Also if there is a need for querying databases then SQL is much more reliable than vector db.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2islf7","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think for NLP the LLM is the most superior technology and so the user query should go to LLM first else it is misinterpreted. Also if there is a need for querying databases then SQL is much more reliable than vector db.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwx77q","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx77q/is_llm_first_rag_better_than_traditional_rag/n2islf7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752227463,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ilxxt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MoneroXGC","can_mod_post":false,"created_utc":1752223664,"send_replies":true,"parent_id":"t3_1lwx77q","score":2,"author_fullname":"t2_8jhze9u4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Vector DBs aren’t an inferior technology, they’re a completely different technology.\\n\\nYou don’t query an LLM, you ask it a question and it responds with what it generates what looks like a correct response to your answer based on the data it’s been trained with. Vector DBs you query with natural language and it fetches chunks of data with similar meaning to that of your query. \\n\\nYou use traditional vector RAG to retrieve live/up-to-date information that the LLM doesn’t have in its training data. For example, you might store notes from a meeting you had in a vector DB so that you can later look up a type of conversation you had and then find out who you had that conversation with. You couldn’t ask for an LLM to give you this information because it isn’t in its training data","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ilxxt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Vector DBs aren’t an inferior technology, they’re a completely different technology.&lt;/p&gt;\\n\\n&lt;p&gt;You don’t query an LLM, you ask it a question and it responds with what it generates what looks like a correct response to your answer based on the data it’s been trained with. Vector DBs you query with natural language and it fetches chunks of data with similar meaning to that of your query. &lt;/p&gt;\\n\\n&lt;p&gt;You use traditional vector RAG to retrieve live/up-to-date information that the LLM doesn’t have in its training data. For example, you might store notes from a meeting you had in a vector DB so that you can later look up a type of conversation you had and then find out who you had that conversation with. You couldn’t ask for an LLM to give you this information because it isn’t in its training data&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx77q/is_llm_first_rag_better_than_traditional_rag/n2ilxxt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752223664,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwx77q","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2hy30l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"I_Short_TSLA","can_mod_post":false,"created_utc":1752210793,"send_replies":true,"parent_id":"t3_1lwx77q","score":1,"author_fullname":"t2_2zs0bfio","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It depends on how much data you are dealing with. Retrieval is a tool in case you are dealing with tons and tons of data, let’s say 100x the model context size or more.\\n\\nYou only use retrieval for grounding if you have no other choice.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2hy30l","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It depends on how much data you are dealing with. Retrieval is a tool in case you are dealing with tons and tons of data, let’s say 100x the model context size or more.&lt;/p&gt;\\n\\n&lt;p&gt;You only use retrieval for grounding if you have no other choice.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx77q/is_llm_first_rag_better_than_traditional_rag/n2hy30l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752210793,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwx77q","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),o=()=>e.jsx(t,{data:l});export{o as default};
