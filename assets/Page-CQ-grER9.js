import{j as l}from"./index-DLSqWzaI.js";import{R as e}from"./RedditPostRenderer-CysRo2D_.js";import"./index-COXiL3Lo.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I'm running the same model on llama.cpp as I do with kobold.cpp. KCPP has very fast outputs while LCPP is considerably more sluggish. I run llama-server with -ngl 100, but the output time is seemingly unchanged. Is this just how it's meant to be, or can I fix it somehow?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"llama.cpp running too slow","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m3rhy2","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.4,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_ll4bvonk1","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752915055,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m running the same model on llama.cpp as I do with kobold.cpp. KCPP has very fast outputs while LCPP is considerably more sluggish. I run llama-server with -ngl 100, but the output time is seemingly unchanged. Is this just how it&amp;#39;s meant to be, or can I fix it somehow?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m3rhy2","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"bridgebucket","discussion_type":null,"num_comments":4,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m3rhy2/llamacpp_running_too_slow/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m3rhy2/llamacpp_running_too_slow/","subreddit_subscribers":501753,"created_utc":1752915055,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3yva64","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"smahs9","can_mod_post":false,"created_utc":1752917417,"send_replies":true,"parent_id":"t3_1m3rhy2","score":13,"author_fullname":"t2_neyagc1uz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"May be post some meaningful information to help others help you? Like all the args you ran the server with? What hardware and OS you're running on, and did you compile llama.cpp with support for your hardware?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3yva64","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;May be post some meaningful information to help others help you? Like all the args you ran the server with? What hardware and OS you&amp;#39;re running on, and did you compile llama.cpp with support for your hardware?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3rhy2/llamacpp_running_too_slow/n3yva64/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752917417,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3rhy2","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":13}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3z1agm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"fizzy1242","can_mod_post":false,"created_utc":1752920767,"send_replies":true,"parent_id":"t3_1m3rhy2","score":5,"author_fullname":"t2_16zcsx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"you're probably missing flags you're using in koboldcpp, like batch size, flash attention, mmq,...","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3z1agm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;you&amp;#39;re probably missing flags you&amp;#39;re using in koboldcpp, like batch size, flash attention, mmq,...&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3rhy2/llamacpp_running_too_slow/n3z1agm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752920767,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3rhy2","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n40yxta","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ArsNeph","can_mod_post":false,"created_utc":1752945676,"send_replies":true,"parent_id":"t3_1m3rhy2","score":2,"author_fullname":"t2_vt0xkv60d","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Make sure you're using the llama.cpp CUDA version, enable flash attention, set your CPU threads, make sure you have your batch size set to like 512, check that the context is fitting in VRAM, so on so forth. It should be like 5% faster than KoboldCPP for most models","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n40yxta","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Make sure you&amp;#39;re using the llama.cpp CUDA version, enable flash attention, set your CPU threads, make sure you have your batch size set to like 512, check that the context is fitting in VRAM, so on so forth. It should be like 5% faster than KoboldCPP for most models&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3rhy2/llamacpp_running_too_slow/n40yxta/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752945676,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3rhy2","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3yxpcr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok_Cow1976","can_mod_post":false,"created_utc":1752918792,"send_replies":true,"parent_id":"t3_1m3rhy2","score":0,"author_fullname":"t2_3pwbsmdr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Try to set context size small like 2048 for llama cpp.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3yxpcr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Try to set context size small like 2048 for llama cpp.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3rhy2/llamacpp_running_too_slow/n3yxpcr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752918792,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3rhy2","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}}]`),s=()=>l.jsx(e,{data:a});export{s as default};
