import{j as e}from"./index-xfnGEtuL.js";import{R as l}from"./RedditPostRenderer-DAZRIZZK.js";import"./index-BaNn5-RR.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"The Creators of Earnie just published a new quantization algorithm that compress Ernie-300B to 85GB and Deepseek-V3 to 184 GB, with minimal (&lt;2%) performance degradation in benchmarks. Paper here: [https://arxiv.org/pdf/2507.07145](https://arxiv.org/pdf/2507.07145)\\n\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"2-bit Quant: CCQ, Convolutional Code for Extreme Low-bit Quantization in LLMs","link_flair_richtext":[{"e":"text","t":"Resources"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lwx50s","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.98,"author_flair_background_color":"#bd9e9e","subreddit_type":"public","ups":81,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":"d2642412-d9ce-11ed-ae30-32b11309f5bd","is_original_content":false,"author_fullname":"t2_g177e","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Resources","can_mod_post":false,"score":81,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[{"e":"text","t":"Alpaca"}],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752206258,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"richtext","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;The Creators of Earnie just published a new quantization algorithm that compress Ernie-300B to 85GB and Deepseek-V3 to 184 GB, with minimal (&amp;lt;2%) performance degradation in benchmarks. Paper here: &lt;a href=\\"https://arxiv.org/pdf/2507.07145\\"&gt;https://arxiv.org/pdf/2507.07145&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":"Alpaca","treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#ccac2b","id":"1lwx50s","is_robot_indexable":true,"num_duplicates":1,"report_reasons":null,"author":"ortegaalfredo","discussion_type":null,"num_comments":29,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":"light","permalink":"/r/LocalLLaMA/comments/1lwx50s/2bit_quant_ccq_convolutional_code_for_extreme/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lwx50s/2bit_quant_ccq_convolutional_code_for_extreme/","subreddit_subscribers":497504,"created_utc":1752206258,"num_crossposts":1,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ie1dx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"pkmxtw","can_mod_post":false,"created_utc":1752219125,"send_replies":true,"parent_id":"t1_n2i54cj","score":12,"author_fullname":"t2_a2gtk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Or you can just run [this 656k model](https://huggingface.co/raincandy-u/TinyStories-656K) that produces grammarly correct stories! Even Q8 fits within a floppy disk!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ie1dx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Or you can just run &lt;a href=\\"https://huggingface.co/raincandy-u/TinyStories-656K\\"&gt;this 656k model&lt;/a&gt; that produces grammarly correct stories! Even Q8 fits within a floppy disk!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwx50s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx50s/2bit_quant_ccq_convolutional_code_for_extreme/n2ie1dx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752219125,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}}],"before":null}},"user_reports":[],"saved":false,"id":"n2i54cj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1752214300,"send_replies":true,"parent_id":"t3_1lwx50s","score":25,"author_fullname":"t2_uz37qfx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yay!I can compress SmollLLM to 35Mb ! And run it on 1998 era computer!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2i54cj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yay!I can compress SmollLLM to 35Mb ! And run it on 1998 era computer!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx50s/2bit_quant_ccq_convolutional_code_for_extreme/n2i54cj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752214300,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwx50s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":25}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2i3e7t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Firepal64","can_mod_post":false,"created_utc":1752213413,"send_replies":true,"parent_id":"t3_1lwx50s","score":13,"author_fullname":"t2_11i4bd","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"i've been burned too many times...","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2i3e7t","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;i&amp;#39;ve been burned too many times...&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx50s/2bit_quant_ccq_convolutional_code_for_extreme/n2i3e7t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752213413,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwx50s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":13}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2id0h0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"roselan","can_mod_post":false,"created_utc":1752218538,"send_replies":true,"parent_id":"t1_n2huhbt","score":10,"author_fullname":"t2_9akn4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This is a freaking huge if indeed.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2id0h0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is a freaking huge if indeed.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwx50s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx50s/2bit_quant_ccq_convolutional_code_for_extreme/n2id0h0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752218538,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"7d1f04e6-4920-11ef-b2e1-2e580594e1a1","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"7d1f04e6-4920-11ef-b2e1-2e580594e1a1","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2kmyg9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"bucolucas","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2kmbjw","score":1,"author_fullname":"t2_ai9fq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":":)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2kmyg9","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 3.1"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;:)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwx50s","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx50s/2bit_quant_ccq_convolutional_code_for_extreme/n2kmyg9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752250501,"author_flair_text":"Llama 3.1","treatment_tags":[],"created_utc":1752250501,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#93b1ba","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2kmbjw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AIEchoesHumanity","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2kl5fe","score":1,"author_fullname":"t2_t4oqvl2rk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"lol took me a minute to get this","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2kmbjw","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;lol took me a minute to get this&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwx50s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx50s/2bit_quant_ccq_convolutional_code_for_extreme/n2kmbjw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752250320,"author_flair_text":null,"treatment_tags":[],"created_utc":1752250320,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2kl5fe","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"bucolucas","can_mod_post":false,"created_utc":1752249984,"send_replies":true,"parent_id":"t1_n2huhbt","score":2,"author_fullname":"t2_ai9fq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Actually, extremely small if true","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2kl5fe","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 3.1"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Actually, extremely small if true&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwx50s","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx50s/2bit_quant_ccq_convolutional_code_for_extreme/n2kl5fe/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752249984,"author_flair_text":"Llama 3.1","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#93b1ba","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"d2642412-d9ce-11ed-ae30-32b11309f5bd","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"d2642412-d9ce-11ed-ae30-32b11309f5bd","distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"d2642412-d9ce-11ed-ae30-32b11309f5bd","distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2j3414","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ortegaalfredo","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2iwl5x","score":1,"author_fullname":"t2_g177e","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I forgot that exl3 also have 2-bit quants. Would be interesting to see a compare, however I think they lose quite a lot more in quality. CCQ here compares even with AWQ.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n2j3414","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"Alpaca"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I forgot that exl3 also have 2-bit quants. Would be interesting to see a compare, however I think they lose quite a lot more in quality. CCQ here compares even with AWQ.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lwx50s","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx50s/2bit_quant_ccq_convolutional_code_for_extreme/n2j3414/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752232640,"author_flair_text":"Alpaca","treatment_tags":[],"created_utc":1752232640,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":"#bd9e9e","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2iwl5x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rerri","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2iv29f","score":2,"author_fullname":"t2_12aeph","approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt;Quantizing is also very heavy and expensive, don't know really about the costs of this techinque but if the lab was able to release a quant of deepseek that is the biggest currently available LLM, cost should not be a lot.\\n\\nQuantizing can be lighter or heavier depending on the method. Afaik the differences between methods can be vast.\\n\\nIf Baidu Inc builds 300B models, I'm not sure I'd rule out the possibility that they have some very different type of compute available for quantization studies like this than the heroes of ours who produce imatrix/exl3 quants for distribution on HF. But I'm no expert, maybe it's reasonable to assume otherwise.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n2iwl5x","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Quantizing is also very heavy and expensive, don&amp;#39;t know really about the costs of this techinque but if the lab was able to release a quant of deepseek that is the biggest currently available LLM, cost should not be a lot.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Quantizing can be lighter or heavier depending on the method. Afaik the differences between methods can be vast.&lt;/p&gt;\\n\\n&lt;p&gt;If Baidu Inc builds 300B models, I&amp;#39;m not sure I&amp;#39;d rule out the possibility that they have some very different type of compute available for quantization studies like this than the heroes of ours who produce imatrix/exl3 quants for distribution on HF. But I&amp;#39;m no expert, maybe it&amp;#39;s reasonable to assume otherwise.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lwx50s","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx50s/2bit_quant_ccq_convolutional_code_for_extreme/n2iwl5x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752229558,"author_flair_text":null,"treatment_tags":[],"created_utc":1752229558,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2iv29f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ortegaalfredo","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2iuibp","score":1,"author_fullname":"t2_g177e","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Oh I see, I was talking about de-quantizing in real time. Quantizing is also very heavy and expensive, don't know really about the costs of this techinque but if the lab was able to release a quant of deepseek that is the biggest currently available LLM, cost should not be a lot.","edited":false,"author_flair_css_class":null,"name":"t1_n2iv29f","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"Alpaca"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Oh I see, I was talking about de-quantizing in real time. Quantizing is also very heavy and expensive, don&amp;#39;t know really about the costs of this techinque but if the lab was able to release a quant of deepseek that is the biggest currently available LLM, cost should not be a lot.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lwx50s","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx50s/2bit_quant_ccq_convolutional_code_for_extreme/n2iv29f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752228774,"author_flair_text":"Alpaca","collapsed":false,"created_utc":1752228774,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":"#bd9e9e","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2iuibp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"rerri","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2irrz9","score":4,"author_fullname":"t2_12aeph","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I meant quantizing a model from say BF16 into this 2-bit CCQ format.\\n\\nThere's been some other new quantization methods lately that showed good results, but exllama author turboderp commented that those methods were unfeasible for projects like exllamav3 because of the the high computational requirements to quantize a model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2iuibp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I meant quantizing a model from say BF16 into this 2-bit CCQ format.&lt;/p&gt;\\n\\n&lt;p&gt;There&amp;#39;s been some other new quantization methods lately that showed good results, but exllama author turboderp commented that those methods were unfeasible for projects like exllamav3 because of the the high computational requirements to quantize a model.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwx50s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx50s/2bit_quant_ccq_convolutional_code_for_extreme/n2iuibp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752228483,"author_flair_text":null,"treatment_tags":[],"created_utc":1752228483,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n2irrz9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ortegaalfredo","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2ijemv","score":1,"author_fullname":"t2_g177e","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That is true but unless you are decoding &gt; \\\\~8 prompts at the same time, in GPUs you are memory-limited, not compute-limited. So this will likely make the models \\\\*faster\\\\* until you start increasing the batching number.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2irrz9","is_submitter":true,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"Alpaca"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That is true but unless you are decoding &amp;gt; ~8 prompts at the same time, in GPUs you are memory-limited, not compute-limited. So this will likely make the models *faster* until you start increasing the batching number.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwx50s","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx50s/2bit_quant_ccq_convolutional_code_for_extreme/n2irrz9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752227014,"author_flair_text":"Alpaca","treatment_tags":[],"created_utc":1752227014,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bd9e9e","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ijemv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rerri","can_mod_post":false,"created_utc":1752222183,"send_replies":true,"parent_id":"t1_n2huhbt","score":1,"author_fullname":"t2_12aeph","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Only if the computational resources required to quantize a model are reasonable though. I don't think this was discussed in the paper.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ijemv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Only if the computational resources required to quantize a model are reasonable though. I don&amp;#39;t think this was discussed in the paper.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwx50s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx50s/2bit_quant_ccq_convolutional_code_for_extreme/n2ijemv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752222183,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2huhbt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AIEchoesHumanity","can_mod_post":false,"created_utc":1752209114,"send_replies":true,"parent_id":"t3_1lwx50s","score":10,"author_fullname":"t2_t4oqvl2rk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"this is fricking huge if true","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2huhbt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;this is fricking huge if true&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx50s/2bit_quant_ccq_convolutional_code_for_extreme/n2huhbt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752209114,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwx50s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ia769","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mpthouse","can_mod_post":false,"created_utc":1752216992,"send_replies":true,"parent_id":"t3_1lwx50s","score":3,"author_fullname":"t2_1ft86zbrel","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Interesting, definitely worth checking out for optimizing those large models!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ia769","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Interesting, definitely worth checking out for optimizing those large models!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx50s/2bit_quant_ccq_convolutional_code_for_extreme/n2ia769/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752216992,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwx50s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"7d921414-5177-11ee-b947-e27b363b98d5","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2jhkik","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ReturningTarzan","can_mod_post":false,"created_utc":1752238234,"send_replies":true,"parent_id":"t3_1lwx50s","score":3,"author_fullname":"t2_4dru3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"There's also [this](https://huggingface.co/turboderp/ERNIE-4.5-300B-A47B-PT-exl3). It would be interesting to try out the Paddle model, though I don't have the facilities to actually run it and make a straight comparison. The average bitrate works out to about 2.5 bits per weight (a mix of 2, 4 and 8-bit quantizations by the looks of it) so even if I did have inference code needed I don't have the hardware for it.\\n\\nThe largest EXL3 version I can run locally is 2.25bpw, which is coherent and \\"seems smart,\\" though to fully benchmark it I would need to set up a RunPod instance. Generally all claims of \\"lossless\\" or \\"minimal accuracy loss\\" should be taken with a grain of salt, but I did run a reduced MMLU test on the 2.25bpw version and got \\"lossless\\" results (87.52% vs their reported 86.5% for WINT8). I'll queue up a full test when I have some downtime later to confirm.\\n\\nMMLU is very limited in scope of course, but it's what I have available that I can easily compare to the paper. I'll see if it's feasible to run a full test suite with HumanEval+ etc. later on.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2jhkik","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"ExLlama Developer"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There&amp;#39;s also &lt;a href=\\"https://huggingface.co/turboderp/ERNIE-4.5-300B-A47B-PT-exl3\\"&gt;this&lt;/a&gt;. It would be interesting to try out the Paddle model, though I don&amp;#39;t have the facilities to actually run it and make a straight comparison. The average bitrate works out to about 2.5 bits per weight (a mix of 2, 4 and 8-bit quantizations by the looks of it) so even if I did have inference code needed I don&amp;#39;t have the hardware for it.&lt;/p&gt;\\n\\n&lt;p&gt;The largest EXL3 version I can run locally is 2.25bpw, which is coherent and &amp;quot;seems smart,&amp;quot; though to fully benchmark it I would need to set up a RunPod instance. Generally all claims of &amp;quot;lossless&amp;quot; or &amp;quot;minimal accuracy loss&amp;quot; should be taken with a grain of salt, but I did run a reduced MMLU test on the 2.25bpw version and got &amp;quot;lossless&amp;quot; results (87.52% vs their reported 86.5% for WINT8). I&amp;#39;ll queue up a full test when I have some downtime later to confirm.&lt;/p&gt;\\n\\n&lt;p&gt;MMLU is very limited in scope of course, but it&amp;#39;s what I have available that I can easily compare to the paper. I&amp;#39;ll see if it&amp;#39;s feasible to run a full test suite with HumanEval+ etc. later on.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx50s/2bit_quant_ccq_convolutional_code_for_extreme/n2jhkik/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752238234,"author_flair_text":"ExLlama Developer","treatment_tags":[],"link_id":"t3_1lwx50s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#5a74cc","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"d2642412-d9ce-11ed-ae30-32b11309f5bd","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ixnz5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Zestyclose_Yak_3174","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2irgr2","score":1,"author_fullname":"t2_o0jgdhlij","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Second this. That would be the dream","edited":false,"author_flair_css_class":null,"name":"t1_n2ixnz5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Second this. That would be the dream&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lwx50s","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx50s/2bit_quant_ccq_convolutional_code_for_extreme/n2ixnz5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752230099,"author_flair_text":null,"collapsed":false,"created_utc":1752230099,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2irgr2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ortegaalfredo","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2iibtg","score":4,"author_fullname":"t2_g177e","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Qwen-235B, If quantized to 2 bits that would make it \\\\~58 GB, and would fit in a 64GB system.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2irgr2","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"Alpaca"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen-235B, If quantized to 2 bits that would make it ~58 GB, and would fit in a 64GB system.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwx50s","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx50s/2bit_quant_ccq_convolutional_code_for_extreme/n2irgr2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752226842,"author_flair_text":"Alpaca","treatment_tags":[],"created_utc":1752226842,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#bd9e9e","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2j3mmi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AIEchoesHumanity","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2iibtg","score":1,"author_fullname":"t2_t4oqvl2rk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"im interested in models that can fit inside 12GB of vram. so around 24B parameters, if my math is right","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2j3mmi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;im interested in models that can fit inside 12GB of vram. so around 24B parameters, if my math is right&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwx50s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx50s/2bit_quant_ccq_convolutional_code_for_extreme/n2j3mmi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752232867,"author_flair_text":null,"treatment_tags":[],"created_utc":1752232867,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2iibtg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Zestyclose-Hurry1063","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2hz1g6","score":9,"author_fullname":"t2_jfv8yktp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Great point. We are extending CCQ to more models... Hopefully more 2-bit models could be released soon. \\n\\nbtw, is there any specific LLM you wanna try on?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2iibtg","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Great point. We are extending CCQ to more models... Hopefully more 2-bit models could be released soon. &lt;/p&gt;\\n\\n&lt;p&gt;btw, is there any specific LLM you wanna try on?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwx50s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx50s/2bit_quant_ccq_convolutional_code_for_extreme/n2iibtg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752221558,"author_flair_text":null,"treatment_tags":[],"created_utc":1752221558,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}}],"before":null}},"user_reports":[],"saved":false,"id":"n2hz1g6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AIEchoesHumanity","can_mod_post":false,"created_utc":1752211253,"send_replies":true,"parent_id":"t1_n2hx6s9","score":6,"author_fullname":"t2_t4oqvl2rk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"im seeing that their github code allows you to run inference and do this quantization on ERNIE models. it would be sick to have a generalized quantization script for all models","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2hz1g6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;im seeing that their github code allows you to run inference and do this quantization on ERNIE models. it would be sick to have a generalized quantization script for all models&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwx50s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx50s/2bit_quant_ccq_convolutional_code_for_extreme/n2hz1g6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752211253,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n2hx6s9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"vasileer","can_mod_post":false,"created_utc":1752210368,"send_replies":true,"parent_id":"t3_1lwx50s","score":5,"author_fullname":"t2_730bgdulm","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"From paper: \\"We publicly release 2-bit quantized ERNIE 4.5. The inference code is available at [https://github.com/PaddlePaddle/FastDeploy/tree/develop\\"](https://github.com/PaddlePaddle/FastDeploy/tree/develop) \\n\\nAnd probably this is the quantized model [https://huggingface.co/baidu/ERNIE-4.5-300B-A47B-2Bits-TP2-Paddle](https://huggingface.co/baidu/ERNIE-4.5-300B-A47B-2Bits-TP2-Paddle)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2hx6s9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;From paper: &amp;quot;We publicly release 2-bit quantized ERNIE 4.5. The inference code is available at &lt;a href=\\"https://github.com/PaddlePaddle/FastDeploy/tree/develop\\"&gt;https://github.com/PaddlePaddle/FastDeploy/tree/develop&amp;quot;&lt;/a&gt; &lt;/p&gt;\\n\\n&lt;p&gt;And probably this is the quantized model &lt;a href=\\"https://huggingface.co/baidu/ERNIE-4.5-300B-A47B-2Bits-TP2-Paddle\\"&gt;https://huggingface.co/baidu/ERNIE-4.5-300B-A47B-2Bits-TP2-Paddle&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx50s/2bit_quant_ccq_convolutional_code_for_extreme/n2hx6s9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752210368,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwx50s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2iigoc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ResidentPositive4122","can_mod_post":false,"created_utc":1752221635,"send_replies":true,"parent_id":"t1_n2ihugt","score":15,"author_fullname":"t2_10nxrjjgay","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Don't confuse pruning with quantising. In pruning you remove data and make the model smaller. In quantising you are limiting the precision of your calculations, basically making less precise estimations, but the signal is still there in the model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2iigoc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Don&amp;#39;t confuse pruning with quantising. In pruning you remove data and make the model smaller. In quantising you are limiting the precision of your calculations, basically making less precise estimations, but the signal is still there in the model.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwx50s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx50s/2bit_quant_ccq_convolutional_code_for_extreme/n2iigoc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752221635,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":15}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ik404","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1752222593,"send_replies":true,"parent_id":"t1_n2ihugt","score":6,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"no, as quantized is never as good as full-precision. The difference like between $1 slice gas station pizza and normal $4 from a good pizzeria.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ik404","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;no, as quantized is never as good as full-precision. The difference like between $1 slice gas station pizza and normal $4 from a good pizzeria.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwx50s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx50s/2bit_quant_ccq_convolutional_code_for_extreme/n2ik404/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752222593,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ja9c4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"audioen","can_mod_post":false,"created_utc":1752235587,"send_replies":true,"parent_id":"t1_n2ihugt","score":1,"author_fullname":"t2_gz6hs","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I have no idea where you took the 98 % from. They reduced model by something like 72 %, not 98 %, with small penalty in accuracy.\\n\\nThere is rapidly diminishing marginal utility to increasing numeric precision of model weights. The most value is in the first few bits, suggesting that weights don't need to be very precise. However, training in practice tends to require higher precision so that gradient optimization can work. 16 bit floating point is common choice during training, though fp8 has also seen some use. The model's \\"official\\" version tends to be the precision it was trained with for practical reasons.\\n\\nThere is some research that suggests that somewhere near between of 2 to 2.5 bits per weight lies the optimal quantization, which has the most quality per byte of memory used. Maybe it can actually be even less, like the 1.58 bits per weight of a bitnet if model is trained to maximize its performance as a bitnet.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ja9c4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have no idea where you took the 98 % from. They reduced model by something like 72 %, not 98 %, with small penalty in accuracy.&lt;/p&gt;\\n\\n&lt;p&gt;There is rapidly diminishing marginal utility to increasing numeric precision of model weights. The most value is in the first few bits, suggesting that weights don&amp;#39;t need to be very precise. However, training in practice tends to require higher precision so that gradient optimization can work. 16 bit floating point is common choice during training, though fp8 has also seen some use. The model&amp;#39;s &amp;quot;official&amp;quot; version tends to be the precision it was trained with for practical reasons.&lt;/p&gt;\\n\\n&lt;p&gt;There is some research that suggests that somewhere near between of 2 to 2.5 bits per weight lies the optimal quantization, which has the most quality per byte of memory used. Maybe it can actually be even less, like the 1.58 bits per weight of a bitnet if model is trained to maximize its performance as a bitnet.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwx50s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx50s/2bit_quant_ccq_convolutional_code_for_extreme/n2ja9c4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752235587,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ihugt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MelodicRecognition7","can_mod_post":false,"created_utc":1752221279,"send_replies":true,"parent_id":"t3_1lwx50s","score":3,"author_fullname":"t2_1eex9ug5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"does that mean that 98% of their 300B is totally useless data?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ihugt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;does that mean that 98% of their 300B is totally useless data?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx50s/2bit_quant_ccq_convolutional_code_for_extreme/n2ihugt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752221279,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwx50s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2jbguo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1752236047,"send_replies":true,"parent_id":"t3_1lwx50s","score":2,"author_fullname":"t2_h5utwre7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Anyone gotten it running? Fully offloaded ernie sounds nice and if they have all the vllm samplers a bonus.\\n\\nI doubt they have quantized KV cache though. I want some ernie in 96gb vram.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2jbguo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Anyone gotten it running? Fully offloaded ernie sounds nice and if they have all the vllm samplers a bonus.&lt;/p&gt;\\n\\n&lt;p&gt;I doubt they have quantized KV cache though. I want some ernie in 96gb vram.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx50s/2bit_quant_ccq_convolutional_code_for_extreme/n2jbguo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752236047,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwx50s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"609bf7d4-01f3-11f0-9760-5611c8333bee","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2jvwj6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"You_Wen_AzzHu","can_mod_post":false,"created_utc":1752242818,"send_replies":true,"parent_id":"t3_1lwx50s","score":1,"author_fullname":"t2_p4oxcufl","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Finally I can run qwen3 235b much faster.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2jvwj6","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"exllama"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Finally I can run qwen3 235b much faster.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx50s/2bit_quant_ccq_convolutional_code_for_extreme/n2jvwj6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752242818,"author_flair_text":"exllama","treatment_tags":[],"link_id":"t3_1lwx50s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2k0bk7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MLDataScientist","can_mod_post":false,"created_utc":1752244116,"send_replies":true,"parent_id":"t3_1lwx50s","score":1,"author_fullname":"t2_3zy7pnf1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"it has always been the case that any 2 bit quantization is on par with GGUF IQ2\\\\_M or slightly better. However, support level for such quantizations was very low (e.g. QTIP - [https://github.com/Cornell-RelaxML/qtip](https://github.com/Cornell-RelaxML/qtip) , vptq - [https://github.com/microsoft/VPTQ](https://github.com/microsoft/VPTQ) , Quip# or AQLM). Unless it is supported by vLLM or llama.cpp, those quantization types become obsolete since they don't keep up with new model releases. I wish researchers added those quants to vLLM/llama.cpp.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2k0bk7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;it has always been the case that any 2 bit quantization is on par with GGUF IQ2_M or slightly better. However, support level for such quantizations was very low (e.g. QTIP - &lt;a href=\\"https://github.com/Cornell-RelaxML/qtip\\"&gt;https://github.com/Cornell-RelaxML/qtip&lt;/a&gt; , vptq - &lt;a href=\\"https://github.com/microsoft/VPTQ\\"&gt;https://github.com/microsoft/VPTQ&lt;/a&gt; , Quip# or AQLM). Unless it is supported by vLLM or llama.cpp, those quantization types become obsolete since they don&amp;#39;t keep up with new model releases. I wish researchers added those quants to vLLM/llama.cpp.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwx50s/2bit_quant_ccq_convolutional_code_for_extreme/n2k0bk7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752244116,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwx50s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(l,{data:t});export{n as default};
