import{j as e}from"./index-xfnGEtuL.js";import{R as t}from"./RedditPostRenderer-DAZRIZZK.js";import"./index-BaNn5-RR.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"When I use LLMs for creative writing tasks, a lot of the time they can write a couple of hundred words just fine, but then sentences break down. \\n\\nThe screenshot shows a typical example of one going off the rails - there are proper sentences, then some barely readable James-Joyce-style stream of consciousness, then just an mediated gush of words without form or meaning. \\n\\nI've tried prompting hard (\\"Use ONLY full complete traditional sentences and grammar, write like Hemingway\\" and variations of the same), and I've tried bringing the Temperature right down, but nothing seems to help. \\n\\nI've had it happen with loads of locally run models, and also with large cloud-based stuff like DeepSeek's R1 and V3. Only the corporate ones (ChatGPT, Claude, Gemini, and interestingly Mistral) seem immune. This particular example is from the new KimiK2. Even though I specified only 400 words (and placed that right at the end of the prompt, which always seems to hit hardest), it kept spitting out this nonsense for thousands of words until I hit Stop.\\n\\nAny advice, or just some bitter commiseration, gratefully accepted.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Responses keep dissolving into word salad - how to stop it?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":97,"top_awarded_type":null,"hide_score":false,"name":"t3_1lzhqz8","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.86,"author_flair_background_color":null,"ups":21,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_i5ettea7e","secure_media":null,"is_reddit_media_domain":true,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":21,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://b.thumbs.redditmedia.com/KT3zIRhU3VM0HArg0M_K5OvqTcxlyaFFQUhqYMvfTZU.jpg","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"image","content_categories":null,"is_self":false,"subreddit_type":"public","created":1752484685,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"i.redd.it","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;When I use LLMs for creative writing tasks, a lot of the time they can write a couple of hundred words just fine, but then sentences break down. &lt;/p&gt;\\n\\n&lt;p&gt;The screenshot shows a typical example of one going off the rails - there are proper sentences, then some barely readable James-Joyce-style stream of consciousness, then just an mediated gush of words without form or meaning. &lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ve tried prompting hard (&amp;quot;Use ONLY full complete traditional sentences and grammar, write like Hemingway&amp;quot; and variations of the same), and I&amp;#39;ve tried bringing the Temperature right down, but nothing seems to help. &lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ve had it happen with loads of locally run models, and also with large cloud-based stuff like DeepSeek&amp;#39;s R1 and V3. Only the corporate ones (ChatGPT, Claude, Gemini, and interestingly Mistral) seem immune. This particular example is from the new KimiK2. Even though I specified only 400 words (and placed that right at the end of the prompt, which always seems to hit hardest), it kept spitting out this nonsense for thousands of words until I hit Stop.&lt;/p&gt;\\n\\n&lt;p&gt;Any advice, or just some bitter commiseration, gratefully accepted.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"url_overridden_by_dest":"https://i.redd.it/lr7kq1452tcf1.png","view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://preview.redd.it/lr7kq1452tcf1.png?auto=webp&amp;s=4b3ae39d0f36d78e751373129a21148da7beecfe","width":2106,"height":1468},"resolutions":[{"url":"https://preview.redd.it/lr7kq1452tcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cc920b45a1223c1528565fd604812590fd688bc1","width":108,"height":75},{"url":"https://preview.redd.it/lr7kq1452tcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=40aefee037971cf3d6b6efb316c7b3e51c7517f4","width":216,"height":150},{"url":"https://preview.redd.it/lr7kq1452tcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2dd6d6bf1658fd7fe587e5003dba9ba42a7ad36a","width":320,"height":223},{"url":"https://preview.redd.it/lr7kq1452tcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a7c9ff380a5c67f510c3b2b1cf4849772e667cf4","width":640,"height":446},{"url":"https://preview.redd.it/lr7kq1452tcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ce4eccc69c232a328a2c24e817b01c27f77cdfd0","width":960,"height":669},{"url":"https://preview.redd.it/lr7kq1452tcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=af4b94b12d7167b70c16f924fc869a65fe847a07","width":1080,"height":752}],"variants":{},"id":"LwTV7b9YJU3kHEoNxw0COXZ7bbZXrcGjWQcefFCOVzU"}],"enabled":true},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"mod_note":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"num_reports":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lzhqz8","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Gilgameshcomputing","discussion_type":null,"num_comments":29,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lzhqz8/responses_keep_dissolving_into_word_salad_how_to/","stickied":false,"url":"https://i.redd.it/lr7kq1452tcf1.png","subreddit_subscribers":499296,"created_utc":1752484685,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n31zvnk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"xmBQWugdxjaA","can_mod_post":false,"send_replies":true,"parent_id":"t1_n31sj98","score":8,"author_fullname":"t2_nyyscwdgr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The output of LLM inference is the probability distribution for the next token, from the final token in the input sequence (which is repeated with the new full sequence going forward, although some calculations are cached in the KV cache).\\n\\nThe sampler decides what you do with that distribution - do you sample across all probabilities according to their weight (but there's a small chance you get a completely crazy token), or sample just from the top 5 tokens? Or just always take the top token in a deterministic manner?\\n\\nIn your case you probably want a lower temperature (lower chance of sampling less likely tokens) and maybe also restrict the set considered to a smaller set (e.g. top 10 tokens) - as the issue is the moment you sample one crazy token, then the sequence is poisoned with that for future inference.","edited":false,"author_flair_css_class":null,"name":"t1_n31zvnk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The output of LLM inference is the probability distribution for the next token, from the final token in the input sequence (which is repeated with the new full sequence going forward, although some calculations are cached in the KV cache).&lt;/p&gt;\\n\\n&lt;p&gt;The sampler decides what you do with that distribution - do you sample across all probabilities according to their weight (but there&amp;#39;s a small chance you get a completely crazy token), or sample just from the top 5 tokens? Or just always take the top token in a deterministic manner?&lt;/p&gt;\\n\\n&lt;p&gt;In your case you probably want a lower temperature (lower chance of sampling less likely tokens) and maybe also restrict the set considered to a smaller set (e.g. top 10 tokens) - as the issue is the moment you sample one crazy token, then the sequence is poisoned with that for future inference.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lzhqz8","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhqz8/responses_keep_dissolving_into_word_salad_how_to/n31zvnk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752490808,"author_flair_text":null,"collapsed":false,"created_utc":1752490808,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":0,"name":"t1__","id":"_","parent_id":"t1_n3289v9","depth":10,"children":[]}}],"before":null}},"user_reports":[],"saved":false,"id":"n3289v9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1752494493,"send_replies":true,"parent_id":"t1_n327ymj","score":1,"author_fullname":"t2_uz37qfx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt; And I agree about lower temperatures making the model harder to steer, \\n\\nhigher temperature make it hardert, not lower.\\n\\n&gt; I hadn't heard that Min-P needs Top P on, though. I've never run into any problems with Top P off using Min-P. Do you have anything I can read about that by chance?\\n\\njust trial and error.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3289v9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;And I agree about lower temperatures making the model harder to steer, &lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;higher temperature make it hardert, not lower.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;I hadn&amp;#39;t heard that Min-P needs Top P on, though. I&amp;#39;ve never run into any problems with Top P off using Min-P. Do you have anything I can read about that by chance?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;just trial and error.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhqz8/responses_keep_dissolving_into_word_salad_how_to/n3289v9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752494493,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzhqz8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":9,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n327ymj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GlowingPulsar","can_mod_post":false,"created_utc":1752494364,"send_replies":true,"parent_id":"t1_n325y56","score":1,"author_fullname":"t2_1eeuvibnme","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I will say that newer model releases are gravitating towards lower temperatures, so I appreciate what you're saying. And I agree about lower temperatures making the model harder to steer, which is why I suggested the ranges I did. I mostly use Mistral models which I've found handle higher temperatures better than others, so I'm probably biased there. Top N Sigma performs well with high temperatures, as far as I've seen, better than Min-P.\\n\\nI hadn't heard that Min-P needs Top P on, though. I've never run into any problems with Top P off using Min-P. Do you have anything I can read about that by chance?","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n327ymj","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I will say that newer model releases are gravitating towards lower temperatures, so I appreciate what you&amp;#39;re saying. And I agree about lower temperatures making the model harder to steer, which is why I suggested the ranges I did. I mostly use Mistral models which I&amp;#39;ve found handle higher temperatures better than others, so I&amp;#39;m probably biased there. Top N Sigma performs well with high temperatures, as far as I&amp;#39;ve seen, better than Min-P.&lt;/p&gt;\\n\\n&lt;p&gt;I hadn&amp;#39;t heard that Min-P needs Top P on, though. I&amp;#39;ve never run into any problems with Top P off using Min-P. Do you have anything I can read about that by chance?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhqz8/responses_keep_dissolving_into_word_salad_how_to/n327ymj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752494364,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzhqz8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":8,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n325y56","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1752493533,"send_replies":true,"parent_id":"t1_n325bms","score":1,"author_fullname":"t2_uz37qfx5","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Min_p and top_p need to be used together.\\n\\n&gt; Low temperatures like 0.5-0.6 may be better for some models like Qwen or Deepseek, usually thinking models, but those aren't necessarily better at creative-writing. Lower temperatures are for when you want accuracy, not creativity. \\n\\nYou may increase temp for the outline and brainstorming, but for actual fleshing out the writing plan you want every part to deviate as little as possible from the plan, and you'also want the chapters to start and end exactly where the plan says. With temperature &gt;= 0.7 models became very difficult to steer, and begin annoyingly deviate from the plan. Yes I do sometimes increase to to 0.8-1 when it gets stuck, but normally I use dynatemp 0.5+-0.25.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n325y56","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Min_p and top_p need to be used together.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Low temperatures like 0.5-0.6 may be better for some models like Qwen or Deepseek, usually thinking models, but those aren&amp;#39;t necessarily better at creative-writing. Lower temperatures are for when you want accuracy, not creativity. &lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;You may increase temp for the outline and brainstorming, but for actual fleshing out the writing plan you want every part to deviate as little as possible from the plan, and you&amp;#39;also want the chapters to start and end exactly where the plan says. With temperature &amp;gt;= 0.7 models became very difficult to steer, and begin annoyingly deviate from the plan. Yes I do sometimes increase to to 0.8-1 when it gets stuck, but normally I use dynatemp 0.5+-0.25.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lzhqz8","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhqz8/responses_keep_dissolving_into_word_salad_how_to/n325y56/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752493533,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n325bms","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GlowingPulsar","can_mod_post":false,"send_replies":true,"parent_id":"t1_n323oud","score":1,"author_fullname":"t2_1eeuvibnme","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Min-P and Top N Sigma are superior to Top P and Top K in my experience, but I'm sure some models do well with Top K and P on. Low temperatures like 0.5-0.6 may be better for some models like Qwen or Deepseek, usually thinking models, but those aren't necessarily better at creative-writing. Lower temperatures are for when you want accuracy, not creativity. I don't use XTC much, but I would agree it can potentially make the model be a little too outlandish or dumb in its responses.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n325bms","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Min-P and Top N Sigma are superior to Top P and Top K in my experience, but I&amp;#39;m sure some models do well with Top K and P on. Low temperatures like 0.5-0.6 may be better for some models like Qwen or Deepseek, usually thinking models, but those aren&amp;#39;t necessarily better at creative-writing. Lower temperatures are for when you want accuracy, not creativity. I don&amp;#39;t use XTC much, but I would agree it can potentially make the model be a little too outlandish or dumb in its responses.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lzhqz8","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhqz8/responses_keep_dissolving_into_word_salad_how_to/n325bms/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752493267,"author_flair_text":null,"treatment_tags":[],"created_utc":1752493267,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n323oud","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n322tep","score":9,"author_fullname":"t2_uz37qfx5","approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; I'd suggest keeping Top P set to 1 (off), Top K set to 0 (off),\\n\\nrecepe to junk in the output. I set Top_P=0.9, Top_K=40.\\n\\n&gt; Temperature: Between 1-1.35 \\n\\nInsanely high, esp if model is already misbehaving. 0.5-0.6 is optimum. \\n\\n&gt; XTC Threshold: 0.15 Probability: 0.5\\n\\ndumbs the model.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n323oud","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;I&amp;#39;d suggest keeping Top P set to 1 (off), Top K set to 0 (off),&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;recepe to junk in the output. I set Top_P=0.9, Top_K=40.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Temperature: Between 1-1.35 &lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Insanely high, esp if model is already misbehaving. 0.5-0.6 is optimum. &lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;XTC Threshold: 0.15 Probability: 0.5&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;dumbs the model.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lzhqz8","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhqz8/responses_keep_dissolving_into_word_salad_how_to/n323oud/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752492555,"author_flair_text":null,"treatment_tags":[],"created_utc":1752492555,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}}],"before":null}},"user_reports":[],"saved":false,"id":"n322tep","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GlowingPulsar","can_mod_post":false,"send_replies":true,"parent_id":"t1_n31sj98","score":3,"author_fullname":"t2_1eeuvibnme","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'd suggest keeping Top P set to 1 (off), Top K set to 0 (off), and repetition penalty set to 1, or very slightly higher if you must use it (no higher than 1.1). I'm going to include DRY and XTC settings, which can help with repetition, but I would only turn them on if you're still seeing too much repetition for your own personal tastes.\\n\\nTemperature: Between 1-1.35 (Adjust as needed. Some models may perform better at lower temperatures, but many that are well-suited for creative-writing will perform well between 1-1.35)\\n\\nDRY Multiplier: 0.8 Base: 1.75 Allowed length: 2\\n\\nXTC Threshold: 0.15 Probability: 0.5\\n\\nMin-P: 0.05 OR (As in only one or the other)\\nTop N Sigma: 1","edited":1752492502,"author_flair_css_class":null,"name":"t1_n322tep","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;d suggest keeping Top P set to 1 (off), Top K set to 0 (off), and repetition penalty set to 1, or very slightly higher if you must use it (no higher than 1.1). I&amp;#39;m going to include DRY and XTC settings, which can help with repetition, but I would only turn them on if you&amp;#39;re still seeing too much repetition for your own personal tastes.&lt;/p&gt;\\n\\n&lt;p&gt;Temperature: Between 1-1.35 (Adjust as needed. Some models may perform better at lower temperatures, but many that are well-suited for creative-writing will perform well between 1-1.35)&lt;/p&gt;\\n\\n&lt;p&gt;DRY Multiplier: 0.8 Base: 1.75 Allowed length: 2&lt;/p&gt;\\n\\n&lt;p&gt;XTC Threshold: 0.15 Probability: 0.5&lt;/p&gt;\\n\\n&lt;p&gt;Min-P: 0.05 OR (As in only one or the other)\\nTop N Sigma: 1&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lzhqz8","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhqz8/responses_keep_dissolving_into_word_salad_how_to/n322tep/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752492167,"author_flair_text":null,"collapsed":false,"created_utc":1752492167,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n356k9t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Limp_Classroom_2645","can_mod_post":false,"send_replies":true,"parent_id":"t1_n31sj98","score":1,"author_fullname":"t2_1lwf5vg68e","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"reset the values to defaults","edited":false,"author_flair_css_class":null,"name":"t1_n356k9t","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;reset the values to defaults&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lzhqz8","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhqz8/responses_keep_dissolving_into_word_salad_how_to/n356k9t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752525999,"author_flair_text":null,"collapsed":false,"created_utc":1752525999,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n31sj98","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Gilgameshcomputing","can_mod_post":false,"send_replies":true,"parent_id":"t1_n31qqey","score":3,"author_fullname":"t2_i5ettea7e","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thankyou. I've been messing with 'em forever and didn't know they were called that!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n31sj98","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thankyou. I&amp;#39;ve been messing with &amp;#39;em forever and didn&amp;#39;t know they were called that!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzhqz8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhqz8/responses_keep_dissolving_into_word_salad_how_to/n31sj98/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752486979,"author_flair_text":null,"treatment_tags":[],"created_utc":1752486979,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n31qqey","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"jabies","can_mod_post":false,"send_replies":true,"parent_id":"t1_n31q5m5","score":13,"author_fullname":"t2_4zcpj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Samplers are the thing you adjust with temperature, min p, etc. ","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n31qqey","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Samplers are the thing you adjust with temperature, min p, etc. &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzhqz8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhqz8/responses_keep_dissolving_into_word_salad_how_to/n31qqey/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752485958,"author_flair_text":null,"treatment_tags":[],"created_utc":1752485958,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":13}}],"before":null}},"user_reports":[],"saved":false,"id":"n31q5m5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Gilgameshcomputing","can_mod_post":false,"created_utc":1752485625,"send_replies":true,"parent_id":"t1_n31pq7z","score":0,"author_fullname":"t2_i5ettea7e","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thank you, interesting. I've not come across samplers for LLM before. I've used use LMStudio, MindMac, and in this case Msty. I've had it happen on all of them, on the default settings.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n31q5m5","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you, interesting. I&amp;#39;ve not come across samplers for LLM before. I&amp;#39;ve used use LMStudio, MindMac, and in this case Msty. I&amp;#39;ve had it happen on all of them, on the default settings.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzhqz8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhqz8/responses_keep_dissolving_into_word_salad_how_to/n31q5m5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752485625,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n31pq7z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"lothariusdark","can_mod_post":false,"created_utc":1752485375,"send_replies":true,"parent_id":"t3_1lzhqz8","score":24,"author_fullname":"t2_idhb522c","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This issue cant be fixed with prompting.\\n\\nThis is related to samplers or maybe the inference code itself if the model isnt supported.\\n\\nWhat are you using to interact with these models? I dont recognize the program.\\n\\nWhat settings did you manually change? Did you try to reset to default settings?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n31pq7z","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This issue cant be fixed with prompting.&lt;/p&gt;\\n\\n&lt;p&gt;This is related to samplers or maybe the inference code itself if the model isnt supported.&lt;/p&gt;\\n\\n&lt;p&gt;What are you using to interact with these models? I dont recognize the program.&lt;/p&gt;\\n\\n&lt;p&gt;What settings did you manually change? Did you try to reset to default settings?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhqz8/responses_keep_dissolving_into_word_salad_how_to/n31pq7z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752485375,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzhqz8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":24}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n31q5xb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Square-Onion-1825","can_mod_post":false,"created_utc":1752485630,"send_replies":true,"parent_id":"t3_1lzhqz8","score":10,"author_fullname":"t2_1mkh7x2yxn","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"sounds like context window overflow","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n31q5xb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;sounds like context window overflow&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhqz8/responses_keep_dissolving_into_word_salad_how_to/n31q5xb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752485630,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzhqz8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n31ta1s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Awwtifishal","can_mod_post":false,"created_utc":1752487393,"send_replies":true,"parent_id":"t3_1lzhqz8","score":5,"author_fullname":"t2_1d96a8k10t","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You may have repetition penalty a bit too high. It's one of the most common samplers to avoid repetition, but in my experience others like DRY and XTC work better.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n31ta1s","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You may have repetition penalty a bit too high. It&amp;#39;s one of the most common samplers to avoid repetition, but in my experience others like DRY and XTC work better.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhqz8/responses_keep_dissolving_into_word_salad_how_to/n31ta1s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752487393,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzhqz8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"7d1f04e6-4920-11ef-b2e1-2e580594e1a1","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n328g0g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"__lawless","can_mod_post":false,"created_utc":1752494562,"send_replies":true,"parent_id":"t3_1lzhqz8","score":2,"author_fullname":"t2_4z4vgfw9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Are you sure EOS is set properly?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n328g0g","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 3.1"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Are you sure EOS is set properly?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhqz8/responses_keep_dissolving_into_word_salad_how_to/n328g0g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752494562,"author_flair_text":"Llama 3.1","treatment_tags":[],"link_id":"t3_1lzhqz8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#93b1ba","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"d2642412-d9ce-11ed-ae30-32b11309f5bd","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n33xajt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n32ojo3","score":2,"author_fullname":"t2_uz37qfx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt; So possibly dumb question - the probability % next to each token is the likelihood that that token will be chosen at the moment of generation? So if the word \\"hi\\" is at 50% and the word \\"hello\\" is at 25%, and the rest of the tokens make up the remaining 25% and if I just keep re-generating that token a million times, then \\"hi\\" will be chosen roughly 50% of the time, and so on?\\n\\nExactly right.\\n\\n&gt; If I understand that right, how does the model know which tokens to put into consideration in the first place? \\n\\nThis is the magic of transformer.\\n\\n&gt; And how many tokens does a model put into that initial list to choose from? Is there some hard-coded number of tokens that the model puts into the running - like say 100, and is there already an inherent probability distribution before the samplers manipulate the probability (and the number of tokens in the running)?\\n\\nWithout top_k set the number of choices is equal to vocabulary size, normaly around 150000 (yes, 150 thousands). Wuth top_k set it is eqaul to top_k.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n33xajt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;So possibly dumb question - the probability % next to each token is the likelihood that that token will be chosen at the moment of generation? So if the word &amp;quot;hi&amp;quot; is at 50% and the word &amp;quot;hello&amp;quot; is at 25%, and the rest of the tokens make up the remaining 25% and if I just keep re-generating that token a million times, then &amp;quot;hi&amp;quot; will be chosen roughly 50% of the time, and so on?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Exactly right.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;If I understand that right, how does the model know which tokens to put into consideration in the first place? &lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;This is the magic of transformer.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;And how many tokens does a model put into that initial list to choose from? Is there some hard-coded number of tokens that the model puts into the running - like say 100, and is there already an inherent probability distribution before the samplers manipulate the probability (and the number of tokens in the running)?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Without top_k set the number of choices is equal to vocabulary size, normaly around 150000 (yes, 150 thousands). Wuth top_k set it is eqaul to top_k.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lzhqz8","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhqz8/responses_keep_dissolving_into_word_salad_how_to/n33xajt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752513277,"author_flair_text":null,"treatment_tags":[],"created_utc":1752513277,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n32ojo3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"YearZero","can_mod_post":false,"send_replies":true,"parent_id":"t1_n324rkr","score":1,"author_fullname":"t2_4kpsn","approved_by":null,"mod_note":null,"all_awardings":[],"body":"So possibly dumb question - the probability % next to each token is the likelihood that that token will be chosen at the moment of generation? So if the word \\"hi\\" is at 50% and the word \\"hello\\" is at 25%, and the rest of the tokens make up the remaining 25% and if I just keep re-generating that token a million times, then \\"hi\\" will be chosen roughly 50% of the time, and so on?\\n\\nEven the token with the lowest probability, as long as it's in the running and not eliminated from the list by top-k or something will always have that % chance of being chosen when the token is generated, right?\\n\\nThe model creates a list of tokens to choose from, but the samplers manipulate their probabilities (and how many are available to choose from)?\\n\\nIf I understand that right, how does the model know which tokens to put into consideration in the first place? And how many tokens does a model put into that initial list to choose from? Is there some hard-coded number of tokens that the model puts into the running - like say 100, and is there already an inherent probability distribution before the samplers manipulate the probability (and the number of tokens in the running)?\\n\\nOk I think I answered my own question by doing a bit of legwork here. All tokens in the vocabulary are up for consideration, and they are all given an initial probability score called logits. Then a sampling function called \\"softmax\\" turns the logit scores into an actual probability distribution. Samplers manipulate it and whittle it down after that. Then to make the final \\"choice\\", it's just a sampling algorithm, but ultimately it just follows whatever the model + samplers have available as far as the tokens and their probabilities, and chooses one of the tokens based on its probability.\\n\\nYou can make it as greedy (always choose the highest probability one by giving it 100% probability every time) or as \\"random/chaotic\\" (the probability of all tokens being very close to one another so it's almost random which one is chosen) as you want using samplers.\\n\\nSo the real \\"magic\\" is the model's initial logit scores of all possible tokens in its vocabulary.","edited":1752502041,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n32ojo3","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So possibly dumb question - the probability % next to each token is the likelihood that that token will be chosen at the moment of generation? So if the word &amp;quot;hi&amp;quot; is at 50% and the word &amp;quot;hello&amp;quot; is at 25%, and the rest of the tokens make up the remaining 25% and if I just keep re-generating that token a million times, then &amp;quot;hi&amp;quot; will be chosen roughly 50% of the time, and so on?&lt;/p&gt;\\n\\n&lt;p&gt;Even the token with the lowest probability, as long as it&amp;#39;s in the running and not eliminated from the list by top-k or something will always have that % chance of being chosen when the token is generated, right?&lt;/p&gt;\\n\\n&lt;p&gt;The model creates a list of tokens to choose from, but the samplers manipulate their probabilities (and how many are available to choose from)?&lt;/p&gt;\\n\\n&lt;p&gt;If I understand that right, how does the model know which tokens to put into consideration in the first place? And how many tokens does a model put into that initial list to choose from? Is there some hard-coded number of tokens that the model puts into the running - like say 100, and is there already an inherent probability distribution before the samplers manipulate the probability (and the number of tokens in the running)?&lt;/p&gt;\\n\\n&lt;p&gt;Ok I think I answered my own question by doing a bit of legwork here. All tokens in the vocabulary are up for consideration, and they are all given an initial probability score called logits. Then a sampling function called &amp;quot;softmax&amp;quot; turns the logit scores into an actual probability distribution. Samplers manipulate it and whittle it down after that. Then to make the final &amp;quot;choice&amp;quot;, it&amp;#39;s just a sampling algorithm, but ultimately it just follows whatever the model + samplers have available as far as the tokens and their probabilities, and chooses one of the tokens based on its probability.&lt;/p&gt;\\n\\n&lt;p&gt;You can make it as greedy (always choose the highest probability one by giving it 100% probability every time) or as &amp;quot;random/chaotic&amp;quot; (the probability of all tokens being very close to one another so it&amp;#39;s almost random which one is chosen) as you want using samplers.&lt;/p&gt;\\n\\n&lt;p&gt;So the real &amp;quot;magic&amp;quot; is the model&amp;#39;s initial logit scores of all possible tokens in its vocabulary.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lzhqz8","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhqz8/responses_keep_dissolving_into_word_salad_how_to/n32ojo3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752500341,"author_flair_text":null,"treatment_tags":[],"created_utc":1752500341,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n324rkr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Vancha","can_mod_post":false,"send_replies":true,"parent_id":"t1_n31rhjw","score":3,"author_fullname":"t2_5htqe","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"[This](https://artefact2.github.io/llm-sampling/index.xhtml) offers a pretty intuitive representation of what a lot of them do.\\n\\nThat said, I'll take an early bet on the repetition penalty being too high.","edited":false,"author_flair_css_class":null,"name":"t1_n324rkr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://artefact2.github.io/llm-sampling/index.xhtml\\"&gt;This&lt;/a&gt; offers a pretty intuitive representation of what a lot of them do.&lt;/p&gt;\\n\\n&lt;p&gt;That said, I&amp;#39;ll take an early bet on the repetition penalty being too high.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lzhqz8","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhqz8/responses_keep_dissolving_into_word_salad_how_to/n324rkr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752493027,"author_flair_text":null,"collapsed":false,"created_utc":1752493027,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n31rhjw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Gilgameshcomputing","can_mod_post":false,"send_replies":true,"parent_id":"t1_n31qt7n","score":1,"author_fullname":"t2_i5ettea7e","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Christ that's a lot of numbers &amp; variables for someone who went to art school! Thank you, I shall dig into this and see how much understanding I can uncover.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n31rhjw","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Christ that&amp;#39;s a lot of numbers &amp;amp; variables for someone who went to art school! Thank you, I shall dig into this and see how much understanding I can uncover.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzhqz8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhqz8/responses_keep_dissolving_into_word_salad_how_to/n31rhjw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752486387,"author_flair_text":null,"treatment_tags":[],"created_utc":1752486387,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n31qt7n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"jabies","can_mod_post":false,"send_replies":true,"parent_id":"t1_n31pomr","score":6,"author_fullname":"t2_4zcpj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"https://www.reddit.com/r/LocalLLaMA/comments/17vonjo/your_settings_are_probably_hurting_your_model_why/","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n31qt7n","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/17vonjo/your_settings_are_probably_hurting_your_model_why/\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/17vonjo/your_settings_are_probably_hurting_your_model_why/&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzhqz8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhqz8/responses_keep_dissolving_into_word_salad_how_to/n31qt7n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752486002,"author_flair_text":null,"treatment_tags":[],"created_utc":1752486002,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n31pomr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Gilgameshcomputing","can_mod_post":false,"created_utc":1752485349,"send_replies":true,"parent_id":"t1_n31ow13","score":1,"author_fullname":"t2_i5ettea7e","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I haven't seen settings for samplers on the frontends I've used (including LMStudio, MindMac, Msty, and another one I'm currently blanking on), so the default one I guess?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n31pomr","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I haven&amp;#39;t seen settings for samplers on the frontends I&amp;#39;ve used (including LMStudio, MindMac, Msty, and another one I&amp;#39;m currently blanking on), so the default one I guess?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzhqz8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhqz8/responses_keep_dissolving_into_word_salad_how_to/n31pomr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752485349,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n31ow13","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RevolutionaryKiwi541","can_mod_post":false,"created_utc":1752484885,"send_replies":true,"parent_id":"t3_1lzhqz8","score":2,"author_fullname":"t2_1o13744qvm","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"what samplers do you have set?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n31ow13","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Alpaca"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;what samplers do you have set?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhqz8/responses_keep_dissolving_into_word_salad_how_to/n31ow13/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752484885,"author_flair_text":"Alpaca","treatment_tags":[],"link_id":"t3_1lzhqz8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bd9e9e","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n32f72p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"createthiscom","can_mod_post":false,"created_utc":1752497145,"send_replies":true,"parent_id":"t3_1lzhqz8","score":1,"author_fullname":"t2_ozxxf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What's your inference engine? ktransformers does stuff like this once there is a memory leak. I've yet to see it happen with llama.cpp though.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n32f72p","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What&amp;#39;s your inference engine? ktransformers does stuff like this once there is a memory leak. I&amp;#39;ve yet to see it happen with llama.cpp though.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhqz8/responses_keep_dissolving_into_word_salad_how_to/n32f72p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752497145,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzhqz8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n32wn7a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kyazoglu","can_mod_post":false,"created_utc":1752502864,"send_replies":true,"parent_id":"t3_1lzhqz8","score":1,"author_fullname":"t2_slwqrxz3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This is a sampler misconfig issue. I have encountered it many times. Try to tune the penalty terms.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n32wn7a","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is a sampler misconfig issue. I have encountered it many times. Try to tune the penalty terms.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhqz8/responses_keep_dissolving_into_word_salad_how_to/n32wn7a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752502864,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzhqz8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n336ihb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"martinerous","can_mod_post":false,"created_utc":1752505729,"send_replies":true,"parent_id":"t3_1lzhqz8","score":1,"author_fullname":"t2_5tp54ey","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I usually experienced this in Backyard AI when I set the context length too large for the model, even when the actual data length in the context was far from the model's limit.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n336ihb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I usually experienced this in Backyard AI when I set the context length too large for the model, even when the actual data length in the context was far from the model&amp;#39;s limit.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhqz8/responses_keep_dissolving_into_word_salad_how_to/n336ihb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752505729,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzhqz8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n37t9wd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"the320x200","can_mod_post":false,"created_utc":1752560444,"send_replies":true,"parent_id":"t3_1lzhqz8","score":1,"author_fullname":"t2_5m9yw","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This often means your temperature parameter is way too high.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n37t9wd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This often means your temperature parameter is way too high.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhqz8/responses_keep_dissolving_into_word_salad_how_to/n37t9wd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752560444,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzhqz8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3872tl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SpiritualWindow3855","can_mod_post":false,"created_utc":1752568279,"send_replies":true,"parent_id":"t3_1lzhqz8","score":1,"author_fullname":"t2_i7pimyxj5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Heads up: Kimi K2 might be served by NovitaAI if you use OpenRouter.\\n\\nTheir models tend to be broken, K2 is broken with them, and frankly I have no idea why they're allowed on OpenRouter.\\n\\nGo to your account settings, and add them to Ignored Providers.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3872tl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Heads up: Kimi K2 might be served by NovitaAI if you use OpenRouter.&lt;/p&gt;\\n\\n&lt;p&gt;Their models tend to be broken, K2 is broken with them, and frankly I have no idea why they&amp;#39;re allowed on OpenRouter.&lt;/p&gt;\\n\\n&lt;p&gt;Go to your account settings, and add them to Ignored Providers.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzhqz8/responses_keep_dissolving_into_word_salad_how_to/n3872tl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752568279,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzhqz8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(t,{data:l});export{r as default};
