import{j as e}from"./index-M4edQi1P.js";import{R as l}from"./RedditPostRenderer-CESBGIIy.js";import"./index-DFpL1mt4.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hello All,\\n\\nI'm new to gen AI. I'm learning the basics, but I know that I will be getting my hands occupied in a couple of weeks with hands-on models. I currently have a very old GPU (1070 TI) which I game on. I want to bring another card (was thinking of the 5060 TI 16 GB version).\\n\\nI know that 24 GB+ (or I think it is) is the sweet spot for LLMs, but I would like to know if I can pair my old 1070 TI, which already has 8 GB, with the 16 GB of the 5060 TI.\\n\\nDoes having 2 separate GPUs affect how your models work?\\n\\nAnd if I'm running both GPUs, will I have to upgrade my current 800 W PSU?\\n\\nBelow are my old GPU specs\\n\\nThank you again for your time.\\n\\nhttps://preview.redd.it/cbq4ynspjhdf1.png?width=851&amp;format=png&amp;auto=webp&amp;s=c3ebfae2f15cfc68ef82cde137fab3344180d5b5\\n\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"GPU advice for running local LLMs","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"media_metadata":{"cbq4ynspjhdf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":103,"x":108,"u":"https://preview.redd.it/cbq4ynspjhdf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c92d7b3385305d4eb1a3813ce86231611fa2f082"},{"y":207,"x":216,"u":"https://preview.redd.it/cbq4ynspjhdf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=dea12fd39cc277e1663262039373d3884ea7c3e4"},{"y":307,"x":320,"u":"https://preview.redd.it/cbq4ynspjhdf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=da41d4f0e12fd21b631387e0265069d452450c1d"},{"y":614,"x":640,"u":"https://preview.redd.it/cbq4ynspjhdf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=234f325a4d1bcfde169ab28a14dbc8be9397403f"}],"s":{"y":817,"x":851,"u":"https://preview.redd.it/cbq4ynspjhdf1.png?width=851&amp;format=png&amp;auto=webp&amp;s=c3ebfae2f15cfc68ef82cde137fab3344180d5b5"},"id":"cbq4ynspjhdf1"}},"name":"t3_1m2gy2t","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.66,"author_flair_background_color":null,"subreddit_type":"public","ups":1,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1tr7w997vf","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":1,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752780708,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hello All,&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m new to gen AI. I&amp;#39;m learning the basics, but I know that I will be getting my hands occupied in a couple of weeks with hands-on models. I currently have a very old GPU (1070 TI) which I game on. I want to bring another card (was thinking of the 5060 TI 16 GB version).&lt;/p&gt;\\n\\n&lt;p&gt;I know that 24 GB+ (or I think it is) is the sweet spot for LLMs, but I would like to know if I can pair my old 1070 TI, which already has 8 GB, with the 16 GB of the 5060 TI.&lt;/p&gt;\\n\\n&lt;p&gt;Does having 2 separate GPUs affect how your models work?&lt;/p&gt;\\n\\n&lt;p&gt;And if I&amp;#39;m running both GPUs, will I have to upgrade my current 800 W PSU?&lt;/p&gt;\\n\\n&lt;p&gt;Below are my old GPU specs&lt;/p&gt;\\n\\n&lt;p&gt;Thank you again for your time.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/cbq4ynspjhdf1.png?width=851&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c3ebfae2f15cfc68ef82cde137fab3344180d5b5\\"&gt;https://preview.redd.it/cbq4ynspjhdf1.png?width=851&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c3ebfae2f15cfc68ef82cde137fab3344180d5b5&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m2gy2t","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Negative_Owl_6623","discussion_type":null,"num_comments":26,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m2gy2t/gpu_advice_for_running_local_llms/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m2gy2t/gpu_advice_for_running_local_llms/","subreddit_subscribers":500896,"created_utc":1752780708,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ri84b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Blizado","can_mod_post":false,"created_utc":1752814691,"send_replies":true,"parent_id":"t1_n3prgbf","score":1,"author_fullname":"t2_j0e2r","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Should be also faster then 5060 TI + normal RAM. If he use GGUF and split it between GPU and CPU.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ri84b","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Should be also faster then 5060 TI + normal RAM. If he use GGUF and split it between GPU and CPU.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2gy2t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2gy2t/gpu_advice_for_running_local_llms/n3ri84b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752814691,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3prgbf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"The_GSingh","can_mod_post":false,"created_utc":1752791328,"send_replies":true,"parent_id":"t3_1m2gy2t","score":2,"author_fullname":"t2_fy4qc98m","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Get the 5060 ti (only if you can’t get a cheap 3090) and then use that only to run most llms. When you can’t fit a llm onto the 5060 ti, then and only then add the second gpu. \\n\\nYour 1070ti will slow the llm considerably but it’ll be way faster than cpu only anyways.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3prgbf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Get the 5060 ti (only if you can’t get a cheap 3090) and then use that only to run most llms. When you can’t fit a llm onto the 5060 ti, then and only then add the second gpu. &lt;/p&gt;\\n\\n&lt;p&gt;Your 1070ti will slow the llm considerably but it’ll be way faster than cpu only anyways.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2gy2t/gpu_advice_for_running_local_llms/n3prgbf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752791328,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2gy2t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3rhw3m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Blizado","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3pjbvk","score":1,"author_fullname":"t2_j0e2r","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"16GB VRAM is a good starting point. More is of course always better, but with 16GB VRAM you can already use very decent LLMs. But it also depends what exactly you want to do, because the context size of your prompt also needs VRAM, so if you want to use a lot of context, you have less VRAM for the LLM itself.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3rhw3m","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;16GB VRAM is a good starting point. More is of course always better, but with 16GB VRAM you can already use very decent LLMs. But it also depends what exactly you want to do, because the context size of your prompt also needs VRAM, so if you want to use a lot of context, you have less VRAM for the LLM itself.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2gy2t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2gy2t/gpu_advice_for_running_local_llms/n3rhw3m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752814534,"author_flair_text":null,"treatment_tags":[],"created_utc":1752814534,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3s01f8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3pjbvk","score":1,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Do not listen to the dude, he does not know what he is talking about and says silly stuff. Keep both cards (I have 3060 and a similar to 1070 mining card too), you can load smaller models into the 5060ti (to preserve performance) only and bigger models split between two. This way it will massively faster than spilling big model to CPU.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3s01f8","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Do not listen to the dude, he does not know what he is talking about and says silly stuff. Keep both cards (I have 3060 and a similar to 1070 mining card too), you can load smaller models into the 5060ti (to preserve performance) only and bigger models split between two. This way it will massively faster than spilling big model to CPU.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2gy2t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2gy2t/gpu_advice_for_running_local_llms/n3s01f8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752823939,"author_flair_text":null,"treatment_tags":[],"created_utc":1752823939,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3pjbvk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Negative_Owl_6623","can_mod_post":false,"created_utc":1752788768,"send_replies":true,"parent_id":"t1_n3p72eo","score":1,"author_fullname":"t2_1tr7w997vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Many thanks mate.\\n\\n  \\nWell, I found this video  \\n[https://www.youtube.com/watch?v=khH2dCs0cM4](https://www.youtube.com/watch?v=khH2dCs0cM4)\\n\\nIt made a comparison between the 50 series and how each VRAM amount can take\\n\\nhttps://preview.redd.it/5zc6p4sr8idf1.png?width=1230&amp;format=png&amp;auto=webp&amp;s=7ae03b80b7dd86da33175a767b015518db0044b1\\n\\n\\n\\n  \\nIs 16GB a good amount of VRam for someone who's just trying to learn and not getting too deep or serious about the field?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3pjbvk","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Many thanks mate.&lt;/p&gt;\\n\\n&lt;p&gt;Well, I found this video&lt;br/&gt;\\n&lt;a href=\\"https://www.youtube.com/watch?v=khH2dCs0cM4\\"&gt;https://www.youtube.com/watch?v=khH2dCs0cM4&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;It made a comparison between the 50 series and how each VRAM amount can take&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/5zc6p4sr8idf1.png?width=1230&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7ae03b80b7dd86da33175a767b015518db0044b1\\"&gt;https://preview.redd.it/5zc6p4sr8idf1.png?width=1230&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7ae03b80b7dd86da33175a767b015518db0044b1&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Is 16GB a good amount of VRam for someone who&amp;#39;s just trying to learn and not getting too deep or serious about the field?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2gy2t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2gy2t/gpu_advice_for_running_local_llms/n3pjbvk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752788768,"media_metadata":{"5zc6p4sr8idf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":39,"x":108,"u":"https://preview.redd.it/5zc6p4sr8idf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c0bc895d9bd050e3cb1d957901d936b57e5d017a"},{"y":78,"x":216,"u":"https://preview.redd.it/5zc6p4sr8idf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=75d10ce4c6f55ba4e38b3f48fd6bd83af140e303"},{"y":116,"x":320,"u":"https://preview.redd.it/5zc6p4sr8idf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b05d4da27351e2255fbe7f11e6ba9d7630695f35"},{"y":233,"x":640,"u":"https://preview.redd.it/5zc6p4sr8idf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=552be5931b362f70f5eb29fc2a7f78642106688f"},{"y":349,"x":960,"u":"https://preview.redd.it/5zc6p4sr8idf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6e7fa4cab4c0884ac8276af0e79e23eb111ee445"},{"y":393,"x":1080,"u":"https://preview.redd.it/5zc6p4sr8idf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=47ca6244a637f04366abd0d5995156440f10b97e"}],"s":{"y":448,"x":1230,"u":"https://preview.redd.it/5zc6p4sr8idf1.png?width=1230&amp;format=png&amp;auto=webp&amp;s=7ae03b80b7dd86da33175a767b015518db0044b1"},"id":"5zc6p4sr8idf1"}},"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3u74mj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Blizado","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3u4vzy","score":1,"author_fullname":"t2_j0e2r","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I do? That was my whole point in my first post.\\n\\n\\"For performance it would be better to use the 5060TI only.\\"\\n\\nWith that I meant exactly this, but as I said, that was maybe not clear enough.","edited":false,"author_flair_css_class":null,"name":"t1_n3u74mj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I do? That was my whole point in my first post.&lt;/p&gt;\\n\\n&lt;p&gt;&amp;quot;For performance it would be better to use the 5060TI only.&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;With that I meant exactly this, but as I said, that was maybe not clear enough.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m2gy2t","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2gy2t/gpu_advice_for_running_local_llms/n3u74mj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752854703,"author_flair_text":null,"collapsed":false,"created_utc":1752854703,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3u4vzy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3ty8fo","score":1,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt; Yeah, I'm not one of the users who give up a lot of performance only to run a bigger model, which are in general slower. So yes, I tend to always point to performance as other tend to sacrifice performance for bigger models. It always feels like two different religions come together on this topic.\\n\\nWhat are talking about? Just disable the smaller GPU when you run models that can fit in 5060 and enable it back when you run bigger models. You are still missing a point.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3u4vzy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Yeah, I&amp;#39;m not one of the users who give up a lot of performance only to run a bigger model, which are in general slower. So yes, I tend to always point to performance as other tend to sacrifice performance for bigger models. It always feels like two different religions come together on this topic.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;What are talking about? Just disable the smaller GPU when you run models that can fit in 5060 and enable it back when you run bigger models. You are still missing a point.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2gy2t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2gy2t/gpu_advice_for_running_local_llms/n3u4vzy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752854066,"author_flair_text":null,"treatment_tags":[],"created_utc":1752854066,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ty8fo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Blizado","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3s04xa","score":1,"author_fullname":"t2_j0e2r","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah, great to begin your post with \\"absolute crap\\". I could do this as well. First I said \\"for performance\\", I didn't said that this GPU working together is not an option. Second I never wrote about splitting a larger model over GPU and CPU, that this should be avoided is crystal clear. And I wrote that I was unsure about that age gap of the both GPU. If they work flawless together, good to know, thanks.\\n\\nBut yes, I could have been a bit more clear on the \\"5060TI only\\" statement, where I meant really only the 5060TI, no CPU split. And you are also right on the more context stuff with a second GPU.\\n\\nYeah, I'm not one of the users who give up a lot of performance only to run a bigger model, which are in general slower. So yes, I tend to always point to performance as other tend to sacrifice performance for bigger models. It always feels like two different religions come together on this topic.","edited":1752852422,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3ty8fo","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, great to begin your post with &amp;quot;absolute crap&amp;quot;. I could do this as well. First I said &amp;quot;for performance&amp;quot;, I didn&amp;#39;t said that this GPU working together is not an option. Second I never wrote about splitting a larger model over GPU and CPU, that this should be avoided is crystal clear. And I wrote that I was unsure about that age gap of the both GPU. If they work flawless together, good to know, thanks.&lt;/p&gt;\\n\\n&lt;p&gt;But yes, I could have been a bit more clear on the &amp;quot;5060TI only&amp;quot; statement, where I meant really only the 5060TI, no CPU split. And you are also right on the more context stuff with a second GPU.&lt;/p&gt;\\n\\n&lt;p&gt;Yeah, I&amp;#39;m not one of the users who give up a lot of performance only to run a bigger model, which are in general slower. So yes, I tend to always point to performance as other tend to sacrifice performance for bigger models. It always feels like two different religions come together on this topic.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2gy2t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2gy2t/gpu_advice_for_running_local_llms/n3ty8fo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752852213,"author_flair_text":null,"treatment_tags":[],"created_utc":1752852213,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3s04xa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1752823996,"send_replies":true,"parent_id":"t1_n3p72eo","score":1,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What you've said is absolute crap; second card is always useful to have  for bigger models as 16 GiB is not enough for anything decent such Mistral Small, Qwen 32b or GLM4. Not using second card will cause spill to CPU which way slower than 1070. Besides second card also useful for larger context. You can also split in such way that the bulk of the model be in 5060ti and only small piece, like 2Gb will be 1070ti. You'll barely see a difference this way.There will be zero problems BTW running two different cards  1070ti and 5060ti on one llama.cpp instance.","edited":1752824213,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3s04xa","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What you&amp;#39;ve said is absolute crap; second card is always useful to have  for bigger models as 16 GiB is not enough for anything decent such Mistral Small, Qwen 32b or GLM4. Not using second card will cause spill to CPU which way slower than 1070. Besides second card also useful for larger context. You can also split in such way that the bulk of the model be in 5060ti and only small piece, like 2Gb will be 1070ti. You&amp;#39;ll barely see a difference this way.There will be zero problems BTW running two different cards  1070ti and 5060ti on one llama.cpp instance.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2gy2t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2gy2t/gpu_advice_for_running_local_llms/n3s04xa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752823996,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3p72eo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Blizado","can_mod_post":false,"created_utc":1752785171,"send_replies":true,"parent_id":"t3_1m2gy2t","score":1,"author_fullname":"t2_j0e2r","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"That are two very different GPUs. If you split a LLM over two GPUs the performance is set by the slower card, so your 1070 TI will limit your 5060 TI a lot. I don't know exactly if there are other problems with such age different generations of GPUs.\\n\\nFor performance it would be better to use the 5060TI only. You could use the 1070 TI for a second model, like for TTS, STT or something like that.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3p72eo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That are two very different GPUs. If you split a LLM over two GPUs the performance is set by the slower card, so your 1070 TI will limit your 5060 TI a lot. I don&amp;#39;t know exactly if there are other problems with such age different generations of GPUs.&lt;/p&gt;\\n\\n&lt;p&gt;For performance it would be better to use the 5060TI only. You could use the 1070 TI for a second model, like for TTS, STT or something like that.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2gy2t/gpu_advice_for_running_local_llms/n3p72eo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752785171,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2gy2t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3pt0od","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Afternoon_4260","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3pjtvm","score":2,"author_fullname":"t2_cj9kap4bx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I understand, out of curiosity where are you?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3pt0od","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I understand, out of curiosity where are you?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2gy2t","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2gy2t/gpu_advice_for_running_local_llms/n3pt0od/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752791843,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752791843,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3pjtvm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Negative_Owl_6623","can_mod_post":false,"created_utc":1752788919,"send_replies":true,"parent_id":"t1_n3p835z","score":1,"author_fullname":"t2_1tr7w997vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thank you very much. The used market in where I live is dead. To get a used one shipped from the EU or the US is going to take money, which I think is better put in a new one from where I live","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3pjtvm","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you very much. The used market in where I live is dead. To get a used one shipped from the EU or the US is going to take money, which I think is better put in a new one from where I live&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2gy2t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2gy2t/gpu_advice_for_running_local_llms/n3pjtvm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752788919,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3p835z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Afternoon_4260","can_mod_post":false,"created_utc":1752785458,"send_replies":true,"parent_id":"t3_1m2gy2t","score":1,"author_fullname":"t2_cj9kap4bx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You could find a used 3090 for ~ the price of a 5060ti. For llm this is a no brainer, if you have other use cases for it then it is your choice","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3p835z","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You could find a used 3090 for ~ the price of a 5060ti. For llm this is a no brainer, if you have other use cases for it then it is your choice&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2gy2t/gpu_advice_for_running_local_llms/n3p835z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752785458,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m2gy2t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3pj7hp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jacek2023","can_mod_post":false,"created_utc":1752788732,"send_replies":true,"parent_id":"t3_1m2gy2t","score":1,"author_fullname":"t2_vqgbql9w","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"check the price of RTX 3060, it's good \\"entry level\\" for local LLMs\\n\\nI currently use 3x3090","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3pj7hp","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;check the price of RTX 3060, it&amp;#39;s good &amp;quot;entry level&amp;quot; for local LLMs&lt;/p&gt;\\n\\n&lt;p&gt;I currently use 3x3090&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2gy2t/gpu_advice_for_running_local_llms/n3pj7hp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752788732,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m2gy2t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3sy23p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GPTrack_ai","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3ssxil","score":1,"author_fullname":"t2_1tpuoj72sa","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I am the same guy. The guy wanted advice for runing LLMs, IMHO the hardware for LLMs begins with RTX Pro 6000, which still does not have HBM memory, right. And it goes up from there.","edited":false,"author_flair_css_class":null,"name":"t1_n3sy23p","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I am the same guy. The guy wanted advice for runing LLMs, IMHO the hardware for LLMs begins with RTX Pro 6000, which still does not have HBM memory, right. And it goes up from there.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m2gy2t","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2gy2t/gpu_advice_for_running_local_llms/n3sy23p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752841060,"author_flair_text":null,"collapsed":false,"created_utc":1752841060,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ssxil","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Afternoon_4260","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3risl9","score":1,"author_fullname":"t2_cj9kap4bx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Funny you are not the same guy but have pretty much the same name x)  \\nImo with a couple of 3090 you can start learning and experimenting with a lot of things. After that, except if you have tens of thousands of dollars you'll pay api anyway if you want something.. bigger.  \\nYou want everyone to buy 30k dols Nvidia h100 or what? Rtx 6000 aren't hbm memory 🤷","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ssxil","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Funny you are not the same guy but have pretty much the same name x)&lt;br/&gt;\\nImo with a couple of 3090 you can start learning and experimenting with a lot of things. After that, except if you have tens of thousands of dollars you&amp;#39;ll pay api anyway if you want something.. bigger.&lt;br/&gt;\\nYou want everyone to buy 30k dols Nvidia h100 or what? Rtx 6000 aren&amp;#39;t hbm memory 🤷&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2gy2t","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2gy2t/gpu_advice_for_running_local_llms/n3ssxil/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752839068,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752839068,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3risl9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GPTrack_ai","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3p8ay9","score":1,"author_fullname":"t2_1tpuoj72sa","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"LLMs are all about memory size and bandwidth. So ideally HBM memory of at least 96GB is IMHO the starting point.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3risl9","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;LLMs are all about memory size and bandwidth. So ideally HBM memory of at least 96GB is IMHO the starting point.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2gy2t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2gy2t/gpu_advice_for_running_local_llms/n3risl9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752814959,"author_flair_text":null,"treatment_tags":[],"created_utc":1752814959,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3p8ay9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Afternoon_4260","can_mod_post":false,"created_utc":1752785518,"send_replies":true,"parent_id":"t1_n3ovs97","score":3,"author_fullname":"t2_cj9kap4bx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You are so wrong, you can experiment with a lot of things with any cuda device, 6 years later the 3090 is still relevant","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3p8ay9","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You are so wrong, you can experiment with a lot of things with any cuda device, 6 years later the 3090 is still relevant&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2gy2t","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2gy2t/gpu_advice_for_running_local_llms/n3p8ay9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752785518,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3rnx8x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GPTshop_ai","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3rix4l","score":1,"author_fullname":"t2_rkmud0isr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Newer systems are more energy efficient. It is not too bad in terms of electricty. Also, you should not buy an expensive powerfull car if you can't afford the gas.","edited":false,"author_flair_css_class":null,"name":"t1_n3rnx8x","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Newer systems are more energy efficient. It is not too bad in terms of electricty. Also, you should not buy an expensive powerfull car if you can&amp;#39;t afford the gas.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m2gy2t","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2gy2t/gpu_advice_for_running_local_llms/n3rnx8x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752817493,"author_flair_text":null,"collapsed":false,"created_utc":1752817493,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3u6jhi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Blizado","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3sts4y","score":1,"author_fullname":"t2_j0e2r","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Well, then you also need to deal with 4 GPUs instead of only one. But did tensor parallelism not only work with specific models who support this? Or do you not need it for more performance with multiple GPUs?\\n\\nI myself have actually only a 4090 and was thinking to get a used 3090, but a 3090 is noticeable slower as a 4090. For performance I use generally smaller models, often Mistral Nemo / Small based ones, because of it's good language support. I don't want to use LLMs in english anymore, that adds only another friction layer for me. But I also use local LLM more for more private stuff, where cloud LLMs are often too censored. For all other stuff I use mostly ChatGPT or Cursor AI for code stuff. So I would say my usecase is different as many who want to use expensive strong hardware.\\n\\nIt is clear that performance is for me more important than a lot of VRAM, but If I would have the money, sure a Pro 6000 or a at least two 5090 would be nice to have. So more VRAM but a slower GPU was never really an option for me. And to make it more clear how limited I am: I had to save money hard for 3/4 year + the money I got for my old RTX 2080 to buy my cheaper 4090 card two years ago when the cards was on their cheapest price point (~1600€) (first I planed to only get a 4080 for ~1200€).\\n\\nA RTX 5090 is already on a price point I would clearly say it is out of my budget, especially since all got more expensive in life here. Also a 4090 was only 400€ more than a 4080, a 5080 cost pretty much the same a 4080 cost that time, but a 5090 cost like two 5080. NVidia has really overdone it with the price, a huge gap between 5080 and 5090. A good gap for a 5080 TI with 24GB VRAM, right NVidia? XD\\n\\nYeah, if I would be rich, it would be clear what hardware would standing in my home. So I can only dream about it and do the best out of my situation and when I read what others have I'm not even that poor. Other would be happy to have at least a 4090.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3u6jhi","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Well, then you also need to deal with 4 GPUs instead of only one. But did tensor parallelism not only work with specific models who support this? Or do you not need it for more performance with multiple GPUs?&lt;/p&gt;\\n\\n&lt;p&gt;I myself have actually only a 4090 and was thinking to get a used 3090, but a 3090 is noticeable slower as a 4090. For performance I use generally smaller models, often Mistral Nemo / Small based ones, because of it&amp;#39;s good language support. I don&amp;#39;t want to use LLMs in english anymore, that adds only another friction layer for me. But I also use local LLM more for more private stuff, where cloud LLMs are often too censored. For all other stuff I use mostly ChatGPT or Cursor AI for code stuff. So I would say my usecase is different as many who want to use expensive strong hardware.&lt;/p&gt;\\n\\n&lt;p&gt;It is clear that performance is for me more important than a lot of VRAM, but If I would have the money, sure a Pro 6000 or a at least two 5090 would be nice to have. So more VRAM but a slower GPU was never really an option for me. And to make it more clear how limited I am: I had to save money hard for 3/4 year + the money I got for my old RTX 2080 to buy my cheaper 4090 card two years ago when the cards was on their cheapest price point (~1600€) (first I planed to only get a 4080 for ~1200€).&lt;/p&gt;\\n\\n&lt;p&gt;A RTX 5090 is already on a price point I would clearly say it is out of my budget, especially since all got more expensive in life here. Also a 4090 was only 400€ more than a 4080, a 5080 cost pretty much the same a 4080 cost that time, but a 5090 cost like two 5080. NVidia has really overdone it with the price, a huge gap between 5080 and 5090. A good gap for a 5080 TI with 24GB VRAM, right NVidia? XD&lt;/p&gt;\\n\\n&lt;p&gt;Yeah, if I would be rich, it would be clear what hardware would standing in my home. So I can only dream about it and do the best out of my situation and when I read what others have I&amp;#39;m not even that poor. Other would be happy to have at least a 4090.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m2gy2t","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2gy2t/gpu_advice_for_running_local_llms/n3u6jhi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752854536,"author_flair_text":null,"treatment_tags":[],"created_utc":1752854536,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3sts4y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Afternoon_4260","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3rix4l","score":1,"author_fullname":"t2_cj9kap4bx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I agree, but there's something about speed that makes me wonder.. I have 3090s and have always been happy with them, I still experiment a lot with them. But since I've started giving money to groq, my.. it's so fast! I can iterate so much faster, it's not me waiting for an answer, it directly me telling the next step or thinking on the answer.  \\nI feel it's more comfortable that my brain is the bottleneck, than having to wait\\n\\nJust to say, maybe 4 5090 can be faster than rtx pro.. idk it's a personal choice","edited":1752839768,"author_flair_css_class":null,"name":"t1_n3sts4y","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I agree, but there&amp;#39;s something about speed that makes me wonder.. I have 3090s and have always been happy with them, I still experiment a lot with them. But since I&amp;#39;ve started giving money to groq, my.. it&amp;#39;s so fast! I can iterate so much faster, it&amp;#39;s not me waiting for an answer, it directly me telling the next step or thinking on the answer.&lt;br/&gt;\\nI feel it&amp;#39;s more comfortable that my brain is the bottleneck, than having to wait&lt;/p&gt;\\n\\n&lt;p&gt;Just to say, maybe 4 5090 can be faster than rtx pro.. idk it&amp;#39;s a personal choice&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m2gy2t","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2gy2t/gpu_advice_for_running_local_llms/n3sts4y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752839412,"author_flair_text":"llama.cpp","collapsed":false,"created_utc":1752839412,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3rix4l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Blizado","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3rew4r","score":1,"author_fullname":"t2_j0e2r","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It is no cheap hobby, right, but it is not that you can't do anything with cheaper hardware, it is only more limited and slower, but not unusable. Some even use cheap old ~200$ cards with a lot of VRAM, only they have much VRAM but so slow in generation that they can go shopping when the let the LLM generate its answer. That would be even for me way too slow. But if you want the best, yes, nothing beats the RTX Pro 6000 actually on the consumer market or you need actual server cards, then it gets even more expensive. But sure, who don't want to have such hardware at home? Perhaps only people who don't want to pay the expected electricity bill. :D","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3rix4l","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It is no cheap hobby, right, but it is not that you can&amp;#39;t do anything with cheaper hardware, it is only more limited and slower, but not unusable. Some even use cheap old ~200$ cards with a lot of VRAM, only they have much VRAM but so slow in generation that they can go shopping when the let the LLM generate its answer. That would be even for me way too slow. But if you want the best, yes, nothing beats the RTX Pro 6000 actually on the consumer market or you need actual server cards, then it gets even more expensive. But sure, who don&amp;#39;t want to have such hardware at home? Perhaps only people who don&amp;#39;t want to pay the expected electricity bill. :D&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2gy2t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2gy2t/gpu_advice_for_running_local_llms/n3rix4l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752815020,"author_flair_text":null,"treatment_tags":[],"created_utc":1752815020,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3rew4r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GPTshop_ai","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3p7a2d","score":1,"author_fullname":"t2_rkmud0isr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Unfortunately, LLMs are a rich mans toy. If you can't afford the hardware, you can rent it.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3rew4r","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Unfortunately, LLMs are a rich mans toy. If you can&amp;#39;t afford the hardware, you can rent it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2gy2t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2gy2t/gpu_advice_for_running_local_llms/n3rew4r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752813164,"author_flair_text":null,"treatment_tags":[],"created_utc":1752813164,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3p7a2d","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Blizado","can_mod_post":false,"created_utc":1752785231,"send_replies":true,"parent_id":"t1_n3ovs97","score":2,"author_fullname":"t2_j0e2r","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Lol, not everyone lives in your rich world. :D","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3p7a2d","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Lol, not everyone lives in your rich world. :D&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2gy2t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2gy2t/gpu_advice_for_running_local_llms/n3p7a2d/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752785231,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3phpy5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Negative_Owl_6623","can_mod_post":false,"created_utc":1752788290,"send_replies":true,"parent_id":"t1_n3ovs97","score":2,"author_fullname":"t2_1tr7w997vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Trust me, if I have that kind of money, then I wouldn't have asked regarding an 8-year-old hardware xD","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3phpy5","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Trust me, if I have that kind of money, then I wouldn&amp;#39;t have asked regarding an 8-year-old hardware xD&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2gy2t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2gy2t/gpu_advice_for_running_local_llms/n3phpy5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752788290,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ovs97","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"GPTshop_ai","can_mod_post":false,"created_utc":1752781975,"send_replies":true,"parent_id":"t3_1m2gy2t","score":-6,"author_fullname":"t2_rkmud0isr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":true,"body":"IMHO, anything less than a RTX Pro 6000 does not make any sense.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ovs97","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;IMHO, anything less than a RTX Pro 6000 does not make any sense.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2gy2t/gpu_advice_for_running_local_llms/n3ovs97/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752781975,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2gy2t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-6}}],"before":null}}]`),n=()=>e.jsx(l,{data:t});export{n as default};
