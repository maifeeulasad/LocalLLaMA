import{j as e}from"./index-Cd3v0jxz.js";import{R as t}from"./RedditPostRenderer-cI96YLhy.js";import"./index-DGBoKyQm.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hi!  \\nI want to fine-tune a small pre-trained LLM to help users write code in a specific language. This language is very specific to a particular machinery and does not have widespread usage. We have a manual in PDF format and a few examples for the code. We want to build a chat agent where users can write code, and the agent writes the code. I am very new to training LLM and willing to learn whatever is necessary. I have a basic understanding of working with LLMs using Ollama and LangChain. Could someone please guide me on where to start? I have a good machine with an NVIDIA RTX 4090, 24 GB GPU. I want to build the entire system on this machine. \\n\\nThanks in advance for all the help. ","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Fine Tune a smaller LLM for Code generation","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lw1qp5","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.93,"author_flair_background_color":null,"subreddit_type":"public","ups":36,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1qt1co6pyc","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":36,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752115392,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi!&lt;br/&gt;\\nI want to fine-tune a small pre-trained LLM to help users write code in a specific language. This language is very specific to a particular machinery and does not have widespread usage. We have a manual in PDF format and a few examples for the code. We want to build a chat agent where users can write code, and the agent writes the code. I am very new to training LLM and willing to learn whatever is necessary. I have a basic understanding of working with LLMs using Ollama and LangChain. Could someone please guide me on where to start? I have a good machine with an NVIDIA RTX 4090, 24 GB GPU. I want to build the entire system on this machine. &lt;/p&gt;\\n\\n&lt;p&gt;Thanks in advance for all the help. &lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lw1qp5","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"GlobeAndGeek","discussion_type":null,"num_comments":8,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lw1qp5/fine_tune_a_smaller_llm_for_code_generation/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lw1qp5/fine_tune_a_smaller_llm_for_code_generation/","subreddit_subscribers":497354,"created_utc":1752115392,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2b1ulm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"dash_bro","can_mod_post":false,"created_utc":1752120987,"send_replies":true,"parent_id":"t3_1lw1qp5","score":19,"author_fullname":"t2_4bzd6saj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Congrats on being clear about what you wanna do! Here's how I'd start:\\n\\n- look up unsloth's fine-tuning notebooks. At the absolute minimum, you need to learn fine-tuning and quantization, to be able to fine-tune a larger model and run on a smaller memory footprint or train a smaller model correctly\\n- I recommend **picking an architecture and training the smallest model you can with it** (to get a hang of issues and errors you'll face barring output quality). This has been a game changer for me when I fine-tune, because failing runs with larger models/larger compute is time and money intensive. I've done 0.6B on 16GB GPUs to get a hang of it, then boosted to 14B on an A100 using this method.\\n- come up with a strategy to collect the data you need. You have pdfs and code samples -- but that likely isn't formatted right for what you need. Unsloth's notebooks also contain the dataset information you'll need, you can format the data you have -&gt; data you'll need. Also, look out for collecting more data\\n- **have a way to measure performance beyond just subjective checks**. Objectivity is what will guide you towards building/changing processes for a better model overall\\n- see if you can enlist an A100 on colab or kaggle instead. It's relatively cheap once you have all the other stuff ready and just need faster training done\\n- iterate! You'll likely realise small steps/missteps you're making that isn't leading to the model performance expected. Only doing it hands on is going to help you understand it - so iterate and build as much as possible!\\n\\nGood luck! If you develop a cool model feel free to make it open source :)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2b1ulm","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Congrats on being clear about what you wanna do! Here&amp;#39;s how I&amp;#39;d start:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;look up unsloth&amp;#39;s fine-tuning notebooks. At the absolute minimum, you need to learn fine-tuning and quantization, to be able to fine-tune a larger model and run on a smaller memory footprint or train a smaller model correctly&lt;/li&gt;\\n&lt;li&gt;I recommend &lt;strong&gt;picking an architecture and training the smallest model you can with it&lt;/strong&gt; (to get a hang of issues and errors you&amp;#39;ll face barring output quality). This has been a game changer for me when I fine-tune, because failing runs with larger models/larger compute is time and money intensive. I&amp;#39;ve done 0.6B on 16GB GPUs to get a hang of it, then boosted to 14B on an A100 using this method.&lt;/li&gt;\\n&lt;li&gt;come up with a strategy to collect the data you need. You have pdfs and code samples -- but that likely isn&amp;#39;t formatted right for what you need. Unsloth&amp;#39;s notebooks also contain the dataset information you&amp;#39;ll need, you can format the data you have -&amp;gt; data you&amp;#39;ll need. Also, look out for collecting more data&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;have a way to measure performance beyond just subjective checks&lt;/strong&gt;. Objectivity is what will guide you towards building/changing processes for a better model overall&lt;/li&gt;\\n&lt;li&gt;see if you can enlist an A100 on colab or kaggle instead. It&amp;#39;s relatively cheap once you have all the other stuff ready and just need faster training done&lt;/li&gt;\\n&lt;li&gt;iterate! You&amp;#39;ll likely realise small steps/missteps you&amp;#39;re making that isn&amp;#39;t leading to the model performance expected. Only doing it hands on is going to help you understand it - so iterate and build as much as possible!&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Good luck! If you develop a cool model feel free to make it open source :)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw1qp5/fine_tune_a_smaller_llm_for_code_generation/n2b1ulm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752120987,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lw1qp5","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":19}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2b1pgk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"lostnuclues","can_mod_post":false,"created_utc":1752120925,"send_replies":true,"parent_id":"t3_1lw1qp5","score":11,"author_fullname":"t2_7spksnox","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You first need to make a dataset. Gather the official documentation (PDF) of that language, pass it to a big LLM like Gemini, DeepSeek, ask it to create question and answer pair responses, once you have 1000+, take it to\\n\\n[https://docs.unsloth.ai/get-started/beginner-start-here](https://docs.unsloth.ai/get-started/beginner-start-here)\\n\\n  \\nAlong with QnA, you can use git repo history to train, use git commit message as task and the git diff output compared to previous commit as its fix.","edited":1752142937,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2b1pgk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You first need to make a dataset. Gather the official documentation (PDF) of that language, pass it to a big LLM like Gemini, DeepSeek, ask it to create question and answer pair responses, once you have 1000+, take it to&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://docs.unsloth.ai/get-started/beginner-start-here\\"&gt;https://docs.unsloth.ai/get-started/beginner-start-here&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Along with QnA, you can use git repo history to train, use git commit message as task and the git diff output compared to previous commit as its fix.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw1qp5/fine_tune_a_smaller_llm_for_code_generation/n2b1pgk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752120925,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lw1qp5","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2c295r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"indicava","can_mod_post":false,"created_utc":1752140247,"send_replies":true,"parent_id":"t3_1lw1qp5","score":7,"author_fullname":"t2_4dvff","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Before even thinking about the fine tuning process itself:\\n\\n\\n- Amass a large corpus of unstructured data like manuals, release notes, slack messages, support correspondence, stackoverflow, Reddit, forums and of course all the code you have/can find. Anywhere that has data specific to your domain, grab it.\\n\\n- As mentioned before me, split the official technical docs into topics, subtopics, etc. Have a SOTA LLM generate an SFT dataset with questions/answer pairs (preferably with some percent being multi turn conversations) across all topics using different Q/A styles (How to, bug fixing, code explaining, etc.). Also augment the topics from the docs with relevant code examples from the actual code you have. \\n\\n- Lastly, setup a robust evaluation benchmark like: a script that compiles/interprets the model’s generated code and provides feedback whether it compiled. You can augment this with pre-written unit tests (or have the model write the unit tests) and validate functionality. Also, have an “unseen” subset of Q/A pairs with grounded answers attached, prompt your trained model to answer and have a SOTA LLM evaluate the response based on the ground truth. \\n\\n\\nOnce you have all that in place, read up on CLM/DAPT/Continued Pre-training, SFT and preferably RL (PPO/GRPO/DPO) and just go with what works for the big boys: CLM -&gt; SFT -&gt; RL\\n\\nIf you’re totally new to this, and depending on the breadth of the domain you’re training for, you’re looking at 6-12 month project (full time, assuming you have solid understanding of python).\\n\\nAlso, you’re probably not going to be able to pull this off on your consumer hardware. For a decent sized coding model (3B parameters is pretty much the bare minimum imo) doing CLM and RL is going to require more VRAM. Just rent out GPU’s on RunPod or vast, an H100 can be rented out for about $1.80 an hour. With a well tested, automated training pipeline, you’re not gonna need more 3-4 days of compute from that machine, which in a commercial setting is peanuts.\\n\\nGood luck!","edited":1752140438,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2c295r","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Before even thinking about the fine tuning process itself:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;p&gt;Amass a large corpus of unstructured data like manuals, release notes, slack messages, support correspondence, stackoverflow, Reddit, forums and of course all the code you have/can find. Anywhere that has data specific to your domain, grab it.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;As mentioned before me, split the official technical docs into topics, subtopics, etc. Have a SOTA LLM generate an SFT dataset with questions/answer pairs (preferably with some percent being multi turn conversations) across all topics using different Q/A styles (How to, bug fixing, code explaining, etc.). Also augment the topics from the docs with relevant code examples from the actual code you have. &lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Lastly, setup a robust evaluation benchmark like: a script that compiles/interprets the model’s generated code and provides feedback whether it compiled. You can augment this with pre-written unit tests (or have the model write the unit tests) and validate functionality. Also, have an “unseen” subset of Q/A pairs with grounded answers attached, prompt your trained model to answer and have a SOTA LLM evaluate the response based on the ground truth. &lt;/p&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Once you have all that in place, read up on CLM/DAPT/Continued Pre-training, SFT and preferably RL (PPO/GRPO/DPO) and just go with what works for the big boys: CLM -&amp;gt; SFT -&amp;gt; RL&lt;/p&gt;\\n\\n&lt;p&gt;If you’re totally new to this, and depending on the breadth of the domain you’re training for, you’re looking at 6-12 month project (full time, assuming you have solid understanding of python).&lt;/p&gt;\\n\\n&lt;p&gt;Also, you’re probably not going to be able to pull this off on your consumer hardware. For a decent sized coding model (3B parameters is pretty much the bare minimum imo) doing CLM and RL is going to require more VRAM. Just rent out GPU’s on RunPod or vast, an H100 can be rented out for about $1.80 an hour. With a well tested, automated training pipeline, you’re not gonna need more 3-4 days of compute from that machine, which in a commercial setting is peanuts.&lt;/p&gt;\\n\\n&lt;p&gt;Good luck!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw1qp5/fine_tune_a_smaller_llm_for_code_generation/n2c295r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752140247,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lw1qp5","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2at73c","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DinoAmino","can_mod_post":false,"created_utc":1752117428,"send_replies":true,"parent_id":"t3_1lw1qp5","score":12,"author_fullname":"t2_j1v7f","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"1. Use custom code to parse the language manual into individual functions, data types, properties or whatever that language is composed of. Decompose each thing into its own tiny document.\\n2. Gather all existing files of your existing codebase that uses this language.\\n3. Learn about codebase RAG.\\n4. Forget about fine-tuning. \\n5. Use your RAG to create datasets for fine-tuning if you insist on skipping step 4. Because good RAG + a good fine-tune rocks if you can pull it off.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2at73c","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;ol&gt;\\n&lt;li&gt;Use custom code to parse the language manual into individual functions, data types, properties or whatever that language is composed of. Decompose each thing into its own tiny document.&lt;/li&gt;\\n&lt;li&gt;Gather all existing files of your existing codebase that uses this language.&lt;/li&gt;\\n&lt;li&gt;Learn about codebase RAG.&lt;/li&gt;\\n&lt;li&gt;Forget about fine-tuning. &lt;/li&gt;\\n&lt;li&gt;Use your RAG to create datasets for fine-tuning if you insist on skipping step 4. Because good RAG + a good fine-tune rocks if you can pull it off.&lt;/li&gt;\\n&lt;/ol&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw1qp5/fine_tune_a_smaller_llm_for_code_generation/n2at73c/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752117428,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lw1qp5","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2dtilr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok_Engineering_1203","can_mod_post":false,"created_utc":1752162834,"send_replies":true,"parent_id":"t3_1lw1qp5","score":2,"author_fullname":"t2_ypm6gpb2j","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Best of luck!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2dtilr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Best of luck!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw1qp5/fine_tune_a_smaller_llm_for_code_generation/n2dtilr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752162834,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lw1qp5","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2e6fua","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"patbhakta","can_mod_post":false,"created_utc":1752166489,"send_replies":true,"parent_id":"t3_1lw1qp5","score":2,"author_fullname":"t2_1e5sho1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Before fine tuning anything, use notebook LLM. Upload your PDF documentation and production ready code. \\n\\nUse that first, document all the questions you and the team ask. \\nRefine the documents you uploaded with the Q&amp;A your team does. \\n\\nOnce your codebase grows and your documents grow, then think about implementing fine tuning + memory architecture. \\n\\nYour hardware is weak for running large context sizes even with the smallest LLMs, you'll have to use creative memory architecture with traditional databases and vector databases for RAG.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2e6fua","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Before fine tuning anything, use notebook LLM. Upload your PDF documentation and production ready code. &lt;/p&gt;\\n\\n&lt;p&gt;Use that first, document all the questions you and the team ask. \\nRefine the documents you uploaded with the Q&amp;amp;A your team does. &lt;/p&gt;\\n\\n&lt;p&gt;Once your codebase grows and your documents grow, then think about implementing fine tuning + memory architecture. &lt;/p&gt;\\n\\n&lt;p&gt;Your hardware is weak for running large context sizes even with the smallest LLMs, you&amp;#39;ll have to use creative memory architecture with traditional databases and vector databases for RAG.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw1qp5/fine_tune_a_smaller_llm_for_code_generation/n2e6fua/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752166489,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lw1qp5","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2at0iu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"hehsteve","can_mod_post":false,"created_utc":1752117357,"send_replies":true,"parent_id":"t3_1lw1qp5","score":1,"author_fullname":"t2_4ucpgien","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Following.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2at0iu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Following.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw1qp5/fine_tune_a_smaller_llm_for_code_generation/n2at0iu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752117357,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lw1qp5","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2fch1c","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"neph1010","can_mod_post":false,"created_utc":1752178264,"send_replies":true,"parent_id":"t3_1lw1qp5","score":3,"author_fullname":"t2_v2z4wgzc","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Fwiw I made a similar project for fun a while back. I decided to finetune Qwen Coder 7B for Unity specialization.  \\nI developed two datasets of my own (but only one ended up in the model). For the first dataset, I scraped all unity example projects I could find and made a Q&amp;A multiturn dataset; take a code file, simulate a user asking questions and follow up questions.  \\nThen I scraped all pdf manuals, extracted the text and did it in a similar manner (I never ended up training on it).\\n\\nIf you want to have a look at the setup and training regime, they're here on hf:  \\n[https://huggingface.co/neph1/Qwen2.5-Coder-7B-Instruct-Unity](https://huggingface.co/neph1/Qwen2.5-Coder-7B-Instruct-Unity)  \\n[https://huggingface.co/datasets/neph1/Unity\\\\_Code\\\\_QnA](https://huggingface.co/datasets/neph1/Unity_Code_QnA)  \\n[https://huggingface.co/datasets/neph1/UnityManualQnA](https://huggingface.co/datasets/neph1/UnityManualQnA)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2fch1c","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Fwiw I made a similar project for fun a while back. I decided to finetune Qwen Coder 7B for Unity specialization.&lt;br/&gt;\\nI developed two datasets of my own (but only one ended up in the model). For the first dataset, I scraped all unity example projects I could find and made a Q&amp;amp;A multiturn dataset; take a code file, simulate a user asking questions and follow up questions.&lt;br/&gt;\\nThen I scraped all pdf manuals, extracted the text and did it in a similar manner (I never ended up training on it).&lt;/p&gt;\\n\\n&lt;p&gt;If you want to have a look at the setup and training regime, they&amp;#39;re here on hf:&lt;br/&gt;\\n&lt;a href=\\"https://huggingface.co/neph1/Qwen2.5-Coder-7B-Instruct-Unity\\"&gt;https://huggingface.co/neph1/Qwen2.5-Coder-7B-Instruct-Unity&lt;/a&gt;&lt;br/&gt;\\n&lt;a href=\\"https://huggingface.co/datasets/neph1/Unity_Code_QnA\\"&gt;https://huggingface.co/datasets/neph1/Unity_Code_QnA&lt;/a&gt;&lt;br/&gt;\\n&lt;a href=\\"https://huggingface.co/datasets/neph1/UnityManualQnA\\"&gt;https://huggingface.co/datasets/neph1/UnityManualQnA&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw1qp5/fine_tune_a_smaller_llm_for_code_generation/n2fch1c/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752178264,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lw1qp5","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}}]`),r=()=>e.jsx(t,{data:a});export{r as default};
