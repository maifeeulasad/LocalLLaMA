import{j as e}from"./index-BUtHYhT3.js";import{R as l}from"./RedditPostRenderer-BaN1Fn7z.js";import"./index-Cli9kp5v.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hey folks,\\n\\nI'm looking into running a larger language model locally and came across Qwen3-14B (or Qwen3\\\\\\\\\\\\_14B depending on naming). I know it's been getting some hype lately, but I wanted to hear from people who’ve actually used it.\\n\\n\\\\* How does it perform compared to other 13B/14B class models like Gemma, Mistral, LLaMA 2/3, Yi, etc.?\\n\\n\\\\* Any real-world performance/benchmark comparisons in terms of speed, context handling, or reasoning?\\n\\n\\\\* How’s the quantization support (GGUF/ExLlama/AutoGPTQ)? Is it efficient enough to run on a single GPU (e.g. 24GB VRAM of Macmini m4, token/secs)?\\n\\n\\\\* How does it do with coding, long-context tasks, or general instruction following?\\n\\nWould like to hear your experience, whether it’s through serious benchmarking or just specific use. Thanks in advance!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"How good is Qwen3-14B for local use? Any benchmarks vs other models?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1ltnpsl","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":1,"author_flair_background_color":null,"subreddit_type":"public","ups":2,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_dnh48atl6","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":2,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751872472,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey folks,&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m looking into running a larger language model locally and came across Qwen3-14B (or Qwen3\\\\_14B depending on naming). I know it&amp;#39;s been getting some hype lately, but I wanted to hear from people who’ve actually used it.&lt;/p&gt;\\n\\n&lt;p&gt;* How does it perform compared to other 13B/14B class models like Gemma, Mistral, LLaMA 2/3, Yi, etc.?&lt;/p&gt;\\n\\n&lt;p&gt;* Any real-world performance/benchmark comparisons in terms of speed, context handling, or reasoning?&lt;/p&gt;\\n\\n&lt;p&gt;* How’s the quantization support (GGUF/ExLlama/AutoGPTQ)? Is it efficient enough to run on a single GPU (e.g. 24GB VRAM of Macmini m4, token/secs)?&lt;/p&gt;\\n\\n&lt;p&gt;* How does it do with coding, long-context tasks, or general instruction following?&lt;/p&gt;\\n\\n&lt;p&gt;Would like to hear your experience, whether it’s through serious benchmarking or just specific use. Thanks in advance!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1ltnpsl","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"abubakkar_s","discussion_type":null,"num_comments":6,"send_replies":false,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1ltnpsl/how_good_is_qwen314b_for_local_use_any_benchmarks/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1ltnpsl/how_good_is_qwen314b_for_local_use_any_benchmarks/","subreddit_subscribers":495395,"created_utc":1751872472,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1rq968","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"custodiam99","can_mod_post":false,"created_utc":1751872874,"send_replies":true,"parent_id":"t3_1ltnpsl","score":2,"author_fullname":"t2_nqnhgqqf5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It is very good for summarization, but Phi-4 thinking plus can beat it in some tasks like mind map creation. I think it is possibly the best all around 14b model right now.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1rq968","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It is very good for summarization, but Phi-4 thinking plus can beat it in some tasks like mind map creation. I think it is possibly the best all around 14b model right now.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltnpsl/how_good_is_qwen314b_for_local_use_any_benchmarks/n1rq968/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751872874,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltnpsl","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1rrhj7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RiskyBizz216","can_mod_post":false,"created_utc":1751873614,"send_replies":true,"parent_id":"t3_1ltnpsl","score":1,"author_fullname":"t2_4eu8nupk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Better than Yi and Gemma, worse that Mistral &amp; Llama models\\n\\nSpeed is not terrible on my 5090  - 75.99 tokens/sec \\n\\nBut it does not follow directions very well. Its a thinking model, and it is great for RP and brainstorming, but terrible at coding. \\n\\nIts the type of model you'd use for short scripts or quick answers, maybe auto complete if you disable thinking.\\n\\nIMO You can use a much better model with 24GB","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1rrhj7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Better than Yi and Gemma, worse that Mistral &amp;amp; Llama models&lt;/p&gt;\\n\\n&lt;p&gt;Speed is not terrible on my 5090  - 75.99 tokens/sec &lt;/p&gt;\\n\\n&lt;p&gt;But it does not follow directions very well. Its a thinking model, and it is great for RP and brainstorming, but terrible at coding. &lt;/p&gt;\\n\\n&lt;p&gt;Its the type of model you&amp;#39;d use for short scripts or quick answers, maybe auto complete if you disable thinking.&lt;/p&gt;\\n\\n&lt;p&gt;IMO You can use a much better model with 24GB&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltnpsl/how_good_is_qwen314b_for_local_use_any_benchmarks/n1rrhj7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751873614,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltnpsl","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1rt6wh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1751874637,"send_replies":true,"parent_id":"t3_1ltnpsl","score":2,"author_fullname":"t2_uz37qfx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It does okay with coding, all Qwen3 are good at it for its size. Creative writing and related stuff such as RP is awful. Sumarries and stem stuff ok. \\n\\nYou can check it online. https://huggingface.co/spaces/Qwen/Qwen3-Demo","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1rt6wh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It does okay with coding, all Qwen3 are good at it for its size. Creative writing and related stuff such as RP is awful. Sumarries and stem stuff ok. &lt;/p&gt;\\n\\n&lt;p&gt;You can check it online. &lt;a href=\\"https://huggingface.co/spaces/Qwen/Qwen3-Demo\\"&gt;https://huggingface.co/spaces/Qwen/Qwen3-Demo&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltnpsl/how_good_is_qwen314b_for_local_use_any_benchmarks/n1rt6wh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751874637,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltnpsl","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1rubxf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Accomplished_Ad9530","can_mod_post":false,"created_utc":1751875309,"send_replies":true,"parent_id":"t3_1ltnpsl","score":1,"author_fullname":"t2_88fma001","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Your last bullet point is the only one that matters. LLMs are tools. Describe your use cases.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1rubxf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Your last bullet point is the only one that matters. LLMs are tools. Describe your use cases.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltnpsl/how_good_is_qwen314b_for_local_use_any_benchmarks/n1rubxf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751875309,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltnpsl","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1rw3jc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AaronFeng47","can_mod_post":false,"created_utc":1751876372,"send_replies":true,"parent_id":"t3_1ltnpsl","score":1,"author_fullname":"t2_4gc7hf3m","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Unless you want to do creative writing and role play stuff, Qwen 3 is the best or at least a great choice ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1rw3jc","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Unless you want to do creative writing and role play stuff, Qwen 3 is the best or at least a great choice &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltnpsl/how_good_is_qwen314b_for_local_use_any_benchmarks/n1rw3jc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751876372,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1ltnpsl","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1rzjkx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"tmvr","can_mod_post":false,"created_utc":1751878460,"send_replies":true,"parent_id":"t3_1ltnpsl","score":1,"author_fullname":"t2_11qlhv","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It tends to overthink and burn tokens and time on that, but this has nothing to do with the 14B specifically, it's a Qwen3 \\"specialty\\".","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1rzjkx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It tends to overthink and burn tokens and time on that, but this has nothing to do with the 14B specifically, it&amp;#39;s a Qwen3 &amp;quot;specialty&amp;quot;.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltnpsl/how_good_is_qwen314b_for_local_use_any_benchmarks/n1rzjkx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751878460,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltnpsl","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(l,{data:t});export{r as default};
