import{j as e}from"./index-DACS7Nh6.js";import{R as t}from"./RedditPostRenderer-Dqa1NZuX.js";import"./index-DiMIVQx4.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Does it involve first building a model that creates generates a dataset with &lt;think&gt; tokens.\\n\\nThen generate a reward model.\\n\\nFinally fine tune model with RL and reward model?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"How are reasonable models built? Are they fine tuned from base non reasoning models?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lwrad1","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.43,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1t3515o2d2","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752189190,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Does it involve first building a model that creates generates a dataset with &amp;lt;think&amp;gt; tokens.&lt;/p&gt;\\n\\n&lt;p&gt;Then generate a reward model.&lt;/p&gt;\\n\\n&lt;p&gt;Finally fine tune model with RL and reward model?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lwrad1","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"rockybaby2025","discussion_type":null,"num_comments":9,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lwrad1/how_are_reasonable_models_built_are_they_fine/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lwrad1/how_are_reasonable_models_built_are_they_fine/","subreddit_subscribers":497504,"created_utc":1752189190,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2gjzvp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Double_Cause4609","can_mod_post":false,"created_utc":1752191718,"send_replies":true,"parent_id":"t3_1lwrad1","score":7,"author_fullname":"t2_1kubzxt2ww","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"There's not really a single way of doing it, but there's a few basic principles you can use.\\n\\nFirst it helps to understand how non reasoning instruct models are made. Basically, you train a base model on a huge variety of information, which is knowledgeable, but not necessarily useful. Then, you show it a ton of examples of an assistant following the instructions of the user. For example\\n\\nsystem: You are a helpful math assistant  \\nuser: How do I solve SQRT(81)  \\nassistant: ...\\n\\nAnd so on.\\n\\nIf this is done well, you get a model that can solve a lot of problems for you, do code, etc.\\n\\nTo convert this from a non reasoning model to a reasoning one, there's basically two broad ways to do it.\\n\\n1) Produce a dataset of examples where the assistant puts reasoning in &lt;think&gt; tokens. This can be done in \\\\*a lot\\\\* of different ways. You can take &lt;think&gt; examples from existing reasoning models, you can do CoT prompting and put the produced CoT in the &lt;think&gt; tags, you can linearize a tree search, etc.\\n\\nThis can work well (see LIMO and S1 simple scaling), but there are a lot of problems with this approach. Notably, SFT has a few issues with catastrophic forgetting, and is quite harsh on the model's built-in knowledge (instilled in the base model pre-training), and it can overwrite a lot of things.\\n\\nGenerally, this is regarded as something of a dead-end, with a few notable exceptions.\\n\\n2) Take a standard instruct model, and tell it something like\\n\\nSystem: You are a deep thinking model, and you can place extended reasoning within a &lt;think&gt; XML tag. Close it with &lt;/think&gt; when you are finished and are ready to answer.\\n\\nThen, you just rate the model's outputs somehow. This could be RLVR (essentially you make a problem where you know the right answer, and if it gives it, you give it a 1, or a 0 otherwise), RLHF (you rate the outputs either literally or via proxy), or RLAIF (you instruct another model to rate the outputs).\\n\\nThen, you produce a gradient by...Well, it's kind of complicated, but at the core, you compare the score of each output against the output that was scored, and you do some math to kind of adjust how likely the model is to have made each output.\\n\\nThis can then be propagated backward as usual with next token prediction.\\n\\nAnyway, the key is that because LLMs are somewhat random, this lets them explore a variety of strategies on their own, and they can find the strategies (within their probability distribution) that are most effective for them, so you're kind of working with what the model already has built into it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2gjzvp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There&amp;#39;s not really a single way of doing it, but there&amp;#39;s a few basic principles you can use.&lt;/p&gt;\\n\\n&lt;p&gt;First it helps to understand how non reasoning instruct models are made. Basically, you train a base model on a huge variety of information, which is knowledgeable, but not necessarily useful. Then, you show it a ton of examples of an assistant following the instructions of the user. For example&lt;/p&gt;\\n\\n&lt;p&gt;system: You are a helpful math assistant&lt;br/&gt;\\nuser: How do I solve SQRT(81)&lt;br/&gt;\\nassistant: ...&lt;/p&gt;\\n\\n&lt;p&gt;And so on.&lt;/p&gt;\\n\\n&lt;p&gt;If this is done well, you get a model that can solve a lot of problems for you, do code, etc.&lt;/p&gt;\\n\\n&lt;p&gt;To convert this from a non reasoning model to a reasoning one, there&amp;#39;s basically two broad ways to do it.&lt;/p&gt;\\n\\n&lt;p&gt;1) Produce a dataset of examples where the assistant puts reasoning in &amp;lt;think&amp;gt; tokens. This can be done in *a lot* of different ways. You can take &amp;lt;think&amp;gt; examples from existing reasoning models, you can do CoT prompting and put the produced CoT in the &amp;lt;think&amp;gt; tags, you can linearize a tree search, etc.&lt;/p&gt;\\n\\n&lt;p&gt;This can work well (see LIMO and S1 simple scaling), but there are a lot of problems with this approach. Notably, SFT has a few issues with catastrophic forgetting, and is quite harsh on the model&amp;#39;s built-in knowledge (instilled in the base model pre-training), and it can overwrite a lot of things.&lt;/p&gt;\\n\\n&lt;p&gt;Generally, this is regarded as something of a dead-end, with a few notable exceptions.&lt;/p&gt;\\n\\n&lt;p&gt;2) Take a standard instruct model, and tell it something like&lt;/p&gt;\\n\\n&lt;p&gt;System: You are a deep thinking model, and you can place extended reasoning within a &amp;lt;think&amp;gt; XML tag. Close it with &amp;lt;/think&amp;gt; when you are finished and are ready to answer.&lt;/p&gt;\\n\\n&lt;p&gt;Then, you just rate the model&amp;#39;s outputs somehow. This could be RLVR (essentially you make a problem where you know the right answer, and if it gives it, you give it a 1, or a 0 otherwise), RLHF (you rate the outputs either literally or via proxy), or RLAIF (you instruct another model to rate the outputs).&lt;/p&gt;\\n\\n&lt;p&gt;Then, you produce a gradient by...Well, it&amp;#39;s kind of complicated, but at the core, you compare the score of each output against the output that was scored, and you do some math to kind of adjust how likely the model is to have made each output.&lt;/p&gt;\\n\\n&lt;p&gt;This can then be propagated backward as usual with next token prediction.&lt;/p&gt;\\n\\n&lt;p&gt;Anyway, the key is that because LLMs are somewhat random, this lets them explore a variety of strategies on their own, and they can find the strategies (within their probability distribution) that are most effective for them, so you&amp;#39;re kind of working with what the model already has built into it.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwrad1/how_are_reasonable_models_built_are_they_fine/n2gjzvp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752191718,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwrad1","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2haupf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rockybaby2025","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2hakzv","score":1,"author_fullname":"t2_1t3515o2d2","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thank you seems like everything is a fine tune!!","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n2haupf","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you seems like everything is a fine tune!!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lwrad1","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwrad1/how_are_reasonable_models_built_are_they_fine/n2haupf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752201131,"author_flair_text":null,"treatment_tags":[],"created_utc":1752201131,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2hakzv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"____vladrad","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2hadce","score":2,"author_fullname":"t2_u6i8a0ay","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It’s all post training. You can pick any model on huggingface and you can probably reason train it. Think of it has behavior training where you can reward it. You can take llama 2 and technically turn it into a reasoning model","edited":false,"author_flair_css_class":null,"name":"t1_n2hakzv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It’s all post training. You can pick any model on huggingface and you can probably reason train it. Think of it has behavior training where you can reward it. You can take llama 2 and technically turn it into a reasoning model&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lwrad1","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwrad1/how_are_reasonable_models_built_are_they_fine/n2hakzv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752201032,"author_flair_text":null,"collapsed":false,"created_utc":1752201032,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2hadce","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rockybaby2025","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2h9plp","score":0,"author_fullname":"t2_1t3515o2d2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"In your view then are reasoning models created from further training? Or are they trained from group up in reasoning manner? \\n\\nOr architecturally are they changed (eg transformer level) to think?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2hadce","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;In your view then are reasoning models created from further training? Or are they trained from group up in reasoning manner? &lt;/p&gt;\\n\\n&lt;p&gt;Or architecturally are they changed (eg transformer level) to think?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwrad1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwrad1/how_are_reasonable_models_built_are_they_fine/n2hadce/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752200953,"author_flair_text":null,"treatment_tags":[],"created_utc":1752200953,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n2h9plp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"____vladrad","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2h71ql","score":3,"author_fullname":"t2_u6i8a0ay","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Great question during training you essentially one shot each attempt. It’s like starting a fresh chat each time. In the prompt you give it instructions to use think tags…\\n\\nLook under data preparation you’ll see the prompt that’s used each time. https://docs.unsloth.ai/basics/reinforcement-learning-rl-guide/tutorial-train-your-own-reasoning-model-with-grpo#data-preparation\\n‘’’\\nSYSTEM_PROMPT = \\"\\"\\"\\nRespond in the following format:\\n&lt;reasoning&gt;\\n...\\n&lt;/reasoning&gt;\\n&lt;answer&gt;\\n...\\n&lt;/answer&gt;\\n\\"\\"\\"\\n\\nXML_COT_FORMAT = \\"\\"\\"\\\\\\n&lt;reasoning&gt;\\n{reasoning}\\n&lt;/reasoning&gt;\\n&lt;answer&gt;\\n{answer}\\n&lt;/answer&gt;\\n\\"\\"\\"\\n‘’’\\n\\nThen when you get an answer you can programmatically score by just parsing the xml. Is it valid, right etc. then you give the model a reward for it doing stuff right. So let’s say you give it 1 snack per correct format and 1 snack for correct answer. On top of it you can give it .01 snack per every word it has written. Eventually the model will write more and more and more… which slowly becomes a reasoning chain.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2h9plp","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Great question during training you essentially one shot each attempt. It’s like starting a fresh chat each time. In the prompt you give it instructions to use think tags…&lt;/p&gt;\\n\\n&lt;p&gt;Look under data preparation you’ll see the prompt that’s used each time. &lt;a href=\\"https://docs.unsloth.ai/basics/reinforcement-learning-rl-guide/tutorial-train-your-own-reasoning-model-with-grpo#data-preparation\\"&gt;https://docs.unsloth.ai/basics/reinforcement-learning-rl-guide/tutorial-train-your-own-reasoning-model-with-grpo#data-preparation&lt;/a&gt;\\n‘’’\\nSYSTEM_PROMPT = &amp;quot;&amp;quot;&amp;quot;\\nRespond in the following format:\\n&amp;lt;reasoning&amp;gt;\\n...\\n&amp;lt;/reasoning&amp;gt;\\n&amp;lt;answer&amp;gt;\\n...\\n&amp;lt;/answer&amp;gt;\\n&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;XML_COT_FORMAT = &amp;quot;&amp;quot;&amp;quot;\\\\\\n&amp;lt;reasoning&amp;gt;\\n{reasoning}\\n&amp;lt;/reasoning&amp;gt;\\n&amp;lt;answer&amp;gt;\\n{answer}\\n&amp;lt;/answer&amp;gt;\\n&amp;quot;&amp;quot;&amp;quot;\\n‘’’&lt;/p&gt;\\n\\n&lt;p&gt;Then when you get an answer you can programmatically score by just parsing the xml. Is it valid, right etc. then you give the model a reward for it doing stuff right. So let’s say you give it 1 snack per correct format and 1 snack for correct answer. On top of it you can give it .01 snack per every word it has written. Eventually the model will write more and more and more… which slowly becomes a reasoning chain.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwrad1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwrad1/how_are_reasonable_models_built_are_they_fine/n2h9plp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752200713,"author_flair_text":null,"treatment_tags":[],"created_utc":1752200713,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2h71ql","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rockybaby2025","can_mod_post":false,"created_utc":1752199769,"send_replies":true,"parent_id":"t1_n2gokf4","score":1,"author_fullname":"t2_1t3515o2d2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"But do you prompt it to use &lt;think&gt; tags so that RL model can score the better \\"thinking\\" model?\\n\\nIt's not possible for humans to manually annotate.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2h71ql","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;But do you prompt it to use &amp;lt;think&amp;gt; tags so that RL model can score the better &amp;quot;thinking&amp;quot; model?&lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s not possible for humans to manually annotate.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwrad1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwrad1/how_are_reasonable_models_built_are_they_fine/n2h71ql/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752199769,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2gokf4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"____vladrad","can_mod_post":false,"created_utc":1752193313,"send_replies":true,"parent_id":"t3_1lwrad1","score":2,"author_fullname":"t2_u6i8a0ay","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"https://kalomaze.bearblog.dev/why-does-grpo-work/ Why does GRPO work? | kalomaze's kalomazing blog\\n\\n\\nYou can make your own with unsloth\\n\\nEssentially during a RL run you ask a model to solve one problem in a specific format 100 times. You can score each sample. Then it essentially gets an idea what went right. This takes a lot of steps. You can score it by giving it a reward for how much it writes, correct wrong etc.\\n\\nIt’s not just think tags it can be anything. I made a battle tag in it a Viking fights a pirate and knight. Each are scored to not copy each other and at the end a robot judges the best answer. So asking them how to do a math problem gives you three ways to do it and something reviews it. The user just sees the answer. In the background they are mad talking smack about each other.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2gokf4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://kalomaze.bearblog.dev/why-does-grpo-work/\\"&gt;https://kalomaze.bearblog.dev/why-does-grpo-work/&lt;/a&gt; Why does GRPO work? | kalomaze&amp;#39;s kalomazing blog&lt;/p&gt;\\n\\n&lt;p&gt;You can make your own with unsloth&lt;/p&gt;\\n\\n&lt;p&gt;Essentially during a RL run you ask a model to solve one problem in a specific format 100 times. You can score each sample. Then it essentially gets an idea what went right. This takes a lot of steps. You can score it by giving it a reward for how much it writes, correct wrong etc.&lt;/p&gt;\\n\\n&lt;p&gt;It’s not just think tags it can be anything. I made a battle tag in it a Viking fights a pirate and knight. Each are scored to not copy each other and at the end a robot judges the best answer. So asking them how to do a math problem gives you three ways to do it and something reviews it. The user just sees the answer. In the background they are mad talking smack about each other.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwrad1/how_are_reasonable_models_built_are_they_fine/n2gokf4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752193313,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwrad1","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2i27ds","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Thomas-Lore","can_mod_post":false,"created_utc":1752212821,"send_replies":true,"parent_id":"t3_1lwrad1","score":2,"author_fullname":"t2_5hobp6m4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Read the Deepseek R1 paper: https://arxiv.org/abs/2501.12948","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2i27ds","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Read the Deepseek R1 paper: &lt;a href=\\"https://arxiv.org/abs/2501.12948\\"&gt;https://arxiv.org/abs/2501.12948&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwrad1/how_are_reasonable_models_built_are_they_fine/n2i27ds/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752212821,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwrad1","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2j84xa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1752234758,"send_replies":true,"parent_id":"t3_1lwrad1","score":1,"author_fullname":"t2_uz37qfx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt;How are **reasonable** models built\\n\\nInteresting question.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2j84xa","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;How are &lt;strong&gt;reasonable&lt;/strong&gt; models built&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Interesting question.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwrad1/how_are_reasonable_models_built_are_they_fine/n2j84xa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752234758,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwrad1","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(t,{data:a});export{r as default};
