import{j as e}from"./index-CjwP30j7.js";import{R as t}from"./RedditPostRenderer-BbYuEq_V.js";import"./index-C-yxLSPN.js";const l=[{kind:"Listing",data:{after:null,dist:1,modhash:"",geo_filter:"",children:[{kind:"t3",data:{approved_at_utc:null,subreddit:"LocalLLaMA",selftext:`The thought progress bar looks cool.

Unfortunately, this needs to train something to modify hidden state.`,user_reports:[],saved:!1,mod_reason_title:null,gilded:0,clicked:!1,title:"[PAPER] Overclocking LLM Reasoning: Monitoring and Controlling Thinking Path Lengths in LLMs",link_flair_richtext:[{e:"text",t:"Resources"}],subreddit_name_prefixed:"r/LocalLLaMA",hidden:!1,pwls:6,link_flair_css_class:"",downs:0,thumbnail_height:null,top_awarded_type:null,hide_score:!1,name:"t3_1ltstdt",quarantine:!1,link_flair_text_color:"light",upvote_ratio:1,author_flair_background_color:null,subreddit_type:"public",ups:6,total_awards_received:0,media_embed:{},thumbnail_width:null,author_flair_template_id:null,is_original_content:!1,author_fullname:"t2_g644e",secure_media:null,is_reddit_media_domain:!1,is_meta:!1,category:null,secure_media_embed:{},link_flair_text:"Resources",can_mod_post:!1,score:6,approved_by:null,is_created_from_ads_ui:!1,author_premium:!1,thumbnail:"default",edited:!1,author_flair_css_class:null,author_flair_richtext:[],gildings:{},content_categories:null,is_self:!1,mod_note:null,created:1751891156,link_flair_type:"richtext",wls:6,removed_by_category:null,banned_by:null,author_flair_type:"text",domain:"royeisen.github.io",allow_live_comments:!1,selftext_html:`&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The thought progress bar looks cool.&lt;/p&gt;

&lt;p&gt;Unfortunately, this needs to train something to modify hidden state.&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;`,likes:null,suggested_sort:null,banned_at_utc:null,url_overridden_by_dest:"https://royeisen.github.io/OverclockingLLMReasoning-paper/",view_count:null,archived:!1,no_follow:!1,is_crosspostable:!1,pinned:!1,over_18:!1,all_awardings:[],awarders:[],media_only:!1,link_flair_template_id:"ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",can_gild:!1,spoiler:!1,locked:!1,author_flair_text:null,treatment_tags:[],visited:!1,removed_by:null,num_reports:null,distinguished:null,subreddit_id:"t5_81eyvm",author_is_blocked:!1,mod_reason_by:null,removal_reason:null,link_flair_background_color:"#ccac2b",id:"1ltstdt",is_robot_indexable:!0,num_duplicates:1,report_reasons:null,author:"foldl-li",discussion_type:null,num_comments:1,send_replies:!0,media:null,contest_mode:!1,author_patreon_flair:!1,author_flair_text_color:null,permalink:"/r/LocalLLaMA/comments/1ltstdt/paper_overclocking_llm_reasoning_monitoring_and/",stickied:!1,url:"https://royeisen.github.io/OverclockingLLMReasoning-paper/",subreddit_subscribers:495645,created_utc:1751891156,num_crossposts:0,mod_reports:[],is_video:!1}}],before:null}},{kind:"Listing",data:{after:null,dist:null,modhash:"",geo_filter:"",children:[{kind:"t1",data:{subreddit_id:"t5_81eyvm",approved_at_utc:null,author_is_blocked:!1,comment_type:null,awarders:[],mod_reason_by:null,banned_by:null,author_flair_type:"text",total_awards_received:0,subreddit:"LocalLLaMA",author_flair_template_id:null,likes:null,replies:"",user_reports:[],saved:!1,id:"n1sq2uc",banned_at_utc:null,mod_reason_title:null,gilded:0,archived:!1,collapsed_reason_code:null,no_follow:!1,author:"Chromix_",can_mod_post:!1,created_utc:1751891752,send_replies:!0,parent_id:"t3_1ltstdt",score:4,author_fullname:"t2_k7w2h",approved_by:null,mod_note:null,all_awardings:[],collapsed:!1,body:`Predicting how long the LLM will reason for until it found an answer is not possible, at least not accurately. Windows doesn't even get it right in simple cases with the progress bars stuck at 99%. The third "Reasoning loading bar" example nicely shows how the progress gets slower and slower as reasoning continues.

&gt;we manipulate the internal progress encoding during inference to reduce unnecessary steps

It's also not possible to decide ahead of time whether or not specific reasoning tokens will lead to an (in)correct result.

The tests were exclusively done on math benchmarks. Maybe it's possible to shave off some tokens there without much loss. I doubt that this will generalize as-is though.`,edited:!1,top_awarded_type:null,author_flair_css_class:null,name:"t1_n1sq2uc",is_submitter:!1,downs:0,author_flair_richtext:[],author_patreon_flair:!1,body_html:`&lt;div class="md"&gt;&lt;p&gt;Predicting how long the LLM will reason for until it found an answer is not possible, at least not accurately. Windows doesn&amp;#39;t even get it right in simple cases with the progress bars stuck at 99%. The third &amp;quot;Reasoning loading bar&amp;quot; example nicely shows how the progress gets slower and slower as reasoning continues.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;we manipulate the internal progress encoding during inference to reduce unnecessary steps&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;It&amp;#39;s also not possible to decide ahead of time whether or not specific reasoning tokens will lead to an (in)correct result.&lt;/p&gt;

&lt;p&gt;The tests were exclusively done on math benchmarks. Maybe it&amp;#39;s possible to shave off some tokens there without much loss. I doubt that this will generalize as-is though.&lt;/p&gt;
&lt;/div&gt;`,removal_reason:null,collapsed_reason:null,distinguished:null,associated_award:null,stickied:!1,author_premium:!1,can_gild:!1,gildings:{},unrepliable_reason:null,author_flair_text_color:null,score_hidden:!1,permalink:"/r/LocalLLaMA/comments/1ltstdt/paper_overclocking_llm_reasoning_monitoring_and/n1sq2uc/",subreddit_type:"public",locked:!1,report_reasons:null,created:1751891752,author_flair_text:null,treatment_tags:[],link_id:"t3_1ltstdt",subreddit_name_prefixed:"r/LocalLLaMA",controversiality:0,depth:0,author_flair_background_color:null,collapsed_because_crowd_control:null,mod_reports:[],num_reports:null,ups:4}}],before:null}}],o=()=>e.jsx(t,{data:l});export{o as default};
