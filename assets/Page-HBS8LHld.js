import{j as e}from"./index-CWmJdUH_.js";import{R as l}from"./RedditPostRenderer-D2iunoQ9.js";import"./index-BCg9RP6g.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I don't mean one-off responses that sound good, I'm thinking more along the lines of: ways in which you've gotten the model working reliably in a workflow or pipeline of some kind, or fine tuned it for a specific task that it performs jus as well as the cloudAI behemoths.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"What's the most complex thing you've been able to (consistently) do with a 4B LLM?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lppg3g","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.96,"author_flair_background_color":null,"subreddit_type":"public","ups":124,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_c9j5fpaz","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":124,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751440542,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t mean one-off responses that sound good, I&amp;#39;m thinking more along the lines of: ways in which you&amp;#39;ve gotten the model working reliably in a workflow or pipeline of some kind, or fine tuned it for a specific task that it performs jus as well as the cloudAI behemoths.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lppg3g","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"noellarkin","discussion_type":null,"num_comments":64,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/","subreddit_subscribers":494001,"created_utc":1751440542,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0xw57f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Ruin-Capable","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0xrx41","score":5,"author_fullname":"t2_2g4fwosy","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thank you.  That's very helpful.  Seems like there would be the possibility for an impedence mismatch between the llm tokenization scheme, and the lexical tokens expected by grammar.  For example if a particular keyword in the language is represented as multiple tokens by the LLM, you would have to be more careful, and  look at whether or not an LLM token is a prefix or part of a prefix to one of the valid lexical tokens.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n0xw57f","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you.  That&amp;#39;s very helpful.  Seems like there would be the possibility for an impedence mismatch between the llm tokenization scheme, and the lexical tokens expected by grammar.  For example if a particular keyword in the language is represented as multiple tokens by the LLM, you would have to be more careful, and  look at whether or not an LLM token is a prefix or part of a prefix to one of the valid lexical tokens.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lppg3g","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0xw57f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751463598,"author_flair_text":null,"treatment_tags":[],"created_utc":1751463598,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0zh8xx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Mkengine","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0xrx41","score":1,"author_fullname":"t2_9p2xe","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I am still trying to understand the use case. What you described I usually solve by using json enum values, is your way better?","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n0zh8xx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I am still trying to understand the use case. What you described I usually solve by using json enum values, is your way better?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lppg3g","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0zh8xx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751479931,"author_flair_text":null,"treatment_tags":[],"created_utc":1751479931,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0xrx41","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"2oby","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0xmwrv","score":14,"author_fullname":"t2_crs71","approved_by":null,"mod_note":null,"all_awardings":[],"body":"a gbnf grammar to restrict only numbers between 1-10 as a JSON looks like this:\\n\\nroot ::= \\"{\\\\\\\\\\"number\\\\\\\\\\":\\" &lt;value&gt; \\"}\\"  \\n&lt;value&gt; ::= \\"1\\" | \\"2\\" | \\"3\\" | \\"4\\" | \\"5\\" | \\"6\\" | \\"7\\" | \\"8\\" | \\"9\\" | \\"10\\"\\n\\nthisgives something like one of these outputs below (depending on the prompt).  \\n{\\"number\\":1}\\n\\n{\\"number\\":7}\\n\\n{\\"number\\":10}\\n\\nBasically you clamp the output and stop it choosing any output token that is not possible according to the grammar definition.\\n\\nthis is separate from the System prompt and is passed to the model when you load it, or when you call it depending on how you are using it.  \\n(Note: I tried outlines, couldn't get it to work, and complied a new version of llama.cpp to get it to work on my Orin Nano using Qwen3)\\n\\n(edits: typo and added example (not run this maybe not 100% correct, but you get the idea))\\n\\ngrammar = LlamaGrammar.from\\\\_file(\\"number\\\\_1\\\\_to\\\\_10.gbnf\\")  \\nllm = Llama(model\\\\_path=\\"models/your-model.gguf\\")  \\nres = llm(\\n\\nprompt=\\"/nothink Give me a number from 1 to 10 in JSON format:\\",\\n\\nmax\\\\_tokens=16,\\n\\ngrammar=grammar,\\n\\nstop=\\\\[\\"\\\\\\\\n\\"\\\\],\\n\\n)","edited":1751462879,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0xrx41","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;a gbnf grammar to restrict only numbers between 1-10 as a JSON looks like this:&lt;/p&gt;\\n\\n&lt;p&gt;root ::= &amp;quot;{\\\\&amp;quot;number\\\\&amp;quot;:&amp;quot; &amp;lt;value&amp;gt; &amp;quot;}&amp;quot;&lt;br/&gt;\\n&amp;lt;value&amp;gt; ::= &amp;quot;1&amp;quot; | &amp;quot;2&amp;quot; | &amp;quot;3&amp;quot; | &amp;quot;4&amp;quot; | &amp;quot;5&amp;quot; | &amp;quot;6&amp;quot; | &amp;quot;7&amp;quot; | &amp;quot;8&amp;quot; | &amp;quot;9&amp;quot; | &amp;quot;10&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;thisgives something like one of these outputs below (depending on the prompt).&lt;br/&gt;\\n{&amp;quot;number&amp;quot;:1}&lt;/p&gt;\\n\\n&lt;p&gt;{&amp;quot;number&amp;quot;:7}&lt;/p&gt;\\n\\n&lt;p&gt;{&amp;quot;number&amp;quot;:10}&lt;/p&gt;\\n\\n&lt;p&gt;Basically you clamp the output and stop it choosing any output token that is not possible according to the grammar definition.&lt;/p&gt;\\n\\n&lt;p&gt;this is separate from the System prompt and is passed to the model when you load it, or when you call it depending on how you are using it.&lt;br/&gt;\\n(Note: I tried outlines, couldn&amp;#39;t get it to work, and complied a new version of llama.cpp to get it to work on my Orin Nano using Qwen3)&lt;/p&gt;\\n\\n&lt;p&gt;(edits: typo and added example (not run this maybe not 100% correct, but you get the idea))&lt;/p&gt;\\n\\n&lt;p&gt;grammar = LlamaGrammar.from_file(&amp;quot;number_1_to_10.gbnf&amp;quot;)&lt;br/&gt;\\nllm = Llama(model_path=&amp;quot;models/your-model.gguf&amp;quot;)&lt;br/&gt;\\nres = llm(&lt;/p&gt;\\n\\n&lt;p&gt;prompt=&amp;quot;/nothink Give me a number from 1 to 10 in JSON format:&amp;quot;,&lt;/p&gt;\\n\\n&lt;p&gt;max_tokens=16,&lt;/p&gt;\\n\\n&lt;p&gt;grammar=grammar,&lt;/p&gt;\\n\\n&lt;p&gt;stop=[&amp;quot;\\\\n&amp;quot;],&lt;/p&gt;\\n\\n&lt;p&gt;)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lppg3g","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0xrx41/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751462213,"author_flair_text":null,"treatment_tags":[],"created_utc":1751462213,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":14}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0xwe0x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ArcaneThoughts","can_mod_post":false,"created_utc":1751463677,"send_replies":true,"parent_id":"t1_n0xsung","score":5,"author_fullname":"t2_hgivzvub","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm not too familiar with grammars, I mostly use JSON schemas when possible, but yes, it's built into the inference engine so it would do a softmax on the valid tokens and sample from that.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n0xwe0x","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m not too familiar with grammars, I mostly use JSON schemas when possible, but yes, it&amp;#39;s built into the inference engine so it would do a softmax on the valid tokens and sample from that.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lppg3g","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0xwe0x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751463677,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n10bx6v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"homarp","can_mod_post":false,"created_utc":1751489041,"send_replies":true,"parent_id":"t1_n0xsung","score":1,"author_fullname":"t2_bbas9","approved_by":null,"mod_note":null,"all_awardings":[],"body":"for example in llama.cpp\\nhttps://github.com/ggml-org/llama.cpp/blob/master/grammars/README.md","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n10bx6v","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;for example in llama.cpp\\n&lt;a href=\\"https://github.com/ggml-org/llama.cpp/blob/master/grammars/README.md\\"&gt;https://github.com/ggml-org/llama.cpp/blob/master/grammars/README.md&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lppg3g","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n10bx6v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751489041,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0xsung","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ruin-Capable","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0xqbl4","score":3,"author_fullname":"t2_2g4fwosy","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"So it's built into actually inferencing engine?   \\n  \\nIf you were using an LALR(1) grammar, you would eliminate (llm) tokens that couldn't be used to construct a valid lexical token in the target language, then do a softmax on the remaining valid (llm) tokens and sample from that? I apologize if the question is unclear.\\n\\nThanks again for your response.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n0xsung","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So it&amp;#39;s built into actually inferencing engine?   &lt;/p&gt;\\n\\n&lt;p&gt;If you were using an LALR(1) grammar, you would eliminate (llm) tokens that couldn&amp;#39;t be used to construct a valid lexical token in the target language, then do a softmax on the remaining valid (llm) tokens and sample from that? I apologize if the question is unclear.&lt;/p&gt;\\n\\n&lt;p&gt;Thanks again for your response.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lppg3g","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0xsung/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751462521,"author_flair_text":null,"treatment_tags":[],"created_utc":1751462521,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0xqbl4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ArcaneThoughts","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0xmwrv","score":12,"author_fullname":"t2_hgivzvub","approved_by":null,"mod_note":null,"all_awardings":[],"body":"It's a little bit more complicated than that. LLMs produce one token at a time, picking from a list of \\"probabilities\\". Grammars and JSON schemas remove the tokens at each step from that list that would not conform with the desired output structure.\\n\\nSo it's neither just prompting nor retries, it's literally forcing the model to adhere to the structure you give it.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0xqbl4","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s a little bit more complicated than that. LLMs produce one token at a time, picking from a list of &amp;quot;probabilities&amp;quot;. Grammars and JSON schemas remove the tokens at each step from that list that would not conform with the desired output structure.&lt;/p&gt;\\n\\n&lt;p&gt;So it&amp;#39;s neither just prompting nor retries, it&amp;#39;s literally forcing the model to adhere to the structure you give it.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lppg3g","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0xqbl4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751461677,"author_flair_text":null,"treatment_tags":[],"created_utc":1751461677,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}}],"before":null}},"user_reports":[],"saved":false,"id":"n0xmwrv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Ruin-Capable","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0xj9g9","score":3,"author_fullname":"t2_2g4fwosy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I was asking how you use grammars and schemas to constrain LLM output.  Do you just tell the model in the system prompt \\"Hey make your responses conform to the following grammar or json-schema: &lt;insert grammar or schema def here\\"?  Or do you internally do something different like have a loop that keeps rejecting responses until it generates one that conforms?  Or is there some other mechanism that I'm unaware of (and how does it work)?\\n\\nThe reason I was asking is that the first option (using a system prompt to tell it to make its responses conform) seems too simplistic and I couldn't see how it could work reliably, but I was willing to be convinced.","edited":1751460689,"author_flair_css_class":null,"name":"t1_n0xmwrv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I was asking how you use grammars and schemas to constrain LLM output.  Do you just tell the model in the system prompt &amp;quot;Hey make your responses conform to the following grammar or json-schema: &amp;lt;insert grammar or schema def here&amp;quot;?  Or do you internally do something different like have a loop that keeps rejecting responses until it generates one that conforms?  Or is there some other mechanism that I&amp;#39;m unaware of (and how does it work)?&lt;/p&gt;\\n\\n&lt;p&gt;The reason I was asking is that the first option (using a system prompt to tell it to make its responses conform) seems too simplistic and I couldn&amp;#39;t see how it could work reliably, but I was willing to be convinced.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lppg3g","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0xmwrv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751460496,"author_flair_text":null,"collapsed":false,"created_utc":1751460496,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0yo3uh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"RegisteredJustToSay","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0xmvpt","score":9,"author_fullname":"t2_41ok7","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Depends on what you're after. Without the schema the model has a tendency to invent new fields, return multiple objects, forget what specific fields are for, etc, even when it *technically* returns JSON. It's really night and day - I went from something like a 50% usable output rate to 100% (not a single issue yet) with non-trivial JSON, which is a big deal to me because my use case is not interactive.\\n\\n\\nI could see a dynamic schema being advantageous if you're after completely free form structured data output for arbitrary inputs (e.g. key facts from articles) which don't need strict programmatic parsing but can be fed to another LLM later, or maybe be presented as a hierarchy of information.","edited":1751471921,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0yo3uh","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Depends on what you&amp;#39;re after. Without the schema the model has a tendency to invent new fields, return multiple objects, forget what specific fields are for, etc, even when it &lt;em&gt;technically&lt;/em&gt; returns JSON. It&amp;#39;s really night and day - I went from something like a 50% usable output rate to 100% (not a single issue yet) with non-trivial JSON, which is a big deal to me because my use case is not interactive.&lt;/p&gt;\\n\\n&lt;p&gt;I could see a dynamic schema being advantageous if you&amp;#39;re after completely free form structured data output for arbitrary inputs (e.g. key facts from articles) which don&amp;#39;t need strict programmatic parsing but can be fed to another LLM later, or maybe be presented as a hierarchy of information.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lppg3g","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0yo3uh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751471690,"author_flair_text":null,"treatment_tags":[],"created_utc":1751471690,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}}],"before":null}},"user_reports":[],"saved":false,"id":"n0xmvpt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Repulsive-Memory-298","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0xj9g9","score":3,"author_fullname":"t2_bdznq4n6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"but the prompt makes the schema satisfaction “highly likely”","edited":false,"author_flair_css_class":null,"name":"t1_n0xmvpt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;but the prompt makes the schema satisfaction “highly likely”&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lppg3g","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0xmvpt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751460485,"author_flair_text":null,"collapsed":false,"created_utc":1751460485,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0xj9g9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ArcaneThoughts","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0xhqbp","score":11,"author_fullname":"t2_hgivzvub","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Not sure if you are asking if that's what it means to use a schema or saying that you could just change the prompt.\\n\\nUsing JSON schemas, for instance, the model is forced internally to produce output that satisfies the schema, instead of just \\"highly likely\\" like when using just prompts. This is more useful the smaller the model is as they would be more likely to make mistakes in the format of the JSON, like missing the amount of parenthesis to close. But even with the biggest models by using JSON schema you ensure you will get output in a consistent format, which helps a lot in production.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0xj9g9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not sure if you are asking if that&amp;#39;s what it means to use a schema or saying that you could just change the prompt.&lt;/p&gt;\\n\\n&lt;p&gt;Using JSON schemas, for instance, the model is forced internally to produce output that satisfies the schema, instead of just &amp;quot;highly likely&amp;quot; like when using just prompts. This is more useful the smaller the model is as they would be more likely to make mistakes in the format of the JSON, like missing the amount of parenthesis to close. But even with the biggest models by using JSON schema you ensure you will get output in a consistent format, which helps a lot in production.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lppg3g","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0xj9g9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751459161,"author_flair_text":null,"treatment_tags":[],"created_utc":1751459161,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}}],"before":null}},"user_reports":[],"saved":false,"id":"n0xhqbp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ruin-Capable","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0x7gdw","score":3,"author_fullname":"t2_2g4fwosy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You use the grammar/schema as part of the prompt and it just naturally understands it, and generates conforming responses?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0xhqbp","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You use the grammar/schema as part of the prompt and it just naturally understands it, and generates conforming responses?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lppg3g","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0xhqbp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751458583,"author_flair_text":null,"treatment_tags":[],"created_utc":1751458583,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ynebw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RegisteredJustToSay","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0x7gdw","score":0,"author_fullname":"t2_41ok7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It does modify the behaviour of the LLM, but yes, I agree. In general the engineering is lagging so far behind the hypetrain.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0ynebw","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It does modify the behaviour of the LLM, but yes, I agree. In general the engineering is lagging so far behind the hypetrain.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lppg3g","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0ynebw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751471495,"author_flair_text":null,"treatment_tags":[],"created_utc":1751471495,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n0x7gdw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ArcaneThoughts","can_mod_post":false,"created_utc":1751454292,"send_replies":true,"parent_id":"t1_n0wyoui","score":38,"author_fullname":"t2_hgivzvub","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Grammars and JSON schemas are so incredibly underrated. I use them on every single project.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0x7gdw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Grammars and JSON schemas are so incredibly underrated. I use them on every single project.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lppg3g","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0x7gdw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751454292,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":38}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0y3chh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"2oby","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0xunei","score":9,"author_fullname":"t2_crs71","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I don't have the prompt here, but /nothink is important, and it is something like:  \\n\\"/nothink you are a JSON generator. Take the users input and return ONLY a valid JSON containing only the keys: device, action, location.   \\ne.g. User: \\"Turn on the bedroom lights\\" JSON generator:{\\"device\\":\\"lights\\", \\"action\\":\\"on\\", \\"location\\",\\"bedroom}\\"\\n\\nI have been over dozens of iterations, there are more examples and there is stuff in there about synonyms...\\n\\nAlso Temp, Top-P and Top-K change things a lot.... \\n\\nBasically once you have got the grammar working, its a lot of tiral and error to get all the parameters tweaked.\\n\\nI have it checked into GIT Hub... but its too horrible to show anybody ;) if you are interested, ask me again in a month!","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0y3chh","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t have the prompt here, but /nothink is important, and it is something like:&lt;br/&gt;\\n&amp;quot;/nothink you are a JSON generator. Take the users input and return ONLY a valid JSON containing only the keys: device, action, location.&lt;br/&gt;\\ne.g. User: &amp;quot;Turn on the bedroom lights&amp;quot; JSON generator:{&amp;quot;device&amp;quot;:&amp;quot;lights&amp;quot;, &amp;quot;action&amp;quot;:&amp;quot;on&amp;quot;, &amp;quot;location&amp;quot;,&amp;quot;bedroom}&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;I have been over dozens of iterations, there are more examples and there is stuff in there about synonyms...&lt;/p&gt;\\n\\n&lt;p&gt;Also Temp, Top-P and Top-K change things a lot.... &lt;/p&gt;\\n\\n&lt;p&gt;Basically once you have got the grammar working, its a lot of tiral and error to get all the parameters tweaked.&lt;/p&gt;\\n\\n&lt;p&gt;I have it checked into GIT Hub... but its too horrible to show anybody ;) if you are interested, ask me again in a month!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lppg3g","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0y3chh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751465811,"author_flair_text":null,"treatment_tags":[],"created_utc":1751465811,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}}],"before":null}},"user_reports":[],"saved":false,"id":"n0xunei","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ASTRdeca","can_mod_post":false,"created_utc":1751463113,"send_replies":true,"parent_id":"t1_n0wyoui","score":3,"author_fullname":"t2_e11po","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm very surprised a 0.6B can do that consistently.. I had issues getting qwen3 14B and 30B A3B to do this consistently, but found success with Gemma3 27B. What's your prompt for these kinds of tasks?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0xunei","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m very surprised a 0.6B can do that consistently.. I had issues getting qwen3 14B and 30B A3B to do this consistently, but found success with Gemma3 27B. What&amp;#39;s your prompt for these kinds of tasks?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lppg3g","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0xunei/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751463113,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0y3sc8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"some_user_2021","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0y1ubo","score":3,"author_fullname":"t2_cmfeo4yq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Where can I go and learn things like this?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0y3sc8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Where can I go and learn things like this?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lppg3g","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0y3sc8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751465939,"author_flair_text":null,"treatment_tags":[],"created_utc":1751465939,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ygn64","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"TopImaginary5996","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0y1ubo","score":2,"author_fullname":"t2_1nf3fhsk8j","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"That's amazing. :) Thank you so much for the generous reply. :)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ygn64","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s amazing. :) Thank you so much for the generous reply. :)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lppg3g","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0ygn64/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751469629,"author_flair_text":null,"treatment_tags":[],"created_utc":1751469629,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0y1ubo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"2oby","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0xj2o3","score":5,"author_fullname":"t2_crs71","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Small, it is things like turn on the kitchen lights, set the bathroom heating to 25 degrees.  \\nI may have to abandon numerical values as things get sketchy as the prompt gets too long (user+system). But for this use case, i.e. read the entities from Home Assistant, create grammar with entities and actions, produce JSON from user free text, it works.\\n\\nIt even sometimes works with vague things like \\"It's too hot in the bathroom\\", but these are less reliable than the simple cases. I have many models, but so far Qwen3 0.6B is the smallest, fastest model for this use case.\\n\\n**QBNF makes an enormous difference!!** The model can literally not return invalid JSON. The contents can be wrong, but being right is the path of least resistance... works surprisingly well.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0y1ubo","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Small, it is things like turn on the kitchen lights, set the bathroom heating to 25 degrees.&lt;br/&gt;\\nI may have to abandon numerical values as things get sketchy as the prompt gets too long (user+system). But for this use case, i.e. read the entities from Home Assistant, create grammar with entities and actions, produce JSON from user free text, it works.&lt;/p&gt;\\n\\n&lt;p&gt;It even sometimes works with vague things like &amp;quot;It&amp;#39;s too hot in the bathroom&amp;quot;, but these are less reliable than the simple cases. I have many models, but so far Qwen3 0.6B is the smallest, fastest model for this use case.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;QBNF makes an enormous difference!!&lt;/strong&gt; The model can literally not return invalid JSON. The contents can be wrong, but being right is the path of least resistance... works surprisingly well.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lppg3g","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0y1ubo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751465366,"author_flair_text":null,"treatment_tags":[],"created_utc":1751465366,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n0xj2o3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"TopImaginary5996","can_mod_post":false,"created_utc":1751459092,"send_replies":true,"parent_id":"t1_n0wyoui","score":1,"author_fullname":"t2_1nf3fhsk8j","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thank you for posting this, it's great to know that you can do that with Qwen(3?)-0.6B!\\n\\nIf I may ask, on average how large (and how complex if you could give a qualitative indication) are your inputs, and how many fields are there in the JSON output?\\n\\nI have a couple of scripts that analyzes various types of text littered in my coding projects to produce JSON outputs. I have spent a few weekend tweaking my prompts but could never get satisfactory results with any model with less than 8B parameters. Or is GBNF the biggest determinant in the quality of outcomes?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0xj2o3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you for posting this, it&amp;#39;s great to know that you can do that with Qwen(3?)-0.6B!&lt;/p&gt;\\n\\n&lt;p&gt;If I may ask, on average how large (and how complex if you could give a qualitative indication) are your inputs, and how many fields are there in the JSON output?&lt;/p&gt;\\n\\n&lt;p&gt;I have a couple of scripts that analyzes various types of text littered in my coding projects to produce JSON outputs. I have spent a few weekend tweaking my prompts but could never get satisfactory results with any model with less than 8B parameters. Or is GBNF the biggest determinant in the quality of outcomes?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lppg3g","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0xj2o3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751459092,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1066ia","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"sumguysr","can_mod_post":false,"created_utc":1751487389,"send_replies":true,"parent_id":"t1_n0wyoui","score":1,"author_fullname":"t2_5iznl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You gave the LLM a response format I'm GBNF in the prompt?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1066ia","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You gave the LLM a response format I&amp;#39;m GBNF in the prompt?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lppg3g","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n1066ia/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751487389,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0wyoui","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"2oby","can_mod_post":false,"created_utc":1751449829,"send_replies":true,"parent_id":"t3_1lppg3g","score":104,"author_fullname":"t2_crs71","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"qwen0.6B 6bit quantised -  used to turn natural language into JSON, e.g. turn on the bathroom lights -&gt;{\\"device\\":\\"lights\\", \\"location:\\"bathroom\\", \\"action\\":\\"on\\"}\\n\\nGetting prompt right was critical, but understanding gbnf grammar is what enabled the tiny LLM to be 'production ready' (I don't see gbnf mentioned much, but it's incredible for constraining well formed responses.)  \\nThe API and LLM run on a 8GB Orin Nano with around 2 sec latency (depending on the size of the System Prompt).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0wyoui","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;qwen0.6B 6bit quantised -  used to turn natural language into JSON, e.g. turn on the bathroom lights -&amp;gt;{&amp;quot;device&amp;quot;:&amp;quot;lights&amp;quot;, &amp;quot;location:&amp;quot;bathroom&amp;quot;, &amp;quot;action&amp;quot;:&amp;quot;on&amp;quot;}&lt;/p&gt;\\n\\n&lt;p&gt;Getting prompt right was critical, but understanding gbnf grammar is what enabled the tiny LLM to be &amp;#39;production ready&amp;#39; (I don&amp;#39;t see gbnf mentioned much, but it&amp;#39;s incredible for constraining well formed responses.)&lt;br/&gt;\\nThe API and LLM run on a 8GB Orin Nano with around 2 sec latency (depending on the size of the System Prompt).&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0wyoui/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751449829,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lppg3g","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":104}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0wwblr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"RMCPhoto","can_mod_post":false,"created_utc":1751448476,"send_replies":true,"parent_id":"t1_n0wpinw","score":9,"author_fullname":"t2_ehhvb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This is a very good example of how to successfully use small language models for complex problems - break down the problem until the individual task is no longer complex.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0wwblr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is a very good example of how to successfully use small language models for complex problems - break down the problem until the individual task is no longer complex.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lppg3g","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0wwblr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751448476,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0x7p1x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"admajic","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0x5c2p","score":2,"author_fullname":"t2_60b9farf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Interesting. Didnt think of that","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0x7p1x","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Interesting. Didnt think of that&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lppg3g","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0x7p1x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751454403,"author_flair_text":null,"treatment_tags":[],"created_utc":1751454403,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0x5c2p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Mescallan","can_mod_post":false,"created_utc":1751453305,"send_replies":true,"parent_id":"t1_n0wpinw","score":5,"author_fullname":"t2_3ykrc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Also speculative decoding. Have small models answer a question/task, then have a larger model confirm with a yes or no. No can reroute to the small model or have the big model take over","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0x5c2p","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Also speculative decoding. Have small models answer a question/task, then have a larger model confirm with a yes or no. No can reroute to the small model or have the big model take over&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lppg3g","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0x5c2p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751453305,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0wq33z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Tenzu9","can_mod_post":false,"created_utc":1751444774,"send_replies":true,"parent_id":"t1_n0wpinw","score":2,"author_fullname":"t2_10wgss","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yep, it's an excellent draft model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0wq33z","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yep, it&amp;#39;s an excellent draft model.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lppg3g","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0wq33z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751444774,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0wpinw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"admajic","can_mod_post":false,"created_utc":1751444430,"send_replies":true,"parent_id":"t3_1lppg3g","score":41,"author_fullname":"t2_60b9farf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Qwen3 0.6b could actually make a better prompt in an agent workflow. 4b better 8b much better. \\n\\nhttps://github.com/adamjen/Prompt_Maker\\n\\nTry it out.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0wpinw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen3 0.6b could actually make a better prompt in an agent workflow. 4b better 8b much better. &lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/adamjen/Prompt_Maker\\"&gt;https://github.com/adamjen/Prompt_Maker&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Try it out.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0wpinw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751444430,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lppg3g","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":41}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0yofaj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Logan_Maransy","can_mod_post":false,"created_utc":1751471776,"send_replies":true,"parent_id":"t1_n0wqicb","score":1,"author_fullname":"t2_8afxa","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Your comment describes almost exactly what I want to do on one of my side projects. Specifically I've tried using Gemma3 4B as a \\"clean raw text data\\" model, where the raw text is output from a locally run Whisper model capturing STT. Sometimes there's just... terrible grammar, punctuation, or spelling mistakes in the raw STT. Because I'm using this text as ground truth data for fine tune training an LLM, I want it to be very clean and nice and without random periods where they shouldn't be.\\n\\n\\nHowever I haven't had much luck with Gemma3 4B... So maybe my starting prompt is wrong or bad. But from what I've read Gemma3 4B doesn't even have the ability to run a system prompt, it just gets added to the first user message. ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0yofaj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Your comment describes almost exactly what I want to do on one of my side projects. Specifically I&amp;#39;ve tried using Gemma3 4B as a &amp;quot;clean raw text data&amp;quot; model, where the raw text is output from a locally run Whisper model capturing STT. Sometimes there&amp;#39;s just... terrible grammar, punctuation, or spelling mistakes in the raw STT. Because I&amp;#39;m using this text as ground truth data for fine tune training an LLM, I want it to be very clean and nice and without random periods where they shouldn&amp;#39;t be.&lt;/p&gt;\\n\\n&lt;p&gt;However I haven&amp;#39;t had much luck with Gemma3 4B... So maybe my starting prompt is wrong or bad. But from what I&amp;#39;ve read Gemma3 4B doesn&amp;#39;t even have the ability to run a system prompt, it just gets added to the first user message. &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lppg3g","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0yofaj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751471776,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0yqq0b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CtrlAltDelve","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0x13zj","score":3,"author_fullname":"t2_1f1tptkzcs","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I use 4B models for low-level spellcheck/grammar check, and I'll use larger models for more nuanced speech (although I won't lie, if it's something like communications with a customer or an investor, I switch over to a cloud LLM like Opus or Pro 2.5. Kerlig is great in this regard because you can swap the model on any action simply by tapping on Tab/Shift-Tab to cycle through your model list.\\nI swear I'm not a shill for Kerlig; I just love the app.\\n\\nHere's a quick demo with some deliberately typo'd up text: \\n\\nhttps://imgur.com/a/245JELv","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0yqq0b","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I use 4B models for low-level spellcheck/grammar check, and I&amp;#39;ll use larger models for more nuanced speech (although I won&amp;#39;t lie, if it&amp;#39;s something like communications with a customer or an investor, I switch over to a cloud LLM like Opus or Pro 2.5. Kerlig is great in this regard because you can swap the model on any action simply by tapping on Tab/Shift-Tab to cycle through your model list.\\nI swear I&amp;#39;m not a shill for Kerlig; I just love the app.&lt;/p&gt;\\n\\n&lt;p&gt;Here&amp;#39;s a quick demo with some deliberately typo&amp;#39;d up text: &lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://imgur.com/a/245JELv\\"&gt;https://imgur.com/a/245JELv&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"link_id":"t3_1lppg3g","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0yqq0b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751472425,"author_flair_text":null,"treatment_tags":[],"created_utc":1751472425,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0x13zj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"wfamily","can_mod_post":false,"created_utc":1751451141,"send_replies":true,"parent_id":"t1_n0wqicb","score":1,"author_fullname":"t2_7vmbx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"have you not seen the repetitiveness from when you do that for a while?\\n\\n  \\nthe speak to text thing is a fair one tho.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0x13zj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;have you not seen the repetitiveness from when you do that for a while?&lt;/p&gt;\\n\\n&lt;p&gt;the speak to text thing is a fair one tho.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lppg3g","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0x13zj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751451141,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0wqicb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"CtrlAltDelve","can_mod_post":false,"created_utc":1751445031,"send_replies":true,"parent_id":"t3_1lppg3g","score":24,"author_fullname":"t2_1f1tptkzcs","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"4B models are *phenomenal* at spell check/grammar check. Sometimes I use them with tools like Kerlig (MacOS) for rapid on-device edits to messages before sending them. It's way faster than clicking on red underlines and you can create custom actions much quicker. I know the idea of using an LLM for spellcheck/grammar sounds like overkill, but because of what it is, it's capable of rephrasing and contextually correcting spelling way quicker and way more easily than any spell check does. \\n\\nPlus if you use tools like Spokenly or Superwhisper for on-device STT, you can combine those with a 4B or even 8B LLM to post-process the transcribed text and fix its grammar or reflow it to account for \\"um...\\"s and whatnot. \\n\\nGemma 3 4B is great for this.\\n\\nEDIT: \\n\\nHere's a quick demo with some deliberately typo'd up text: \\n\\nhttps://imgur.com/a/245JELv","edited":1751472444,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0wqicb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;4B models are &lt;em&gt;phenomenal&lt;/em&gt; at spell check/grammar check. Sometimes I use them with tools like Kerlig (MacOS) for rapid on-device edits to messages before sending them. It&amp;#39;s way faster than clicking on red underlines and you can create custom actions much quicker. I know the idea of using an LLM for spellcheck/grammar sounds like overkill, but because of what it is, it&amp;#39;s capable of rephrasing and contextually correcting spelling way quicker and way more easily than any spell check does. &lt;/p&gt;\\n\\n&lt;p&gt;Plus if you use tools like Spokenly or Superwhisper for on-device STT, you can combine those with a 4B or even 8B LLM to post-process the transcribed text and fix its grammar or reflow it to account for &amp;quot;um...&amp;quot;s and whatnot. &lt;/p&gt;\\n\\n&lt;p&gt;Gemma 3 4B is great for this.&lt;/p&gt;\\n\\n&lt;p&gt;EDIT: &lt;/p&gt;\\n\\n&lt;p&gt;Here&amp;#39;s a quick demo with some deliberately typo&amp;#39;d up text: &lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://imgur.com/a/245JELv\\"&gt;https://imgur.com/a/245JELv&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0wqicb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751445031,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lppg3g","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":24}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0xfo5e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fgoricha","can_mod_post":false,"created_utc":1751457781,"send_replies":true,"parent_id":"t1_n0wxvge","score":2,"author_fullname":"t2_40xsg56g","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Which fine tuning technique would you consider for models? Like a full fine tune or something like a QLORA?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0xfo5e","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Which fine tuning technique would you consider for models? Like a full fine tune or something like a QLORA?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lppg3g","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0xfo5e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751457781,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0wxvge","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"RMCPhoto","can_mod_post":false,"created_utc":1751449370,"send_replies":true,"parent_id":"t3_1lppg3g","score":10,"author_fullname":"t2_ehhvb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Small LLM's can be just as good for many highly specific tasks in an otherwise complex workflow - even without fine tuning (to a point).  \\n  \\n\\\\* Classification with few-shot examples:   A 4b model will often be just as good as a giant model for few shot or zero shot classification problems, Sentiment, stance, topic, emotion, party-position, etc.   They will still begin to fall apart as the input context grows or near edge conditions, but are often \\"good enough\\".  \\n\\n\\\\* Data extraction from a few paragraphs (named entity extraction etc).\\n\\n\\\\* Short-form summarization (≤ 1 K tokens in / \\\\~200 tokens out)\\n\\n\\\\* Structured data re-formatting / annotation - ie going to specific date standards from non-standard unstructured date, checking &lt; 1k token json blobs for correct structure and fixing, translating between different structures (with low complexity).\\n\\nBreak the overall pipeline into **atomic subtasks** (classify ➔ extract ➔ transform ➔ generate) and assign the *smallest viable model* to each slice.\\n\\nNow, fine tuning... the sky is the limit. This is a different beast. The issue here is making your domain narrow enough so that it fits within the parameter limit of the model.   IE tiny LLM's like Gorilla and Jan-Nano can be just as good or better at function calling / tool use / mcp as the giants when fine tuned on this task. Same for classifcation [https://arxiv.org/html/2406.08660v2](https://arxiv.org/html/2406.08660v2) or any other narrow task.  But they will fall apart when asked to classify/reason/summarize/etc.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0wxvge","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Small LLM&amp;#39;s can be just as good for many highly specific tasks in an otherwise complex workflow - even without fine tuning (to a point).  &lt;/p&gt;\\n\\n&lt;p&gt;* Classification with few-shot examples:   A 4b model will often be just as good as a giant model for few shot or zero shot classification problems, Sentiment, stance, topic, emotion, party-position, etc.   They will still begin to fall apart as the input context grows or near edge conditions, but are often &amp;quot;good enough&amp;quot;.  &lt;/p&gt;\\n\\n&lt;p&gt;* Data extraction from a few paragraphs (named entity extraction etc).&lt;/p&gt;\\n\\n&lt;p&gt;* Short-form summarization (≤ 1 K tokens in / ~200 tokens out)&lt;/p&gt;\\n\\n&lt;p&gt;* Structured data re-formatting / annotation - ie going to specific date standards from non-standard unstructured date, checking &amp;lt; 1k token json blobs for correct structure and fixing, translating between different structures (with low complexity).&lt;/p&gt;\\n\\n&lt;p&gt;Break the overall pipeline into &lt;strong&gt;atomic subtasks&lt;/strong&gt; (classify ➔ extract ➔ transform ➔ generate) and assign the &lt;em&gt;smallest viable model&lt;/em&gt; to each slice.&lt;/p&gt;\\n\\n&lt;p&gt;Now, fine tuning... the sky is the limit. This is a different beast. The issue here is making your domain narrow enough so that it fits within the parameter limit of the model.   IE tiny LLM&amp;#39;s like Gorilla and Jan-Nano can be just as good or better at function calling / tool use / mcp as the giants when fine tuned on this task. Same for classifcation &lt;a href=\\"https://arxiv.org/html/2406.08660v2\\"&gt;https://arxiv.org/html/2406.08660v2&lt;/a&gt; or any other narrow task.  But they will fall apart when asked to classify/reason/summarize/etc.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0wxvge/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751449370,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lppg3g","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0xskig","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_slay_nub","can_mod_post":false,"created_utc":1751462429,"send_replies":true,"parent_id":"t1_n0wk6z7","score":4,"author_fullname":"t2_u8o4d","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It's fun until your battery drains within 5 minutes lol.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0xskig","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s fun until your battery drains within 5 minutes lol.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lppg3g","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0xskig/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751462429,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0wrgdw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ThinkExtension2328","can_mod_post":false,"created_utc":1751445601,"send_replies":true,"parent_id":"t1_n0wk6z7","score":0,"author_fullname":"t2_8eneodlk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Gemma 3n works surprisingly well overall and it’s only a 2b model (E2B) if only they gave it 128 context window.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0wrgdw","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Gemma 3n works surprisingly well overall and it’s only a 2b model (E2B) if only they gave it 128 context window.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lppg3g","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0wrgdw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751445601,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n0wk6z7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"typeryu","can_mod_post":false,"created_utc":1751441269,"send_replies":true,"parent_id":"t3_1lppg3g","score":12,"author_fullname":"t2_me4xe61","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The best I could get it to do was summaries lol. It struggles for most productive tasks I would say, but fun to use it on planes with no wifi.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0wk6z7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The best I could get it to do was summaries lol. It struggles for most productive tasks I would say, but fun to use it on planes with no wifi.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0wk6z7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751441269,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lppg3g","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0z5ck8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GrehgyHils","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ycwsj","score":1,"author_fullname":"t2_cho421r","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Interesting thanks for explaining this","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0z5ck8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Interesting thanks for explaining this&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lppg3g","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0z5ck8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751476565,"author_flair_text":null,"treatment_tags":[],"created_utc":1751476565,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ycwsj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Lesser-than","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0xh1d0","score":1,"author_fullname":"t2_98d256k","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"it would not be that brief because my code is a Spaghetti mess, but for the most part with the --jinja option in llamacpp I have not had any problems getting these 2 models to call tools the trick is you need to use them as agents even if your using it as your main conversational llm. You need to make a seperate call for the function that is a one off prompt and not polluted with previos context. then have the agent report back with its findings.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0ycwsj","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;it would not be that brief because my code is a Spaghetti mess, but for the most part with the --jinja option in llamacpp I have not had any problems getting these 2 models to call tools the trick is you need to use them as agents even if your using it as your main conversational llm. You need to make a seperate call for the function that is a one off prompt and not polluted with previos context. then have the agent report back with its findings.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lppg3g","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0ycwsj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751468568,"author_flair_text":null,"treatment_tags":[],"created_utc":1751468568,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0xh1d0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GrehgyHils","can_mod_post":false,"created_utc":1751458316,"send_replies":true,"parent_id":"t1_n0x5et7","score":2,"author_fullname":"t2_cho421r","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"May you show a brief example of how you achieve this?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0xh1d0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;May you show a brief example of how you achieve this?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lppg3g","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0xh1d0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751458316,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0zav12","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Lesser-than","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0z3bzk","score":2,"author_fullname":"t2_98d256k","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"gemma-3-4b you are correct! , I meant the 4b","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0zav12","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;gemma-3-4b you are correct! , I meant the 4b&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lppg3g","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0zav12/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751478109,"author_flair_text":null,"treatment_tags":[],"created_utc":1751478109,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0z3bzk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"YearnMar10","can_mod_post":false,"created_utc":1751476009,"send_replies":true,"parent_id":"t1_n0x5et7","score":1,"author_fullname":"t2_zldrkfl0t","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What 3b gemma model are you talking about?  Do you mean the 4b version?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0z3bzk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What 3b gemma model are you talking about?  Do you mean the 4b version?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lppg3g","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0z3bzk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751476009,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0x5et7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Lesser-than","can_mod_post":false,"created_utc":1751453342,"send_replies":true,"parent_id":"t3_1lppg3g","score":6,"author_fullname":"t2_98d256k","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I find the 4b qwen3 and 4b gemma3 models exelent function callers.","edited":1751478141,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0x5et7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I find the 4b qwen3 and 4b gemma3 models exelent function callers.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0x5et7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751453342,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lppg3g","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0wwa71","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Mescallan","can_mod_post":false,"created_utc":1751448451,"send_replies":true,"parent_id":"t3_1lppg3g","score":3,"author_fullname":"t2_3ykrc","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"categorizing unstructured data into JSON, you need quite a bit of scaffolding, but I have 11 categories (3 individual calls + 1 call that merges the other 8 categories). Alone it can't really handle it single shot, but with pre-filtering and some other techniques I can reliably categorize unstructured natural language into JSON into {category, subcategory, time, duration, one sentence note.} \\n\\nIt's possible to do it with LLM only using multi-step procedures, where you slowly structure the data each step and get an equivalent JSON, but it's 5-8x slower without scaffolding and lower recall, as you are basically calling the model multiple times for each categorization.\\n\\n  \\nrelative to the cloud API, it's lower quality, but passes the threshold of useable, without actually sending data off my device. I use sonnet 3.5 (because im too lazy to update not for any specific reason) and it's 98% recall with like 95% accuracy in categorization and takes \\\\~90 seconds. Gemma 3 4b with scaffolding for the same task is 85% recall with 90ish% accuracy in 7 minutes on my macbook, but i have it tuned for false positives over false negatives so i can delete items when manually reviewing instead of having to manually add things.  \\n  \\n  \\nI wish I could go more indepth in the use case but it's for a product. AMA and i will answer the best I can.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0wwa71","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;categorizing unstructured data into JSON, you need quite a bit of scaffolding, but I have 11 categories (3 individual calls + 1 call that merges the other 8 categories). Alone it can&amp;#39;t really handle it single shot, but with pre-filtering and some other techniques I can reliably categorize unstructured natural language into JSON into {category, subcategory, time, duration, one sentence note.} &lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s possible to do it with LLM only using multi-step procedures, where you slowly structure the data each step and get an equivalent JSON, but it&amp;#39;s 5-8x slower without scaffolding and lower recall, as you are basically calling the model multiple times for each categorization.&lt;/p&gt;\\n\\n&lt;p&gt;relative to the cloud API, it&amp;#39;s lower quality, but passes the threshold of useable, without actually sending data off my device. I use sonnet 3.5 (because im too lazy to update not for any specific reason) and it&amp;#39;s 98% recall with like 95% accuracy in categorization and takes ~90 seconds. Gemma 3 4b with scaffolding for the same task is 85% recall with 90ish% accuracy in 7 minutes on my macbook, but i have it tuned for false positives over false negatives so i can delete items when manually reviewing instead of having to manually add things.  &lt;/p&gt;\\n\\n&lt;p&gt;I wish I could go more indepth in the use case but it&amp;#39;s for a product. AMA and i will answer the best I can.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0wwa71/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751448451,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lppg3g","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n10jrq2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Daemontatox","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0xs5b3","score":1,"author_fullname":"t2_1rm9syq1nb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I have been using for managing my files , building dummy apps , generating academic papers about very random things .\\n\\nNothing major / real life useful sadly.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n10jrq2","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have been using for managing my files , building dummy apps , generating academic papers about very random things .&lt;/p&gt;\\n\\n&lt;p&gt;Nothing major / real life useful sadly.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lppg3g","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n10jrq2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751491327,"author_flair_text":null,"treatment_tags":[],"created_utc":1751491327,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0xs5b3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"TjFr00","can_mod_post":false,"created_utc":1751462289,"send_replies":true,"parent_id":"t1_n0xb3xc","score":1,"author_fullname":"t2_5lmlxzw0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What’s a real world example you could use it for?  Having trouble to understand how a multi model approach is useful, if the flow is more or less static given from something like flow and specialization between / of the agents.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0xs5b3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What’s a real world example you could use it for?  Having trouble to understand how a multi model approach is useful, if the flow is more or less static given from something like flow and specialization between / of the agents.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lppg3g","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0xs5b3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751462289,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0xb3xc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Daemontatox","can_mod_post":false,"created_utc":1751455912,"send_replies":true,"parent_id":"t3_1lppg3g","score":2,"author_fullname":"t2_1rm9syq1nb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Multi-Agents cooperative workflows , been working on something like a 1 command input and the agents figure the rest themselves.\\n\\nFrom understanding requirements,  planning,  generation,  reflection,  revision,  correction,  testing ...etc.\\n\\nCurrently its only local file systems and i plan on adding browser use and GUI.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0xb3xc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Multi-Agents cooperative workflows , been working on something like a 1 command input and the agents figure the rest themselves.&lt;/p&gt;\\n\\n&lt;p&gt;From understanding requirements,  planning,  generation,  reflection,  revision,  correction,  testing ...etc.&lt;/p&gt;\\n\\n&lt;p&gt;Currently its only local file systems and i plan on adding browser use and GUI.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0xb3xc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751455912,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lppg3g","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0xrtoy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Competitive_Ideal866","can_mod_post":false,"created_utc":1751462182,"send_replies":true,"parent_id":"t3_1lppg3g","score":2,"author_fullname":"t2_1d13xm6n7f","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Mostly gemma3:4b for summarization. I use a script that takes a URL to an article, downloads and summarizes it for me so I can decide whether or not it is worth reading the full article.\\n\\nLarger models do give substantially better summaries but they are much slower.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0xrtoy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Mostly gemma3:4b for summarization. I use a script that takes a URL to an article, downloads and summarizes it for me so I can decide whether or not it is worth reading the full article.&lt;/p&gt;\\n\\n&lt;p&gt;Larger models do give substantially better summaries but they are much slower.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0xrtoy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751462182,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lppg3g","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0xyl1w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"InsideResolve4517","can_mod_post":false,"created_utc":1751464375,"send_replies":true,"parent_id":"t3_1lppg3g","score":2,"author_fullname":"t2_1b8utegv8t","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I have constently got proper tool calling for my personal voice assistant.\\n\\nAnd it 80\\\\~90% of the time picks correct tool &amp; gives proper responses.\\n\\nIn my senario I have apprx 20 tools which I directly provide to llm and more then 50 different type of actions I perform\\n\\nEdit:1\\n\\nI use qwen2.5-coder:3b (1.9GB) which is too small and runs faster. Even if it's qwen fine tuned coder variant but for it it outperforms llama2, llama3, qwen3 in same size. I also feel it's reasoning &amp; understanding is too good based on size","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0xyl1w","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have constently got proper tool calling for my personal voice assistant.&lt;/p&gt;\\n\\n&lt;p&gt;And it 80~90% of the time picks correct tool &amp;amp; gives proper responses.&lt;/p&gt;\\n\\n&lt;p&gt;In my senario I have apprx 20 tools which I directly provide to llm and more then 50 different type of actions I perform&lt;/p&gt;\\n\\n&lt;p&gt;Edit:1&lt;/p&gt;\\n\\n&lt;p&gt;I use qwen2.5-coder:3b (1.9GB) which is too small and runs faster. Even if it&amp;#39;s qwen fine tuned coder variant but for it it outperforms llama2, llama3, qwen3 in same size. I also feel it&amp;#39;s reasoning &amp;amp; understanding is too good based on size&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0xyl1w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751464375,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lppg3g","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"7dba5c08-72f1-11ee-9b6f-ca195bc297d4","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0zlqwt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"PraxisOG","can_mod_post":false,"created_utc":1751481272,"send_replies":true,"parent_id":"t3_1lppg3g","score":2,"author_fullname":"t2_3f9vjjno","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Qwen 3's tool calling. Works well enough that a well prompted 4b can write its own tools and call them","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0zlqwt","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 70B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen 3&amp;#39;s tool calling. Works well enough that a well prompted 4b can write its own tools and call them&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0zlqwt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751481272,"author_flair_text":"Llama 70B","treatment_tags":[],"link_id":"t3_1lppg3g","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0zscfo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"claythearc","can_mod_post":false,"created_utc":1751483236,"send_replies":true,"parent_id":"t3_1lppg3g","score":2,"author_fullname":"t2_65rk0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"To be honest there’s really not a lot. For zero shot tasks they’re ok - function calling, summarizations, prompt generation, isolated tab complete, etc but when you start adding conversation turns or even like 1k of context the performance nose dives. \\n\\nIf you can stay in those constraints though, you can do some meaningful work.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0zscfo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;To be honest there’s really not a lot. For zero shot tasks they’re ok - function calling, summarizations, prompt generation, isolated tab complete, etc but when you start adding conversation turns or even like 1k of context the performance nose dives. &lt;/p&gt;\\n\\n&lt;p&gt;If you can stay in those constraints though, you can do some meaningful work.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0zscfo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751483236,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lppg3g","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0wkrn4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1751441610,"send_replies":true,"parent_id":"t3_1lppg3g","score":3,"author_fullname":"t2_uz37qfx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I did briefly use Llama 3.2 3b for coding assistant, for very very dumb stuff like renaming methods that accept certain parameter. And Gemma 2 2b is surprisingly good at summaries.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0wkrn4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I did briefly use Llama 3.2 3b for coding assistant, for very very dumb stuff like renaming methods that accept certain parameter. And Gemma 2 2b is surprisingly good at summaries.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0wkrn4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751441610,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lppg3g","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0y3w0h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MrBloham","can_mod_post":false,"created_utc":1751465970,"send_replies":true,"parent_id":"t3_1lppg3g","score":1,"author_fullname":"t2_8qfx9sd","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Add emojis to boring lists xD","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0y3w0h","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Add emojis to boring lists xD&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0y3w0h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751465970,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lppg3g","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0zgiuw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"b0tbuilder","can_mod_post":false,"created_utc":1751479718,"send_replies":true,"parent_id":"t3_1lppg3g","score":1,"author_fullname":"t2_bxf9roj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Route to more complex models.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0zgiuw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Route to more complex models.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0zgiuw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751479718,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lppg3g","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0zllug","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RonHarrods","can_mod_post":false,"created_utc":1751481229,"send_replies":true,"parent_id":"t3_1lppg3g","score":1,"author_fullname":"t2_a4486q5q","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Shellcheck correction on the easy rules.\\n\\nI'm working on a cli tool where if you fail to write a command like a complex find or some simple bash script, you can just type \\"vibe\\" and it captures the output of your command and generates a proompt and opens vscode for you to formulate your desire.\\n\\nLLMs consistently fail with escaping and following shellcheck rules. So I've built in an entire hardcoded step that inserts rulebreaking comments and then a 4b model is very capable of fixing those","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0zllug","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Shellcheck correction on the easy rules.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m working on a cli tool where if you fail to write a command like a complex find or some simple bash script, you can just type &amp;quot;vibe&amp;quot; and it captures the output of your command and generates a proompt and opens vscode for you to formulate your desire.&lt;/p&gt;\\n\\n&lt;p&gt;LLMs consistently fail with escaping and following shellcheck rules. So I&amp;#39;ve built in an entire hardcoded step that inserts rulebreaking comments and then a 4b model is very capable of fixing those&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0zllug/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751481229,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lppg3g","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n10onq4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Background-Ad-5398","can_mod_post":false,"created_utc":1751492815,"send_replies":true,"parent_id":"t3_1lppg3g","score":1,"author_fullname":"t2_71b6nl31","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"gemma 3 4b scores higher then darkest muse and gemma 3 12b at creative writing, ive never tested it but the samples were impressive","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n10onq4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;gemma 3 4b scores higher then darkest muse and gemma 3 12b at creative writing, ive never tested it but the samples were impressive&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n10onq4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751492815,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lppg3g","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n11vfou","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"PykeAtBanquet","can_mod_post":false,"created_utc":1751506749,"send_replies":true,"parent_id":"t3_1lppg3g","score":1,"author_fullname":"t2_31k9rz9i","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Google Gemma 4b 4_k_m has done everything I needed for the past several days: written code correctly; successfully parsed 50 page pdf file documentation and generated correct responses based on it; produced JSON format responses reliably; and multilingual - amazing model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n11vfou","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Google Gemma 4b 4_k_m has done everything I needed for the past several days: written code correctly; successfully parsed 50 page pdf file documentation and generated correct responses based on it; produced JSON format responses reliably; and multilingual - amazing model.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n11vfou/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751506749,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lppg3g","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n13reol","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Far-Incident822","can_mod_post":false,"created_utc":1751539385,"send_replies":true,"parent_id":"t3_1lppg3g","score":1,"author_fullname":"t2_gc8qgt9n","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I built a productivity tracking application that uses Gemma3:4BB quantized to keep me on track for being productive. It currently processes window titles but I’ve also coded in a screenshot capability for it to use its vision to determine if the task I’m working on matches the task I’ve declared I want to work on. Happy to open source it if it sounds useful.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n13reol","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I built a productivity tracking application that uses Gemma3:4BB quantized to keep me on track for being productive. It currently processes window titles but I’ve also coded in a screenshot capability for it to use its vision to determine if the task I’m working on matches the task I’ve declared I want to work on. Happy to open source it if it sounds useful.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n13reol/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751539385,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lppg3g","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0wsfrx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"jojacode","can_mod_post":false,"created_utc":1751446184,"send_replies":true,"parent_id":"t1_n0wmwcv","score":8,"author_fullname":"t2_1lnovydjbe","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Nah AI clouds are mostly made out of VC and perverse incentives","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0wsfrx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nah AI clouds are mostly made out of VC and perverse incentives&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lppg3g","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0wsfrx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751446184,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0wvgwo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"mxforest","can_mod_post":false,"created_utc":1751447974,"send_replies":true,"parent_id":"t1_n0wmwcv","score":4,"author_fullname":"t2_kenmq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The cloud behemoths are basically really fast humans typing it out. You can't change my mind as long as I am living on this flat Earth.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0wvgwo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The cloud behemoths are basically really fast humans typing it out. You can&amp;#39;t change my mind as long as I am living on this flat Earth.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lppg3g","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0wvgwo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751447974,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0z4mm8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"WitAndWonder","can_mod_post":false,"created_utc":1751476368,"send_replies":true,"parent_id":"t1_n0wmwcv","score":-1,"author_fullname":"t2_58u1t2z","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Actually, you can absolutely build an agent network of smaller models that replicates the functionality of a larger model. You'd basically have an initial model whose sole purpose is to classify the goal of the initial prompt and convert it into a format that one of your specialized models is prepared and trained to handle. While this still requires a large amount of VRAM to have 5, 10, or more models all loaded and ready to go depending on the data received, it does allow for much faster token processing, as well as parallel processing when different models are in use from varied prompts, taking advantage of the smaller sizes in that way. Possibly the best writing model right now is likely a 4-8B parameter model (I suspect it's close to 4, as they have several variations that are all auto-complete format), which is Sudowrite's Muse model. It was exclusively trained on high quality writing materials. No parameters wasted on coding or other datasets irrelevant to its purpose. You could run 50-100 models of that size (or more) on the same hardware that runs a single Open AI/Anthropic model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0z4mm8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Actually, you can absolutely build an agent network of smaller models that replicates the functionality of a larger model. You&amp;#39;d basically have an initial model whose sole purpose is to classify the goal of the initial prompt and convert it into a format that one of your specialized models is prepared and trained to handle. While this still requires a large amount of VRAM to have 5, 10, or more models all loaded and ready to go depending on the data received, it does allow for much faster token processing, as well as parallel processing when different models are in use from varied prompts, taking advantage of the smaller sizes in that way. Possibly the best writing model right now is likely a 4-8B parameter model (I suspect it&amp;#39;s close to 4, as they have several variations that are all auto-complete format), which is Sudowrite&amp;#39;s Muse model. It was exclusively trained on high quality writing materials. No parameters wasted on coding or other datasets irrelevant to its purpose. You could run 50-100 models of that size (or more) on the same hardware that runs a single Open AI/Anthropic model.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lppg3g","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0z4mm8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751476368,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0wmwcv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Fit-Produce420","can_mod_post":false,"created_utc":1751442882,"send_replies":true,"parent_id":"t3_1lppg3g","score":1,"author_fullname":"t2_tewf9bdwg","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You can't currently replicate a \\"cloud behemoth\\" performance on an edge device, if you could then clouds would be made from lots of tiny models. ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0wmwcv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can&amp;#39;t currently replicate a &amp;quot;cloud behemoth&amp;quot; performance on an edge device, if you could then clouds would be made from lots of tiny models. &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0wmwcv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751442882,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lppg3g","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0x1j37","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"wfamily","can_mod_post":false,"created_utc":1751451364,"send_replies":true,"parent_id":"t1_n0wrbfw","score":2,"author_fullname":"t2_7vmbx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"did you set yours up right?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0x1j37","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;did you set yours up right?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lppg3g","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0x1j37/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751451364,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0wrbfw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"AnomalyNexus","can_mod_post":false,"created_utc":1751445517,"send_replies":true,"parent_id":"t3_1lppg3g","score":-7,"author_fullname":"t2_3q8dd","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":true,"body":"Very hard to get 4Bs to do anything consistently whatsoever.\\n\\nSmall = higher chance of it going on tangents","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0wrbfw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Very hard to get 4Bs to do anything consistently whatsoever.&lt;/p&gt;\\n\\n&lt;p&gt;Small = higher chance of it going on tangents&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/n0wrbfw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751445517,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lppg3g","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-7}}],"before":null}}]`),n=()=>e.jsx(l,{data:t});export{n as default};
