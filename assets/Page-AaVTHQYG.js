import{j as e}from"./index-F0NXdzZX.js";import{R as t}from"./RedditPostRenderer-CoEZB1dt.js";import"./index-DrtravXm.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hello!\\n\\nI feel like there have been a lot of new releases in the past few weeks after a relatively quiet period following the Qwen3 release.\\n\\nOf course, there was the new Deepseek model, and now Kimi. But what is the consensus on the other, somewhat smaller LLMs that came out? Models like Jamba-Mini-1.7, Hunyuan-A13B-Instruct or ERNIE-4.5-21B-A3B?\\n\\nWhat's everyone's go-to model these days?\\n\\nAnd what are some other LLMs, tools, or research papers that you think flew under the radar because of the many big releases recently? For example, things like the recently released [FlexOlmo](https://huggingface.co/allenai/FlexOlmo-7x7B-1T) LLM/paradigm?\\n\\nThanks! ","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Which LLMs, tools, or research have been overlooked or deserve more attention?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m5827d","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.93,"author_flair_background_color":null,"subreddit_type":"public","ups":26,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_h8yrica5","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":26,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1753067598,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\\n\\n&lt;p&gt;I feel like there have been a lot of new releases in the past few weeks after a relatively quiet period following the Qwen3 release.&lt;/p&gt;\\n\\n&lt;p&gt;Of course, there was the new Deepseek model, and now Kimi. But what is the consensus on the other, somewhat smaller LLMs that came out? Models like Jamba-Mini-1.7, Hunyuan-A13B-Instruct or ERNIE-4.5-21B-A3B?&lt;/p&gt;\\n\\n&lt;p&gt;What&amp;#39;s everyone&amp;#39;s go-to model these days?&lt;/p&gt;\\n\\n&lt;p&gt;And what are some other LLMs, tools, or research papers that you think flew under the radar because of the many big releases recently? For example, things like the recently released &lt;a href=\\"https://huggingface.co/allenai/FlexOlmo-7x7B-1T\\"&gt;FlexOlmo&lt;/a&gt; LLM/paradigm?&lt;/p&gt;\\n\\n&lt;p&gt;Thanks! &lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/KasKHMjBTqO8qG3YViHMjFsBwsBOo-TVs2fkqqj8qBo.png?auto=webp&amp;s=cc904ba70b4ddd36e094ee5d02e948b2bbc3fe87","width":1200,"height":648},"resolutions":[{"url":"https://external-preview.redd.it/KasKHMjBTqO8qG3YViHMjFsBwsBOo-TVs2fkqqj8qBo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=326925660fb97ea07b8f47320d9a931b6f3b8850","width":108,"height":58},{"url":"https://external-preview.redd.it/KasKHMjBTqO8qG3YViHMjFsBwsBOo-TVs2fkqqj8qBo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e17265857ab969bc54807e5a91f994978fb57506","width":216,"height":116},{"url":"https://external-preview.redd.it/KasKHMjBTqO8qG3YViHMjFsBwsBOo-TVs2fkqqj8qBo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ea0cfa586f8cd24ecc53311134a6d90dd01a14b7","width":320,"height":172},{"url":"https://external-preview.redd.it/KasKHMjBTqO8qG3YViHMjFsBwsBOo-TVs2fkqqj8qBo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d67cbf4fd08636ac1f8772745df0258dc4168228","width":640,"height":345},{"url":"https://external-preview.redd.it/KasKHMjBTqO8qG3YViHMjFsBwsBOo-TVs2fkqqj8qBo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1b5d406f0a9d22f5434c5bd24f0856052cdd8bb0","width":960,"height":518},{"url":"https://external-preview.redd.it/KasKHMjBTqO8qG3YViHMjFsBwsBOo-TVs2fkqqj8qBo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2a148304a5d367a2a7e14ff2ea9f57df4e05ed2f","width":1080,"height":583}],"variants":{},"id":"KasKHMjBTqO8qG3YViHMjFsBwsBOo-TVs2fkqqj8qBo"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1m5827d","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"MDT-49","discussion_type":null,"num_comments":13,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m5827d/which_llms_tools_or_research_have_been_overlooked/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m5827d/which_llms_tools_or_research_have_been_overlooked/","subreddit_subscribers":502273,"created_utc":1753067598,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ad0kg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"RobotRobotWhatDoUSee","can_mod_post":false,"created_utc":1753072674,"send_replies":true,"parent_id":"t1_n4a6tl5","score":5,"author_fullname":"t2_m78cdz1nv","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Whoops replied to your post instead of the OP (*Edit*: just moved it). BUT, I also meant to reply to your post anyway, hah, so now you'll just have two direct replies from me instead of one...\\n\\nYes, I was *just* reading the FlexOlmo paper, came here to post it, searched first and found your post from a week or two ago -- highly unfortunate it didn't get any engagement! Very much agree that it seems extremely relevant to /r/LocalLLaMA interests. \\n\\nI'm not familiar with dense passthrough-merging, but will look into it now that you've mentioned it.  \\n\\n[GemmaScope](https://huggingface.co/google/gemma-scope) also looks very interesting, related to some of Anthropic's mechanistic interpretability perhaps?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ad0kg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Whoops replied to your post instead of the OP (&lt;em&gt;Edit&lt;/em&gt;: just moved it). BUT, I also meant to reply to your post anyway, hah, so now you&amp;#39;ll just have two direct replies from me instead of one...&lt;/p&gt;\\n\\n&lt;p&gt;Yes, I was &lt;em&gt;just&lt;/em&gt; reading the FlexOlmo paper, came here to post it, searched first and found your post from a week or two ago -- highly unfortunate it didn&amp;#39;t get any engagement! Very much agree that it seems extremely relevant to &lt;a href=\\"/r/LocalLLaMA\\"&gt;/r/LocalLLaMA&lt;/a&gt; interests. &lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m not familiar with dense passthrough-merging, but will look into it now that you&amp;#39;ve mentioned it.  &lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/google/gemma-scope\\"&gt;GemmaScope&lt;/a&gt; also looks very interesting, related to some of Anthropic&amp;#39;s mechanistic interpretability perhaps?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5827d","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5827d/which_llms_tools_or_research_have_been_overlooked/n4ad0kg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753072674,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ajznf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LicensedTerrapin","can_mod_post":false,"created_utc":1753076095,"send_replies":true,"parent_id":"t1_n4a6tl5","score":1,"author_fullname":"t2_97zi8wea","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Am I seeing it correctly that there's no gguf? It's there a pr for llamacpp?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ajznf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Am I seeing it correctly that there&amp;#39;s no gguf? It&amp;#39;s there a pr for llamacpp?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5827d","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5827d/which_llms_tools_or_research_have_been_overlooked/n4ajznf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753076095,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4a6tl5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ttkciar","can_mod_post":false,"created_utc":1753069916,"send_replies":true,"parent_id":"t3_1m5827d","score":17,"author_fullname":"t2_cpegz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yes indeedy!  I talked about FlexOlmo [in a post a few days ago](https://old.reddit.com/r/LocalLLaMA/comments/1lxehv3/flexolmo_open_language_models_for_flexible_data/) but nobody responded.  They have definitely paved the way for better technology, and I suspect (but have not yet confirmed) that their approach may be applicable to dense models as well, trained separately and then passthrough-merged.\\n\\nOther exciting but overlooked research:\\n\\n* https://arxiv.org/abs/2505.24832 demonstrated that as the ratio of training data to model parameters increases, parameters which encode \\"memorized\\" world knowledge get cannibalized for generalization heuristics.  That should have a profound impact on how we train models, once we have a better idea of the ideal mix of memorized and generalized knowledge.\\n\\n* GemmaScope and other layer-probing research have revealed that some parts of some layers consist of high numbers of very simple, narrow heuristics, which also seems highly relevant to training.  I suspect continued pretraining of duplicated rows with just the heuristic parameters unfrozen should give us much smarter models for a given compute budget.\\n\\nI have plans to investigate these possibilities, if nobody else does, but right now my hardware is meager and my backlog of neglected projects is long.  It doesn't help that I've shut down half of my homelab during the summer to keep it from overheating, and I've been using the hardware which is still running for paid work instead of personal interests.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4a6tl5","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes indeedy!  I talked about FlexOlmo &lt;a href=\\"https://old.reddit.com/r/LocalLLaMA/comments/1lxehv3/flexolmo_open_language_models_for_flexible_data/\\"&gt;in a post a few days ago&lt;/a&gt; but nobody responded.  They have definitely paved the way for better technology, and I suspect (but have not yet confirmed) that their approach may be applicable to dense models as well, trained separately and then passthrough-merged.&lt;/p&gt;\\n\\n&lt;p&gt;Other exciting but overlooked research:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;p&gt;&lt;a href=\\"https://arxiv.org/abs/2505.24832\\"&gt;https://arxiv.org/abs/2505.24832&lt;/a&gt; demonstrated that as the ratio of training data to model parameters increases, parameters which encode &amp;quot;memorized&amp;quot; world knowledge get cannibalized for generalization heuristics.  That should have a profound impact on how we train models, once we have a better idea of the ideal mix of memorized and generalized knowledge.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;GemmaScope and other layer-probing research have revealed that some parts of some layers consist of high numbers of very simple, narrow heuristics, which also seems highly relevant to training.  I suspect continued pretraining of duplicated rows with just the heuristic parameters unfrozen should give us much smarter models for a given compute budget.&lt;/p&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;I have plans to investigate these possibilities, if nobody else does, but right now my hardware is meager and my backlog of neglected projects is long.  It doesn&amp;#39;t help that I&amp;#39;ve shut down half of my homelab during the summer to keep it from overheating, and I&amp;#39;ve been using the hardware which is still running for paid work instead of personal interests.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5827d/which_llms_tools_or_research_have_been_overlooked/n4a6tl5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753069916,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m5827d","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":17}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4bsil3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RobotRobotWhatDoUSee","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4attc3","score":1,"author_fullname":"t2_m78cdz1nv","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Very interested in such a paper. I'll go look, but if you have title or link handy, please share!","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4bsil3","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Very interested in such a paper. I&amp;#39;ll go look, but if you have title or link handy, please share!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5827d","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5827d/which_llms_tools_or_research_have_been_overlooked/n4bsil3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753099618,"author_flair_text":null,"treatment_tags":[],"created_utc":1753099618,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4attc3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"InsideYork","can_mod_post":false,"created_utc":1753081372,"send_replies":true,"parent_id":"t1_n4ad2jh","score":1,"author_fullname":"t2_12s3hn4y0b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Didn't bytedance just have about this and actual training being not worth it?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4attc3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Didn&amp;#39;t bytedance just have about this and actual training being not worth it?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5827d","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5827d/which_llms_tools_or_research_have_been_overlooked/n4attc3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753081372,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4bf35r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1753093529,"send_replies":true,"parent_id":"t1_n4ad2jh","score":1,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I wonder if it possible to \\"downcycle\\" existing big moes. say extract a 37b model out of full deepseek.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4bf35r","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I wonder if it possible to &amp;quot;downcycle&amp;quot; existing big moes. say extract a 37b model out of full deepseek.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5827d","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5827d/which_llms_tools_or_research_have_been_overlooked/n4bf35r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753093529,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4ad2jh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"RobotRobotWhatDoUSee","can_mod_post":false,"created_utc":1753072700,"send_replies":true,"parent_id":"t3_1m5827d","score":7,"author_fullname":"t2_m78cdz1nv","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"\\"Upcycling\\" in general seems to be highly under-discussed here -- creating MoE models by using pre-trained base models to cut down on total compute time needed to train a MoE. NVIDIA wrote the [first paper](https://arxiv.org/html/2410.07524v1) but the literature has been growing quickly since then. \\n\\nIt is closely related to Goddard's [clowncar MoE](https://goddard.blog/posts/clown-moe/) approach that you can implement using \`mergekit-moe\` style merging if you initialize the router randomly and then do some post-training on the full MoE. \\n\\nThere is a fast-growing literature here, see for example this fun paper on [Scaling Laws for Upcycling](https://arxiv.org/abs/2502.03009) -- the scaling law they estimate (see eg. Figure 1 and equation 2) can probably be used for backing out how much new data and compute you want to use if you choose some other things (eg. total size of model, amount of pre-training in 'base' models, etc). \\n\\nI was playing around with different potential ways to combine Goddard's calibration of routers + post-training the router network as a way to extend the training, but I *just* ran across FlexOlmo and they appear to already be doing some of the things I was vaguely considering, very exciting to see.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ad2jh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&amp;quot;Upcycling&amp;quot; in general seems to be highly under-discussed here -- creating MoE models by using pre-trained base models to cut down on total compute time needed to train a MoE. NVIDIA wrote the &lt;a href=\\"https://arxiv.org/html/2410.07524v1\\"&gt;first paper&lt;/a&gt; but the literature has been growing quickly since then. &lt;/p&gt;\\n\\n&lt;p&gt;It is closely related to Goddard&amp;#39;s &lt;a href=\\"https://goddard.blog/posts/clown-moe/\\"&gt;clowncar MoE&lt;/a&gt; approach that you can implement using &lt;code&gt;mergekit-moe&lt;/code&gt; style merging if you initialize the router randomly and then do some post-training on the full MoE. &lt;/p&gt;\\n\\n&lt;p&gt;There is a fast-growing literature here, see for example this fun paper on &lt;a href=\\"https://arxiv.org/abs/2502.03009\\"&gt;Scaling Laws for Upcycling&lt;/a&gt; -- the scaling law they estimate (see eg. Figure 1 and equation 2) can probably be used for backing out how much new data and compute you want to use if you choose some other things (eg. total size of model, amount of pre-training in &amp;#39;base&amp;#39; models, etc). &lt;/p&gt;\\n\\n&lt;p&gt;I was playing around with different potential ways to combine Goddard&amp;#39;s calibration of routers + post-training the router network as a way to extend the training, but I &lt;em&gt;just&lt;/em&gt; ran across FlexOlmo and they appear to already be doing some of the things I was vaguely considering, very exciting to see.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5827d/which_llms_tools_or_research_have_been_overlooked/n4ad2jh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753072700,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5827d","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4addr5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RobotRobotWhatDoUSee","can_mod_post":false,"created_utc":1753072847,"send_replies":true,"parent_id":"t1_n4a4nzq","score":3,"author_fullname":"t2_m78cdz1nv","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes, I was *just* reading their paper and came here to see if there was any discussion -- only [one post](https://old.reddit.com/r/LocalLLaMA/comments/1lxehv3/flexolmo_open_language_models_for_flexible_data/) from another person in this thread, which didn't get any engagement at the time. Unfortunate! \\n\\nStrongly agree that federated learning is extremely relevant. I'm highly interested to learn more about this. They seem to have made progress on some very tricky parts of moe-merging disparate small dense models.\\n\\nI do kind of wish they tried this with a smaller model, like Olmo 1B, or Llama 3.2 3B, or Llama Llama 3.1 Minitron 4B, all of which could be used make MoE models with 5-10B active parameters, which would be usable even with relatively moderately powered local machines (I'm thinking laptops with AMD APUs or Apple Silicon).","edited":1753073230,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4addr5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, I was &lt;em&gt;just&lt;/em&gt; reading their paper and came here to see if there was any discussion -- only &lt;a href=\\"https://old.reddit.com/r/LocalLLaMA/comments/1lxehv3/flexolmo_open_language_models_for_flexible_data/\\"&gt;one post&lt;/a&gt; from another person in this thread, which didn&amp;#39;t get any engagement at the time. Unfortunate! &lt;/p&gt;\\n\\n&lt;p&gt;Strongly agree that federated learning is extremely relevant. I&amp;#39;m highly interested to learn more about this. They seem to have made progress on some very tricky parts of moe-merging disparate small dense models.&lt;/p&gt;\\n\\n&lt;p&gt;I do kind of wish they tried this with a smaller model, like Olmo 1B, or Llama 3.2 3B, or Llama Llama 3.1 Minitron 4B, all of which could be used make MoE models with 5-10B active parameters, which would be usable even with relatively moderately powered local machines (I&amp;#39;m thinking laptops with AMD APUs or Apple Silicon).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5827d","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5827d/which_llms_tools_or_research_have_been_overlooked/n4addr5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753072847,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n4a4nzq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"GL-AI","can_mod_post":false,"created_utc":1753069007,"send_replies":true,"parent_id":"t3_1m5827d","score":3,"author_fullname":"t2_1sr5yw3yg0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Wow, I'm surprised I didn't see FlexOlmo mentioned anywhere. I really think using federated learning is the future of open-weight models. It looks like allenai is only working with big organizations with sensitive data for this one, I hope in the future it could be open the general public somehow.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4a4nzq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Wow, I&amp;#39;m surprised I didn&amp;#39;t see FlexOlmo mentioned anywhere. I really think using federated learning is the future of open-weight models. It looks like allenai is only working with big organizations with sensitive data for this one, I hope in the future it could be open the general public somehow.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5827d/which_llms_tools_or_research_have_been_overlooked/n4a4nzq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753069007,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5827d","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4bkaqc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Porespellar","can_mod_post":false,"created_utc":1753096076,"send_replies":true,"parent_id":"t3_1m5827d","score":2,"author_fullname":"t2_y35oj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"A couple projects related to AI computer use don’t get enough attention in my opinion. They’re a couple of Microsoft research releases that I’m surprised haven’t been forked or built upon in some way yet \\n\\nOmniparser v2 / OmniTool / Omnibox\\n\\nwhich is a really interesting tool that breaks down screen interface elements for use in computer control. Omnitool is the control element for using the parser and Omnibox is the Sandbox to run it all in. \\n\\nhttps://github.com/microsoft/OmniParser\\n\\nMagentic UI which is a browser use agentic tool that lets you use a team of different LLMs to accomplish a computer vision-related end goal. I haven’t found a good open vision model to drive it with but when someone does figure that part out I think it’s going to be a really cool tool. The current version even has instructions for running it with Ollama \\n\\nhttps://github.com/microsoft/magentic-ui\\n\\nI really wish someone would combine the best parts of both projects and make a decent computer use agent tool.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4bkaqc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;A couple projects related to AI computer use don’t get enough attention in my opinion. They’re a couple of Microsoft research releases that I’m surprised haven’t been forked or built upon in some way yet &lt;/p&gt;\\n\\n&lt;p&gt;Omniparser v2 / OmniTool / Omnibox&lt;/p&gt;\\n\\n&lt;p&gt;which is a really interesting tool that breaks down screen interface elements for use in computer control. Omnitool is the control element for using the parser and Omnibox is the Sandbox to run it all in. &lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/microsoft/OmniParser\\"&gt;https://github.com/microsoft/OmniParser&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Magentic UI which is a browser use agentic tool that lets you use a team of different LLMs to accomplish a computer vision-related end goal. I haven’t found a good open vision model to drive it with but when someone does figure that part out I think it’s going to be a really cool tool. The current version even has instructions for running it with Ollama &lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/microsoft/magentic-ui\\"&gt;https://github.com/microsoft/magentic-ui&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;I really wish someone would combine the best parts of both projects and make a decent computer use agent tool.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5827d/which_llms_tools_or_research_have_been_overlooked/n4bkaqc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753096076,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5827d","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4a4zgs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AbortedFajitas","can_mod_post":false,"created_utc":1753069142,"send_replies":true,"parent_id":"t3_1m5827d","score":1,"author_fullname":"t2_3hv9p","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"https://github.com/genxnetwork/confidential-ai-example","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4a4zgs","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://github.com/genxnetwork/confidential-ai-example\\"&gt;https://github.com/genxnetwork/confidential-ai-example&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5827d/which_llms_tools_or_research_have_been_overlooked/n4a4zgs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753069142,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5827d","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4betkl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1753093391,"send_replies":true,"parent_id":"t1_n4bb2ce","score":2,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"both models, glm-4-32b and glm-experimental are very interesting, great at avoiding detection by zerogpt, but suffer of poor context handling. I personally use GLM-4-0414-32b locally almost exclusively as fiction writing assistant, as for the code I write (low level c/c++) it is not as good as Qwens.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4betkl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;both models, glm-4-32b and glm-experimental are very interesting, great at avoiding detection by zerogpt, but suffer of poor context handling. I personally use GLM-4-0414-32b locally almost exclusively as fiction writing assistant, as for the code I write (low level c/c++) it is not as good as Qwens.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5827d","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5827d/which_llms_tools_or_research_have_been_overlooked/n4betkl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753093391,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4bb2ce","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"roselan","can_mod_post":false,"created_utc":1753091371,"send_replies":true,"parent_id":"t3_1m5827d","score":1,"author_fullname":"t2_9akn4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It's not local, but as everyone is speaking about kimi, I found https://chat.z.ai/ to work pretty well (I didn't test it extensively yet)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4bb2ce","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s not local, but as everyone is speaking about kimi, I found &lt;a href=\\"https://chat.z.ai/\\"&gt;https://chat.z.ai/&lt;/a&gt; to work pretty well (I didn&amp;#39;t test it extensively yet)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5827d/which_llms_tools_or_research_have_been_overlooked/n4bb2ce/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753091371,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5827d","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(t,{data:l});export{n as default};
