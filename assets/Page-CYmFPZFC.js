import{j as e}from"./index-DACS7Nh6.js";import{R as t}from"./RedditPostRenderer-Dqa1NZuX.js";import"./index-DiMIVQx4.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"It seems compared with llama.cpp, mlx has greatly improved LLM inference with Apple Silicone. \\n\\nI was looking at the Qwen3 inference benchmarks [https://x.com/awnihannun/status/1917050679467835880?s=61](https://x.com/awnihannun/status/1917050679467835880?s=61)\\n\\nhttps://preview.redd.it/5q47tjbuecbf1.png?width=1213&amp;format=png&amp;auto=webp&amp;s=e48cf219d4b204f29646cb709a4ac00dfc7a0165\\n\\nI believe it was done on unbinned M4 max, and I get the corresponding numbers with my M3 ultra (binned version, 28c CPU, 60c GPU).\\n\\n\\\\- 0.6B: 394 t/s\\n\\n\\\\- 1.7B: 294 t/s\\n\\n\\\\- 4B: 173 t/s\\n\\n\\\\- 8B: 116 t/s\\n\\n\\\\- 14B: 71 t/s\\n\\n\\\\- 30B /A3B: 101 t/s\\n\\n\\\\- 32B: 33 t/s\\n\\n  \\nFrom this comparison, it seems\\n\\n\\\\- M3U binned only get faster when activated parameters exceed 4B, and the advanges are actually not that big.\\n\\n\\\\- For small LLMs with &lt;=3B activated parameters, including 30B/A3B moe, M4 max is significantly faster.\\n\\n  \\nThere are many previous discussions on choosing between two models, and I was also so hesitant when I made the choice and I ended up with M3U binned.\\n\\nBut from this results, it seems from a local LLM inference perspective, maxed M4 max should be the to-go choice? My rationals are\\n\\n\\\\- M4 max has much better single core cpu/gpu performance, which is more helpful for most daily tasks and programming tasks.\\n\\n\\\\- max M4 max has 128gb memory, which allows you try a even bigger model, e.g., Qwen3 235B A22B\\n\\n\\\\- For local LLM inference, small LLMs are more usable, it's barely feasible to use &gt;32B models in daily tasks. And with this assumption, M4 max seems to win in most cases?\\n\\n  \\nWhat should be the correct take-aways from this comparison?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"M4 Max VS M3 Ultra Qwen3 mlx inference","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":123,"top_awarded_type":null,"hide_score":false,"media_metadata":{"5q47tjbuecbf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":95,"x":108,"u":"https://preview.redd.it/5q47tjbuecbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cd57b4a74cbb0af72ca76bb9aafc06fdaa450f87"},{"y":190,"x":216,"u":"https://preview.redd.it/5q47tjbuecbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=354b8a68fc98403a5fd463e633e5ecb34678c98f"},{"y":281,"x":320,"u":"https://preview.redd.it/5q47tjbuecbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=482d39a1077f0d05df1c7a6de50f63a38357ac15"},{"y":562,"x":640,"u":"https://preview.redd.it/5q47tjbuecbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=451815e093a4b340f5928b8e55f79b669b9c8310"},{"y":844,"x":960,"u":"https://preview.redd.it/5q47tjbuecbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b76348995929aed9d5b7762c3c31802fa8840e7a"},{"y":950,"x":1080,"u":"https://preview.redd.it/5q47tjbuecbf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e970af28e905f0bbb2c388029579f247408ebac8"}],"s":{"y":1067,"x":1213,"u":"https://preview.redd.it/5q47tjbuecbf1.png?width=1213&amp;format=png&amp;auto=webp&amp;s=e48cf219d4b204f29646cb709a4ac00dfc7a0165"},"id":"5q47tjbuecbf1"}},"name":"t3_1ltg9ji","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.73,"author_flair_background_color":null,"ups":13,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_wtezmzgh7","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":13,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://b.thumbs.redditmedia.com/38fjQ8FdkFPe3JErw4C8LVworPDzNKhdj-UY_BSD-DM.jpg","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"subreddit_type":"public","created":1751847393,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;It seems compared with llama.cpp, mlx has greatly improved LLM inference with Apple Silicone. &lt;/p&gt;\\n\\n&lt;p&gt;I was looking at the Qwen3 inference benchmarks &lt;a href=\\"https://x.com/awnihannun/status/1917050679467835880?s=61\\"&gt;https://x.com/awnihannun/status/1917050679467835880?s=61&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/5q47tjbuecbf1.png?width=1213&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e48cf219d4b204f29646cb709a4ac00dfc7a0165\\"&gt;https://preview.redd.it/5q47tjbuecbf1.png?width=1213&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e48cf219d4b204f29646cb709a4ac00dfc7a0165&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;I believe it was done on unbinned M4 max, and I get the corresponding numbers with my M3 ultra (binned version, 28c CPU, 60c GPU).&lt;/p&gt;\\n\\n&lt;p&gt;- 0.6B: 394 t/s&lt;/p&gt;\\n\\n&lt;p&gt;- 1.7B: 294 t/s&lt;/p&gt;\\n\\n&lt;p&gt;- 4B: 173 t/s&lt;/p&gt;\\n\\n&lt;p&gt;- 8B: 116 t/s&lt;/p&gt;\\n\\n&lt;p&gt;- 14B: 71 t/s&lt;/p&gt;\\n\\n&lt;p&gt;- 30B /A3B: 101 t/s&lt;/p&gt;\\n\\n&lt;p&gt;- 32B: 33 t/s&lt;/p&gt;\\n\\n&lt;p&gt;From this comparison, it seems&lt;/p&gt;\\n\\n&lt;p&gt;- M3U binned only get faster when activated parameters exceed 4B, and the advanges are actually not that big.&lt;/p&gt;\\n\\n&lt;p&gt;- For small LLMs with &amp;lt;=3B activated parameters, including 30B/A3B moe, M4 max is significantly faster.&lt;/p&gt;\\n\\n&lt;p&gt;There are many previous discussions on choosing between two models, and I was also so hesitant when I made the choice and I ended up with M3U binned.&lt;/p&gt;\\n\\n&lt;p&gt;But from this results, it seems from a local LLM inference perspective, maxed M4 max should be the to-go choice? My rationals are&lt;/p&gt;\\n\\n&lt;p&gt;- M4 max has much better single core cpu/gpu performance, which is more helpful for most daily tasks and programming tasks.&lt;/p&gt;\\n\\n&lt;p&gt;- max M4 max has 128gb memory, which allows you try a even bigger model, e.g., Qwen3 235B A22B&lt;/p&gt;\\n\\n&lt;p&gt;- For local LLM inference, small LLMs are more usable, it&amp;#39;s barely feasible to use &amp;gt;32B models in daily tasks. And with this assumption, M4 max seems to win in most cases?&lt;/p&gt;\\n\\n&lt;p&gt;What should be the correct take-aways from this comparison?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/z-W_hwNGqZcVQKlbi62gXxLy_SqHFi2P1iczkOmcdTI.jpg?auto=webp&amp;s=d95cf6063affe4da61c86b6b6a44d15c51df0ef6","width":700,"height":616},"resolutions":[{"url":"https://external-preview.redd.it/z-W_hwNGqZcVQKlbi62gXxLy_SqHFi2P1iczkOmcdTI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c1b223931987f4a81c1661dc66820eee7f7bea2c","width":108,"height":95},{"url":"https://external-preview.redd.it/z-W_hwNGqZcVQKlbi62gXxLy_SqHFi2P1iczkOmcdTI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5f0b16f37414d24b8e046a624f32dfa8bc663eab","width":216,"height":190},{"url":"https://external-preview.redd.it/z-W_hwNGqZcVQKlbi62gXxLy_SqHFi2P1iczkOmcdTI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b5963da5182b3dfdb8958f58808f1609121e4f00","width":320,"height":281},{"url":"https://external-preview.redd.it/z-W_hwNGqZcVQKlbi62gXxLy_SqHFi2P1iczkOmcdTI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fdf1d0b9ee516b55b56aba895742768fd4fba77f","width":640,"height":563}],"variants":{},"id":"gnJQk3_BiObsdER_rJi69pQW2NuwnCE169S22KMhlgo"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"mod_note":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"num_reports":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1ltg9ji","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"SuperPumpkin314","discussion_type":null,"num_comments":10,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1ltg9ji/m4_max_vs_m3_ultra_qwen3_mlx_inference/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1ltg9ji/m4_max_vs_m3_ultra_qwen3_mlx_inference/","subreddit_subscribers":496034,"created_utc":1751847393,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1tjd69","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SuperPumpkin314","can_mod_post":false,"created_utc":1751901293,"send_replies":true,"parent_id":"t1_n1sxswj","score":1,"author_fullname":"t2_wtezmzgh7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"yeah, it’s almost ensured the m5 max to come in a couple of month will definitely beat m3ultra in all aspects.\\nYes, guess large memory is the only reason now….","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1tjd69","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;yeah, it’s almost ensured the m5 max to come in a couple of month will definitely beat m3ultra in all aspects.\\nYes, guess large memory is the only reason now….&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltg9ji","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltg9ji/m4_max_vs_m3_ultra_qwen3_mlx_inference/n1tjd69/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751901293,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1sxswj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Baldur-Norddahl","can_mod_post":false,"created_utc":1751894569,"send_replies":true,"parent_id":"t3_1ltg9ji","score":1,"author_fullname":"t2_bvqb8ng0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I am disappointed in the M3 Ultra performance compared to M4 Max. It has 50% more memory bandwidth and double GPU cores and yet only manages slightly better performance. Maybe it is the chip generation with M4 just being better than M3 or that M3 Ultra is really two chips glued together and something is lost in the interconnect.\\n\\nThat said, if I could afford the 256 GB or 512 GB version, I would not blink. Being able to run the model beats everything. Who isn't dreaming about a local DeepSeek R1?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1sxswj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I am disappointed in the M3 Ultra performance compared to M4 Max. It has 50% more memory bandwidth and double GPU cores and yet only manages slightly better performance. Maybe it is the chip generation with M4 just being better than M3 or that M3 Ultra is really two chips glued together and something is lost in the interconnect.&lt;/p&gt;\\n\\n&lt;p&gt;That said, if I could afford the 256 GB or 512 GB version, I would not blink. Being able to run the model beats everything. Who isn&amp;#39;t dreaming about a local DeepSeek R1?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltg9ji/m4_max_vs_m3_ultra_qwen3_mlx_inference/n1sxswj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751894569,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltg9ji","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1tvsdb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"irrationalfab","can_mod_post":false,"created_utc":1751904887,"send_replies":true,"parent_id":"t3_1ltg9ji","score":2,"author_fullname":"t2_5slhc","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Did you compare Time to first token (TTFT) as well?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1tvsdb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Did you compare Time to first token (TTFT) as well?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltg9ji/m4_max_vs_m3_ultra_qwen3_mlx_inference/n1tvsdb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751904887,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltg9ji","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1v88a0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MrPecunius","can_mod_post":false,"created_utc":1751919935,"send_replies":true,"parent_id":"t3_1ltg9ji","score":1,"author_fullname":"t2_or0ok9hd7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Interesting.\\n\\nMy binned M4 Pro/48GB gets about 65t/s with 30B-A3B @ 4-bit MLX (16GB model size, downloaded for this test, about 5k tokens of output for a 174 token prompt), so your M3 Ultra is just over 50% faster at that quant (and you have triple the memory bandwidth, more than double the CPU cores, and nearly 4X the GPU cores).\\n\\nMy daily driver is the 8-bit MLX model, which is almost double the size at 30.23GB. I get 45-50t/s, which isn't a huge penalty. This degrades to about 25t/s @ 20k+ context, which I still find super usable.\\n\\nIt seems like the main LLM use case for the M3 Ultra is large MoE models, which are not easy to run otherwise on off-the-shelf hardware. I continue to feel the sweet spot for Macs otherwise is the M4 Pro (probably binned) for portable use (i.e. Macbook Pro--for thermal, performance, and cost reasons) and maybe the M4 Max in a Mac Studio.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1v88a0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Interesting.&lt;/p&gt;\\n\\n&lt;p&gt;My binned M4 Pro/48GB gets about 65t/s with 30B-A3B @ 4-bit MLX (16GB model size, downloaded for this test, about 5k tokens of output for a 174 token prompt), so your M3 Ultra is just over 50% faster at that quant (and you have triple the memory bandwidth, more than double the CPU cores, and nearly 4X the GPU cores).&lt;/p&gt;\\n\\n&lt;p&gt;My daily driver is the 8-bit MLX model, which is almost double the size at 30.23GB. I get 45-50t/s, which isn&amp;#39;t a huge penalty. This degrades to about 25t/s @ 20k+ context, which I still find super usable.&lt;/p&gt;\\n\\n&lt;p&gt;It seems like the main LLM use case for the M3 Ultra is large MoE models, which are not easy to run otherwise on off-the-shelf hardware. I continue to feel the sweet spot for Macs otherwise is the M4 Pro (probably binned) for portable use (i.e. Macbook Pro--for thermal, performance, and cost reasons) and maybe the M4 Max in a Mac Studio.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltg9ji/m4_max_vs_m3_ultra_qwen3_mlx_inference/n1v88a0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751919935,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltg9ji","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1qfazl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Hanthunius","can_mod_post":false,"created_utc":1751851405,"send_replies":true,"parent_id":"t3_1ltg9ji","score":1,"author_fullname":"t2_d2gb9jhgg","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"kind of matches what we see on this table:\\n\\n[https://github.com/ggml-org/llama.cpp/discussions/4167](https://github.com/ggml-org/llama.cpp/discussions/4167)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1qfazl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;kind of matches what we see on this table:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/ggml-org/llama.cpp/discussions/4167\\"&gt;https://github.com/ggml-org/llama.cpp/discussions/4167&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltg9ji/m4_max_vs_m3_ultra_qwen3_mlx_inference/n1qfazl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751851405,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltg9ji","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1qp42m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"pseudonerv","can_mod_post":false,"created_utc":1751855073,"send_replies":true,"parent_id":"t1_n1qdiib","score":5,"author_fullname":"t2_eerln","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Given the memory usage, they are all standard 4bit q4_0","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1qp42m","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Given the memory usage, they are all standard 4bit q4_0&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltg9ji","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltg9ji/m4_max_vs_m3_ultra_qwen3_mlx_inference/n1qp42m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751855073,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1qr3ho","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SuperPumpkin314","can_mod_post":false,"created_utc":1751855842,"send_replies":true,"parent_id":"t1_n1qdiib","score":1,"author_fullname":"t2_wtezmzgh7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"yes, 4bit","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1qr3ho","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;yes, 4bit&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltg9ji","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltg9ji/m4_max_vs_m3_ultra_qwen3_mlx_inference/n1qr3ho/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751855842,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1qdiib","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"idesireawill","can_mod_post":false,"created_utc":1751850753,"send_replies":true,"parent_id":"t3_1ltg9ji","score":1,"author_fullname":"t2_60p5qnji","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Hi, thank you for the numbers. Is it possible for you to share the quantization for the modeşs that you have posted?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1qdiib","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi, thank you for the numbers. Is it possible for you to share the quantization for the modeşs that you have posted?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltg9ji/m4_max_vs_m3_ultra_qwen3_mlx_inference/n1qdiib/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751850753,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltg9ji","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1r14i9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SuperPumpkin314","can_mod_post":false,"created_utc":1751859970,"send_replies":true,"parent_id":"t1_n1qu8q8","score":-3,"author_fullname":"t2_wtezmzgh7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"yeah, also it seems m4 max has more potentials as apple continue to optimize MLX, while m3 ultra is already matured. So for the use case of running daily (relatively) small LLMs, M4 max is better than m3 utlra?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1r14i9","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;yeah, also it seems m4 max has more potentials as apple continue to optimize MLX, while m3 ultra is already matured. So for the use case of running daily (relatively) small LLMs, M4 max is better than m3 utlra?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltg9ji","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltg9ji/m4_max_vs_m3_ultra_qwen3_mlx_inference/n1r14i9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751859970,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-3}}],"before":null}},"user_reports":[],"saved":false,"id":"n1qu8q8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Conversation9561","can_mod_post":false,"created_utc":1751857091,"send_replies":true,"parent_id":"t3_1ltg9ji","score":1,"author_fullname":"t2_jqxb4pte","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I assume you’re comparing between M4 Max 128GB unbinned vs M3 Ultra 96 GB binned. M4 Max is cheaper and has 32 GB extra ram. However if you can spend more then definitely go for M3 ultra binned 256 GB or unbinned 512 GB. That’s where the M3 ultra truly shines.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1qu8q8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I assume you’re comparing between M4 Max 128GB unbinned vs M3 Ultra 96 GB binned. M4 Max is cheaper and has 32 GB extra ram. However if you can spend more then definitely go for M3 ultra binned 256 GB or unbinned 512 GB. That’s where the M3 ultra truly shines.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltg9ji/m4_max_vs_m3_ultra_qwen3_mlx_inference/n1qu8q8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751857091,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltg9ji","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),s=()=>e.jsx(t,{data:a});export{s as default};
