import{j as e}from"./index-BpC9hjVs.js";import{R as l}from"./RedditPostRenderer-BEo6AnSR.js";import"./index-DwkJHX1_.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Simple question, how offloading layers work in LLM, so for example if i have 24Gig rtx 3090 and offloading layers, lets say 5 gig each, so the model will offload only 4 of them leaving remaining 4 giga dormant or it will utilize it somehow as well?  Asking because many time seeing task menager under performance tab I see unused Vram even though only few layers has been offloaded out of 40 or 60. So it is kind of waste of resources then. Right? ","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Offloading layers","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m4zogr","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.5,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_12dubk","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753044500,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Simple question, how offloading layers work in LLM, so for example if i have 24Gig rtx 3090 and offloading layers, lets say 5 gig each, so the model will offload only 4 of them leaving remaining 4 giga dormant or it will utilize it somehow as well?  Asking because many time seeing task menager under performance tab I see unused Vram even though only few layers has been offloaded out of 40 or 60. So it is kind of waste of resources then. Right? &lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m4zogr","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"PawelSalsa","discussion_type":null,"num_comments":6,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m4zogr/offloading_layers/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m4zogr/offloading_layers/","subreddit_subscribers":502273,"created_utc":1753044500,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n48cdok","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fp4guru","can_mod_post":false,"created_utc":1753045787,"send_replies":true,"parent_id":"t3_1m4zogr","score":1,"author_fullname":"t2_1tp8zldw5g","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What backend are you using? Ollama has a recent fix regarding memory allocation. Llamacpp allows you to offload layers at your will.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n48cdok","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What backend are you using? Ollama has a recent fix regarding memory allocation. Llamacpp allows you to offload layers at your will.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4zogr/offloading_layers/n48cdok/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753045787,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4zogr","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n48ddpr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Conscious_Cut_6144","can_mod_post":false,"created_utc":1753046106,"send_replies":true,"parent_id":"t3_1m4zogr","score":1,"author_fullname":"t2_9hl4ymvj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It's not only the layer size, because context also fills them up.  \\nBut yes with llama.cpp you can run into divisibility issues.\\n\\nIt's not so bad with 24gb gpus.  \\nBut I have a system with 8x p102-100's and it can really start to mess you up. (8x 10GB each)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n48ddpr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s not only the layer size, because context also fills them up.&lt;br/&gt;\\nBut yes with llama.cpp you can run into divisibility issues.&lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s not so bad with 24gb gpus.&lt;br/&gt;\\nBut I have a system with 8x p102-100&amp;#39;s and it can really start to mess you up. (8x 10GB each)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4zogr/offloading_layers/n48ddpr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753046106,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4zogr","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4945n3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Afternoon_4260","can_mod_post":false,"send_replies":true,"parent_id":"t1_n490ell","score":1,"author_fullname":"t2_cj9kap4bx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"LM Studio I have no ide how it's made \\nLlama cpp would load more stuff to the first gpu and you could select how to split to layer on all gpu, do it by hand sure but you have control. Lm studio idk","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4945n3","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;LM Studio I have no ide how it&amp;#39;s made \\nLlama cpp would load more stuff to the first gpu and you could select how to split to layer on all gpu, do it by hand sure but you have control. Lm studio idk&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4zogr","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4zogr/offloading_layers/n4945n3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753055172,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1753055172,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n490ell","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"PawelSalsa","can_mod_post":false,"created_utc":1753053856,"send_replies":true,"parent_id":"t1_n48sryj","score":1,"author_fullname":"t2_12dubk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm using LM Studio, also have 5x rtx 3090 but this whole allocation makes me confuse. What i see offloading as much as LM Studio allow me that some of my cards allocate 19Gigs other 22,  20 or 22 sometimes 17 so it means all remaining VRam is  not allocated then. So what is the point having as much in the first place?  But the thing is, I have also rtx 4070ti super with 16gigs and this card always allocate full VRam more than 15gigs, any one have ideas why it's that?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n490ell","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m using LM Studio, also have 5x rtx 3090 but this whole allocation makes me confuse. What i see offloading as much as LM Studio allow me that some of my cards allocate 19Gigs other 22,  20 or 22 sometimes 17 so it means all remaining VRam is  not allocated then. So what is the point having as much in the first place?  But the thing is, I have also rtx 4070ti super with 16gigs and this card always allocate full VRam more than 15gigs, any one have ideas why it&amp;#39;s that?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4zogr","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4zogr/offloading_layers/n490ell/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753053856,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n48sryj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Afternoon_4260","can_mod_post":false,"created_utc":1753051220,"send_replies":true,"parent_id":"t3_1m4zogr","score":1,"author_fullname":"t2_cj9kap4bx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yeah what backend are you using?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n48sryj","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah what backend are you using?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4zogr/offloading_layers/n48sryj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753051220,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m4zogr","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n49ldgk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"skatardude10","can_mod_post":false,"created_utc":1753061546,"send_replies":true,"parent_id":"t3_1m4zogr","score":1,"author_fullname":"t2_8k4ah","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Look into tensor overrides. Especially for newer DDR5 systems, overriding tensors (say, FFN down tensors) to stay on the CPU, to free up enough space for ALL layers to offload to GPU (minus the tensors overridden to stay on CPU) can result in way faster processing and generation speeds. I always shoot to fill nearly all vram this way.\\n\\nhttps://www.reddit.com/r/LocalLLaMA/s/Bf6YLLFiOv","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n49ldgk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Look into tensor overrides. Especially for newer DDR5 systems, overriding tensors (say, FFN down tensors) to stay on the CPU, to free up enough space for ALL layers to offload to GPU (minus the tensors overridden to stay on CPU) can result in way faster processing and generation speeds. I always shoot to fill nearly all vram this way.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/s/Bf6YLLFiOv\\"&gt;https://www.reddit.com/r/LocalLLaMA/s/Bf6YLLFiOv&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4zogr/offloading_layers/n49ldgk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753061546,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4zogr","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),s=()=>e.jsx(l,{data:a});export{s as default};
