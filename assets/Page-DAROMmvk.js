import{j as e}from"./index-BOnf-UhU.js";import{R as t}from"./RedditPostRenderer-Ce39qICS.js";import"./index-CDase6VD.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I'm experimenting with a home server setup and wondering if anyone has managed to run both an LLM (e.g. LM Studio, Ollama) **and** an image generation model (e.g. Stable Diffusion via Forge or SD WebUI) **on the same GPU**.\\n\\nIf you had a chatbot that needs to handle both text and image generation, would it be feasible to dynamically swap model weights (e.g. using a queuing system), or is that too inefficient in practice?\\n\\nI realize calling APIs would be easier, but I'm prioritizing **local inference for privacy**.  \\nHere’s a small [GitHub repo](https://github.com/adnjoo/TGBot) I’m working on — it connects a local LLM to Telegram with Chroma (a rough LTM approximation).\\n\\nWould love to hear how others have tackled this!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Any way to serve images and text from a single GPU?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m4ojg7","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.5,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1odrj3pe27","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1753017018,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m experimenting with a home server setup and wondering if anyone has managed to run both an LLM (e.g. LM Studio, Ollama) &lt;strong&gt;and&lt;/strong&gt; an image generation model (e.g. Stable Diffusion via Forge or SD WebUI) &lt;strong&gt;on the same GPU&lt;/strong&gt;.&lt;/p&gt;\\n\\n&lt;p&gt;If you had a chatbot that needs to handle both text and image generation, would it be feasible to dynamically swap model weights (e.g. using a queuing system), or is that too inefficient in practice?&lt;/p&gt;\\n\\n&lt;p&gt;I realize calling APIs would be easier, but I&amp;#39;m prioritizing &lt;strong&gt;local inference for privacy&lt;/strong&gt;.&lt;br/&gt;\\nHere’s a small &lt;a href=\\"https://github.com/adnjoo/TGBot\\"&gt;GitHub repo&lt;/a&gt; I’m working on — it connects a local LLM to Telegram with Chroma (a rough LTM approximation).&lt;/p&gt;\\n\\n&lt;p&gt;Would love to hear how others have tackled this!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/dR9kYi9a7ReLHd2VI7w1O3Kh2bFTNtklcCPDAxjZolI.png?auto=webp&amp;s=0625ef9a6b90540a57b3081b50b1987ddbf36c09","width":1200,"height":600},"resolutions":[{"url":"https://external-preview.redd.it/dR9kYi9a7ReLHd2VI7w1O3Kh2bFTNtklcCPDAxjZolI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6e827c8e19c5323146ee624628bffdcacf9b3427","width":108,"height":54},{"url":"https://external-preview.redd.it/dR9kYi9a7ReLHd2VI7w1O3Kh2bFTNtklcCPDAxjZolI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a0d203c95f583d8719b6abcbb308554b3a1054e9","width":216,"height":108},{"url":"https://external-preview.redd.it/dR9kYi9a7ReLHd2VI7w1O3Kh2bFTNtklcCPDAxjZolI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7263a02c0ba8b09a62247c47a7621023215089c3","width":320,"height":160},{"url":"https://external-preview.redd.it/dR9kYi9a7ReLHd2VI7w1O3Kh2bFTNtklcCPDAxjZolI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fbb701424b77fbdde8c89525dfc6def60f93831b","width":640,"height":320},{"url":"https://external-preview.redd.it/dR9kYi9a7ReLHd2VI7w1O3Kh2bFTNtklcCPDAxjZolI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e6be44535f6226827f407776ced88c92e8639abb","width":960,"height":480},{"url":"https://external-preview.redd.it/dR9kYi9a7ReLHd2VI7w1O3Kh2bFTNtklcCPDAxjZolI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0c5c82c264bb08be17b7512928bcfcd075bbd25a","width":1080,"height":540}],"variants":{},"id":"dR9kYi9a7ReLHd2VI7w1O3Kh2bFTNtklcCPDAxjZolI"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m4ojg7","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Realistic_Age6660","discussion_type":null,"num_comments":6,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m4ojg7/any_way_to_serve_images_and_text_from_a_single_gpu/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m4ojg7/any_way_to_serve_images_and_text_from_a_single_gpu/","subreddit_subscribers":502274,"created_utc":1753017018,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n463vda","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"henfiber","can_mod_post":false,"send_replies":true,"parent_id":"t1_n461thv","score":1,"author_fullname":"t2_lw9me25","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"They don't seem to support the image generation endpoints yet, but maybe you can hack something using a curl script, a dummy model to force unloading of the text model, and a custom \`cmdStop\` command to unload the external image generation model. \\n\\nI have not played with image generation locally yet (but I use llama-swap for multiple text or vision models), so just throwing out ideas.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n463vda","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They don&amp;#39;t seem to support the image generation endpoints yet, but maybe you can hack something using a curl script, a dummy model to force unloading of the text model, and a custom &lt;code&gt;cmdStop&lt;/code&gt; command to unload the external image generation model. &lt;/p&gt;\\n\\n&lt;p&gt;I have not played with image generation locally yet (but I use llama-swap for multiple text or vision models), so just throwing out ideas.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4ojg7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4ojg7/any_way_to_serve_images_and_text_from_a_single_gpu/n463vda/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753021083,"author_flair_text":null,"treatment_tags":[],"created_utc":1753021083,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n461thv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Realistic_Age6660","can_mod_post":false,"created_utc":1753020393,"send_replies":true,"parent_id":"t1_n45xz7e","score":1,"author_fullname":"t2_1odrj3pe27","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Currently only have 16GB from a 5060 Ti.\\n\\nThanks! I'll definitely check llama-swap out..","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n461thv","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Currently only have 16GB from a 5060 Ti.&lt;/p&gt;\\n\\n&lt;p&gt;Thanks! I&amp;#39;ll definitely check llama-swap out..&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4ojg7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4ojg7/any_way_to_serve_images_and_text_from_a_single_gpu/n461thv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753020393,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n45xz7e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"henfiber","can_mod_post":false,"created_utc":1753019061,"send_replies":true,"parent_id":"t3_1m4ojg7","score":2,"author_fullname":"t2_lw9me25","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you have the VRAM to hold both in memory, you can do that.\\n\\nOtherwise, check [llama-swap](https://github.com/mostlygeek/llama-swap) and use OpenAI compatible endpoints for image generation. With appropriate configuration, you can swap which model is running based on the last request and ttl (time to live) settings. Check their wiki for examples.","edited":1753019260,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n45xz7e","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you have the VRAM to hold both in memory, you can do that.&lt;/p&gt;\\n\\n&lt;p&gt;Otherwise, check &lt;a href=\\"https://github.com/mostlygeek/llama-swap\\"&gt;llama-swap&lt;/a&gt; and use OpenAI compatible endpoints for image generation. With appropriate configuration, you can swap which model is running based on the last request and ttl (time to live) settings. Check their wiki for examples.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4ojg7/any_way_to_serve_images_and_text_from_a_single_gpu/n45xz7e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753019061,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4ojg7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n45yt0i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kironlau","can_mod_post":false,"created_utc":1753019350,"send_replies":true,"parent_id":"t3_1m4ojg7","score":1,"author_fullname":"t2_tb0dz2ds","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"maybe you could try 'Qwen3-30B-A3B' offloading all the layer to CPU, so that it would not hold any vram when idle (in theory, at least).(If you're concerning token speed, use ik\\\\_llama.cpp, hold 3GB vram for a Q4 model, for MOE optimization.)\\n\\nFor my personal expericnece, using Comfyui Nunchaku Flux model, offload the clip models to CPU, it using less than 6GB vram, without loading/unloading models.\\n\\nIf you want a dynamically swap model workflow.... I suggest using comfyui, it has many nodes to connect ollama, which could unload the model after a prompt.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n45yt0i","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;maybe you could try &amp;#39;Qwen3-30B-A3B&amp;#39; offloading all the layer to CPU, so that it would not hold any vram when idle (in theory, at least).(If you&amp;#39;re concerning token speed, use ik_llama.cpp, hold 3GB vram for a Q4 model, for MOE optimization.)&lt;/p&gt;\\n\\n&lt;p&gt;For my personal expericnece, using Comfyui Nunchaku Flux model, offload the clip models to CPU, it using less than 6GB vram, without loading/unloading models.&lt;/p&gt;\\n\\n&lt;p&gt;If you want a dynamically swap model workflow.... I suggest using comfyui, it has many nodes to connect ollama, which could unload the model after a prompt.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4ojg7/any_way_to_serve_images_and_text_from_a_single_gpu/n45yt0i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753019350,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4ojg7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n463b34","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Casual-Godzilla","can_mod_post":false,"created_utc":1753020896,"send_replies":true,"parent_id":"t3_1m4ojg7","score":1,"author_fullname":"t2_1fw4w48cp6","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"At the risk of crossing the threshold of unacceptable self-promotion, I'd like to suggest you take a look at [AI Model Juggler](https://github.com/makedin/AI-Model-Juggler) that I just [announced](https://www.reddit.com/r/LocalLLaMA/comments/1m4n7fh/ai_model_juggler_automatically_and_transparently/) here.\\n\\nIt currently only supports llama.cpp and Stable Diffusion web UI / Forge, but if that's fine with you, it might be just what you're looking for. You will still need enough VRAM to fit the largest model you wish to use (well, CPU offloading is an option too, but you get the point) but no more, as unused models are automatically swapped off the GPU.","edited":1753021364,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n463b34","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;At the risk of crossing the threshold of unacceptable self-promotion, I&amp;#39;d like to suggest you take a look at &lt;a href=\\"https://github.com/makedin/AI-Model-Juggler\\"&gt;AI Model Juggler&lt;/a&gt; that I just &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1m4n7fh/ai_model_juggler_automatically_and_transparently/\\"&gt;announced&lt;/a&gt; here.&lt;/p&gt;\\n\\n&lt;p&gt;It currently only supports llama.cpp and Stable Diffusion web UI / Forge, but if that&amp;#39;s fine with you, it might be just what you&amp;#39;re looking for. You will still need enough VRAM to fit the largest model you wish to use (well, CPU offloading is an option too, but you get the point) but no more, as unused models are automatically swapped off the GPU.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4ojg7/any_way_to_serve_images_and_text_from_a_single_gpu/n463b34/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753020896,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4ojg7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n47uz0f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Not4Fame","can_mod_post":false,"created_utc":1753040394,"send_replies":true,"parent_id":"t3_1m4ojg7","score":1,"author_fullname":"t2_9vjlnmv0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'm currently running a 30B A3B qwen3 as main, calling: SDXL for image generation, qwen2.5 omni for image/audio input and qwen3 embedding for augmented memory retrieval semantic lookups . All together, simultaneously, on a 5090. Through smart juggling. so yes, in short, my chatbot can and does handle the LLM calling everything I've listed in real-time, in the same flask server for UI, no issues.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n47uz0f","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m currently running a 30B A3B qwen3 as main, calling: SDXL for image generation, qwen2.5 omni for image/audio input and qwen3 embedding for augmented memory retrieval semantic lookups . All together, simultaneously, on a 5090. Through smart juggling. so yes, in short, my chatbot can and does handle the LLM calling everything I&amp;#39;ve listed in real-time, in the same flask server for UI, no issues.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4ojg7/any_way_to_serve_images_and_text_from_a_single_gpu/n47uz0f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753040394,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4ojg7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(t,{data:l});export{r as default};
