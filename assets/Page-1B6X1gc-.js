import{j as l}from"./index-DOAmItP2.js";import{R as e}from"./RedditPostRenderer-KKgzpPpv.js";import"./index-YSfz60vQ.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I'm looking for the best open-source LLM for local use, focused on programming. I have a 2 RTX 5090.\\n\\nIs Codestral 22B still the best choice for local code related tasks (code completion, refactoring, understanding context etc.), or are there better alternatives now like DeepSeek-Coder V2, StarCoder2, or WizardCoder?\\n\\nLooking for models that run locally (preferably via GGUF with llama.cpp or LM Studio) and give good real-world coding performance – not just benchmark wins. C/C++, python and Js.\\n\\nThanks in advance.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Is Codestral 22B still the best open LLM for local coding on 32–64 GB VRAM?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lsmtzr","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.93,"author_flair_background_color":null,"subreddit_type":"public","ups":46,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_77k54e2b","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":46,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751757237,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m looking for the best open-source LLM for local use, focused on programming. I have a 2 RTX 5090.&lt;/p&gt;\\n\\n&lt;p&gt;Is Codestral 22B still the best choice for local code related tasks (code completion, refactoring, understanding context etc.), or are there better alternatives now like DeepSeek-Coder V2, StarCoder2, or WizardCoder?&lt;/p&gt;\\n\\n&lt;p&gt;Looking for models that run locally (preferably via GGUF with llama.cpp or LM Studio) and give good real-world coding performance – not just benchmark wins. C/C++, python and Js.&lt;/p&gt;\\n\\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lsmtzr","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"One-Stress-6734","discussion_type":null,"num_comments":23,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lsmtzr/is_codestral_22b_still_the_best_open_llm_for/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lsmtzr/is_codestral_22b_still_the_best_open_llm_for/","subreddit_subscribers":494897,"created_utc":1751757237,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1k9lfq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DinoAmino","can_mod_post":false,"created_utc":1751764135,"send_replies":true,"parent_id":"t1_n1jzvpv","score":1,"author_fullname":"t2_j1v7f","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It was","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1k9lfq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It was&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsmtzr","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsmtzr/is_codestral_22b_still_the_best_open_llm_for/n1k9lfq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751764135,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1jzvpv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"CheatCodesOfLife","can_mod_post":false,"created_utc":1751760285,"send_replies":true,"parent_id":"t3_1lsmtzr","score":27,"author_fullname":"t2_32el727b","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt; Is Codestral 22B \\n\\nWas it ever? You'd probably want Devstral 24B if that's the case.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1jzvpv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Is Codestral 22B &lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Was it ever? You&amp;#39;d probably want Devstral 24B if that&amp;#39;s the case.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsmtzr/is_codestral_22b_still_the_best_open_llm_for/n1jzvpv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751760285,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lsmtzr","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":27}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1kv6bs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mp3m4k3r","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1jywdn","score":1,"author_fullname":"t2_542ztvqj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"So far I've found continue to be pretty solid overall though it can be a little tricky to setup. I've been using it with qwen3-32b for a while as well as a phi4 and qwen2.5-coder before that. Still having a bit of trouble getting auto complete working but its been great imo with what I'm largely using it for on 90k context","edited":false,"author_flair_css_class":null,"name":"t1_n1kv6bs","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So far I&amp;#39;ve found continue to be pretty solid overall though it can be a little tricky to setup. I&amp;#39;ve been using it with qwen3-32b for a while as well as a phi4 and qwen2.5-coder before that. Still having a bit of trouble getting auto complete working but its been great imo with what I&amp;#39;m largely using it for on 90k context&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lsmtzr","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsmtzr/is_codestral_22b_still_the_best_open_llm_for/n1kv6bs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751773189,"author_flair_text":null,"collapsed":false,"created_utc":1751773189,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1jywdn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"One-Stress-6734","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1jxwkw","score":6,"author_fullname":"t2_77k54e2b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Aaah okay. That was the missing piece in my puzzle. So something like continue.dev. Perfect. I’ll give it a try. Thanks so much!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1jywdn","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Aaah okay. That was the missing piece in my puzzle. So something like continue.dev. Perfect. I’ll give it a try. Thanks so much!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsmtzr","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsmtzr/is_codestral_22b_still_the_best_open_llm_for/n1jywdn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751759913,"author_flair_text":null,"treatment_tags":[],"created_utc":1751759913,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n1jxwkw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Lazy-Pattern-5171","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1jwjqc","score":15,"author_fullname":"t2_1lyjk8is25","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That’s where extensions come in. They do those things for you programmatically and then create the final prompt for LLM to work with. LLMs are as of today still just next token generators. The harness around it all is still very much about programming.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1jxwkw","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That’s where extensions come in. They do those things for you programmatically and then create the final prompt for LLM to work with. LLMs are as of today still just next token generators. The harness around it all is still very much about programming.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsmtzr","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsmtzr/is_codestral_22b_still_the_best_open_llm_for/n1jxwkw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751759541,"author_flair_text":null,"treatment_tags":[],"created_utc":1751759541,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":15}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1kidt8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"YouDontSeemRight","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1jwjqc","score":2,"author_fullname":"t2_1b7gjxtue9","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You asked compared to codestral. Codestrals really old now. Qwen 3 32B is probably better and not even a coding model.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1kidt8","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You asked compared to codestral. Codestrals really old now. Qwen 3 32B is probably better and not even a coding model.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsmtzr","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsmtzr/is_codestral_22b_still_the_best_open_llm_for/n1kidt8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751767722,"author_flair_text":null,"treatment_tags":[],"created_utc":1751767722,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1jwjqc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"One-Stress-6734","can_mod_post":false,"created_utc":1751759042,"send_replies":true,"parent_id":"t1_n1jsrxy","score":9,"author_fullname":"t2_77k54e2b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah, Qwen2.5-Code definitely looks solid on paper..\\n\\nBut do you know how well it handles actual multi-file projects in a manual coding setup.  I'm not using coding agents just working in VS Code with local models, so the ability to track structure across multiple .h, .cpp, etc. files is the key for me.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1jwjqc","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, Qwen2.5-Code definitely looks solid on paper..&lt;/p&gt;\\n\\n&lt;p&gt;But do you know how well it handles actual multi-file projects in a manual coding setup.  I&amp;#39;m not using coding agents just working in VS Code with local models, so the ability to track structure across multiple .h, .cpp, etc. files is the key for me.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsmtzr","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsmtzr/is_codestral_22b_still_the_best_open_llm_for/n1jwjqc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751759042,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1k32zl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"robiinn","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1jul52","score":8,"author_fullname":"t2_709lt","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"GLM-4-32B has been very weak for long context and large codebases, in my experience.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1k32zl","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;GLM-4-32B has been very weak for long context and large codebases, in my experience.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsmtzr","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsmtzr/is_codestral_22b_still_the_best_open_llm_for/n1k32zl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751761524,"author_flair_text":null,"treatment_tags":[],"created_utc":1751761524,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}}],"before":null}},"user_reports":[],"saved":false,"id":"n1jul52","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"JumpyAbies","can_mod_post":false,"created_utc":1751758329,"send_replies":true,"parent_id":"t1_n1jsrxy","score":9,"author_fullname":"t2_8gh0gowt","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"GLM-4-32B &gt; Qwen3-32B","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1jul52","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;GLM-4-32B &amp;gt; Qwen3-32B&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsmtzr","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsmtzr/is_codestral_22b_still_the_best_open_llm_for/n1jul52/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751758329,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}}],"before":null}},"user_reports":[],"saved":false,"id":"n1jsrxy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"xtremx12","can_mod_post":false,"created_utc":1751757673,"send_replies":true,"parent_id":"t3_1lsmtzr","score":39,"author_fullname":"t2_3op05s7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"qwen2.5 code is one of the best if u can go with 32b or 14b","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1jsrxy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;qwen2.5 code is one of the best if u can go with 32b or 14b&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsmtzr/is_codestral_22b_still_the_best_open_llm_for/n1jsrxy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751757673,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lsmtzr","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":39}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1jvujt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"One-Stress-6734","can_mod_post":false,"created_utc":1751758788,"send_replies":true,"parent_id":"t1_n1js4c3","score":1,"author_fullname":"t2_77k54e2b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thank you :) – I'm actually not using coding agents like GPT-Engineer or SWE-agent.  \\nWhat i want to do is more like vibecoding and working manually on a full local codebase.  \\nSo I’m mainly looking for something that handles: full multi-file project understanding, persistent context, strong code generation and refactoring. I’ll keep Deep SWE  in mind if I ever start working with agents.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1jvujt","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you :) – I&amp;#39;m actually not using coding agents like GPT-Engineer or SWE-agent.&lt;br/&gt;\\nWhat i want to do is more like vibecoding and working manually on a full local codebase.&lt;br/&gt;\\nSo I’m mainly looking for something that handles: full multi-file project understanding, persistent context, strong code generation and refactoring. I’ll keep Deep SWE  in mind if I ever start working with agents.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsmtzr","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsmtzr/is_codestral_22b_still_the_best_open_llm_for/n1jvujt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751758788,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1js4c3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Sorry_Ad191","can_mod_post":false,"created_utc":1751757435,"send_replies":true,"parent_id":"t3_1lsmtzr","score":8,"author_fullname":"t2_1rig07ocmc","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think maybe DeepSWE-Preview-32B if you are using coding agents? It's based on Qwen3-32B","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1js4c3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think maybe DeepSWE-Preview-32B if you are using coding agents? It&amp;#39;s based on Qwen3-32B&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsmtzr/is_codestral_22b_still_the_best_open_llm_for/n1js4c3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751757435,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lsmtzr","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"body":"I prefer GLM-4 0414 for C++ although Qwen 3 and Qwen2.5 Coder weren't far behind for my use case.","subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1jyssu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CheatCodesOfLife","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1jxxbx","score":3,"author_fullname":"t2_32el727b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; Would you say GLM-4 actually follows long context chains across multiple files? Or is it more like it generates nice isolated code once you narrow the context manually?\\n\\nGLM-4 is great at really short contexts but no, it'll break down if you try to do that","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1jyssu","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Would you say GLM-4 actually follows long context chains across multiple files? Or is it more like it generates nice isolated code once you narrow the context manually?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;GLM-4 is great at really short contexts but no, it&amp;#39;ll break down if you try to do that&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsmtzr","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsmtzr/is_codestral_22b_still_the_best_open_llm_for/n1jyssu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751759875,"author_flair_text":null,"treatment_tags":[],"created_utc":1751759875,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"body":"I have limited VRAM, so I only feed it relevant code snippets","subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1k5dja","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"sxales","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1jxxbx","score":1,"author_fullname":"t2_5h1ye","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"author_cakeday":true,"edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1k5dja","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have limited VRAM, so I only feed it relevant code snippets&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsmtzr","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsmtzr/is_codestral_22b_still_the_best_open_llm_for/n1k5dja/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751762431,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1751762431,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1jxxbx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"One-Stress-6734","can_mod_post":false,"created_utc":1751759548,"send_replies":true,"parent_id":"t1_n1julm7","score":2,"author_fullname":"t2_77k54e2b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Would you say GLM-4 actually follows long context chains across multiple files? Or is it more like it generates nice isolated code once you narrow the context manually?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1jxxbx","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Would you say GLM-4 actually follows long context chains across multiple files? Or is it more like it generates nice isolated code once you narrow the context manually?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsmtzr","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsmtzr/is_codestral_22b_still_the_best_open_llm_for/n1jxxbx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751759548,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1julm7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"sxales","can_mod_post":false,"created_utc":1751758334,"send_replies":true,"parent_id":"t3_1lsmtzr","score":7,"author_fullname":"t2_5h1ye","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"author_cakeday":true,"edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1julm7","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I prefer GLM-4 0414 for C++ although Qwen 3 and Qwen2.5 Coder weren&amp;#39;t far behind for my use case.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsmtzr/is_codestral_22b_still_the_best_open_llm_for/n1julm7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751758334,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lsmtzr","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"609bf7d4-01f3-11f0-9760-5611c8333bee","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1jz96u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"You_Wen_AzzHu","can_mod_post":false,"created_utc":1751760046,"send_replies":true,"parent_id":"t3_1lsmtzr","score":8,"author_fullname":"t2_p4oxcufl","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Qwen3 32b q4 is the only q4 that can solve my python UI problems. I vote for it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1jz96u","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"exllama"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen3 32b q4 is the only q4 that can solve my python UI problems. I vote for it.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsmtzr/is_codestral_22b_still_the_best_open_llm_for/n1jz96u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751760046,"author_flair_text":"exllama","treatment_tags":[],"link_id":"t3_1lsmtzr","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1kjzej","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rorowhat","can_mod_post":false,"created_utc":1751768376,"send_replies":true,"parent_id":"t1_n1k2t7s","score":3,"author_fullname":"t2_yq51a","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Wouldn't qwen3 32b be better?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1kjzej","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Wouldn&amp;#39;t qwen3 32b be better?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsmtzr","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsmtzr/is_codestral_22b_still_the_best_open_llm_for/n1kjzej/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751768376,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n1k2t7s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HumbleTech905","can_mod_post":false,"created_utc":1751761416,"send_replies":true,"parent_id":"t3_1lsmtzr","score":4,"author_fullname":"t2_stw6dyd68","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Qwen2.5 coder 32B q8 , forget q4, q6.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1k2t7s","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen2.5 coder 32B q8 , forget q4, q6.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsmtzr/is_codestral_22b_still_the_best_open_llm_for/n1k2t7s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751761416,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lsmtzr","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1kierw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Alkeryn","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1ki6cx","score":1,"author_fullname":"t2_xbcpo","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Even then, he could get 60B-90B models at q5 easily.\\nQ5 is pm lossless with modern quant, especially for bigger models.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1kierw","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Even then, he could get 60B-90B models at q5 easily.\\nQ5 is pm lossless with modern quant, especially for bigger models.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsmtzr","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsmtzr/is_codestral_22b_still_the_best_open_llm_for/n1kierw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751767733,"author_flair_text":null,"treatment_tags":[],"created_utc":1751767733,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1ki6cx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"skrshawk","can_mod_post":false,"created_utc":1751767638,"send_replies":true,"parent_id":"t1_n1kaact","score":1,"author_fullname":"t2_36e47","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Coding models are run at much higher precision than chat models.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ki6cx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Coding models are run at much higher precision than chat models.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsmtzr","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsmtzr/is_codestral_22b_still_the_best_open_llm_for/n1ki6cx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751767638,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1kaact","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Alkeryn","can_mod_post":false,"created_utc":1751764415,"send_replies":true,"parent_id":"t3_1lsmtzr","score":0,"author_fullname":"t2_xbcpo","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"if you got 64GB of vram you can run the 100B models.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1kaact","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;if you got 64GB of vram you can run the 100B models.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsmtzr/is_codestral_22b_still_the_best_open_llm_for/n1kaact/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751764415,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lsmtzr","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}}]`),s=()=>l.jsx(e,{data:t});export{s as default};
