import{j as e}from"./index-DACS7Nh6.js";import{R as l}from"./RedditPostRenderer-Dqa1NZuX.js";import"./index-DiMIVQx4.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hello everyone! At home I run various LLM models (text and image generation). I use for this a PC with 3060ti, 16gb RAM and another PC with 3060(12gb) and 32gb RAM.\\n\\nWhen working on 3060ti, the video card is loaded at 100%, and 3060 only at 20%. The generation speed is about the same, but is this a sensor error or is there a bottleneck in my system?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"GPU bottleneck?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m2xewp","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.75,"author_flair_background_color":null,"subreddit_type":"public","ups":2,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_e1ruuaofl","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":2,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752829561,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hello everyone! At home I run various LLM models (text and image generation). I use for this a PC with 3060ti, 16gb RAM and another PC with 3060(12gb) and 32gb RAM.&lt;/p&gt;\\n\\n&lt;p&gt;When working on 3060ti, the video card is loaded at 100%, and 3060 only at 20%. The generation speed is about the same, but is this a sensor error or is there a bottleneck in my system?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m2xewp","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Solid_Studio167","discussion_type":null,"num_comments":8,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m2xewp/gpu_bottleneck/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m2xewp/gpu_bottleneck/","subreddit_subscribers":501232,"created_utc":1752829561,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3sjgqk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GeekyBit","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3sg04k","score":1,"author_fullname":"t2_zq180","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"okay sure again not really answer the question but if you feel like your self generated answer is good for you, than great. Happy to be a sounding board I suppose :P","edited":false,"author_flair_css_class":null,"name":"t1_n3sjgqk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;okay sure again not really answer the question but if you feel like your self generated answer is good for you, than great. Happy to be a sounding board I suppose :P&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m2xewp","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xewp/gpu_bottleneck/n3sjgqk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752834788,"author_flair_text":null,"collapsed":false,"created_utc":1752834788,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3sg04k","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Solid_Studio167","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3sep8b","score":1,"author_fullname":"t2_e1ruuaofl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thank you for answering me with my problem.\\n\\nThe GPU core is loaded at 100%, and the video memory is filled to 8 out of 8 GB on the ti version, perhaps the rest of the model is distributed to RAM, unlike the 12 GB video card","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3sg04k","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you for answering me with my problem.&lt;/p&gt;\\n\\n&lt;p&gt;The GPU core is loaded at 100%, and the video memory is filled to 8 out of 8 GB on the ti version, perhaps the rest of the model is distributed to RAM, unlike the 12 GB video card&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xewp","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xewp/gpu_bottleneck/n3sg04k/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752832975,"author_flair_text":null,"treatment_tags":[],"created_utc":1752832975,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3sep8b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GeekyBit","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3scnys","score":1,"author_fullname":"t2_zq180","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"OMG... This has nothing to do with PCIe Bandwidth... first off once the model is loaded on to the GPU it shouldn't be pulling from the RAM pool unless it doesn't fit on the card... \\n\\nYou can run LLMs on a 1x slot of PCIe and it runs as fast as the card in the slot can go, with a card that can hold the full capacity of the model you are running... . \\n\\nThe only down side is load times. You see The models Ideally run on the GPU... Also here is something for you, if you run it across several cards in the same system the load time will be slow, but even if one card is in a 1x slot, once the model is fully loaded it will run fast.\\n\\nAlso please, please, PLEASE, answer the question... When you say load do you mean load on the vram or GPU usage load... this is a fairly simple question I am asking... One that can help answer your question, and also one that you seem compelled to ignore and posit your own idea on what the problem is. If you just want to ignore someone tying to help we are done here.\\n\\n  \\nYou just seem to have gotten it stuck in your head it is a PCIe Bandwidth issue... Also Just an FYI... IF both system are running say PCIE 4.0 and both have the Card in the 16x slot there would be no way for the systems to have a different load time for the model unless their is a loading not RUNING... but LOADING TIME bottleneck some where else in the system. For example  SATA storage, Low SYSTEM RAM.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3sep8b","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;OMG... This has nothing to do with PCIe Bandwidth... first off once the model is loaded on to the GPU it shouldn&amp;#39;t be pulling from the RAM pool unless it doesn&amp;#39;t fit on the card... &lt;/p&gt;\\n\\n&lt;p&gt;You can run LLMs on a 1x slot of PCIe and it runs as fast as the card in the slot can go, with a card that can hold the full capacity of the model you are running... . &lt;/p&gt;\\n\\n&lt;p&gt;The only down side is load times. You see The models Ideally run on the GPU... Also here is something for you, if you run it across several cards in the same system the load time will be slow, but even if one card is in a 1x slot, once the model is fully loaded it will run fast.&lt;/p&gt;\\n\\n&lt;p&gt;Also please, please, PLEASE, answer the question... When you say load do you mean load on the vram or GPU usage load... this is a fairly simple question I am asking... One that can help answer your question, and also one that you seem compelled to ignore and posit your own idea on what the problem is. If you just want to ignore someone tying to help we are done here.&lt;/p&gt;\\n\\n&lt;p&gt;You just seem to have gotten it stuck in your head it is a PCIe Bandwidth issue... Also Just an FYI... IF both system are running say PCIE 4.0 and both have the Card in the 16x slot there would be no way for the systems to have a different load time for the model unless their is a loading not RUNING... but LOADING TIME bottleneck some where else in the system. For example  SATA storage, Low SYSTEM RAM.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xewp","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xewp/gpu_bottleneck/n3sep8b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752832278,"author_flair_text":null,"treatment_tags":[],"created_utc":1752832278,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3scnys","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Solid_Studio167","can_mod_post":false,"created_utc":1752831145,"send_replies":true,"parent_id":"t1_n3sc98q","score":1,"author_fullname":"t2_e1ruuaofl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"During image generation, the 3060 also loads at a low percentage, unlike Ti version. And they consume the same amount of video memory. It might be a PCI bandwidth issue, but the difference in performance is huge.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3scnys","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;During image generation, the 3060 also loads at a low percentage, unlike Ti version. And they consume the same amount of video memory. It might be a PCI bandwidth issue, but the difference in performance is huge.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xewp","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xewp/gpu_bottleneck/n3scnys/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752831145,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3sc98q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GeekyBit","can_mod_post":false,"created_utc":1752830911,"send_replies":true,"parent_id":"t3_1m2xewp","score":1,"author_fullname":"t2_zq180","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"well they both run the same because both have the same speed of vram more or less.... the 3060ti is only 8gb... the 3060 on the other hand is 12gb...\\n\\nBut when you say load do you mean used Vram or GPU usage...\\n\\nBecause those are two different things \\n\\nFor example in Text generation it is almost completely about the Vram Bandwidth. If it is image generation it is also a lot about GPU speeds... and the GPU should 100% most of the time, unless their is a Ram/Vram size bottleneck.\\n\\nI hope that clears it up to at lest a muddy level for you.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3sc98q","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;well they both run the same because both have the same speed of vram more or less.... the 3060ti is only 8gb... the 3060 on the other hand is 12gb...&lt;/p&gt;\\n\\n&lt;p&gt;But when you say load do you mean used Vram or GPU usage...&lt;/p&gt;\\n\\n&lt;p&gt;Because those are two different things &lt;/p&gt;\\n\\n&lt;p&gt;For example in Text generation it is almost completely about the Vram Bandwidth. If it is image generation it is also a lot about GPU speeds... and the GPU should 100% most of the time, unless their is a Ram/Vram size bottleneck.&lt;/p&gt;\\n\\n&lt;p&gt;I hope that clears it up to at lest a muddy level for you.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xewp/gpu_bottleneck/n3sc98q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752830911,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2xewp","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3sgza6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GPTshop_ai","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3sggdk","score":0,"author_fullname":"t2_rkmud0isr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Don't waste you time.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3sgza6","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Don&amp;#39;t waste you time.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xewp","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xewp/gpu_bottleneck/n3sgza6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752833496,"author_flair_text":null,"treatment_tags":[],"created_utc":1752833496,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n3sggdk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Solid_Studio167","can_mod_post":false,"created_utc":1752833217,"send_replies":true,"parent_id":"t1_n3sekg4","score":1,"author_fullname":"t2_e1ruuaofl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This equipment is suitable for my needs, I'm just trying to understand why, with the same tasks, the load on the GPU core of two similar GPUs is very different","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3sggdk","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This equipment is suitable for my needs, I&amp;#39;m just trying to understand why, with the same tasks, the load on the GPU core of two similar GPUs is very different&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xewp","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xewp/gpu_bottleneck/n3sggdk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752833217,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3sekg4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GPTshop_ai","can_mod_post":false,"created_utc":1752832208,"send_replies":true,"parent_id":"t3_1m2xewp","score":-1,"author_fullname":"t2_rkmud0isr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Just buy some hardware that is suitable.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3sekg4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just buy some hardware that is suitable.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xewp/gpu_bottleneck/n3sekg4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752832208,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2xewp","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}}]`),n=()=>e.jsx(l,{data:t});export{n as default};
