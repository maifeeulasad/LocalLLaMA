import{j as e}from"./index-DQXiEb7D.js";import{R as l}from"./RedditPostRenderer-BjndLgq8.js";import"./index-B-ILyjT1.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"[Here's the YouTube Playlist](https://www.youtube.com/playlist?list=PLoROMvodv4rOY23Y0BoGoBGgQ1zmU_MT_)\\n\\n[Here's the CS336 website with assignments, slides etc](https://stanford-cs336.github.io/spring2025/)\\n\\nI've been studying it for a week and it's the best course on LLMs I've seen online. The assignments are **huge**, very in-depth, and they require you to write **a lot** of code from scratch. For example, the [1st assignment pdf](https://github.com/stanford-cs336/assignment1-basics/blob/main/cs336_spring2025_assignment1_basics.pdf) is 50 pages long and it requires you to implement the BPE tokenizer, a simple transformer LM, cross-entropy loss and AdamW and train models on OpenWebText","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Stanford's CS336 2025 (Language Modeling from Scratch) is now available on YouTube","link_flair_richtext":[{"e":"text","t":"Resources"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lxgb9q","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.98,"author_flair_background_color":null,"subreddit_type":"public","ups":212,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_qzfad","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Resources","can_mod_post":false,"score":212,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1752262867,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://www.youtube.com/playlist?list=PLoROMvodv4rOY23Y0BoGoBGgQ1zmU_MT_\\"&gt;Here&amp;#39;s the YouTube Playlist&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://stanford-cs336.github.io/spring2025/\\"&gt;Here&amp;#39;s the CS336 website with assignments, slides etc&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ve been studying it for a week and it&amp;#39;s the best course on LLMs I&amp;#39;ve seen online. The assignments are &lt;strong&gt;huge&lt;/strong&gt;, very in-depth, and they require you to write &lt;strong&gt;a lot&lt;/strong&gt; of code from scratch. For example, the &lt;a href=\\"https://github.com/stanford-cs336/assignment1-basics/blob/main/cs336_spring2025_assignment1_basics.pdf\\"&gt;1st assignment pdf&lt;/a&gt; is 50 pages long and it requires you to implement the BPE tokenizer, a simple transformer LM, cross-entropy loss and AdamW and train models on OpenWebText&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/9XmReZR8sZe4EwJETS_bT_kZhCOn3jpR_yHrOPaaruc.jpeg?auto=webp&amp;s=015a3619316838acb64cc4b58ef2bd6d744a87cf","width":480,"height":270},"resolutions":[{"url":"https://external-preview.redd.it/9XmReZR8sZe4EwJETS_bT_kZhCOn3jpR_yHrOPaaruc.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1a73b19b73b083dd96c1d55121a321e063838715","width":108,"height":60},{"url":"https://external-preview.redd.it/9XmReZR8sZe4EwJETS_bT_kZhCOn3jpR_yHrOPaaruc.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c3a42750c097b19e93b5a75ec8f80e7c323f71fd","width":216,"height":121},{"url":"https://external-preview.redd.it/9XmReZR8sZe4EwJETS_bT_kZhCOn3jpR_yHrOPaaruc.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2c36bf5bbd205d5e1253f6d71da886c9d0583343","width":320,"height":180}],"variants":{},"id":"9XmReZR8sZe4EwJETS_bT_kZhCOn3jpR_yHrOPaaruc"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#ccac2b","id":"1lxgb9q","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"realmvp77","discussion_type":null,"num_comments":21,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lxgb9q/stanfords_cs336_2025_language_modeling_from/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lxgb9q/stanfords_cs336_2025_language_modeling_from/","subreddit_subscribers":498346,"created_utc":1752262867,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2n3gtt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Lazy-Pattern-5171","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2n0laz","score":6,"author_fullname":"t2_1lyjk8is25","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Even more important to race to the finish line then. Would know if it’s for me or not faster.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2n3gtt","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Even more important to race to the finish line then. Would know if it’s for me or not faster.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxgb9q","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgb9q/stanfords_cs336_2025_language_modeling_from/n2n3gtt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752277196,"author_flair_text":null,"treatment_tags":[],"created_utc":1752277196,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n2n0laz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"realmvp77","can_mod_post":false,"created_utc":1752276200,"send_replies":true,"parent_id":"t1_n2min1f","score":19,"author_fullname":"t2_qzfad","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"just as a warning, even though the course is called \\"Language Modeling *from Scratch*\\", it ramps up pretty fast, so it's not meant for total beginners. I wouldn't go into it without some basic LLM knowledge. I read Sebastian Raschka's \\"Build a LLM\\" book and thought it was great prep for this course. Karpathy's playlist is great too, I watched that before I read the book","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2n0laz","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;just as a warning, even though the course is called &amp;quot;Language Modeling &lt;em&gt;from Scratch&lt;/em&gt;&amp;quot;, it ramps up pretty fast, so it&amp;#39;s not meant for total beginners. I wouldn&amp;#39;t go into it without some basic LLM knowledge. I read Sebastian Raschka&amp;#39;s &amp;quot;Build a LLM&amp;quot; book and thought it was great prep for this course. Karpathy&amp;#39;s playlist is great too, I watched that before I read the book&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxgb9q","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgb9q/stanfords_cs336_2025_language_modeling_from/n2n0laz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752276200,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":19}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2uap6p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"sleepy_roger","can_mod_post":false,"created_utc":1752378183,"send_replies":true,"parent_id":"t1_n2qwa93","score":2,"author_fullname":"t2_usojvms","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Honestly, I wouldn’t take Expensive-Apricot’s comments too seriously. If you dig into their history, it’s clear they speak with a lot of certainty on topics they don’t necessarily have deep experience in. The kind of black-and-white thinking they’re showing, “you can’t do X,” “you won’t make Y” is exactly what kills innovation before it starts.\\n\\nYou’ve already shown you're open to feedback and willing to iterate, which is half the battle in this space. 2x3090s is plenty to do some serious work. You might not build a model that dethrones GPT-4, but setting an ambitious goal, learning along the way, and seeing how far you can push a 100M or even 500M model is absolutely worthwhile.\\n\\nDon’t let people with rigid mindsets set your ceiling. Just make sure you're getting feedback from folks who actually build things and always look at their history before treating what they say as gospel.\\n\\nKeep going. You’re asking the right questions.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n2uap6p","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Honestly, I wouldn’t take Expensive-Apricot’s comments too seriously. If you dig into their history, it’s clear they speak with a lot of certainty on topics they don’t necessarily have deep experience in. The kind of black-and-white thinking they’re showing, “you can’t do X,” “you won’t make Y” is exactly what kills innovation before it starts.&lt;/p&gt;\\n\\n&lt;p&gt;You’ve already shown you&amp;#39;re open to feedback and willing to iterate, which is half the battle in this space. 2x3090s is plenty to do some serious work. You might not build a model that dethrones GPT-4, but setting an ambitious goal, learning along the way, and seeing how far you can push a 100M or even 500M model is absolutely worthwhile.&lt;/p&gt;\\n\\n&lt;p&gt;Don’t let people with rigid mindsets set your ceiling. Just make sure you&amp;#39;re getting feedback from folks who actually build things and always look at their history before treating what they say as gospel.&lt;/p&gt;\\n\\n&lt;p&gt;Keep going. You’re asking the right questions.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lxgb9q","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgb9q/stanfords_cs336_2025_language_modeling_from/n2uap6p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752378183,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":0,"name":"t1__","id":"_","parent_id":"t1_n2t019r","depth":10,"children":[]}}],"before":null}},"user_reports":[],"saved":false,"id":"n2t019r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Expensive-Apricot-25","can_mod_post":false,"created_utc":1752360535,"send_replies":true,"parent_id":"t1_n2suu7y","score":1,"author_fullname":"t2_idqkwio0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"make your own model completely from scratch that is able to actually produce legible output, and have basic Q/A abilities \\n\\n(it is at the very least able to understand that it is being asked a question, and attempts to answer)\\n\\nTrust me, this is harder than you think. from scratch no pre-trained model, only pytorch.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2t019r","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;make your own model completely from scratch that is able to actually produce legible output, and have basic Q/A abilities &lt;/p&gt;\\n\\n&lt;p&gt;(it is at the very least able to understand that it is being asked a question, and attempts to answer)&lt;/p&gt;\\n\\n&lt;p&gt;Trust me, this is harder than you think. from scratch no pre-trained model, only pytorch.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgb9q/stanfords_cs336_2025_language_modeling_from/n2t019r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752360535,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxgb9q","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":9,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2suu7y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Lazy-Pattern-5171","can_mod_post":false,"created_utc":1752358721,"send_replies":true,"parent_id":"t1_n2su639","score":1,"author_fullname":"t2_1lyjk8is25","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Fair. What would be a good challenge then that’s also you know like, a challenge?","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n2suu7y","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Fair. What would be a good challenge then that’s also you know like, a challenge?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgb9q/stanfords_cs336_2025_language_modeling_from/n2suu7y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752358721,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxgb9q","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":8,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2su639","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Expensive-Apricot-25","can_mod_post":false,"created_utc":1752358483,"send_replies":true,"parent_id":"t1_n2qwa93","score":1,"author_fullname":"t2_idqkwio0","approved_by":null,"mod_note":null,"all_awardings":[],"body":"No, you’re not. You won’t be able to make SOTA at any size.\\n\\nAgain, there are companies that hire full teams of people with decades of experience, and infinite compute resources that are working on this 24/7.\\n\\nYou don’t even have any experience. You simply can’t compete.\\n\\nRemember, SOTA means better than everything else, not “using SOTA techniques”.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n2su639","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No, you’re not. You won’t be able to make SOTA at any size.&lt;/p&gt;\\n\\n&lt;p&gt;Again, there are companies that hire full teams of people with decades of experience, and infinite compute resources that are working on this 24/7.&lt;/p&gt;\\n\\n&lt;p&gt;You don’t even have any experience. You simply can’t compete.&lt;/p&gt;\\n\\n&lt;p&gt;Remember, SOTA means better than everything else, not “using SOTA techniques”.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lxgb9q","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgb9q/stanfords_cs336_2025_language_modeling_from/n2su639/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752358483,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2qwa93","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Lazy-Pattern-5171","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2qv6nm","score":3,"author_fullname":"t2_1lyjk8is25","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Can I make a SOTA 100M? I want to give myself a constraint motivating enough to bet 1000$ on myself and also finish it. That’s why dreaming of the leaderboard right now seems to be the only goal people are talking about.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n2qwa93","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Can I make a SOTA 100M? I want to give myself a constraint motivating enough to bet 1000$ on myself and also finish it. That’s why dreaming of the leaderboard right now seems to be the only goal people are talking about.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lxgb9q","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgb9q/stanfords_cs336_2025_language_modeling_from/n2qwa93/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752336231,"author_flair_text":null,"treatment_tags":[],"created_utc":1752336231,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2qv6nm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Expensive-Apricot-25","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2qrfps","score":0,"author_fullname":"t2_idqkwio0","approved_by":null,"mod_note":null,"all_awardings":[],"body":"oh wow, thats really good, but you're still going bottlenecked by compute not memory. training uses way more compute than inference does.\\n\\nBut again, you are not going to make a SOTA model. thats the main issue","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n2qv6nm","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;oh wow, thats really good, but you&amp;#39;re still going bottlenecked by compute not memory. training uses way more compute than inference does.&lt;/p&gt;\\n\\n&lt;p&gt;But again, you are not going to make a SOTA model. thats the main issue&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lxgb9q","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgb9q/stanfords_cs336_2025_language_modeling_from/n2qv6nm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752335890,"author_flair_text":null,"treatment_tags":[],"created_utc":1752335890,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n2qrfps","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Lazy-Pattern-5171","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2qr0qr","score":3,"author_fullname":"t2_1lyjk8is25","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I’ve the classic 2x3090","edited":false,"author_flair_css_class":null,"name":"t1_n2qrfps","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I’ve the classic 2x3090&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lxgb9q","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgb9q/stanfords_cs336_2025_language_modeling_from/n2qrfps/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752334726,"author_flair_text":null,"collapsed":false,"created_utc":1752334726,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2qr0qr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Expensive-Apricot-25","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2q9fsm","score":0,"author_fullname":"t2_idqkwio0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"if you have a dedicated mid-high range consumer GPU, probably around 100-200 million. I would say around 20-50 million is more realistic though since you can train it in a matter of hours rather than days.\\n\\nThats not the problem though, the problem is thinking you are going to make a \\"state of the art model\\", that is not going to happen. \\n\\nThere are teams of people with decades of experience, access to thousands of industrial GPUs, who get paid massive amounts of money to do this, there is no way you are going to be able to compete with them.\\n\\nYou need huge amounts of resources to make these models, thats the reason why only huge companies are the ones able to release open source models","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2qr0qr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;if you have a dedicated mid-high range consumer GPU, probably around 100-200 million. I would say around 20-50 million is more realistic though since you can train it in a matter of hours rather than days.&lt;/p&gt;\\n\\n&lt;p&gt;Thats not the problem though, the problem is thinking you are going to make a &amp;quot;state of the art model&amp;quot;, that is not going to happen. &lt;/p&gt;\\n\\n&lt;p&gt;There are teams of people with decades of experience, access to thousands of industrial GPUs, who get paid massive amounts of money to do this, there is no way you are going to be able to compete with them.&lt;/p&gt;\\n\\n&lt;p&gt;You need huge amounts of resources to make these models, thats the reason why only huge companies are the ones able to release open source models&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxgb9q","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgb9q/stanfords_cs336_2025_language_modeling_from/n2qr0qr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752334599,"author_flair_text":null,"treatment_tags":[],"created_utc":1752334599,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n2q9fsm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Lazy-Pattern-5171","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2q6003","score":2,"author_fullname":"t2_1lyjk8is25","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What’s the largest I can hope to make realistically?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2q9fsm","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What’s the largest I can hope to make realistically?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxgb9q","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgb9q/stanfords_cs336_2025_language_modeling_from/n2q9fsm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752328920,"author_flair_text":null,"treatment_tags":[],"created_utc":1752328920,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2q6003","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Expensive-Apricot-25","can_mod_post":false,"created_utc":1752327715,"send_replies":true,"parent_id":"t1_n2min1f","score":1,"author_fullname":"t2_idqkwio0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You’re not going to be able to make a state of the art 1B model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2q6003","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You’re not going to be able to make a state of the art 1B model.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxgb9q","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgb9q/stanfords_cs336_2025_language_modeling_from/n2q6003/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752327715,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2min1f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Lazy-Pattern-5171","can_mod_post":false,"created_utc":1752270204,"send_replies":true,"parent_id":"t3_1lxgb9q","score":15,"author_fullname":"t2_1lyjk8is25","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Finally. Anyone wants to race to the finish on this one? We can track goals and metrics on Discord. first one to SOTA 1B model wins 1000$. You can’t have prior LLM knowledge or should’ve watched and implemented Karpathy’s videos obviously but using AI should be allowed so my guess is that eventually systems will align.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2min1f","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Finally. Anyone wants to race to the finish on this one? We can track goals and metrics on Discord. first one to SOTA 1B model wins 1000$. You can’t have prior LLM knowledge or should’ve watched and implemented Karpathy’s videos obviously but using AI should be allowed so my guess is that eventually systems will align.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgb9q/stanfords_cs336_2025_language_modeling_from/n2min1f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752270204,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxgb9q","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":15}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2mls7z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Accomplished_Mode170","can_mod_post":false,"created_utc":1752271199,"send_replies":true,"parent_id":"t3_1lxgb9q","score":8,"author_fullname":"t2_4hfmiefj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Will check later; love 3Blue1Browns visuals in particular so I’m interested in similar versions for [NSA because sparsity itself](https://www.tilderesearch.com/blog/sparse-attn) seems [fundamental to reasoning](https://arxiv.org/html/2402.09099v3) (read: [spline fitting the circuit](https://transformer-circuits.pub/2025/attribution-graphs/methods.html))","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2mls7z","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Will check later; love 3Blue1Browns visuals in particular so I’m interested in similar versions for &lt;a href=\\"https://www.tilderesearch.com/blog/sparse-attn\\"&gt;NSA because sparsity itself&lt;/a&gt; seems &lt;a href=\\"https://arxiv.org/html/2402.09099v3\\"&gt;fundamental to reasoning&lt;/a&gt; (read: &lt;a href=\\"https://transformer-circuits.pub/2025/attribution-graphs/methods.html\\"&gt;spline fitting the circuit&lt;/a&gt;)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgb9q/stanfords_cs336_2025_language_modeling_from/n2mls7z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752271199,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxgb9q","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2q5fpn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"realmvp77","can_mod_post":false,"created_utc":1752327511,"send_replies":true,"parent_id":"t1_n2pcx2n","score":4,"author_fullname":"t2_qzfad","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I recently finished reading that book and it's great. you should read the appendix's links too and do the bonus sections on github. CS336 goes deeper than it, and it requires you to write lots of code on your own, so if you wanna study further, you should read the book and then do CS336","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2q5fpn","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I recently finished reading that book and it&amp;#39;s great. you should read the appendix&amp;#39;s links too and do the bonus sections on github. CS336 goes deeper than it, and it requires you to write lots of code on your own, so if you wanna study further, you should read the book and then do CS336&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxgb9q","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgb9q/stanfords_cs336_2025_language_modeling_from/n2q5fpn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752327511,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n2pcx2n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Kathane37","can_mod_post":false,"created_utc":1752314603,"send_replies":true,"parent_id":"t3_1lxgb9q","score":3,"author_fullname":"t2_91nqzv8x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1633437167\\n\\nI have started to dig this book, do you think I need to watch the classes or will I be fine ?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2pcx2n","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1633437167\\"&gt;https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1633437167&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;I have started to dig this book, do you think I need to watch the classes or will I be fine ?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgb9q/stanfords_cs336_2025_language_modeling_from/n2pcx2n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752314603,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxgb9q","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2n3lj6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Sea-Rope-31","can_mod_post":false,"created_utc":1752277242,"send_replies":true,"parent_id":"t3_1lxgb9q","score":3,"author_fullname":"t2_1sdssbj1gj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thanks for sharing!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2n3lj6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for sharing!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgb9q/stanfords_cs336_2025_language_modeling_from/n2n3lj6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752277242,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxgb9q","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2vpzhk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fandogh5","can_mod_post":false,"created_utc":1752405666,"send_replies":true,"parent_id":"t3_1lxgb9q","score":1,"author_fullname":"t2_58wlu2d1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Is it finished?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2vpzhk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Is it finished?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgb9q/stanfords_cs336_2025_language_modeling_from/n2vpzhk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752405666,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxgb9q","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(l,{data:t});export{n as default};
