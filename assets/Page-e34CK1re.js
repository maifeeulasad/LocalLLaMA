import{j as e}from"./index-BgwOAK4-.js";import{R as l}from"./RedditPostRenderer-BOBjDTFu.js";import"./index-BL22wVg5.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hi Folks, Iâ€™m scraping some listing pages and want to extract structured info like title, location, and link â€” but the HTML varies a lot between sites.\\n\\nIâ€™m looking for a fast, local LLM that can handle this kind of messy data and give me clean results. Ideally something lightweight (quantized is fine), and works well with prompts like:  \\n*\\"Extract all detailed listings from this HTML with title, location, and URL.\\"*\\n\\nAny recommendations? Would love to hear whatâ€™s working for you!\\n\\nUpdate #1:  \\n\\\\- I tried Gemma3 4b and 12b -&gt; Im not staisfaied with the results at all  \\n\\\\- I tried Qwen2.5 vl 3b -&gt; doing okay but still add wrong data   \\n\\\\- Qwen2.5 vl 7b -&gt; The best but takes long time ","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Best fast local model for extracting data from scraped HTML?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lrsdne","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":1,"author_flair_background_color":null,"subreddit_type":"public","ups":2,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_3op05s7","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":2,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1751726240,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751660367,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi Folks, Iâ€™m scraping some listing pages and want to extract structured info like title, location, and link â€” but the HTML varies a lot between sites.&lt;/p&gt;\\n\\n&lt;p&gt;Iâ€™m looking for a fast, local LLM that can handle this kind of messy data and give me clean results. Ideally something lightweight (quantized is fine), and works well with prompts like:&lt;br/&gt;\\n&lt;em&gt;&amp;quot;Extract all detailed listings from this HTML with title, location, and URL.&amp;quot;&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Any recommendations? Would love to hear whatâ€™s working for you!&lt;/p&gt;\\n\\n&lt;p&gt;Update #1:&lt;br/&gt;\\n- I tried Gemma3 4b and 12b -&amp;gt; Im not staisfaied with the results at all&lt;br/&gt;\\n- I tried Qwen2.5 vl 3b -&amp;gt; doing okay but still add wrong data&lt;br/&gt;\\n- Qwen2.5 vl 7b -&amp;gt; The best but takes long time &lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lrsdne","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"xtremx12","discussion_type":null,"num_comments":6,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lrsdne/best_fast_local_model_for_extracting_data_from/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lrsdne/best_fast_local_model_for_extracting_data_from/","subreddit_subscribers":494898,"created_utc":1751660367,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1da961","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Last-Progress18","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1d9o8z","score":2,"author_fullname":"t2_amiviq1r","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Like I said, I find Llama 3 8b much faster and it gives good responses ðŸ™‚ðŸ‘\\n\\nAlthough have found it gives better answers with higher context levels.\\n\\nWith my setup (32GB VRAM), even smaller Qwen 3 models can take 3x - 4x response times compared to Llama 3","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1da961","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Like I said, I find Llama 3 8b much faster and it gives good responses ðŸ™‚ðŸ‘&lt;/p&gt;\\n\\n&lt;p&gt;Although have found it gives better answers with higher context levels.&lt;/p&gt;\\n\\n&lt;p&gt;With my setup (32GB VRAM), even smaller Qwen 3 models can take 3x - 4x response times compared to Llama 3&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrsdne","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrsdne/best_fast_local_model_for_extracting_data_from/n1da961/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751662241,"author_flair_text":null,"treatment_tags":[],"created_utc":1751662241,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1d9o8z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"xtremx12","can_mod_post":false,"created_utc":1751662052,"send_replies":true,"parent_id":"t1_n1d69z9","score":1,"author_fullname":"t2_3op05s7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I tested qwen2.5 3b and 7b  .. 7b is much better but actually it's slow","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1d9o8z","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I tested qwen2.5 3b and 7b  .. 7b is much better but actually it&amp;#39;s slow&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrsdne","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrsdne/best_fast_local_model_for_extracting_data_from/n1d9o8z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751662052,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1dcw12","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Last-Progress18","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1dc5lh","score":1,"author_fullname":"t2_amiviq1r","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"On my setup, think itâ€™s a bottleneck caused by running older kernel versions.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1dcw12","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;On my setup, think itâ€™s a bottleneck caused by running older kernel versions.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrsdne","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrsdne/best_fast_local_model_for_extracting_data_from/n1dcw12/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751663098,"author_flair_text":null,"treatment_tags":[],"created_utc":1751663098,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1dc5lh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1751662856,"send_replies":true,"parent_id":"t1_n1d69z9","score":1,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt;  but find the tokeniser much slower, especially Qwen 3 on older enterprise level GPUs.\\n\\nTokenisers run on CPU's, not GPU's and extremely, super cheap in terms of resources. Slow down might be because of more expensive attention in Qwen. I did not notice much difference between Qwen 3 8b and LLama 3.1 though.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1dc5lh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;but find the tokeniser much slower, especially Qwen 3 on older enterprise level GPUs.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Tokenisers run on CPU&amp;#39;s, not GPU&amp;#39;s and extremely, super cheap in terms of resources. Slow down might be because of more expensive attention in Qwen. I did not notice much difference between Qwen 3 8b and LLama 3.1 though.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrsdne","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrsdne/best_fast_local_model_for_extracting_data_from/n1dc5lh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751662856,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1d69z9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Last-Progress18","can_mod_post":false,"created_utc":1751660944,"send_replies":true,"parent_id":"t3_1lrsdne","score":1,"author_fullname":"t2_amiviq1r","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Llama 3 8b or Gemma 3 4b â€” theyâ€™re remarkably accurate for small models. Llama 3 is much better with anything involving math / science etc\\n\\nQwen models are good â€” but find the tokeniser much slower, especially Qwen 3 on older enterprise level GPUs.","edited":1751662654,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1d69z9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Llama 3 8b or Gemma 3 4b â€” theyâ€™re remarkably accurate for small models. Llama 3 is much better with anything involving math / science etc&lt;/p&gt;\\n\\n&lt;p&gt;Qwen models are good â€” but find the tokeniser much slower, especially Qwen 3 on older enterprise level GPUs.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrsdne/best_fast_local_model_for_extracting_data_from/n1d69z9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751660944,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrsdne","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1dg7hd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"brown2green","can_mod_post":false,"created_utc":1751664222,"send_replies":true,"parent_id":"t3_1lrsdne","score":3,"author_fullname":"t2_f010l","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Gemma 3 got pretrained on large amounts of HTML code (you can easily see that by making the pretrained model generate random documents), so I think that should work well.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1dg7hd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Gemma 3 got pretrained on large amounts of HTML code (you can easily see that by making the pretrained model generate random documents), so I think that should work well.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrsdne/best_fast_local_model_for_extracting_data_from/n1dg7hd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751664222,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrsdne","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}}]`),s=()=>e.jsx(l,{data:t});export{s as default};
