import{j as e}from"./index-CWmJdUH_.js";import{R as t}from"./RedditPostRenderer-D2iunoQ9.js";import"./index-BCg9RP6g.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hey r/LocalLlama! Wanted to share something interesting I've been working on that might be relevant for folks running models locally on Apple Silicon.\\n\\n**What I did**\\n\\nUsed evolutionary programming to automatically optimize Metal GPU kernels for transformer attention. Specifically targeted Qwen3-0.6B's grouped query attention (40:8 head ratio) running on Apple M-series GPUs through MLX.\\n\\n**Results**\\n\\nTested across 20 different inference scenarios against MLX's \`scaled_dot_product_attention\` baseline:\\n\\n* **Average decode speed improvement: +12.5%** (σ = 38.3%)\\n* **Peak improvement: +106%** on repetitive pattern generation\\n* **Best category: +24.8%** average on general tasks\\n* **Memory usage: -0.99%** (slight reduction)\\n\\n**The honest picture:** It's workload dependent. Some scenarios saw big gains (+46.6% on dialogue, +73.9% on extreme-length generation), but others regressed (-16.5% on code generation). Success rate was 7/20 benchmarks with &gt;25% improvements.\\n\\n**How it works**\\n\\nThe system automatically evolves the Metal kernel source code using LLMs while preserving the MLX integration. No human GPU programming expertise was provided - it discovered optimizations like:\\n\\n1. **Perfect SIMD vectorization**: Found that \`vec&lt;T, 8&gt;\` operations match Apple Silicon's capabilities for 128-dim attention heads\\n2. **Two-pass online softmax**: Fused softmax normalization with value accumulation, reducing memory bandwidth\\n3. **GQA-specific memory patterns**: Optimized for the 40:8 head structure with coalesced access patterns\\n\\n# Why this might matter for local inference\\n\\n* Shows automated optimization can compete with expert-engineered kernels\\n* Demonstrates potential for hardware-specific optimizations without manual tuning\\n* Could be applied to other transformer components or different model architectures\\n* All open source - you can reproduce and extend this work\\n\\n**Try it yourself**\\n\\nThe code and all benchmarks are available in the [OpenEvolve repo](https://github.com/codelion/openevolve). The MLX kernel optimization example is at \`examples/mlx_metal_kernel_opt/\`.\\n\\nRequirements:\\n\\n* Apple Silicon Mac\\n* MLX framework\\n* Qwen3-0.6B model\\n\\n# Limitations\\n\\n* Currently specific to Apple Silicon and this exact model configuration\\n* Performance improvements are highly workload-dependent\\n* Takes \\\\~25 evolutionary generations to converge (few hours on M3)\\n* No guarantees it'll work better for your specific use case\\n\\n**Technical write-up**\\n\\nFull details with code diffs and benchmark methodology: [https://huggingface.co/blog/codelion/openevolve-gpu-kernel-discovery](https://huggingface.co/blog/codelion/openevolve-gpu-kernel-discovery)\\n\\nCurious to hear thoughts from folks who've done MLX optimization work, or if anyone wants to try this on different models/configurations. The evolutionary approach seems promising but definitely has room for improvement.\\n\\nHas anyone else experimented with automated kernel optimization for local inference?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Automated GPU kernel optimization for Qwen3 attention - 12.5% average speedup on Apple Silicon using evolutionary programming","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lm98z7","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.95,"author_flair_background_color":"#93b1ba","subreddit_type":"public","ups":160,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":"7d1f04e6-4920-11ef-b2e1-2e580594e1a1","is_original_content":false,"author_fullname":"t2_e0bph","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":160,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[{"e":"text","t":"Llama 3.1"}],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1751069414,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"richtext","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey &lt;a href=\\"/r/LocalLlama\\"&gt;r/LocalLlama&lt;/a&gt;! Wanted to share something interesting I&amp;#39;ve been working on that might be relevant for folks running models locally on Apple Silicon.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;What I did&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Used evolutionary programming to automatically optimize Metal GPU kernels for transformer attention. Specifically targeted Qwen3-0.6B&amp;#39;s grouped query attention (40:8 head ratio) running on Apple M-series GPUs through MLX.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Tested across 20 different inference scenarios against MLX&amp;#39;s &lt;code&gt;scaled_dot_product_attention&lt;/code&gt; baseline:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;strong&gt;Average decode speed improvement: +12.5%&lt;/strong&gt; (σ = 38.3%)&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Peak improvement: +106%&lt;/strong&gt; on repetitive pattern generation&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Best category: +24.8%&lt;/strong&gt; average on general tasks&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Memory usage: -0.99%&lt;/strong&gt; (slight reduction)&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;&lt;strong&gt;The honest picture:&lt;/strong&gt; It&amp;#39;s workload dependent. Some scenarios saw big gains (+46.6% on dialogue, +73.9% on extreme-length generation), but others regressed (-16.5% on code generation). Success rate was 7/20 benchmarks with &amp;gt;25% improvements.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;How it works&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;The system automatically evolves the Metal kernel source code using LLMs while preserving the MLX integration. No human GPU programming expertise was provided - it discovered optimizations like:&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;&lt;strong&gt;Perfect SIMD vectorization&lt;/strong&gt;: Found that &lt;code&gt;vec&amp;lt;T, 8&amp;gt;&lt;/code&gt; operations match Apple Silicon&amp;#39;s capabilities for 128-dim attention heads&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Two-pass online softmax&lt;/strong&gt;: Fused softmax normalization with value accumulation, reducing memory bandwidth&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;GQA-specific memory patterns&lt;/strong&gt;: Optimized for the 40:8 head structure with coalesced access patterns&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;h1&gt;Why this might matter for local inference&lt;/h1&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Shows automated optimization can compete with expert-engineered kernels&lt;/li&gt;\\n&lt;li&gt;Demonstrates potential for hardware-specific optimizations without manual tuning&lt;/li&gt;\\n&lt;li&gt;Could be applied to other transformer components or different model architectures&lt;/li&gt;\\n&lt;li&gt;All open source - you can reproduce and extend this work&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Try it yourself&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;The code and all benchmarks are available in the &lt;a href=\\"https://github.com/codelion/openevolve\\"&gt;OpenEvolve repo&lt;/a&gt;. The MLX kernel optimization example is at &lt;code&gt;examples/mlx_metal_kernel_opt/&lt;/code&gt;.&lt;/p&gt;\\n\\n&lt;p&gt;Requirements:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Apple Silicon Mac&lt;/li&gt;\\n&lt;li&gt;MLX framework&lt;/li&gt;\\n&lt;li&gt;Qwen3-0.6B model&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;h1&gt;Limitations&lt;/h1&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Currently specific to Apple Silicon and this exact model configuration&lt;/li&gt;\\n&lt;li&gt;Performance improvements are highly workload-dependent&lt;/li&gt;\\n&lt;li&gt;Takes ~25 evolutionary generations to converge (few hours on M3)&lt;/li&gt;\\n&lt;li&gt;No guarantees it&amp;#39;ll work better for your specific use case&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Technical write-up&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Full details with code diffs and benchmark methodology: &lt;a href=\\"https://huggingface.co/blog/codelion/openevolve-gpu-kernel-discovery\\"&gt;https://huggingface.co/blog/codelion/openevolve-gpu-kernel-discovery&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Curious to hear thoughts from folks who&amp;#39;ve done MLX optimization work, or if anyone wants to try this on different models/configurations. The evolutionary approach seems promising but definitely has room for improvement.&lt;/p&gt;\\n\\n&lt;p&gt;Has anyone else experimented with automated kernel optimization for local inference?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/QL1cai36O6GA_8oWnC5FZk8axBPFbQvVFTkZtsdqnL8.png?auto=webp&amp;s=15507f764af82777148bfae53f7d8f3683c8acb8","width":1200,"height":600},"resolutions":[{"url":"https://external-preview.redd.it/QL1cai36O6GA_8oWnC5FZk8axBPFbQvVFTkZtsdqnL8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=12e6ec993ab25f895d2e736f6d119b26e8ee29d6","width":108,"height":54},{"url":"https://external-preview.redd.it/QL1cai36O6GA_8oWnC5FZk8axBPFbQvVFTkZtsdqnL8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=58a169cb264f9a17e8629c8f571b16f3b3fecf28","width":216,"height":108},{"url":"https://external-preview.redd.it/QL1cai36O6GA_8oWnC5FZk8axBPFbQvVFTkZtsdqnL8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a8b8a51b859ef5ceb7b137e9a2ee876421291229","width":320,"height":160},{"url":"https://external-preview.redd.it/QL1cai36O6GA_8oWnC5FZk8axBPFbQvVFTkZtsdqnL8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d286a39829746db3b240e11b8b92286bbf8dd5f7","width":640,"height":320},{"url":"https://external-preview.redd.it/QL1cai36O6GA_8oWnC5FZk8axBPFbQvVFTkZtsdqnL8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c0ed0561f1426198e2836ece693d0edfc3945ec3","width":960,"height":480},{"url":"https://external-preview.redd.it/QL1cai36O6GA_8oWnC5FZk8axBPFbQvVFTkZtsdqnL8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8ba93e9744a8287c3b561a276b075dfeee9180d1","width":1080,"height":540}],"variants":{},"id":"QL1cai36O6GA_8oWnC5FZk8axBPFbQvVFTkZtsdqnL8"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":"Llama 3.1","treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lm98z7","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"asankhs","discussion_type":null,"num_comments":14,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":"light","permalink":"/r/LocalLLaMA/comments/1lm98z7/automated_gpu_kernel_optimization_for_qwen3/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lm98z7/automated_gpu_kernel_optimization_for_qwen3/","subreddit_subscribers":492929,"created_utc":1751069414,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"7d1f04e6-4920-11ef-b2e1-2e580594e1a1","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n05v2h6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"asankhs","can_mod_post":false,"created_utc":1751070736,"send_replies":true,"parent_id":"t1_n05tauf","score":8,"author_fullname":"t2_e0bph","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I ran experiments on 0.6 due to ease of speed in testing. The evolved kernel itself does work with bigger qwen3 models and some of the optimisations would carry to any model using GQA.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n05v2h6","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 3.1"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I ran experiments on 0.6 due to ease of speed in testing. The evolved kernel itself does work with bigger qwen3 models and some of the optimisations would carry to any model using GQA.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm98z7","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm98z7/automated_gpu_kernel_optimization_for_qwen3/n05v2h6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751070736,"author_flair_text":"Llama 3.1","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#93b1ba","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}}],"before":null}},"user_reports":[],"saved":false,"id":"n05tauf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"SomeOddCodeGuy","can_mod_post":false,"created_utc":1751070085,"send_replies":true,"parent_id":"t3_1lm98z7","score":24,"author_fullname":"t2_kle75fbd6","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This is fantastic. Even if some scenarios regress, having someone out there tinkering with possible ways to further speed up decoding gets me excited; I honestly thought we'd hit the limit of what kind of speed we'd see on the Mac side by way of prompt processing, so just knowing you're out there doing this makes me really happy.\\n\\nYou mention specifically the requirements being the 0.6b; is that just to repeat your results and it could theoretically work on the larger models, or is it very specific to the 0.6b atm?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n05tauf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is fantastic. Even if some scenarios regress, having someone out there tinkering with possible ways to further speed up decoding gets me excited; I honestly thought we&amp;#39;d hit the limit of what kind of speed we&amp;#39;d see on the Mac side by way of prompt processing, so just knowing you&amp;#39;re out there doing this makes me really happy.&lt;/p&gt;\\n\\n&lt;p&gt;You mention specifically the requirements being the 0.6b; is that just to repeat your results and it could theoretically work on the larger models, or is it very specific to the 0.6b atm?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm98z7/automated_gpu_kernel_optimization_for_qwen3/n05tauf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751070085,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lm98z7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":24}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n064lv3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"minnsoup","can_mod_post":false,"created_utc":1751074289,"send_replies":true,"parent_id":"t3_1lm98z7","score":8,"author_fullname":"t2_ba9bm","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Love this. Been using the OpenEvolve playing around. Love this example as it might be able to adapt to some methodology I'm interested in. Thank you for working on this.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n064lv3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Love this. Been using the OpenEvolve playing around. Love this example as it might be able to adapt to some methodology I&amp;#39;m interested in. Thank you for working on this.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm98z7/automated_gpu_kernel_optimization_for_qwen3/n064lv3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751074289,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lm98z7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n08gvm9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Accomplished_Mode170","can_mod_post":false,"created_utc":1751115704,"send_replies":true,"parent_id":"t3_1lm98z7","score":4,"author_fullname":"t2_4hfmiefj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Any interest in using openevolve et al. for [sparse attention mechanisms](https://www.tilderesearch.com/blog/sparse-attn)? 📊 \\n\\nFigure we’ll eventually see those gains everywhere; just trying to self-pace also 🏃","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n08gvm9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Any interest in using openevolve et al. for &lt;a href=\\"https://www.tilderesearch.com/blog/sparse-attn\\"&gt;sparse attention mechanisms&lt;/a&gt;? 📊 &lt;/p&gt;\\n\\n&lt;p&gt;Figure we’ll eventually see those gains everywhere; just trying to self-pace also 🏃&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm98z7/automated_gpu_kernel_optimization_for_qwen3/n08gvm9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751115704,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lm98z7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n064sfu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Worth_Contract7903","can_mod_post":false,"created_utc":1751074359,"send_replies":true,"parent_id":"t3_1lm98z7","score":2,"author_fullname":"t2_939owsox","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This is awesome, I enjoyed learning about it!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n064sfu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is awesome, I enjoyed learning about it!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm98z7/automated_gpu_kernel_optimization_for_qwen3/n064sfu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751074359,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lm98z7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n066oy1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DumaDuma","can_mod_post":false,"created_utc":1751075087,"send_replies":true,"parent_id":"t3_1lm98z7","score":2,"author_fullname":"t2_7k8dr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thank you for the write-up! This is very inspiring","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n066oy1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you for the write-up! This is very inspiring&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm98z7/automated_gpu_kernel_optimization_for_qwen3/n066oy1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751075087,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lm98z7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"7d1f04e6-4920-11ef-b2e1-2e580594e1a1","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"7d1f04e6-4920-11ef-b2e1-2e580594e1a1","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0aey48","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jazir5","can_mod_post":false,"send_replies":true,"parent_id":"t1_n09eg0o","score":2,"author_fullname":"t2_8u27g","approved_by":null,"mod_note":null,"all_awardings":[],"body":"That would be preferable, at least to have as an option.","edited":1751142696,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0aey48","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That would be preferable, at least to have as an option.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lm98z7","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm98z7/automated_gpu_kernel_optimization_for_qwen3/n0aey48/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751138386,"author_flair_text":null,"treatment_tags":[],"created_utc":1751138386,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n09eg0o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Accomplished_Mode170","can_mod_post":false,"send_replies":true,"parent_id":"t1_n07uh7b","score":2,"author_fullname":"t2_4hfmiefj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Or just keep going with the /v1 endpoints example and spin up another dockerized instance locally (wherever that is); like auto scaling starts with matching sure you have horizontal and vertical binning","edited":false,"author_flair_css_class":null,"name":"t1_n09eg0o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Or just keep going with the /v1 endpoints example and spin up another dockerized instance locally (wherever that is); like auto scaling starts with matching sure you have horizontal and vertical binning&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lm98z7","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm98z7/automated_gpu_kernel_optimization_for_qwen3/n09eg0o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751126980,"author_flair_text":null,"collapsed":false,"created_utc":1751126980,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0asim9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jazir5","can_mod_post":false,"send_replies":true,"parent_id":"t1_n07uh7b","score":1,"author_fullname":"t2_8u27g","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That would very much so help. Preferably it would be great to have another option to do so locally like accomplished mode recommended using that method or something similar.\\n\\nAs it is right now, it looks really esoteric and hard to configure, so a simplified UI would be amazing.","edited":false,"author_flair_css_class":null,"name":"t1_n0asim9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That would very much so help. Preferably it would be great to have another option to do so locally like accomplished mode recommended using that method or something similar.&lt;/p&gt;\\n\\n&lt;p&gt;As it is right now, it looks really esoteric and hard to configure, so a simplified UI would be amazing.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lm98z7","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm98z7/automated_gpu_kernel_optimization_for_qwen3/n0asim9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751142801,"author_flair_text":null,"collapsed":false,"created_utc":1751142801,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n07uh7b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"asankhs","can_mod_post":false,"send_replies":true,"parent_id":"t1_n07u7zn","score":1,"author_fullname":"t2_e0bph","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yeah that is also in plan but would ideally be backed by a cloud service that can run the evolutions at scale easily for the users.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n07uh7b","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 3.1"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah that is also in plan but would ideally be backed by a cloud service that can run the evolutions at scale easily for the users.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm98z7","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm98z7/automated_gpu_kernel_optimization_for_qwen3/n07uh7b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751105032,"author_flair_text":"Llama 3.1","treatment_tags":[],"created_utc":1751105032,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#93b1ba","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n07u7zn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jazir5","can_mod_post":false,"send_replies":true,"parent_id":"t1_n07tygh","score":1,"author_fullname":"t2_8u27g","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I meant more about a way to configure and structure the problems given to OpenEvolve as well. Essentially a full GUI for every aspect.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n07u7zn","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I meant more about a way to configure and structure the problems given to OpenEvolve as well. Essentially a full GUI for every aspect.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm98z7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm98z7/automated_gpu_kernel_optimization_for_qwen3/n07u7zn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751104882,"author_flair_text":null,"treatment_tags":[],"created_utc":1751104882,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n07tygh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"asankhs","can_mod_post":false,"created_utc":1751104728,"send_replies":true,"parent_id":"t1_n07sfu0","score":3,"author_fullname":"t2_e0bph","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"There is a visualizer already in the repo [https://github.com/codelion/openevolve?tab=readme-ov-file#visualizing-the-evolution-tree](https://github.com/codelion/openevolve?tab=readme-ov-file#visualizing-the-evolution-tree) that can help see the programs as they evolve. \\n\\nFor the actual initial program and evaluator and running of evolution, I am actually experimenting if it can all be packaged as an openAi comptatible endpoint so that you an juse use openevolve as a evolutionary test time compute technique and may be also add it to optiLLM.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n07tygh","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 3.1"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There is a visualizer already in the repo &lt;a href=\\"https://github.com/codelion/openevolve?tab=readme-ov-file#visualizing-the-evolution-tree\\"&gt;https://github.com/codelion/openevolve?tab=readme-ov-file#visualizing-the-evolution-tree&lt;/a&gt; that can help see the programs as they evolve. &lt;/p&gt;\\n\\n&lt;p&gt;For the actual initial program and evaluator and running of evolution, I am actually experimenting if it can all be packaged as an openAi comptatible endpoint so that you an juse use openevolve as a evolutionary test time compute technique and may be also add it to optiLLM.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm98z7","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm98z7/automated_gpu_kernel_optimization_for_qwen3/n07tygh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751104728,"author_flair_text":"Llama 3.1","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#93b1ba","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n07sfu0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jazir5","can_mod_post":false,"created_utc":1751103836,"send_replies":true,"parent_id":"t3_1lm98z7","score":2,"author_fullname":"t2_8u27g","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Do you have plans to add a UI for OpenEvolve? Could you?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n07sfu0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Do you have plans to add a UI for OpenEvolve? Could you?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm98z7/automated_gpu_kernel_optimization_for_qwen3/n07sfu0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751103836,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lm98z7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0en7nj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Strange_Test7665","can_mod_post":false,"created_utc":1751204625,"send_replies":true,"parent_id":"t3_1lm98z7","score":2,"author_fullname":"t2_t0zjq9mi","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Haven’t tried out your demo yet but the concept and project are great","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0en7nj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Haven’t tried out your demo yet but the concept and project are great&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm98z7/automated_gpu_kernel_optimization_for_qwen3/n0en7nj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751204625,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lm98z7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),r=()=>e.jsx(t,{data:l});export{r as default};
