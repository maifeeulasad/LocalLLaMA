import{j as e}from"./index-Cd3v0jxz.js";import{R as t}from"./RedditPostRenderer-cI96YLhy.js";import"./index-DGBoKyQm.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I am trying to fine tune a LlaVa model, I have a training set of 7800 high quality conversations, each with an image. \\n\\nI am using qlora to fine tune the model, and regardless of the batch size, the lr, and the rank, so far all of my trials were resulted in gibberish on evaluation. \\n\\nI did some reading, and in order to avoid catastrophic forgetting, it says that we should limit our tuning of the lora model to three epochs max. In addition, I understand that the data size I have is allegedly enough. Together there is something that I am not sure about. The qlora model has about 10m weights (even without bias terms). It looks like much too many to be able to fit on my miniature data. \\n\\nAny tips would be greatly appreciated. ","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"First time using QLoRa results in gibberish","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m58qf3","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.87,"author_flair_background_color":null,"subreddit_type":"public","ups":11,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1t6vmqt87p","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":11,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753069731,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I am trying to fine tune a LlaVa model, I have a training set of 7800 high quality conversations, each with an image. &lt;/p&gt;\\n\\n&lt;p&gt;I am using qlora to fine tune the model, and regardless of the batch size, the lr, and the rank, so far all of my trials were resulted in gibberish on evaluation. &lt;/p&gt;\\n\\n&lt;p&gt;I did some reading, and in order to avoid catastrophic forgetting, it says that we should limit our tuning of the lora model to three epochs max. In addition, I understand that the data size I have is allegedly enough. Together there is something that I am not sure about. The qlora model has about 10m weights (even without bias terms). It looks like much too many to be able to fit on my miniature data. &lt;/p&gt;\\n\\n&lt;p&gt;Any tips would be greatly appreciated. &lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m58qf3","is_robot_indexable":true,"num_duplicates":1,"report_reasons":null,"author":"Emotional-Sundae4075","discussion_type":null,"num_comments":4,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m58qf3/first_time_using_qlora_results_in_gibberish/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m58qf3/first_time_using_qlora_results_in_gibberish/","subreddit_subscribers":502722,"created_utc":1753069731,"num_crossposts":1,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4bv9g2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Emotional-Sundae4075","can_mod_post":false,"created_utc":1753100681,"send_replies":true,"parent_id":"t1_n4ab8rg","score":1,"author_fullname":"t2_1t6vmqt87p","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks for your answer!\\n\\n&gt;\\\\- What's the base model?\\n\\nA LlaVa architecture with Clip as an image processor, Ancuna for the language model\\n\\n&gt;\\\\- What framework are you using? Unsloth? Axolotl? Llama Factory\\n\\nThis one caught me off guard. I am using Python with HF interface, I will look into the names you've mentioned, thanks!\\n\\nMy outputs after one iteration look like:\\n\\n\`neighborhood to the of the highway with access via drive leading toages 13 16 The isized with- green andature, a of homes areed in proxim to other, a of- housing. properties to the areed wither side the, the isized with a of homes areed in proxim to other , a of- housing. neighborhood to the of the highway with access via drive leading toages 13 16 The isized with- green andature, a of homes areed in proxim to other , a of- housing.\`\\n\\nIt looks similar to English, but like sometimes it chooses the wrong next token","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4bv9g2","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for your answer!&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;- What&amp;#39;s the base model?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;A LlaVa architecture with Clip as an image processor, Ancuna for the language model&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;- What framework are you using? Unsloth? Axolotl? Llama Factory&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;This one caught me off guard. I am using Python with HF interface, I will look into the names you&amp;#39;ve mentioned, thanks!&lt;/p&gt;\\n\\n&lt;p&gt;My outputs after one iteration look like:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;neighborhood to the of the highway with access via drive leading toages 13 16 The isized with- green andature, a of homes areed in proxim to other, a of- housing. properties to the areed wither side the, the isized with a of homes areed in proxim to other , a of- housing. neighborhood to the of the highway with access via drive leading toages 13 16 The isized with- green andature, a of homes areed in proxim to other , a of- housing.&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;It looks similar to English, but like sometimes it chooses the wrong next token&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58qf3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58qf3/first_time_using_qlora_results_in_gibberish/n4bv9g2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753100681,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4ab8rg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"random-tomato","can_mod_post":false,"created_utc":1753071861,"send_replies":true,"parent_id":"t3_1m58qf3","score":2,"author_fullname":"t2_fmd6oq5v6","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"First a few questions to help us:\\n\\n\\\\- What's the base model?  \\n\\\\- What framework are you using? Unsloth? Axolotl? Llama Factory  \\n\\\\- What specifically are you getting as output? Can you share an example?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ab8rg","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;First a few questions to help us:&lt;/p&gt;\\n\\n&lt;p&gt;- What&amp;#39;s the base model?&lt;br/&gt;\\n- What framework are you using? Unsloth? Axolotl? Llama Factory&lt;br/&gt;\\n- What specifically are you getting as output? Can you share an example?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58qf3/first_time_using_qlora_results_in_gibberish/n4ab8rg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753071861,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m58qf3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4bues1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Emotional-Sundae4075","can_mod_post":false,"created_utc":1753100355,"send_replies":true,"parent_id":"t1_n4amcnw","score":2,"author_fullname":"t2_1t6vmqt87p","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Haha, nope, actually :)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4bues1","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Haha, nope, actually :)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58qf3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58qf3/first_time_using_qlora_results_in_gibberish/n4bues1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753100355,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4amcnw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Comrade_Vodkin","can_mod_post":false,"created_utc":1753077319,"send_replies":true,"parent_id":"t3_1m58qf3","score":1,"author_fullname":"t2_wa8ul","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"\\\\&gt;high quality conversations, each with an image\\n\\nIt's 4chan, isn't it?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4amcnw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&amp;gt;high quality conversations, each with an image&lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s 4chan, isn&amp;#39;t it?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58qf3/first_time_using_qlora_results_in_gibberish/n4amcnw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753077319,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m58qf3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(t,{data:a});export{n as default};
