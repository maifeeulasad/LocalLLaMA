import{j as e}from"./index-BOnf-UhU.js";import{R as l}from"./RedditPostRenderer-Ce39qICS.js";import"./index-CDase6VD.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I'm a layperson when it comes to large language models. Just like learning about them and think local models are fascinating.\\n\\nI want to take the 2018 International Building Code (pdf or other text file) and create a focused AI model to converse with. The input would be something like\\" give me a building code analysis for this floor plan I just put in the chat.\\n\\n  \\nIf one wants to just limit a LLM to one specific document, and get really focused, accurate data, is that reasonable/possible? Either with cloud models or with local models really.\\n\\nOr, will I actually just get better input with a good prompt on Chatgpt?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"For a very specific text knowledge resource, can a local model outperform cloud models?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m461jh","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":1,"author_flair_background_color":null,"subreddit_type":"public","ups":2,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_p8xs","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":2,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752956506,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m a layperson when it comes to large language models. Just like learning about them and think local models are fascinating.&lt;/p&gt;\\n\\n&lt;p&gt;I want to take the 2018 International Building Code (pdf or other text file) and create a focused AI model to converse with. The input would be something like&amp;quot; give me a building code analysis for this floor plan I just put in the chat.&lt;/p&gt;\\n\\n&lt;p&gt;If one wants to just limit a LLM to one specific document, and get really focused, accurate data, is that reasonable/possible? Either with cloud models or with local models really.&lt;/p&gt;\\n\\n&lt;p&gt;Or, will I actually just get better input with a good prompt on Chatgpt?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m461jh","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"loac","discussion_type":null,"num_comments":2,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m461jh/for_a_very_specific_text_knowledge_resource_can_a/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m461jh/for_a_very_specific_text_knowledge_resource_can_a/","subreddit_subscribers":502030,"created_utc":1752956506,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n423ejk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fp4guru","can_mod_post":false,"created_utc":1752958594,"send_replies":true,"parent_id":"t3_1m461jh","score":3,"author_fullname":"t2_1tp8zldw5g","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If your input is properly structured within reasonable context, the difference won't be too much.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n423ejk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If your input is properly structured within reasonable context, the difference won&amp;#39;t be too much.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m461jh/for_a_very_specific_text_knowledge_resource_can_a/n423ejk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752958594,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m461jh","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n44wrqw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ClearApartment2627","can_mod_post":false,"created_utc":1753001131,"send_replies":true,"parent_id":"t3_1m461jh","score":1,"author_fullname":"t2_1p0o7y7278","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You do not need much for fine tuning, but a single document? I guess that won't bei enough, especially since for your use case you will need a combined vision-language model. It would take many planning documents with written descriptions to make the llm \\"understand\\" what it sees. If you are lucky, the Open weights multi modal Gemma/Llama/Qwen2.5 Models have been trained one this. Otherwise, you would bei better off with O3/Gemini Pro etc.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44wrqw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You do not need much for fine tuning, but a single document? I guess that won&amp;#39;t bei enough, especially since for your use case you will need a combined vision-language model. It would take many planning documents with written descriptions to make the llm &amp;quot;understand&amp;quot; what it sees. If you are lucky, the Open weights multi modal Gemma/Llama/Qwen2.5 Models have been trained one this. Otherwise, you would bei better off with O3/Gemini Pro etc.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m461jh/for_a_very_specific_text_knowledge_resource_can_a/n44wrqw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753001131,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m461jh","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(l,{data:t});export{r as default};
