import{j as e}from"./index-BOnf-UhU.js";import{R as a}from"./RedditPostRenderer-Ce39qICS.js";import"./index-CDase6VD.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"So I’ve been diving into alternative architectures to transformers recently, and I came across a few interesting ones. liquid foundation models (lfm), Mamba (ssm based) and RWKV. I’m curious about what these new architectures offer and what their limitations are. From what I understand, they all seem to be better at handling long sequences, SSMs and LFMs are more resource efficient and LFMs seem to struggle with wide area applications (?)\\nI’m still trying to fully grasp how these models compare to transformers, so I’d love to hear more about the strengths and weaknesses of these newer architectures. Any insights would be appreciated!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"What do new architectures offer and what are their limits?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m76df6","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.89,"author_flair_background_color":null,"subreddit_type":"public","ups":7,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_y1vyie97k","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":7,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753269030,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;So I’ve been diving into alternative architectures to transformers recently, and I came across a few interesting ones. liquid foundation models (lfm), Mamba (ssm based) and RWKV. I’m curious about what these new architectures offer and what their limitations are. From what I understand, they all seem to be better at handling long sequences, SSMs and LFMs are more resource efficient and LFMs seem to struggle with wide area applications (?)\\nI’m still trying to fully grasp how these models compare to transformers, so I’d love to hear more about the strengths and weaknesses of these newer architectures. Any insights would be appreciated!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1m76df6","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"ba2sYd","discussion_type":null,"num_comments":4,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m76df6/what_do_new_architectures_offer_and_what_are/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m76df6/what_do_new_architectures_offer_and_what_are/","subreddit_subscribers":503518,"created_utc":1753269030,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4q3p2d","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ba2sYd","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4pg7zp","score":1,"author_fullname":"t2_y1vyie97k","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I can't run it either, I just asked if you know something about it's architecture. though as I know it is not really good.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4q3p2d","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I can&amp;#39;t run it either, I just asked if you know something about it&amp;#39;s architecture. though as I know it is not really good.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m76df6","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m76df6/what_do_new_architectures_offer_and_what_are/n4q3p2d/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753283101,"author_flair_text":null,"treatment_tags":[],"created_utc":1753283101,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4pg7zp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Maykey","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4p9reo","score":1,"author_fullname":"t2_17tuu7pv","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"They also have some extra props like not depending on position encoding. Which leads to easily using bigger context than it was trained on. Transformers also support NoPE but it's not popular\\n\\nJamba is too big for my 16GB vram.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4pg7zp","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They also have some extra props like not depending on position encoding. Which leads to easily using bigger context than it was trained on. Transformers also support NoPE but it&amp;#39;s not popular&lt;/p&gt;\\n\\n&lt;p&gt;Jamba is too big for my 16GB vram.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m76df6","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m76df6/what_do_new_architectures_offer_and_what_are/n4pg7zp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753276085,"author_flair_text":null,"treatment_tags":[],"created_utc":1753276085,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4p9reo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ba2sYd","can_mod_post":false,"created_utc":1753273903,"send_replies":true,"parent_id":"t1_n4p751v","score":2,"author_fullname":"t2_y1vyie97k","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"so only thing mamba and rwkv offer is O(N) inference time? And do you know anything about the jamba (combination of both mamba and transformers)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4p9reo","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;so only thing mamba and rwkv offer is O(N) inference time? And do you know anything about the jamba (combination of both mamba and transformers)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m76df6","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m76df6/what_do_new_architectures_offer_and_what_are/n4p9reo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753273903,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4p751v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Maykey","can_mod_post":false,"created_utc":1753272952,"send_replies":true,"parent_id":"t3_1m76df6","score":3,"author_fullname":"t2_17tuu7pv","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Don't know anything about LFM(And after playing with liquid model on lambda chat, don't particularly want to know), but Mamba and RWKV need O(N) time time inference N tokens, which is very good for long prompts.\\n\\nProblems, is there since there is no access to previous tokens. All history is stored into fixed-size state so it's easy for mamba to lose history. It's fun  to feed pixels-based info instead, for example many moons ago I tried to see what will happen if you try to rotate an image using very small  mamba2 + conv2d, it'll turn into [this](https://imgur.com/a/TkkG8fE)  which was quite funny\\n\\nAlso mamba is very sensitive to precision.  Transformers can easily be quantized and still will be okayish. Mamba will lose its shit.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4p751v","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Don&amp;#39;t know anything about LFM(And after playing with liquid model on lambda chat, don&amp;#39;t particularly want to know), but Mamba and RWKV need O(N) time time inference N tokens, which is very good for long prompts.&lt;/p&gt;\\n\\n&lt;p&gt;Problems, is there since there is no access to previous tokens. All history is stored into fixed-size state so it&amp;#39;s easy for mamba to lose history. It&amp;#39;s fun  to feed pixels-based info instead, for example many moons ago I tried to see what will happen if you try to rotate an image using very small  mamba2 + conv2d, it&amp;#39;ll turn into &lt;a href=\\"https://imgur.com/a/TkkG8fE\\"&gt;this&lt;/a&gt;  which was quite funny&lt;/p&gt;\\n\\n&lt;p&gt;Also mamba is very sensitive to precision.  Transformers can easily be quantized and still will be okayish. Mamba will lose its shit.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m76df6/what_do_new_architectures_offer_and_what_are/n4p751v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753272952,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m76df6","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}}]`),s=()=>e.jsx(a,{data:t});export{s as default};
