import{j as e}from"./index-CWmJdUH_.js";import{R as t}from"./RedditPostRenderer-D2iunoQ9.js";import"./index-BCg9RP6g.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Inspired by the brain's hierarchical processing, HRM unlocks unprecedented reasoning capabilities on complex tasks like ARC-AGI and solving master-level Sudoku using just 1k training examples, without any pretraining or CoT.\\n\\nThough not a general language model yet, with significant computational depth, HRM possibly unlocks next-gen reasoning and long-horizon planning paradigm beyond CoT. ðŸŒŸ\\n\\nhttps://preview.redd.it/uslhwa2nh8ef1.png?width=2026&amp;format=png&amp;auto=webp&amp;s=b7572924d3c565f89605da339cda1df0dc96354f\\n\\nðŸ“„Paper: [https://arxiv.org/abs/2506.21734](https://arxiv.org/abs/2506.21734)\\n\\nðŸ’»Code: [https://github.com/sapientinc/HRM](https://github.com/sapientinc/HRM)  \\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"[New Architecture] Hierarchical Reasoning Model","link_flair_richtext":[{"e":"text","t":"New Model"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":34,"top_awarded_type":null,"hide_score":false,"media_metadata":{"uslhwa2nh8ef1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":26,"x":108,"u":"https://preview.redd.it/uslhwa2nh8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5ab6ae97e133c2649b92b0c8c0ef857c3d79bcc2"},{"y":53,"x":216,"u":"https://preview.redd.it/uslhwa2nh8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0bc3c540c30cdf2b3b12433e760b60c3ceef936c"},{"y":79,"x":320,"u":"https://preview.redd.it/uslhwa2nh8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=056aa9e724c8d47937b0e044af74a221fc2a9886"},{"y":158,"x":640,"u":"https://preview.redd.it/uslhwa2nh8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7a710e5fa6141d56c1a472f9ac1de521e98a86a5"},{"y":237,"x":960,"u":"https://preview.redd.it/uslhwa2nh8ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7ccd9b2f1992c3c691f918fd9309433d1e043fe2"},{"y":267,"x":1080,"u":"https://preview.redd.it/uslhwa2nh8ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bc4733d2af3bd39a37a4c49868c5e8afeca4ff94"}],"s":{"y":501,"x":2026,"u":"https://preview.redd.it/uslhwa2nh8ef1.png?width=2026&amp;format=png&amp;auto=webp&amp;s=b7572924d3c565f89605da339cda1df0dc96354f"},"id":"uslhwa2nh8ef1"}},"name":"t3_1m5jr1v","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.98,"author_flair_background_color":null,"subreddit_type":"public","ups":83,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_dkj51uv0","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"New Model","can_mod_post":false,"score":83,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://a.thumbs.redditmedia.com/jTPeTr-ZhJfvZ_xmMKlbONUjqH188dSuwhEIvPibXE8.jpg","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753106572,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Inspired by the brain&amp;#39;s hierarchical processing, HRM unlocks unprecedented reasoning capabilities on complex tasks like ARC-AGI and solving master-level Sudoku using just 1k training examples, without any pretraining or CoT.&lt;/p&gt;\\n\\n&lt;p&gt;Though not a general language model yet, with significant computational depth, HRM possibly unlocks next-gen reasoning and long-horizon planning paradigm beyond CoT. ðŸŒŸ&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/uslhwa2nh8ef1.png?width=2026&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b7572924d3c565f89605da339cda1df0dc96354f\\"&gt;https://preview.redd.it/uslhwa2nh8ef1.png?width=2026&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b7572924d3c565f89605da339cda1df0dc96354f&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;ðŸ“„Paper: &lt;a href=\\"https://arxiv.org/abs/2506.21734\\"&gt;https://arxiv.org/abs/2506.21734&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;ðŸ’»Code: &lt;a href=\\"https://github.com/sapientinc/HRM\\"&gt;https://github.com/sapientinc/HRM&lt;/a&gt;  &lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ced98442-f5d3-11ed-b657-66d3b15490c6","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#ffb000","id":"1m5jr1v","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"imonenext","discussion_type":null,"num_comments":8,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m5jr1v/new_architecture_hierarchical_reasoning_model/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m5jr1v/new_architecture_hierarchical_reasoning_model/","subreddit_subscribers":502981,"created_utc":1753106572,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4g3ypk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"rickyhatespeas","can_mod_post":false,"created_utc":1753147115,"send_replies":true,"parent_id":"t1_n4d2dpf","score":6,"author_fullname":"t2_bfw4k","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"To be fair, part of the trick is that reasoning might happen at crunch time between weights that is not explicitly what is stated to be in the thinking tokens. This is already known and I'd be surprised if many leaders at AI companies weren't aware.\\n\\nhttps://arxiv.org/html/2504.09762v2","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4g3ypk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;To be fair, part of the trick is that reasoning might happen at crunch time between weights that is not explicitly what is stated to be in the thinking tokens. This is already known and I&amp;#39;d be surprised if many leaders at AI companies weren&amp;#39;t aware.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://arxiv.org/html/2504.09762v2\\"&gt;https://arxiv.org/html/2504.09762v2&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5jr1v","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5jr1v/new_architecture_hierarchical_reasoning_model/n4g3ypk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753147115,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ilg94","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"CommunityTough1","can_mod_post":false,"created_utc":1753188390,"send_replies":true,"parent_id":"t1_n4d2dpf","score":4,"author_fullname":"t2_1iuzpxw7eg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Anthropic did a study on this, and what the models output in the \\"thinking\\" text is not what they're actually thinking. It's really just instructions to \\"show your work while reasoning\\" anyways. The actual reasoning that's happening inside the 'black box' often isn't well represented in the thinking output.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ilg94","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Anthropic did a study on this, and what the models output in the &amp;quot;thinking&amp;quot; text is not what they&amp;#39;re actually thinking. It&amp;#39;s really just instructions to &amp;quot;show your work while reasoning&amp;quot; anyways. The actual reasoning that&amp;#39;s happening inside the &amp;#39;black box&amp;#39; often isn&amp;#39;t well represented in the thinking output.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5jr1v","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5jr1v/new_architecture_hierarchical_reasoning_model/n4ilg94/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753188390,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n4d2dpf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"SignalCompetitive582","can_mod_post":false,"created_utc":1753114133,"send_replies":true,"parent_id":"t3_1m5jr1v","score":28,"author_fullname":"t2_cyec8jul","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thatâ€™s what Iâ€™ve been saying forever, models that â€œreasonâ€ with words is not the way to goâ€¦\\n\\nâ€œTowards this goal, we explore â€œlatent reasoningâ€, where the model conducts computations within\\nits internal hidden state space. This aligns with the understanding that language is a tool for\\nhuman communication, not the substrate of thought itself; the brain sustains lengthy, coherent\\nchains of reasoning with remarkable efficiency in a latent space, without constant translation back\\nto language.â€","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4d2dpf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thatâ€™s what Iâ€™ve been saying forever, models that â€œreasonâ€ with words is not the way to goâ€¦&lt;/p&gt;\\n\\n&lt;p&gt;â€œTowards this goal, we explore â€œlatent reasoningâ€, where the model conducts computations within\\nits internal hidden state space. This aligns with the understanding that language is a tool for\\nhuman communication, not the substrate of thought itself; the brain sustains lengthy, coherent\\nchains of reasoning with remarkable efficiency in a latent space, without constant translation back\\nto language.â€&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5jr1v/new_architecture_hierarchical_reasoning_model/n4d2dpf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753114133,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5jr1v","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":28}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4fw4ag","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"and-nothing-hurt","can_mod_post":false,"created_utc":1753144387,"send_replies":true,"parent_id":"t1_n4e38u1","score":7,"author_fullname":"t2_i5wwbp0z1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes, a lot of good architectures come out in an initial paper, only to never be heard of again - assuming because they didn't scale!\\n\\nOne thing I don't understand here is that the authors claim that the quadratic memory and time of the standard transformer attention mechanism is somehow a negative aspect of attention, while using a recurrent system is better because it processes \\"input tokens sequentially...predicting the next token at each time step\\" (Discussions section - Linear Attention header).\\n\\nI thought the whole point of attention is that it allows you to process tokens in parallel, as in that was a *design feature*, not a bug. The parallel token processing in standard attention allows for things like processing an entire prompt in one run through the network when generating the first response token, which is able to scale well with increasing prompt size. And when prompts contain entire documents/codebases to be searched, this parallel processing starts to matter, where sequential processing would be expected to be much slower.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4fw4ag","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, a lot of good architectures come out in an initial paper, only to never be heard of again - assuming because they didn&amp;#39;t scale!&lt;/p&gt;\\n\\n&lt;p&gt;One thing I don&amp;#39;t understand here is that the authors claim that the quadratic memory and time of the standard transformer attention mechanism is somehow a negative aspect of attention, while using a recurrent system is better because it processes &amp;quot;input tokens sequentially...predicting the next token at each time step&amp;quot; (Discussions section - Linear Attention header).&lt;/p&gt;\\n\\n&lt;p&gt;I thought the whole point of attention is that it allows you to process tokens in parallel, as in that was a &lt;em&gt;design feature&lt;/em&gt;, not a bug. The parallel token processing in standard attention allows for things like processing an entire prompt in one run through the network when generating the first response token, which is able to scale well with increasing prompt size. And when prompts contain entire documents/codebases to be searched, this parallel processing starts to matter, where sequential processing would be expected to be much slower.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5jr1v","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5jr1v/new_architecture_hierarchical_reasoning_model/n4fw4ag/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753144387,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}}],"before":null}},"user_reports":[],"saved":false,"id":"n4e38u1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"oderi","can_mod_post":false,"created_utc":1753124363,"send_replies":true,"parent_id":"t3_1m5jr1v","score":7,"author_fullname":"t2_8jccd","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Seems quite an elegant architecture. How much they've seemingly been able optimise memory use with the DEQ adjacent shenanigans makes me wonder if the fact they've not talked about their training process in terms of hardware means it really is as computationally efficient as it seems. This in turn raises the question or prospect of e.g. having an agentic system roll custom HRMs for specific problems. Would of course always need a sufficient dataset.\\n\\n\\nWhat's also fun to see is the neuro angle - haven't seen the concept of participation ratio since 2018 and back then we called it dimension after Litwin-Kumar et al.\\n\\n\\nEDIT: Will be interesting to see how it scales, and in particular whether there's any scaling to be had with further layers of hierarchy. I'm not smart enough to tell how that would affect the maths in terms of computational efficiency.","edited":1753124881,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4e38u1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Seems quite an elegant architecture. How much they&amp;#39;ve seemingly been able optimise memory use with the DEQ adjacent shenanigans makes me wonder if the fact they&amp;#39;ve not talked about their training process in terms of hardware means it really is as computationally efficient as it seems. This in turn raises the question or prospect of e.g. having an agentic system roll custom HRMs for specific problems. Would of course always need a sufficient dataset.&lt;/p&gt;\\n\\n&lt;p&gt;What&amp;#39;s also fun to see is the neuro angle - haven&amp;#39;t seen the concept of participation ratio since 2018 and back then we called it dimension after Litwin-Kumar et al.&lt;/p&gt;\\n\\n&lt;p&gt;EDIT: Will be interesting to see how it scales, and in particular whether there&amp;#39;s any scaling to be had with further layers of hierarchy. I&amp;#39;m not smart enough to tell how that would affect the maths in terms of computational efficiency.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5jr1v/new_architecture_hierarchical_reasoning_model/n4e38u1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753124363,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5jr1v","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4dximp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Formal_Drop526","can_mod_post":false,"created_utc":1753122743,"send_replies":true,"parent_id":"t3_1m5jr1v","score":3,"author_fullname":"t2_dtsa6gxt","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It's an RNN model, does this architecture work on state-space? or energy-based transformers or whatever?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4dximp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s an RNN model, does this architecture work on state-space? or energy-based transformers or whatever?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5jr1v/new_architecture_hierarchical_reasoning_model/n4dximp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753122743,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5jr1v","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4dxvc8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ninjasaid13","can_mod_post":false,"created_utc":1753122843,"send_replies":true,"parent_id":"t3_1m5jr1v","score":3,"author_fullname":"t2_qjpsv","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"only 27 million parameters? I wonder how it does on ARC-AGI 3.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4dxvc8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;only 27 million parameters? I wonder how it does on ARC-AGI 3.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5jr1v/new_architecture_hierarchical_reasoning_model/n4dxvc8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753122843,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5jr1v","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4fsbs0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"terminoid_","can_mod_post":false,"created_utc":1753143065,"send_replies":true,"parent_id":"t3_1m5jr1v","score":1,"author_fullname":"t2_1iu07dnz2i","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"okay, now i'm interested in reasoning models","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4fsbs0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;okay, now i&amp;#39;m interested in reasoning models&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5jr1v/new_architecture_hierarchical_reasoning_model/n4fsbs0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753143065,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5jr1v","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),s=()=>e.jsx(t,{data:a});export{s as default};
