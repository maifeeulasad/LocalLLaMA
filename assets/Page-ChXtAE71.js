import{j as e}from"./index-F0NXdzZX.js";import{R as l}from"./RedditPostRenderer-CoEZB1dt.js";import"./index-DrtravXm.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Looking forward to the GGUF quants to give it a shot. Would love if the awesome Unsloth team did their magic here, too.\\n\\n  \\n[https://huggingface.co/THUDM/GLM-4.1V-9B-Thinking](https://huggingface.co/THUDM/GLM-4.1V-9B-Thinking)","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"THUDM/GLM-4.1V-9B-Thinking looks impressive","link_flair_richtext":[{"e":"text","t":"New Model"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":60,"top_awarded_type":null,"hide_score":false,"name":"t3_1lrss4u","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.87,"author_flair_background_color":null,"ups":119,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_jip8rft4d","secure_media":null,"is_reddit_media_domain":true,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"New Model","can_mod_post":false,"score":119,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://b.thumbs.redditmedia.com/idh4Gmb5EiyF75my7tifZeUzHzbypwJivniL4vcgyZw.jpg","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"image","content_categories":null,"is_self":false,"subreddit_type":"public","created":1751661469,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"i.redd.it","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Looking forward to the GGUF quants to give it a shot. Would love if the awesome Unsloth team did their magic here, too.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/THUDM/GLM-4.1V-9B-Thinking\\"&gt;https://huggingface.co/THUDM/GLM-4.1V-9B-Thinking&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"url_overridden_by_dest":"https://i.redd.it/62vkwepq4xaf1.jpeg","view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://preview.redd.it/62vkwepq4xaf1.jpeg?auto=webp&amp;s=47b4a0ac5b3ff7c55707c305f18aa5050819fad3","width":1880,"height":817},"resolutions":[{"url":"https://preview.redd.it/62vkwepq4xaf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e48334db0b608a0149e13ef26625c23ab6d950f5","width":108,"height":46},{"url":"https://preview.redd.it/62vkwepq4xaf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7a0adffe54dca86e2f243f886bd70bfb4adeec7e","width":216,"height":93},{"url":"https://preview.redd.it/62vkwepq4xaf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7a3cf3c7b47dec2fa728dffaaca1e20fbe7fd48b","width":320,"height":139},{"url":"https://preview.redd.it/62vkwepq4xaf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ded30c7e3562f37aa41502883d7aa8b656c68551","width":640,"height":278},{"url":"https://preview.redd.it/62vkwepq4xaf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4bba18cd703124b505c4f466cea3f48dab329314","width":960,"height":417},{"url":"https://preview.redd.it/62vkwepq4xaf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f949903dcbbd4917c219dad41bf2c369c7c664bf","width":1080,"height":469}],"variants":{},"id":"xwCchr8Lt12drvPmOfSw-f2wfP9Iz1_9fFJuW2C6gvw"}],"enabled":true},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ced98442-f5d3-11ed-b657-66d3b15490c6","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"mod_note":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"num_reports":null,"removal_reason":null,"link_flair_background_color":"#ffb000","id":"1lrss4u","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"ConfidentTrifle7247","discussion_type":null,"num_comments":39,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/","stickied":false,"url":"https://i.redd.it/62vkwepq4xaf1.jpeg","subreddit_subscribers":494987,"created_utc":1751661469,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"609bf7d4-01f3-11f0-9760-5611c8333bee","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1dbmjx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"You_Wen_AzzHu","can_mod_post":false,"created_utc":1751662683,"send_replies":true,"parent_id":"t3_1lrss4u","score":60,"author_fullname":"t2_p4oxcufl","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Please at least give it a spin before posting like this.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1dbmjx","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"exllama"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Please at least give it a spin before posting like this.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/n1dbmjx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751662683,"author_flair_text":"exllama","treatment_tags":[],"link_id":"t3_1lrss4u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":60}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1etq5a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"JuicedFuck","can_mod_post":false,"created_utc":1751684010,"send_replies":true,"parent_id":"t1_n1d9s7q","score":9,"author_fullname":"t2_11oug7q664","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"VL space has been completely stagnant for &gt;1 year in terms of image understanding. Models now have CoT + VL just so they can solve benchmarks like \\"Solve the equation on the blackboard\\", but the CoT does absolutely nothing to help it understand complicated images which previous models struggled with.\\n\\nOn my private test set, I have seen no improvements made with any vision model except google gemini's 2.5 pro model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1etq5a","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;VL space has been completely stagnant for &amp;gt;1 year in terms of image understanding. Models now have CoT + VL just so they can solve benchmarks like &amp;quot;Solve the equation on the blackboard&amp;quot;, but the CoT does absolutely nothing to help it understand complicated images which previous models struggled with.&lt;/p&gt;\\n\\n&lt;p&gt;On my private test set, I have seen no improvements made with any vision model except google gemini&amp;#39;s 2.5 pro model.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrss4u","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/n1etq5a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751684010,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1e2zvy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"butsicle","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1dg6er","score":5,"author_fullname":"t2_acbu3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Bring some BBQ back for the rest of us please","edited":false,"author_flair_css_class":null,"name":"t1_n1e2zvy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Bring some BBQ back for the rest of us please&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lrss4u","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/n1e2zvy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751672618,"author_flair_text":null,"collapsed":false,"created_utc":1751672618,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1fg1lk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1dg6er","score":1,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"number of hidden layers are different though; 32 in 8b and 40 in 11b. The original layers might as well be frozen, but extra layers are not. And those extra are not \\"vision layers\\", those are normal FFN ones.","edited":false,"author_flair_css_class":null,"name":"t1_n1fg1lk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;number of hidden layers are different though; 32 in 8b and 40 in 11b. The original layers might as well be frozen, but extra layers are not. And those extra are not &amp;quot;vision layers&amp;quot;, those are normal FFN ones.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lrss4u","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/n1fg1lk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751695078,"author_flair_text":null,"collapsed":false,"created_utc":1751695078,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1dg6er","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DepthHour1669","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1deapp","score":12,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"\\nhttps://huggingface.co/blog/llama32\\n\\n&gt; The architecture of these models is based on the combination of Llama 3.1 LLMs combined with a vision tower and an image adapter. The text models used are Llama 3.1 8B for the Llama 3.2 11B Vision model, and Llama 3.1 70B for the 3.2 90B Vision model. To the best of our understanding, the text models were frozen during the training of the vision models to preserve text-only performance.\\n\\nThis seems like a dumb thing to argue about. It’d be very easy to use Captum to look at both models and instantly tell if the text weights were frozen or not. I don’t have time today because I’m about to head out to a BBQ, but you can show proof of your statement if you have time. Otherwise I’ll pull it up tomorrow and compare them.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1dg6er","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://huggingface.co/blog/llama32\\"&gt;https://huggingface.co/blog/llama32&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;The architecture of these models is based on the combination of Llama 3.1 LLMs combined with a vision tower and an image adapter. The text models used are Llama 3.1 8B for the Llama 3.2 11B Vision model, and Llama 3.1 70B for the 3.2 90B Vision model. To the best of our understanding, the text models were frozen during the training of the vision models to preserve text-only performance.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;This seems like a dumb thing to argue about. It’d be very easy to use Captum to look at both models and instantly tell if the text weights were frozen or not. I don’t have time today because I’m about to head out to a BBQ, but you can show proof of your statement if you have time. Otherwise I’ll pull it up tomorrow and compare them.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrss4u","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/n1dg6er/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751664212,"author_flair_text":null,"treatment_tags":[],"created_utc":1751664212,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1frzxj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CheatCodesOfLife","can_mod_post":false,"created_utc":1751702041,"send_replies":true,"parent_id":"t1_n1fr3g9","score":1,"author_fullname":"t2_32el727b","approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; I get downvoted into oblivion\\n\\nProbably why I haven't seen this mentioned before :s","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n1frzxj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;I get downvoted into oblivion&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Probably why I haven&amp;#39;t seen this mentioned before :s&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lrss4u","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/n1frzxj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751702041,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1fr3g9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1fqnwm","score":1,"author_fullname":"t2_uz37qfx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt; Only explanation is you're right, Meta must have used a larger text model for the vision models!\\n\\nI know, every time I bring it up, I get downvoted into oblivion. 3.2 and 3.1 are different models.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n1fr3g9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Only explanation is you&amp;#39;re right, Meta must have used a larger text model for the vision models!&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I know, every time I bring it up, I get downvoted into oblivion. 3.2 and 3.1 are different models.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lrss4u","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/n1fr3g9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751701497,"author_flair_text":null,"treatment_tags":[],"created_utc":1751701497,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1fqnwm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CheatCodesOfLife","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1ffnv2","score":2,"author_fullname":"t2_32el727b","approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; Deepseek v3-0324 and Mistral Small 3.2 work almost exactly, often word-to word same for textgen; check lmarena if you do not believe\\n\\nI believe you, for simple prompts, but if you use the model for real tasks, DS is nothing like MS.\\nI used the 90b image model for about a week in place of the 70b and regenerated the last reply in some of the 70b's chats to test it (this was ages ago)\\n\\n&gt; OTOH in my experiment on build.nvidia.com show that 11b is far more unhinged in the output than 3.1 8b\\n\\nOkay admittedly I haven't used the 11b or 8b so I'll take your word for it.\\n\\n&gt; Anyway here config.json for 3.1 8B:\\n\\nOkay you got me with this one! You're right, 32 layers vs 40 for the 11b, 80 vs 100 for the 90b.\\n\\n&gt; Feel free to explain how a model with 40 layers is same with one with 32 layers\\n\\nOnly explanation is you're right, Meta must have used a larger text model for the vision models!\\n\\nNow I want to strip out the vision weights from the base model and see how it takes to fine tuning...","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n1fqnwm","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Deepseek v3-0324 and Mistral Small 3.2 work almost exactly, often word-to word same for textgen; check lmarena if you do not believe&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I believe you, for simple prompts, but if you use the model for real tasks, DS is nothing like MS.\\nI used the 90b image model for about a week in place of the 70b and regenerated the last reply in some of the 70b&amp;#39;s chats to test it (this was ages ago)&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;OTOH in my experiment on build.nvidia.com show that 11b is far more unhinged in the output than 3.1 8b&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Okay admittedly I haven&amp;#39;t used the 11b or 8b so I&amp;#39;ll take your word for it.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Anyway here config.json for 3.1 8B:&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Okay you got me with this one! You&amp;#39;re right, 32 layers vs 40 for the 11b, 80 vs 100 for the 90b.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Feel free to explain how a model with 40 layers is same with one with 32 layers&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Only explanation is you&amp;#39;re right, Meta must have used a larger text model for the vision models!&lt;/p&gt;\\n\\n&lt;p&gt;Now I want to strip out the vision weights from the base model and see how it takes to fine tuning...&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lrss4u","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/n1fqnwm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751701235,"author_flair_text":null,"treatment_tags":[],"created_utc":1751701235,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1ffnv2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1ehwo8","score":3,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; And it worked exactly like Llama3.3-90b for textgen when I tried it.\\n\\nDeepseek v3-0324 and Mistral Small 3.2 work almost exactly, often word-to word same for textgen; check lmarena if you do not believe; internally they massively different though. OTOH in my experiment on build.nvidia.com show that 11b is far more unhinged in the output than 3.1 8b.\\n\\nAnyway here config.json for 3.1 8B:\\n\\n    \\"num_hidden_layers\\": 32,\\n\\nconfig.json 3.2 11b: \\n\\n    \\"num_hidden_layers\\": 40,\\n\\nFeel free to explain how a model with 40 layers is same with one with 32 layers, and also feel free to test the output on build.nvidia.com with T=0 and other sampler settings set to be same both models with the prompt of your choice.","edited":false,"author_flair_css_class":null,"name":"t1_n1ffnv2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;And it worked exactly like Llama3.3-90b for textgen when I tried it.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Deepseek v3-0324 and Mistral Small 3.2 work almost exactly, often word-to word same for textgen; check lmarena if you do not believe; internally they massively different though. OTOH in my experiment on build.nvidia.com show that 11b is far more unhinged in the output than 3.1 8b.&lt;/p&gt;\\n\\n&lt;p&gt;Anyway here config.json for 3.1 8B:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;&amp;quot;num_hidden_layers&amp;quot;: 32,\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;config.json 3.2 11b: &lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;&amp;quot;num_hidden_layers&amp;quot;: 40,\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;Feel free to explain how a model with 40 layers is same with one with 32 layers, and also feel free to test the output on build.nvidia.com with T=0 and other sampler settings set to be same both models with the prompt of your choice.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lrss4u","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/n1ffnv2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751694868,"author_flair_text":null,"collapsed":false,"created_utc":1751694868,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n1ehwo8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CheatCodesOfLife","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1deapp","score":3,"author_fullname":"t2_32el727b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt; It is not true\\n\\nIt is true. There's even a Llama 3.2-90b with the text layers swapped with the Llama 3.3 70b model [Llama-3.3-90B-Vision-merged](https://huggingface.co/gghfez/Llama-3.3-90B-Vision-merged).\\n\\nAnd it worked exactly like Llama3.3-90b for textgen when I tried it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ehwo8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;It is not true&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;It is true. There&amp;#39;s even a Llama 3.2-90b with the text layers swapped with the Llama 3.3 70b model &lt;a href=\\"https://huggingface.co/gghfez/Llama-3.3-90B-Vision-merged\\"&gt;Llama-3.3-90B-Vision-merged&lt;/a&gt;.&lt;/p&gt;\\n\\n&lt;p&gt;And it worked exactly like Llama3.3-90b for textgen when I tried it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrss4u","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/n1ehwo8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751678808,"author_flair_text":null,"treatment_tags":[],"created_utc":1751678808,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n1deapp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1dcc0i","score":-5,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt;  the Llama 3.1 8b it was built off of\\n\\nIt is not true; it is is widespread misconception but it is incorrect. Visual layers are less 1b in size, textual layer of 3.2 11b is bigger than Llama 3.1 8b","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1deapp","is_submitter":false,"collapsed":true,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;the Llama 3.1 8b it was built off of&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;It is not true; it is is widespread misconception but it is incorrect. Visual layers are less 1b in size, textual layer of 3.2 11b is bigger than Llama 3.1 8b&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrss4u","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/n1deapp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751663569,"author_flair_text":null,"treatment_tags":[],"created_utc":1751663569,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-5}}],"before":null}},"user_reports":[],"saved":false,"id":"n1dcc0i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DepthHour1669","can_mod_post":false,"created_utc":1751662915,"send_replies":true,"parent_id":"t1_n1d9s7q","score":4,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Vision is surprisingly tiny, to be fair. Llama 3.2 11b vision is just 3b more than the Llama 3.1 8b it was built off of.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1dcc0i","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Vision is surprisingly tiny, to be fair. Llama 3.2 11b vision is just 3b more than the Llama 3.1 8b it was built off of.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrss4u","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/n1dcc0i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751662915,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n1d9s7q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"noage","can_mod_post":false,"created_utc":1751662088,"send_replies":true,"parent_id":"t3_1lrss4u","score":11,"author_fullname":"t2_5ao30","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It does seem like the VL landscape has a lot of room for growth. Every time there's a benchmark for a vl model it's like 'here's our tiny model compared to several 72b models. ' don't see that with normal llms.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1d9s7q","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It does seem like the VL landscape has a lot of room for growth. Every time there&amp;#39;s a benchmark for a vl model it&amp;#39;s like &amp;#39;here&amp;#39;s our tiny model compared to several 72b models. &amp;#39; don&amp;#39;t see that with normal llms.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/n1d9s7q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751662088,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrss4u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1ek723","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Kooshi_Govno","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1eiqbh","score":8,"author_fullname":"t2_7kg5p","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah, cus that's the only thing you can do for small models to make them look good. Granted they're benchmaxxing for big models now too. We need some benches just for the LocalLlama community.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1ek723","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, cus that&amp;#39;s the only thing you can do for small models to make them look good. Granted they&amp;#39;re benchmaxxing for big models now too. We need some benches just for the LocalLlama community.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrss4u","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/n1ek723/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751679819,"author_flair_text":null,"treatment_tags":[],"created_utc":1751679819,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}}],"before":null}},"user_reports":[],"saved":false,"id":"n1eiqbh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"rymn","can_mod_post":false,"created_utc":1751679174,"send_replies":true,"parent_id":"t1_n1dajxq","score":13,"author_fullname":"t2_53nkx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Is it just me or does it seem like more of these smaller budge models are just trained for the tests, and not for real world use?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1eiqbh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Is it just me or does it seem like more of these smaller budge models are just trained for the tests, and not for real world use?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrss4u","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/n1eiqbh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751679174,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":13}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1ec778","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Lazy-Pattern-5171","can_mod_post":false,"created_utc":1751676359,"send_replies":true,"parent_id":"t1_n1dajxq","score":11,"author_fullname":"t2_1lyjk8is25","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I got downvoted into oblivion when I said it and now yours in the top comment. SMH 🤦‍♂️","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ec778","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I got downvoted into oblivion when I said it and now yours in the top comment. SMH 🤦‍♂️&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrss4u","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/n1ec778/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751676359,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1en1zn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Commercial-Celery769","can_mod_post":false,"created_utc":1751681060,"send_replies":true,"parent_id":"t1_n1dajxq","score":5,"author_fullname":"t2_zws5yqyow","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It might be a while until we get good small models","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1en1zn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It might be a while until we get good small models&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrss4u","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/n1en1zn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751681060,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1flgqc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Beneficial-Good660","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1fj0ho","score":6,"author_fullname":"t2_7nrq7r2a","approved_by":null,"mod_note":null,"all_awardings":[],"body":"It doesn't get through to you at all — here's a quote from the official card: *\\"designed to explore the upper limits of reasoning in vision-language models\\"*. All tests come from understanding images. Can you even grasp that or not? You can read up on what models are used for — both regular text ones and VL. Study a bit, maybe your stupidity comes precisely from the fact that you don’t know anything. And then, maybe, you’ll start writing slightly smarter comments.\\nIn all vision models, MMLU and other text tasks drop significantly. So when vision integration is added, they’ll still be able to maintain their quality — *that* will be fire. Even with GPT-4o, it's not even sure if it's a single model — probably just OCR attached. And when reasoning comes from images, the coding performance will drop too.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n1flgqc","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It doesn&amp;#39;t get through to you at all — here&amp;#39;s a quote from the official card: &lt;em&gt;&amp;quot;designed to explore the upper limits of reasoning in vision-language models&amp;quot;&lt;/em&gt;. All tests come from understanding images. Can you even grasp that or not? You can read up on what models are used for — both regular text ones and VL. Study a bit, maybe your stupidity comes precisely from the fact that you don’t know anything. And then, maybe, you’ll start writing slightly smarter comments.\\nIn all vision models, MMLU and other text tasks drop significantly. So when vision integration is added, they’ll still be able to maintain their quality — &lt;em&gt;that&lt;/em&gt; will be fire. Even with GPT-4o, it&amp;#39;s not even sure if it&amp;#39;s a single model — probably just OCR attached. And when reasoning comes from images, the coding performance will drop too.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lrss4u","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/n1flgqc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751698166,"author_flair_text":null,"treatment_tags":[],"created_utc":1751698166,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n1fj0ho","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1fi1ut","score":-6,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; The infographic I tested wasn’t these images here — take any infographic from Pinterest for tasks like that. You're extremely stupid and keep spreading lies constantly.\\n\\nMofo what are you talking about? The op linked infographics that shows this model is better than 4o at coding.\\n\\n&gt;  \\"I didn't test the vision\\" — that’s a VL model.\\n\\nSo is GPT-4o they reference infographic. So you are saing this 9b POS  is better coding that 4o. LMAO. I bet you have no fucking idea how to code, and therefore cannot test performance yourself.","edited":false,"author_flair_css_class":null,"name":"t1_n1fj0ho","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;The infographic I tested wasn’t these images here — take any infographic from Pinterest for tasks like that. You&amp;#39;re extremely stupid and keep spreading lies constantly.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Mofo what are you talking about? The op linked infographics that shows this model is better than 4o at coding.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;&amp;quot;I didn&amp;#39;t test the vision&amp;quot; — that’s a VL model.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;So is GPT-4o they reference infographic. So you are saing this 9b POS  is better coding that 4o. LMAO. I bet you have no fucking idea how to code, and therefore cannot test performance yourself.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":"comment score below threshold","link_id":"t3_1lrss4u","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/n1fj0ho/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751696746,"author_flair_text":null,"collapsed":true,"created_utc":1751696746,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-6}}],"before":null}},"user_reports":[],"saved":false,"id":"n1fi1ut","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Beneficial-Good660","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1fgk8z","score":10,"author_fullname":"t2_7nrq7r2a","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"See, you're the complete idiot who writes all sorts of nonsense and doesn’t understand anything. \\"I didn't test the vision\\" — that’s a VL model. The infographic I tested wasn’t these images here — take any infographic from Pinterest for tasks like that. You're extremely stupid and keep spreading lies constantly.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1fi1ut","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;See, you&amp;#39;re the complete idiot who writes all sorts of nonsense and doesn’t understand anything. &amp;quot;I didn&amp;#39;t test the vision&amp;quot; — that’s a VL model. The infographic I tested wasn’t these images here — take any infographic from Pinterest for tasks like that. You&amp;#39;re extremely stupid and keep spreading lies constantly.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrss4u","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/n1fi1ut/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751696201,"author_flair_text":null,"treatment_tags":[],"created_utc":1751696201,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}}],"before":null}},"user_reports":[],"saved":false,"id":"n1fgk8z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1fe1pq","score":-8,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"If you referring as \\"fuckwit\\" to me than look at the mirror, fuckwit. As a vision model it might be or might not be good, I did not test the vision, but, if you, moron look at the linked infographic it shows it as excellent coder, but in fact it is not, it makes trivial errors in the generated code say Qwen 3 8b does not, or even Llama 3.1 8b does not make.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1fgk8z","is_submitter":false,"collapsed":true,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you referring as &amp;quot;fuckwit&amp;quot; to me than look at the mirror, fuckwit. As a vision model it might be or might not be good, I did not test the vision, but, if you, moron look at the linked infographic it shows it as excellent coder, but in fact it is not, it makes trivial errors in the generated code say Qwen 3 8b does not, or even Llama 3.1 8b does not make.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrss4u","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/n1fgk8z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751695363,"author_flair_text":null,"treatment_tags":[],"created_utc":1751695363,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-8}}],"before":null}},"user_reports":[],"saved":false,"id":"n1fe1pq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Beneficial-Good660","can_mod_post":false,"created_utc":1751693984,"send_replies":true,"parent_id":"t1_n1dajxq","score":5,"author_fullname":"t2_7nrq7r2a","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Before taking this fuckwit's words seriously and liking them, you should understand that he doesn't know how LLMs or VL models work — he's testing on \\"creative writing.\\" I tested it on an infographic: the model identified all the words and objects, expanded the meaning, and provided a detailed plan with examples of different tools. It's actually decent and convenient since, in its thinking, it combined everything it found in the image.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1fe1pq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Before taking this fuckwit&amp;#39;s words seriously and liking them, you should understand that he doesn&amp;#39;t know how LLMs or VL models work — he&amp;#39;s testing on &amp;quot;creative writing.&amp;quot; I tested it on an infographic: the model identified all the words and objects, expanded the meaning, and provided a detailed plan with examples of different tools. It&amp;#39;s actually decent and convenient since, in its thinking, it combined everything it found in the image.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrss4u","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/n1fe1pq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751693984,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1g0mxe","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AmazinglyObliviouse","can_mod_post":false,"created_utc":1751707381,"send_replies":true,"parent_id":"t1_n1dajxq","score":1,"author_fullname":"t2_c1qzfso6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"But wait— but wait—but wait—but wait—but wait—but wait—but wait—but wait—but wait—\\n\\n(incorrect answer)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1g0mxe","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;But wait— but wait—but wait—but wait—but wait—but wait—but wait—but wait—but wait—&lt;/p&gt;\\n\\n&lt;p&gt;(incorrect answer)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrss4u","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/n1g0mxe/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751707381,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1dajxq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1751662336,"send_replies":true,"parent_id":"t3_1lrss4u","score":62,"author_fullname":"t2_uz37qfx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Did you try it? I did. It is shit. Utter crap.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1dajxq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Did you try it? I did. It is shit. Utter crap.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/n1dajxq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751662336,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrss4u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":62}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1hyk9x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1gw6wo","score":1,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Not true for Mistral 2503 vs 2501. Also Qwen 2.5 vl 32b was to my taste better than normal qwen 2.5, and Pixtral Large is not worse than Mistral Large at all. I do not think what you said is true.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1hyk9x","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not true for Mistral 2503 vs 2501. Also Qwen 2.5 vl 32b was to my taste better than normal qwen 2.5, and Pixtral Large is not worse than Mistral Large at all. I do not think what you said is true.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrss4u","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/n1hyk9x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751735550,"author_flair_text":null,"treatment_tags":[],"created_utc":1751735550,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1gw6wo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"lompocus","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1fh52e","score":2,"author_fullname":"t2_f4boq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This is true of all vision models compared to their non-vision models in the same family of models.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1gw6wo","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is true of all vision models compared to their non-vision models in the same family of models.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrss4u","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/n1gw6wo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751723045,"author_flair_text":null,"treatment_tags":[],"created_utc":1751723045,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1fh52e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1751695688,"send_replies":true,"parent_id":"t1_n1dtau2","score":0,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It might be good vision model, but it is not a good model in general sense of the word.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1fh52e","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It might be good vision model, but it is not a good model in general sense of the word.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrss4u","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/n1fh52e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751695688,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n1dtau2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"lompocus","can_mod_post":false,"created_utc":1751668956,"send_replies":true,"parent_id":"t3_1lrss4u","score":12,"author_fullname":"t2_f4boq","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Why are people saying it is bad. It is the first vision model that can actually give me good answers.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1dtau2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why are people saying it is bad. It is the first vision model that can actually give me good answers.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/n1dtau2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751668956,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrss4u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1f655i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"llmentry","can_mod_post":false,"created_utc":1751689811,"send_replies":true,"parent_id":"t3_1lrss4u","score":3,"author_fullname":"t2_1lufy6yx6z","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Well, I hope their model didn't produce their misleading charts.  \\n\\n(Inconsistent axis values on the baseline comparison, truncating the Y axis to start at 30 for the RL gains to create a false impression of performance increases ... I would not trust that model for anything STEM-related.)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1f655i","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Well, I hope their model didn&amp;#39;t produce their misleading charts.  &lt;/p&gt;\\n\\n&lt;p&gt;(Inconsistent axis values on the baseline comparison, truncating the Y axis to start at 30 for the RL gains to create a false impression of performance increases ... I would not trust that model for anything STEM-related.)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/n1f655i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751689811,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrss4u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1eed5r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"pcdacks","can_mod_post":false,"created_utc":1751677282,"send_replies":true,"parent_id":"t3_1lrss4u","score":1,"author_fullname":"t2_6suhydu8g","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I want to understand the reasons for the significant divergence, and see if it’s worth spending time to adapt it to llama.cpp.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1eed5r","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I want to understand the reasons for the significant divergence, and see if it’s worth spending time to adapt it to llama.cpp.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/n1eed5r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751677282,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrss4u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1elasd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"poli-cya","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1ejcou","score":1,"author_fullname":"t2_q8g93lhv4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Please provide any source saying that, it's absolutely not correct. I asked o3 to give an evaluation-\\n\\n\\n\\n\\nIssue | Who's right| Why\\n---|---|----\\n“You can’t use a generic you when replying directly.”\\t | emprahsFury is off-base | English lets you use the generic you even in a direct reply. It can be confusing, but it isn’t grammatically outlawed.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n1elasd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Please provide any source saying that, it&amp;#39;s absolutely not correct. I asked o3 to give an evaluation-&lt;/p&gt;\\n\\n&lt;table&gt;&lt;thead&gt;\\n&lt;tr&gt;\\n&lt;th&gt;Issue&lt;/th&gt;\\n&lt;th&gt;Who&amp;#39;s right&lt;/th&gt;\\n&lt;th&gt;Why&lt;/th&gt;\\n&lt;/tr&gt;\\n&lt;/thead&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;td&gt;“You can’t use a generic you when replying directly.”&lt;/td&gt;\\n&lt;td&gt;emprahsFury is off-base&lt;/td&gt;\\n&lt;td&gt;English lets you use the generic you even in a direct reply. It can be confusing, but it isn’t grammatically outlawed.&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;/tbody&gt;&lt;/table&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lrss4u","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/n1elasd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751680302,"author_flair_text":null,"treatment_tags":[],"created_utc":1751680302,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1ejcou","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"emprahsFury","can_mod_post":false,"send_replies":false,"parent_id":"t1_n1e2s31","score":-1,"author_fullname":"t2_177r8n","approved_by":null,"mod_note":null,"all_awardings":[],"body":"If you're addressing someone directly (as in responding to them when they responded to you) then there is no generic you, and we can forgive the dude for not disambiguating perfectly.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n1ejcou","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you&amp;#39;re addressing someone directly (as in responding to them when they responded to you) then there is no generic you, and we can forgive the dude for not disambiguating perfectly.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lrss4u","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/n1ejcou/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751679447,"author_flair_text":null,"treatment_tags":[],"created_utc":1751679447,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1e2s31","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"poli-cya","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1dkta2","score":2,"author_fullname":"t2_q8g93lhv4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"https://en.wikipedia.org/wiki/Generic_you\\n\\nThe guy clearly wasn't talking about you in particular...","edited":false,"author_flair_css_class":null,"name":"t1_n1e2s31","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://en.wikipedia.org/wiki/Generic_you\\"&gt;https://en.wikipedia.org/wiki/Generic_you&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;The guy clearly wasn&amp;#39;t talking about you in particular...&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lrss4u","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/n1e2s31/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751672535,"author_flair_text":null,"collapsed":false,"created_utc":1751672535,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1dkta2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1dh4vv","score":2,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I made no claims.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1dkta2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I made no claims.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrss4u","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/n1dkta2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751665825,"author_flair_text":null,"treatment_tags":[],"created_utc":1751665825,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1dh4vv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"r4in311","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1dcjkz","score":2,"author_fullname":"t2_isspn","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ok but why no coding benchmark when you essentially claim strong coding performance? That's not really vision related?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1dh4vv","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ok but why no coding benchmark when you essentially claim strong coding performance? That&amp;#39;s not really vision related?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrss4u","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/n1dh4vv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751664542,"author_flair_text":null,"treatment_tags":[],"created_utc":1751664542,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1dcjkz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DepthHour1669","can_mod_post":false,"created_utc":1751662985,"send_replies":true,"parent_id":"t1_n1dacht","score":9,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"FLAME is a vision benchmark","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1dcjkz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;FLAME is a vision benchmark&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrss4u","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/n1dcjkz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751662985,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}}],"before":null}},"user_reports":[],"saved":false,"id":"n1dacht","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"r4in311","can_mod_post":false,"created_utc":1751662270,"send_replies":true,"parent_id":"t3_1lrss4u","score":1,"author_fullname":"t2_isspn","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Huge results, if true. So this 9B casually beats 4o in coding... amazing! But so far, we only see a lot of uncommon weird benchmarks, whats Flame-VLM-Code? Wheres HumanEval, MBPP or SWE-bench? If I'd claim near SOTA results, I'd probably not benchmark against Qwen2.5 7B ;-)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1dacht","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Huge results, if true. So this 9B casually beats 4o in coding... amazing! But so far, we only see a lot of uncommon weird benchmarks, whats Flame-VLM-Code? Wheres HumanEval, MBPP or SWE-bench? If I&amp;#39;d claim near SOTA results, I&amp;#39;d probably not benchmark against Qwen2.5 7B ;-)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrss4u/thudmglm41v9bthinking_looks_impressive/n1dacht/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751662270,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrss4u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(l,{data:t});export{n as default};
