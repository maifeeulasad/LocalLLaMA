import{j as e}from"./index-CWmJdUH_.js";import{R as t}from"./RedditPostRenderer-D2iunoQ9.js";import"./index-BCg9RP6g.js";const l=[{kind:"Listing",data:{after:null,dist:1,modhash:"",geo_filter:"",children:[{kind:"t3",data:{approved_at_utc:null,subreddit:"LocalLLaMA",selftext:`I'm working on a low-level experimental setup where, instead of just using embeddings generated by the model, I inject custom embeddings directly into a LLaMA model (specifically a GGUF version using llama.cpp).

These embeddings come from another domain (e.g. images), but I project them into the same space as LLaMA’s token embeddings using a learned encoder.

No fine-tuning, no LoRA, no weight modification.

My idea is:

* Compute cosine similarity between each custom embedding and the model's token embeddings.
* Find the nearest token ID.
* Replace that token in the prompt.
* Let LLaMA generate from there.

So far, I haven’t seen anyone try this with llama.cpp and GGUF.

Anyone doing something similar? Or know how to cleanly access tok\\_embeddings.weight in GGUF?`,user_reports:[],saved:!1,mod_reason_title:null,gilded:0,clicked:!1,title:"Injecting custom embeddings into LLaMA 3.2 GGUF model",link_flair_richtext:[{e:"text",t:"Question | Help"}],subreddit_name_prefixed:"r/LocalLLaMA",hidden:!1,pwls:6,link_flair_css_class:"",downs:0,thumbnail_height:null,top_awarded_type:null,hide_score:!1,name:"t3_1m6xrfj",quarantine:!1,link_flair_text_color:"dark",upvote_ratio:.43,author_flair_background_color:null,subreddit_type:"public",ups:0,total_awards_received:0,media_embed:{},thumbnail_width:null,author_flair_template_id:null,is_original_content:!1,author_fullname:"t2_dwnsr8pxq",secure_media:null,is_reddit_media_domain:!1,is_meta:!1,category:null,secure_media_embed:{},link_flair_text:"Question | Help",can_mod_post:!1,score:0,approved_by:null,is_created_from_ads_ui:!1,author_premium:!1,thumbnail:"self",edited:!1,author_flair_css_class:null,author_flair_richtext:[],gildings:{},content_categories:null,is_self:!0,mod_note:null,created:1753238458,link_flair_type:"richtext",wls:6,removed_by_category:null,banned_by:null,author_flair_type:"text",domain:"self.LocalLLaMA",allow_live_comments:!1,selftext_html:`&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I&amp;#39;m working on a low-level experimental setup where, instead of just using embeddings generated by the model, I inject custom embeddings directly into a LLaMA model (specifically a GGUF version using llama.cpp).&lt;/p&gt;

&lt;p&gt;These embeddings come from another domain (e.g. images), but I project them into the same space as LLaMA’s token embeddings using a learned encoder.&lt;/p&gt;

&lt;p&gt;No fine-tuning, no LoRA, no weight modification.&lt;/p&gt;

&lt;p&gt;My idea is:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Compute cosine similarity between each custom embedding and the model&amp;#39;s token embeddings.&lt;/li&gt;
&lt;li&gt;Find the nearest token ID.&lt;/li&gt;
&lt;li&gt;Replace that token in the prompt.&lt;/li&gt;
&lt;li&gt;Let LLaMA generate from there.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So far, I haven’t seen anyone try this with llama.cpp and GGUF.&lt;/p&gt;

&lt;p&gt;Anyone doing something similar? Or know how to cleanly access tok_embeddings.weight in GGUF?&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;`,likes:null,suggested_sort:null,banned_at_utc:null,view_count:null,archived:!1,no_follow:!0,is_crosspostable:!1,pinned:!1,over_18:!1,all_awardings:[],awarders:[],media_only:!1,link_flair_template_id:"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",can_gild:!1,spoiler:!1,locked:!1,author_flair_text:null,treatment_tags:[],visited:!1,removed_by:null,num_reports:null,distinguished:null,subreddit_id:"t5_81eyvm",author_is_blocked:!1,mod_reason_by:null,removal_reason:null,link_flair_background_color:"#5a74cc",id:"1m6xrfj",is_robot_indexable:!0,num_duplicates:0,report_reasons:null,author:"Old-Toe6442",discussion_type:null,num_comments:1,send_replies:!0,media:null,contest_mode:!1,author_patreon_flair:!1,author_flair_text_color:null,permalink:"/r/LocalLLaMA/comments/1m6xrfj/injecting_custom_embeddings_into_llama_32_gguf/",stickied:!1,url:"https://www.reddit.com/r/LocalLLaMA/comments/1m6xrfj/injecting_custom_embeddings_into_llama_32_gguf/",subreddit_subscribers:503255,created_utc:1753238458,num_crossposts:0,mod_reports:[],is_video:!1}}],before:null}},{kind:"Listing",data:{after:null,dist:null,modhash:"",geo_filter:"",children:[{kind:"t1",data:{subreddit_id:"t5_81eyvm",approved_at_utc:null,author_is_blocked:!1,comment_type:null,awarders:[],mod_reason_by:null,banned_by:null,author_flair_type:"text",total_awards_received:0,subreddit:"LocalLLaMA",author_flair_template_id:null,likes:null,replies:"",user_reports:[],saved:!1,id:"n4nhs84",banned_at_utc:null,mod_reason_title:null,gilded:0,archived:!1,collapsed_reason_code:null,no_follow:!1,author:"laser_man6",can_mod_post:!1,created_utc:1753242342,send_replies:!0,parent_id:"t3_1m6xrfj",score:4,author_fullname:"t2_xx06z",approved_by:null,mod_note:null,all_awardings:[],collapsed:!1,body:"You could also try just injecting the vector directly instead of finding the nearest token, the whole space has meaning, not just the specific points tokens sit on. Look into the ' petertodd' stuff and other noken research for some more info, they might also have technical details but I'm not sure",edited:!1,top_awarded_type:null,author_flair_css_class:null,name:"t1_n4nhs84",is_submitter:!1,downs:0,author_flair_richtext:[],author_patreon_flair:!1,body_html:`&lt;div class="md"&gt;&lt;p&gt;You could also try just injecting the vector directly instead of finding the nearest token, the whole space has meaning, not just the specific points tokens sit on. Look into the &amp;#39; petertodd&amp;#39; stuff and other noken research for some more info, they might also have technical details but I&amp;#39;m not sure&lt;/p&gt;
&lt;/div&gt;`,removal_reason:null,collapsed_reason:null,distinguished:null,associated_award:null,stickied:!1,author_premium:!1,can_gild:!1,gildings:{},unrepliable_reason:null,author_flair_text_color:null,score_hidden:!1,permalink:"/r/LocalLLaMA/comments/1m6xrfj/injecting_custom_embeddings_into_llama_32_gguf/n4nhs84/",subreddit_type:"public",locked:!1,report_reasons:null,created:1753242342,author_flair_text:null,treatment_tags:[],link_id:"t3_1m6xrfj",subreddit_name_prefixed:"r/LocalLLaMA",controversiality:0,depth:0,author_flair_background_color:null,collapsed_because_crowd_control:null,mod_reports:[],num_reports:null,ups:4}}],before:null}}],o=()=>e.jsx(t,{data:l});export{o as default};
