import{j as e}from"./index-CjwP30j7.js";import{R as l}from"./RedditPostRenderer-BbYuEq_V.js";import"./index-C-yxLSPN.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hello fellow redditors,\\n\\n  \\nI am trying to run Gemma-3n-E2B and E4B advertised as 2gb-3gb VRAM models. However, I couldn't run E4B due to torch outOfMemory, but when I ran E2B it took 10gbs and after few requests I went out of memory.\\n\\n  \\nI am trying to understand, is there a way to run these models really on 2gb-3gb VRAM, and if yes how so, and what I missed?\\n\\n  \\nThank you all","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Gemma-3n VRAM usage","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lojd3e","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.84,"author_flair_background_color":null,"subreddit_type":"public","ups":9,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_6hmjiu4n","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":9,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751317817,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hello fellow redditors,&lt;/p&gt;\\n\\n&lt;p&gt;I am trying to run Gemma-3n-E2B and E4B advertised as 2gb-3gb VRAM models. However, I couldn&amp;#39;t run E4B due to torch outOfMemory, but when I ran E2B it took 10gbs and after few requests I went out of memory.&lt;/p&gt;\\n\\n&lt;p&gt;I am trying to understand, is there a way to run these models really on 2gb-3gb VRAM, and if yes how so, and what I missed?&lt;/p&gt;\\n\\n&lt;p&gt;Thank you all&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lojd3e","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"el_pr3sid3nt3","discussion_type":null,"num_comments":8,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lojd3e/gemma3n_vram_usage/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lojd3e/gemma3n_vram_usage/","subreddit_subscribers":493458,"created_utc":1751317817,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0sbt5a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"vk3r","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0qd1my","score":1,"author_fullname":"t2_hyklw8a","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I think you don't understand enough of the subject. That the model mentions that it occupies 2-3GB is an approximate weight that will depend on the architecture for which it was made, the tools that are occupying, the context that it has and the quantization occupied.\\n\\nIt is never exact.\\n\\nAbout the quantizations, search in Hugginface and depending on the tool you use to build the model, you can find one quantized by someone. Unsloth and Bartwski are known for their work.","edited":false,"author_flair_css_class":null,"name":"t1_n0sbt5a","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think you don&amp;#39;t understand enough of the subject. That the model mentions that it occupies 2-3GB is an approximate weight that will depend on the architecture for which it was made, the tools that are occupying, the context that it has and the quantization occupied.&lt;/p&gt;\\n\\n&lt;p&gt;It is never exact.&lt;/p&gt;\\n\\n&lt;p&gt;About the quantizations, search in Hugginface and depending on the tool you use to build the model, you can find one quantized by someone. Unsloth and Bartwski are known for their work.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lojd3e","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lojd3e/gemma3n_vram_usage/n0sbt5a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751388313,"author_flair_text":null,"collapsed":false,"created_utc":1751388313,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0qd1my","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"el_pr3sid3nt3","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0oc69x","score":0,"author_fullname":"t2_6hmjiu4n","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I understood from the papers that you don’t need to quantize to run it on advertised 3gb VRAM. Are there quantized models available?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0qd1my","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I understood from the papers that you don’t need to quantize to run it on advertised 3gb VRAM. Are there quantized models available?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lojd3e","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lojd3e/gemma3n_vram_usage/n0qd1my/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751363632,"author_flair_text":null,"treatment_tags":[],"created_utc":1751363632,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n0oc69x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"vk3r","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0oa62i","score":1,"author_fullname":"t2_hyklw8a","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Forgot to mention quantization. A q8 is bigger than a q4","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0oc69x","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Forgot to mention quantization. A q8 is bigger than a q4&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lojd3e","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lojd3e/gemma3n_vram_usage/n0oc69x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751329791,"author_flair_text":null,"treatment_tags":[],"created_utc":1751329791,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0oa62i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"el_pr3sid3nt3","can_mod_post":false,"created_utc":1751329074,"send_replies":true,"parent_id":"t1_n0nykoe","score":1,"author_fullname":"t2_6hmjiu4n","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Reasonable answer, but these models take way too much memory before any context is given","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0oa62i","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Reasonable answer, but these models take way too much memory before any context is given&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lojd3e","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lojd3e/gemma3n_vram_usage/n0oa62i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751329074,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0nykoe","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"vk3r","can_mod_post":false,"created_utc":1751325210,"send_replies":true,"parent_id":"t3_1lojd3e","score":6,"author_fullname":"t2_hyklw8a","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The context you give to the model also takes up RAM.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0nykoe","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The context you give to the model also takes up RAM.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lojd3e/gemma3n_vram_usage/n0nykoe/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751325210,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lojd3e","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0rasvn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"sciencewarrior","can_mod_post":false,"created_utc":1751377674,"send_replies":true,"parent_id":"t3_1lojd3e","score":4,"author_fullname":"t2_4feaa","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"From what their model cards suggest, the software needs to support their architecture to make it work. Make sure you are running the latest version of llama.cpp. This tutorial should be handy: https://docs.unsloth.ai/basics/gemma-3n-how-to-run-and-fine-tune","edited":1751404517,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0rasvn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;From what their model cards suggest, the software needs to support their architecture to make it work. Make sure you are running the latest version of llama.cpp. This tutorial should be handy: &lt;a href=\\"https://docs.unsloth.ai/basics/gemma-3n-how-to-run-and-fine-tune\\"&gt;https://docs.unsloth.ai/basics/gemma-3n-how-to-run-and-fine-tune&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lojd3e/gemma3n_vram_usage/n0rasvn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751377674,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lojd3e","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0qcuee","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"el_pr3sid3nt3","can_mod_post":false,"created_utc":1751363521,"send_replies":true,"parent_id":"t1_n0pyq44","score":1,"author_fullname":"t2_6hmjiu4n","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah it is slow af, in some cases llama3.1 performed better","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0qcuee","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah it is slow af, in some cases llama3.1 performed better&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lojd3e","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lojd3e/gemma3n_vram_usage/n0qcuee/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751363521,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0pyq44","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Crafty-Celery-2466","can_mod_post":false,"created_utc":1751355007,"send_replies":true,"parent_id":"t3_1lojd3e","score":1,"author_fullname":"t2_8x13k917","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It was slow to run on my 3080. Qwen3-8B was so fast.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0pyq44","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It was slow to run on my 3080. Qwen3-8B was so fast.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lojd3e/gemma3n_vram_usage/n0pyq44/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751355007,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lojd3e","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(l,{data:a});export{n as default};
