import{j as e}from"./index-BOnf-UhU.js";import{R as l}from"./RedditPostRenderer-Ce39qICS.js";import"./index-CDase6VD.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I've come to the conclusion that Qwen's 235b at Q2K~, perhaps unsurprisingly, is not better than Qwen3 32b Q4KL but I still wonder about the Q3? Gemma2 27b Q3KS used to be awesome, for example. Perhaps Qwen's 235b at Q3 will be amazing? Amazing enough to warrant 10 t/s?\\n\\nI'm in the process of getting a mish mash of RAM I have in the cupboard together to go from 96GB to 128GB which should allow me to test Q3... if it'll POST.\\n\\nIs anyone already running the Q3? Is it better for code / design work than the current 32b GOAT?\\n\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Is a heavily quantised Q235b any better than Q32b?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lx2dw4","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.98,"author_flair_background_color":null,"subreddit_type":"public","ups":46,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_by77ogdhr","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":46,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752225846,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve come to the conclusion that Qwen&amp;#39;s 235b at Q2K~, perhaps unsurprisingly, is not better than Qwen3 32b Q4KL but I still wonder about the Q3? Gemma2 27b Q3KS used to be awesome, for example. Perhaps Qwen&amp;#39;s 235b at Q3 will be amazing? Amazing enough to warrant 10 t/s?&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m in the process of getting a mish mash of RAM I have in the cupboard together to go from 96GB to 128GB which should allow me to test Q3... if it&amp;#39;ll POST.&lt;/p&gt;\\n\\n&lt;p&gt;Is anyone already running the Q3? Is it better for code / design work than the current 32b GOAT?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lx2dw4","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Secure_Reflection409","discussion_type":null,"num_comments":33,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/","subreddit_subscribers":497503,"created_utc":1752225846,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2itq93","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ZBoblq","can_mod_post":false,"created_utc":1752228063,"send_replies":true,"parent_id":"t1_n2iry5d","score":9,"author_fullname":"t2_48bdcl5o","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What are the weak points?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2itq93","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What are the weak points?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx2dw4","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/n2itq93/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752228063,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2jj7sv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"EmergencyLetter135","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2iy0e8","score":2,"author_fullname":"t2_ioyqqx8pe","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thank you for your kind advice. Do you know a comparison with results between the GGUF and MLX models of Qwen 235B? The background is that I had the subjective impression that all MLX models I had tried could not keep up with the output quality of Unsloth. I even found the Q2 from Unsloth better than a 3-bit MLX.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2jj7sv","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you for your kind advice. Do you know a comparison with results between the GGUF and MLX models of Qwen 235B? The background is that I had the subjective impression that all MLX models I had tried could not keep up with the output quality of Unsloth. I even found the Q2 from Unsloth better than a 3-bit MLX.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx2dw4","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/n2jj7sv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752238792,"author_flair_text":null,"treatment_tags":[],"created_utc":1752238792,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2jpqy3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LA_rent_Aficionado","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2jl4oz","score":3,"author_fullname":"t2_t8zbiflk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For real, the Cursor system prompt is 15k, I imagine Roo is similar","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2jpqy3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For real, the Cursor system prompt is 15k, I imagine Roo is similar&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx2dw4","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/n2jpqy3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752240932,"author_flair_text":null,"treatment_tags":[],"created_utc":1752240932,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2jm8mv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Baldur-Norddahl","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2jlszs","score":3,"author_fullname":"t2_bvqb8ng0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The large system prompt gets cached and reused, so it is not so bad with regards to prompt processing speed.","edited":false,"author_flair_css_class":null,"name":"t1_n2jm8mv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The large system prompt gets cached and reused, so it is not so bad with regards to prompt processing speed.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lx2dw4","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/n2jm8mv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752239799,"author_flair_text":null,"collapsed":false,"created_utc":1752239799,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2jlszs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mxforest","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2jl4oz","score":2,"author_fullname":"t2_kenmq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Makes sense. I was mostly commenting on the most capable model that can be run on it with a usable context. 40k is plenty for a lot of purposes because prompt processing is ass anyway.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2jlszs","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Makes sense. I was mostly commenting on the most capable model that can be run on it with a usable context. 40k is plenty for a lot of purposes because prompt processing is ass anyway.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx2dw4","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/n2jlszs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752239657,"author_flair_text":null,"treatment_tags":[],"created_utc":1752239657,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2jl4oz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Baldur-Norddahl","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2iy0e8","score":2,"author_fullname":"t2_bvqb8ng0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I need 128k context for my use case. I am using RooCode and the standard system prompt eats up a lot of space, so the models with 40k context feels too limited.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2jl4oz","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I need 128k context for my use case. I am using RooCode and the standard system prompt eats up a lot of space, so the models with 40k context feels too limited.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx2dw4","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/n2jl4oz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752239431,"author_flair_text":null,"treatment_tags":[],"created_utc":1752239431,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2iy0e8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"mxforest","can_mod_post":false,"created_utc":1752230273,"send_replies":true,"parent_id":"t1_n2iry5d","score":9,"author_fullname":"t2_kenmq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Try DWQ. It is dynamic 3-6 bits. I run it on 40k context without a problem on my m4 max.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2iy0e8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Try DWQ. It is dynamic 3-6 bits. I run it on 40k context without a problem on my m4 max.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx2dw4","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/n2iy0e8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752230273,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2jfcur","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Caffdy","can_mod_post":false,"created_utc":1752237466,"send_replies":true,"parent_id":"t1_n2iry5d","score":2,"author_fullname":"t2_ql2vu0wz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"can confirm, best model that fits 128GB, R1 in dynamic quant needs over 140GBs","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2jfcur","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;can confirm, best model that fits 128GB, R1 in dynamic quant needs over 140GBs&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx2dw4","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/n2jfcur/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752237466,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2jrj6v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Secure_Reflection409","can_mod_post":false,"created_utc":1752241490,"send_replies":true,"parent_id":"t1_n2iry5d","score":1,"author_fullname":"t2_by77ogdhr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Nice.\\n\\n\\nWhich / Who's quant are you using exactly?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2jrj6v","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nice.&lt;/p&gt;\\n\\n&lt;p&gt;Which / Who&amp;#39;s quant are you using exactly?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx2dw4","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/n2jrj6v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752241490,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2iry5d","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Baldur-Norddahl","can_mod_post":false,"created_utc":1752227110,"send_replies":true,"parent_id":"t3_1lx2dw4","score":42,"author_fullname":"t2_bvqb8ng0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I am running Qwen3 235b at q3 on my 128 GB M4 Max MacBook Pro. It is the best model and the last resort before going cloud. But I would not call it amazing. It is no DeepSeek R1.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2iry5d","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I am running Qwen3 235b at q3 on my 128 GB M4 Max MacBook Pro. It is the best model and the last resort before going cloud. But I would not call it amazing. It is no DeepSeek R1.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/n2iry5d/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752227110,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lx2dw4","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":42}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2jclo7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"silenceimpaired","can_mod_post":false,"created_utc":1752236468,"send_replies":true,"parent_id":"t1_n2j5nla","score":3,"author_fullname":"t2_dissgzyl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I hope they stick to Apache licensing","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2jclo7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I hope they stick to Apache licensing&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx2dw4","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/n2jclo7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752236468,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2j5nla","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Sabin_Stargem","can_mod_post":false,"created_utc":1752233741,"send_replies":true,"parent_id":"t3_1lx2dw4","score":9,"author_fullname":"t2_3lhg8wa1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"A thing to keep an eye on, is Cognitive Computer's enlarged versions of Qwen3 32b that include a distillation of Qwen 235b.   Right now, they have a checkpoint of Qwen3 58b, Stage 2.   Hopefully the final version of these 58b and 72b models would be worth using.\\n\\nhttps://huggingface.co/cognitivecomputations/Qwen3-58B-Distill-Stage2","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2j5nla","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;A thing to keep an eye on, is Cognitive Computer&amp;#39;s enlarged versions of Qwen3 32b that include a distillation of Qwen 235b.   Right now, they have a checkpoint of Qwen3 58b, Stage 2.   Hopefully the final version of these 58b and 72b models would be worth using.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/cognitivecomputations/Qwen3-58B-Distill-Stage2\\"&gt;https://huggingface.co/cognitivecomputations/Qwen3-58B-Distill-Stage2&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/n2j5nla/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752233741,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lx2dw4","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2jq6gj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"smflx","can_mod_post":false,"created_utc":1752241067,"send_replies":true,"parent_id":"t1_n2iwuwx","score":3,"author_fullname":"t2_16qe65sqwe","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I agree. R1 is better &amp; not much slower in token generation. But, prompt processing of Qwen3 235B q4 is quite faster.\\n\\nI also tested Qwen3 235B q4 is better than Qwen3 32B Q8","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2jq6gj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I agree. R1 is better &amp;amp; not much slower in token generation. But, prompt processing of Qwen3 235B q4 is quite faster.&lt;/p&gt;\\n\\n&lt;p&gt;I also tested Qwen3 235B q4 is better than Qwen3 32B Q8&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx2dw4","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/n2jq6gj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752241067,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2iwuwx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Lissanro","can_mod_post":false,"created_utc":1752229693,"send_replies":true,"parent_id":"t3_1lx2dw4","score":15,"author_fullname":"t2_fpfao9g","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Qwen3 is MoE trained at high 16-bit precision, which makes it quite sensitive to quantization - more so than DeepSeek R1 which even though is MoE too but was trained at FP8 precision (MoE are more sensitive to quantization in general because they only use part of their parameters at a time, unlike dense models).\\n\\nI cannot recommend going below IQ4 even with R1 because I notice quality degradation beyond that point (I downloaded the original FP8 version of R1 and tested few quants: IQ3, IQ4 and Q8), and for Qwen3 I would recommend at least Q6 or Q8. This is actually the main reason why I ended up not using it much beyond some testing... At Q8 it still behind R1 IQ4\\\\_K\\\\_M in many areas, including general coding, creative writing and agentic workflows, while not being much faster. So I just use R1 0528 as my daily driver.\\n\\nThat said, Q3 of Qwen3 235B may still be better than 32B, but likely much slower if you are short on VRAM and still have some quality issues associated with heavy quantization. I did not test Qwen3 235B at quantization lower than IQ4, so please keep in mind that this is just a guess based on my experience. Testing yourself for your use case is a good idea - for creative writing and role play quantization issues are usually less noticeable than for programming.\\n\\nAlternatively, if you are memory limited, but still have enough to run Qwen3 at Q2K, then using Qwen3 32 Q8 may be a good option, especially if you do programming and need the best accuracy. New Mistral Devstral 2507 24B may be another alternative to try if you are looking for a lightweight model.","edited":1752230058,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2iwuwx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen3 is MoE trained at high 16-bit precision, which makes it quite sensitive to quantization - more so than DeepSeek R1 which even though is MoE too but was trained at FP8 precision (MoE are more sensitive to quantization in general because they only use part of their parameters at a time, unlike dense models).&lt;/p&gt;\\n\\n&lt;p&gt;I cannot recommend going below IQ4 even with R1 because I notice quality degradation beyond that point (I downloaded the original FP8 version of R1 and tested few quants: IQ3, IQ4 and Q8), and for Qwen3 I would recommend at least Q6 or Q8. This is actually the main reason why I ended up not using it much beyond some testing... At Q8 it still behind R1 IQ4_K_M in many areas, including general coding, creative writing and agentic workflows, while not being much faster. So I just use R1 0528 as my daily driver.&lt;/p&gt;\\n\\n&lt;p&gt;That said, Q3 of Qwen3 235B may still be better than 32B, but likely much slower if you are short on VRAM and still have some quality issues associated with heavy quantization. I did not test Qwen3 235B at quantization lower than IQ4, so please keep in mind that this is just a guess based on my experience. Testing yourself for your use case is a good idea - for creative writing and role play quantization issues are usually less noticeable than for programming.&lt;/p&gt;\\n\\n&lt;p&gt;Alternatively, if you are memory limited, but still have enough to run Qwen3 at Q2K, then using Qwen3 32 Q8 may be a good option, especially if you do programming and need the best accuracy. New Mistral Devstral 2507 24B may be another alternative to try if you are looking for a lightweight model.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/n2iwuwx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752229693,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lx2dw4","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":15}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2j1qrr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2ivv7e","score":2,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Just PPL and KLD and delta Probs. good ole barty made a good post on this a while back\\n\\n[https://www.reddit.com/r/LocalLLaMA/comments/1jvlf6m/llama\\\\_4\\\\_scout\\\\_sub\\\\_50gb\\\\_gguf\\\\_quantization\\\\_showdown/](https://www.reddit.com/r/LocalLLaMA/comments/1jvlf6m/llama_4_scout_sub_50gb_gguf_quantization_showdown/)\\n\\nDon't trust PPL numbers, those are often weird, esp with gemma quants. MMLU and GPQA are the easiest full e2e benchmarks. Very compute heavy though.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2j1qrr","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just PPL and KLD and delta Probs. good ole barty made a good post on this a while back&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1jvlf6m/llama_4_scout_sub_50gb_gguf_quantization_showdown/\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1jvlf6m/llama_4_scout_sub_50gb_gguf_quantization_showdown/&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Don&amp;#39;t trust PPL numbers, those are often weird, esp with gemma quants. MMLU and GPQA are the easiest full e2e benchmarks. Very compute heavy though.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx2dw4","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/n2j1qrr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752232023,"author_flair_text":null,"treatment_tags":[],"created_utc":1752232023,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ivv7e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Secure_Reflection409","can_mod_post":false,"created_utc":1752229193,"send_replies":true,"parent_id":"t1_n2ir37t","score":2,"author_fullname":"t2_by77ogdhr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"We've had a few people do it here in the past with MMLU-Pro but I do wonder if there's a less compute intensive way to do it.\\n\\n\\nMMLU-Pro is arguably not a good enough proxy for codegen / design, either.\\n\\n\\nNo way around burning millions of tokens, perhaps and if you're at home doing it yourself on your own kit, tens++ of hours of your time, too.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ivv7e","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;We&amp;#39;ve had a few people do it here in the past with MMLU-Pro but I do wonder if there&amp;#39;s a less compute intensive way to do it.&lt;/p&gt;\\n\\n&lt;p&gt;MMLU-Pro is arguably not a good enough proxy for codegen / design, either.&lt;/p&gt;\\n\\n&lt;p&gt;No way around burning millions of tokens, perhaps and if you&amp;#39;re at home doing it yourself on your own kit, tens++ of hours of your time, too.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx2dw4","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/n2ivv7e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752229193,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2j16u9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2iz2x2","score":3,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":" That's weird. Link?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2j16u9","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s weird. Link?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx2dw4","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/n2j16u9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752231766,"author_flair_text":null,"treatment_tags":[],"created_utc":1752231766,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2iz2x2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1752230796,"send_replies":true,"parent_id":"t1_n2ir37t","score":2,"author_fullname":"t2_h5utwre7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That's strange because they posted graphs on how it wasn't affected so much, down to even low quants.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2iz2x2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s strange because they posted graphs on how it wasn&amp;#39;t affected so much, down to even low quants.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx2dw4","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/n2iz2x2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752230796,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2jfrb6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Caffdy","can_mod_post":false,"created_utc":1752237607,"send_replies":true,"parent_id":"t1_n2ir37t","score":2,"author_fullname":"t2_ql2vu0wz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"[Just leaving this here. I know is not Qwen3, but I think is relevant for it as well.](https://arxiv.org/pdf/2505.02390)\\n\\nTL;DR: Dynamic Quants can perform as good as Q4 with generous savings in memory","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2jfrb6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://arxiv.org/pdf/2505.02390\\"&gt;Just leaving this here. I know is not Qwen3, but I think is relevant for it as well.&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;TL;DR: Dynamic Quants can perform as good as Q4 with generous savings in memory&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx2dw4","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/n2jfrb6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752237607,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ir37t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MaxKruse96","can_mod_post":false,"created_utc":1752226631,"send_replies":true,"parent_id":"t3_1lx2dw4","score":13,"author_fullname":"t2_pfi81","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"qwen3 is extremely sensitive to quant for some reason, so the higher u go, the (un)proportionally better it gets. Testing specific quants of different sizes against each other is something so insanely compute heavy, i dont think anyone does that.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ir37t","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;qwen3 is extremely sensitive to quant for some reason, so the higher u go, the (un)proportionally better it gets. Testing specific quants of different sizes against each other is something so insanely compute heavy, i dont think anyone does that.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/n2ir37t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752226631,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lx2dw4","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":13}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2j2cbm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1752232296,"send_replies":true,"parent_id":"t3_1lx2dw4","score":4,"author_fullname":"t2_h5utwre7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I can use it at Q4. Seems samey between it and exl3 3.0bpw where I can fully offload it. The API on OR gets a couple of things slightly more right but that's about all.\\n\\nYou're running into the MoE problem of low active params. For me around ~30b is the cutoff where models start getting decent. 235b is not quite there but almost, the other params don't necessarily make up for it. Training data between them all was similar and so you have cases where the 32b does comparably. You wonder what all that extra memory is for.\\n\\nEven deepseek makes ~30b style mistakes. Truly has a lot of knowledge in those params so it's less likely. These smaller MoE don't have that luxury or as good training data. 235b has all the stem stuff but *not* all the intelligence. Code and conversations need the latter. They force the model to generalize.\\n\\nThat we're even having this conversation shows it's not a free lunch of model go fast.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2j2cbm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I can use it at Q4. Seems samey between it and exl3 3.0bpw where I can fully offload it. The API on OR gets a couple of things slightly more right but that&amp;#39;s about all.&lt;/p&gt;\\n\\n&lt;p&gt;You&amp;#39;re running into the MoE problem of low active params. For me around ~30b is the cutoff where models start getting decent. 235b is not quite there but almost, the other params don&amp;#39;t necessarily make up for it. Training data between them all was similar and so you have cases where the 32b does comparably. You wonder what all that extra memory is for.&lt;/p&gt;\\n\\n&lt;p&gt;Even deepseek makes ~30b style mistakes. Truly has a lot of knowledge in those params so it&amp;#39;s less likely. These smaller MoE don&amp;#39;t have that luxury or as good training data. 235b has all the stem stuff but &lt;em&gt;not&lt;/em&gt; all the intelligence. Code and conversations need the latter. They force the model to generalize.&lt;/p&gt;\\n\\n&lt;p&gt;That we&amp;#39;re even having this conversation shows it&amp;#39;s not a free lunch of model go fast.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/n2j2cbm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752232296,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lx2dw4","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2iybqf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Zestyclose_Yak_3174","can_mod_post":false,"created_utc":1752230427,"send_replies":true,"parent_id":"t3_1lx2dw4","score":3,"author_fullname":"t2_o0jgdhlij","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It feels like even unsloth's Q2_K is quite decent. I do think it has more to pull from and rooted in more real world knowledge VS Qwen 3 32B, however the difference might become really noticeable when accuracy is a must: coding, classification, etc","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2iybqf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It feels like even unsloth&amp;#39;s Q2_K is quite decent. I do think it has more to pull from and rooted in more real world knowledge VS Qwen 3 32B, however the difference might become really noticeable when accuracy is a must: coding, classification, etc&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/n2iybqf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752230427,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lx2dw4","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2kbocc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"__JockY__","can_mod_post":false,"created_utc":1752247338,"send_replies":true,"parent_id":"t3_1lx2dw4","score":3,"author_fullname":"t2_qf8h7ka8","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I run the official Qwen3 235B A22B INT4 GPTQ quant in vLLM using Qwen’s recommended settings.\\n\\nIt’s fabulous for coding and technical work. I love it. Destroys Qwen2.5 72B 8bpw exl2 in all my use cases.\\n\\nHowever it drops off quickly at larger contexts. Once you get past ~ 16k it gets significantly dumber, makes syntax mistakes, etc. close to 32k tokens and it’s pretty bad.\\n\\nBut working inside that first 16k feels like I have a SOTA model right next to me. Fantastic.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2kbocc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I run the official Qwen3 235B A22B INT4 GPTQ quant in vLLM using Qwen’s recommended settings.&lt;/p&gt;\\n\\n&lt;p&gt;It’s fabulous for coding and technical work. I love it. Destroys Qwen2.5 72B 8bpw exl2 in all my use cases.&lt;/p&gt;\\n\\n&lt;p&gt;However it drops off quickly at larger contexts. Once you get past ~ 16k it gets significantly dumber, makes syntax mistakes, etc. close to 32k tokens and it’s pretty bad.&lt;/p&gt;\\n\\n&lt;p&gt;But working inside that first 16k feels like I have a SOTA model right next to me. Fantastic.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/n2kbocc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752247338,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lx2dw4","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2jngty","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Secure_Reflection409","can_mod_post":false,"created_utc":1752240197,"send_replies":true,"parent_id":"t1_n2jgsky","score":2,"author_fullname":"t2_by77ogdhr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Awesome!\\n\\n\\nThe more people we got testing this the better. Slow internet connection here so bear with me :)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2jngty","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Awesome!&lt;/p&gt;\\n\\n&lt;p&gt;The more people we got testing this the better. Slow internet connection here so bear with me :)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx2dw4","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/n2jngty/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752240197,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2jgsky","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Karim_acing_it","can_mod_post":false,"created_utc":1752237971,"send_replies":true,"parent_id":"t3_1lx2dw4","score":2,"author_fullname":"t2_omawcpyf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I run the IQ4\\\\_XS on 128GB DDR5 at a mere 3 tps in LM Studio, just made a post. Do you have any questions specifically? I personally couldn't witness a big increase in quality from Qwen 32B to Qwen 235B in my very initial testing, but it were most generic prompts too.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2jgsky","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I run the IQ4_XS on 128GB DDR5 at a mere 3 tps in LM Studio, just made a post. Do you have any questions specifically? I personally couldn&amp;#39;t witness a big increase in quality from Qwen 32B to Qwen 235B in my very initial testing, but it were most generic prompts too.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/n2jgsky/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752237971,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lx2dw4","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2jq8tj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DemonsHW-","can_mod_post":false,"created_utc":1752241088,"send_replies":true,"parent_id":"t3_1lx2dw4","score":2,"author_fullname":"t2_vsv74a","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I used different Q5 and Q4 quants and they were extremely bad for code generation. It would produce a lot of syntax errors in the generated code and would go into a infinite loop generating random tokens.\\n\\nEven DeepSeek-R1 with TQ1\\\\_0 quant performed better.\\n\\nNot sure about other tasks.\\n\\n  \\nEdit: Also 10t/s is a bit low for Qwen3 if you are planning to enable thinking. In my tests it would sometimes think for over 10 minutes at 40 t/s","edited":1752241441,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2jq8tj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I used different Q5 and Q4 quants and they were extremely bad for code generation. It would produce a lot of syntax errors in the generated code and would go into a infinite loop generating random tokens.&lt;/p&gt;\\n\\n&lt;p&gt;Even DeepSeek-R1 with TQ1_0 quant performed better.&lt;/p&gt;\\n\\n&lt;p&gt;Not sure about other tasks.&lt;/p&gt;\\n\\n&lt;p&gt;Edit: Also 10t/s is a bit low for Qwen3 if you are planning to enable thinking. In my tests it would sometimes think for over 10 minutes at 40 t/s&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/n2jq8tj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752241088,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lx2dw4","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2k3vp4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Secure_Reflection409","can_mod_post":false,"created_utc":1752245145,"send_replies":true,"parent_id":"t1_n2k125p","score":2,"author_fullname":"t2_by77ogdhr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That's great info, thanks.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2k3vp4","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s great info, thanks.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx2dw4","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/n2k3vp4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752245145,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2k125p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Shape_3423","can_mod_post":false,"created_utc":1752244332,"send_replies":true,"parent_id":"t3_1lx2dw4","score":2,"author_fullname":"t2_1mpbnkwidj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I've tested a number of local LLMs and quants for document work with long detailed prompts that generate or grade long documents placed in the ctx (4x3090). My observation generally is that the impact of quantization is undersold. It may be fine for your use case, but not for mine. The first thing to go is IF. BF16 is better than Q8, which may or may not get it done. By the time I get to Q4, IF becomes useless for my workflow.  Qwen3 235b Q3KL could not IF enough to be useful. FWIW the consistent winners on my rig with sufficiently long ctx were Qwen3 32b BF16/Q8, QwQ BF16/Q8, and Qwen2.5 70b (Athene v2) Q8.  llama 3.3 70b Q8 would IF but even at Q8 didn't have enough smarts to be useful. Qwen3 30ab BF16 128k ctx is my daily driver.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2k125p","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve tested a number of local LLMs and quants for document work with long detailed prompts that generate or grade long documents placed in the ctx (4x3090). My observation generally is that the impact of quantization is undersold. It may be fine for your use case, but not for mine. The first thing to go is IF. BF16 is better than Q8, which may or may not get it done. By the time I get to Q4, IF becomes useless for my workflow.  Qwen3 235b Q3KL could not IF enough to be useful. FWIW the consistent winners on my rig with sufficiently long ctx were Qwen3 32b BF16/Q8, QwQ BF16/Q8, and Qwen2.5 70b (Athene v2) Q8.  llama 3.3 70b Q8 would IF but even at Q8 didn&amp;#39;t have enough smarts to be useful. Qwen3 30ab BF16 128k ctx is my daily driver.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/n2k125p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752244332,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lx2dw4","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2l0fxt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"tempetemplar","can_mod_post":false,"created_utc":1752254287,"send_replies":true,"parent_id":"t3_1lx2dw4","score":2,"author_fullname":"t2_atvw2aj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Q3 is still good for my use cases! I've even tried to make some insane exercise of using iq2_xxs of qwen3 32b. To reduce insanity, you have to use many tools. Otherwise, well, you've got totally insane results 😂","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2l0fxt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Q3 is still good for my use cases! I&amp;#39;ve even tried to make some insane exercise of using iq2_xxs of qwen3 32b. To reduce insanity, you have to use many tools. Otherwise, well, you&amp;#39;ve got totally insane results 😂&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/n2l0fxt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752254287,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lx2dw4","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ixvm7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"uti24","can_mod_post":false,"created_utc":1752230205,"send_replies":true,"parent_id":"t3_1lx2dw4","score":3,"author_fullname":"t2_13hbro","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I tried Qwen 235B at Q2\\\\_K; it's definitely not lobotomized at that point, it performed quite well in my test. \\n\\nI guess it might even be better than the 32B at Q8, but that would require a deeper comparison, which I haven’t done.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ixvm7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I tried Qwen 235B at Q2_K; it&amp;#39;s definitely not lobotomized at that point, it performed quite well in my test. &lt;/p&gt;\\n\\n&lt;p&gt;I guess it might even be better than the 32B at Q8, but that would require a deeper comparison, which I haven’t done.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/n2ixvm7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752230205,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lx2dw4","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2iyrev","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Red_Redditor_Reddit","can_mod_post":false,"created_utc":1752230639,"send_replies":true,"parent_id":"t3_1lx2dw4","score":4,"author_fullname":"t2_8eelmfjg","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I use 2q from unsloth. It's better than the 32b, but it also uses like 6x the memory the 32b @ 4q does. The main advantage is speed. If it wasn't that, there's more bang for your RAM with other dense models. ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2iyrev","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I use 2q from unsloth. It&amp;#39;s better than the 32b, but it also uses like 6x the memory the 32b @ 4q does. The main advantage is speed. If it wasn&amp;#39;t that, there&amp;#39;s more bang for your RAM with other dense models. &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/n2iyrev/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752230639,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lx2dw4","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2l4pe2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Few-Yam9901","can_mod_post":false,"created_utc":1752255456,"send_replies":true,"parent_id":"t3_1lx2dw4","score":2,"author_fullname":"t2_1rhlf3bcfk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think Unsloth Q6\\\\_K\\\\_XL 32b is better than 235b Q5?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2l4pe2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think Unsloth Q6_K_XL 32b is better than 235b Q5?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/n2l4pe2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752255456,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lx2dw4","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),r=()=>e.jsx(l,{data:t});export{r as default};
