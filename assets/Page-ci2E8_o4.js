import{j as e}from"./index-DLSqWzaI.js";import{R as t}from"./RedditPostRenderer-CysRo2D_.js";import"./index-COXiL3Lo.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"For a long time I have spent hours and hours testing all the open source models (high performance gaming PCs) so they all work well for me and I must say that ollama in all its variants is truly an excellent model. Lately I've been interested in LLMs that help you program and I've noticed that almost all of them are inadequate to carry out this task unless you get a subscription to cloude 4 etc. So I said to myself, how can I get around this obstacle? Simple (just saying obviously) just do a fine Turing with a performance dataset created specifically. Here, after a long time and sleepless nights, I created a 1.4tb performance and competitive dataset to train my ollama code. Unfortunately, even to do Turing's job, my hardware is not enough but an investment of thousands of euros must be made. If you have the resources you get the results otherwise you just watch. Sorry I went on too long but I am very passionate about this subject","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"The ollama models are excellent models that can be installed locally as a starting point but.....","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lmtlgp","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.21,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1s56c0u2mx","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751134777,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;For a long time I have spent hours and hours testing all the open source models (high performance gaming PCs) so they all work well for me and I must say that ollama in all its variants is truly an excellent model. Lately I&amp;#39;ve been interested in LLMs that help you program and I&amp;#39;ve noticed that almost all of them are inadequate to carry out this task unless you get a subscription to cloude 4 etc. So I said to myself, how can I get around this obstacle? Simple (just saying obviously) just do a fine Turing with a performance dataset created specifically. Here, after a long time and sleepless nights, I created a 1.4tb performance and competitive dataset to train my ollama code. Unfortunately, even to do Turing&amp;#39;s job, my hardware is not enough but an investment of thousands of euros must be made. If you have the resources you get the results otherwise you just watch. Sorry I went on too long but I am very passionate about this subject&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lmtlgp","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"CodeStackDev","discussion_type":null,"num_comments":6,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lmtlgp/the_ollama_models_are_excellent_models_that_can/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lmtlgp/the_ollama_models_are_excellent_models_that_can/","subreddit_subscribers":492840,"created_utc":1751134777,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0adxnb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"hainesk","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0a4vl2","score":1,"author_fullname":"t2_5rprd","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Unsloth is very active in this sub. You might check out their fine tuning guide for some ideas on how to use your dataset. \\n\\n[https://docs.unsloth.ai/get-started/fine-tuning-guide](https://docs.unsloth.ai/get-started/fine-tuning-guide)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0adxnb","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Unsloth is very active in this sub. You might check out their fine tuning guide for some ideas on how to use your dataset. &lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://docs.unsloth.ai/get-started/fine-tuning-guide\\"&gt;https://docs.unsloth.ai/get-started/fine-tuning-guide&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmtlgp","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmtlgp/the_ollama_models_are_excellent_models_that_can/n0adxnb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751138062,"author_flair_text":null,"treatment_tags":[],"created_utc":1751138062,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0a4vl2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CodeStackDev","can_mod_post":false,"created_utc":1751135188,"send_replies":true,"parent_id":"t1_n0a4dud","score":2,"author_fullname":"t2_1s56c0u2mx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks for the tip, to be honest I looked at antropich and saw that it was very interesting. I'm very sorry because my programming-oriented dataset is really high quality and I can't use it. I'm very sorry.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0a4vl2","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for the tip, to be honest I looked at antropich and saw that it was very interesting. I&amp;#39;m very sorry because my programming-oriented dataset is really high quality and I can&amp;#39;t use it. I&amp;#39;m very sorry.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmtlgp","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmtlgp/the_ollama_models_are_excellent_models_that_can/n0a4vl2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751135188,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0dzxc4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CodeStackDev","can_mod_post":false,"created_utc":1751194371,"send_replies":true,"parent_id":"t1_n0a4dud","score":1,"author_fullname":"t2_1s56c0u2mx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I followed your advice and studied a bit. Devstal is not free, you have to purchase a pay-as-you-go API. I'm honest, I'm a big fan of Antropich and to date no LLM is more powerful than Claude 4 on coding. Thanks so much for your advice","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0dzxc4","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I followed your advice and studied a bit. Devstal is not free, you have to purchase a pay-as-you-go API. I&amp;#39;m honest, I&amp;#39;m a big fan of Antropich and to date no LLM is more powerful than Claude 4 on coding. Thanks so much for your advice&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmtlgp","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmtlgp/the_ollama_models_are_excellent_models_that_can/n0dzxc4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751194371,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0a4dud","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No_Afternoon_4260","can_mod_post":false,"created_utc":1751135032,"send_replies":true,"parent_id":"t3_1lmtlgp","score":10,"author_fullname":"t2_cj9kap4bx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Check devstral and use it with openhands/roo code\\nForget about ollmaa, it is a backend (llm inference engine) not a model. And clearly not the backend with which you'll learn how that tech works. It is too userfriendly imho and people mix topics","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0a4dud","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Check devstral and use it with openhands/roo code\\nForget about ollmaa, it is a backend (llm inference engine) not a model. And clearly not the backend with which you&amp;#39;ll learn how that tech works. It is too userfriendly imho and people mix topics&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmtlgp/the_ollama_models_are_excellent_models_that_can/n0a4dud/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751135032,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lmtlgp","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ado2m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"hainesk","can_mod_post":false,"created_utc":1751137977,"send_replies":true,"parent_id":"t1_n0a9tth","score":0,"author_fullname":"t2_5rprd","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I think OP may have been talking about the different versions of llama.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ado2m","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think OP may have been talking about the different versions of llama.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmtlgp","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmtlgp/the_ollama_models_are_excellent_models_that_can/n0ado2m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751137977,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n0a9tth","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"LagOps91","can_mod_post":false,"created_utc":1751136756,"send_replies":true,"parent_id":"t3_1lmtlgp","score":4,"author_fullname":"t2_3wi6j7vwh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'm very confused. olama is just a backend, not a model. and in terms of training, you don't \\"program\\" or \\"train\\" the code of the ai. what you want is to train the model to reproduce the training data and to generalize to a validation dataset. for the most part, it is unlikely that you as an individual can finetune a coding model to be meaningfully better (or better at all) than currently available open source models of the same size.\\n\\nI think what you should do, is learn more about LLMs actually work and what finetuning is and does before trying to proceed with any training of sorts. most likely, you need to try and find the right model for what you are trying to accomplish and be happy with what is on offer.\\n\\nif not, then most likely your hardware is just not good enough to do the job and you either need to return to a paid subscription or shell out a significant amount of money to run something like R1 locally to match sota ai models.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0a9tth","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m very confused. olama is just a backend, not a model. and in terms of training, you don&amp;#39;t &amp;quot;program&amp;quot; or &amp;quot;train&amp;quot; the code of the ai. what you want is to train the model to reproduce the training data and to generalize to a validation dataset. for the most part, it is unlikely that you as an individual can finetune a coding model to be meaningfully better (or better at all) than currently available open source models of the same size.&lt;/p&gt;\\n\\n&lt;p&gt;I think what you should do, is learn more about LLMs actually work and what finetuning is and does before trying to proceed with any training of sorts. most likely, you need to try and find the right model for what you are trying to accomplish and be happy with what is on offer.&lt;/p&gt;\\n\\n&lt;p&gt;if not, then most likely your hardware is just not good enough to do the job and you either need to return to a paid subscription or shell out a significant amount of money to run something like R1 locally to match sota ai models.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmtlgp/the_ollama_models_are_excellent_models_that_can/n0a9tth/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751136756,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmtlgp","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}}]`),r=()=>e.jsx(t,{data:a});export{r as default};
