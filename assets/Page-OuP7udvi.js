import{j as e}from"./index-Bu7qcPAU.js";import{R as t}from"./RedditPostRenderer-CbHA7O5q.js";import"./index-BKgbfxhf.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"https://preview.redd.it/h7ayiozbt7cf1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=f25b54d35a6eaf43451f5d78d2046edf55d974c4\\n\\nHey there,\\n\\nin the last few weeks, I've spent a lot of time learning about Local LLMs but always noticed one glaringly obvious, missing thing: *good* tools to run LLM benchmarks on (in terms of output quality, not talking about speed here!) in order to decide which LLM is best suited for a given task.\\n\\nThus, I've built a small Python tool to run the new and often-discussed HLE benchmark on local Ollama models. Grok 4 just passed the 40% milestone, but how far can our local models go?\\n\\nMy tool supports:\\n\\n* both **vision-based** and **text-only** prompting\\n* **automatic judging** by a third-party model not involved in answering the exam questions\\n* **question randomization** and only testing for a small subset of HLE\\n* export of the results to **machine-readable JSON**\\n* running **several evaluations** for different models all in one go\\n* support for **external Ollama instances** with Bearer Authentication\\n\\nThe entire source code is on GitHub! [https://github.com/mags0ft/hle-eval-ollama](https://github.com/mags0ft/hle-eval-ollama)\\n\\n**To anyone new to HLE (Humanity's Last Exam)**, let me give you a quick rundown: The benchmark has been made by the Center for AI Safety and is known to be one of the hardest currently available. Many people believe that once models reach close to 100%, we're only a few steps away from AGI (sounds more like buzz than actual facts to me, but whatever...)\\n\\nMy project extends the usability of HLE to local Ollama models. It also improves code quality over the provided benchmarking scripts by the HLE authors (because those only provided support for OpenAI API endpoints) due to more documentation and extended formatting efforts.\\n\\nI'd love to get some feedback, so don't hesitate to comment! Have fun trying it out!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"I built a tool to run Humanity's Last Exam on your favorite local models!","link_flair_richtext":[{"e":"text","t":"Resources"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":70,"top_awarded_type":null,"hide_score":false,"media_metadata":{"h7ayiozbt7cf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":60,"x":108,"u":"https://preview.redd.it/h7ayiozbt7cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b9b654093abcaead62062dd06d844d299331fd7a"},{"y":121,"x":216,"u":"https://preview.redd.it/h7ayiozbt7cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c4effb970945f88d3442b1c15fe8c167d8bd5781"},{"y":180,"x":320,"u":"https://preview.redd.it/h7ayiozbt7cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6f6ee76a86e3f3284a9f63766f56f445c9fdfd82"},{"y":360,"x":640,"u":"https://preview.redd.it/h7ayiozbt7cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b45b13f85b604fa1bd5a558d66c904b36c5603bc"},{"y":540,"x":960,"u":"https://preview.redd.it/h7ayiozbt7cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=734b07f48eec345736023b7f2a14fcfd437c86ce"},{"y":607,"x":1080,"u":"https://preview.redd.it/h7ayiozbt7cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=120eff57181b5e5f7de80333a223d795d381b98d"}],"s":{"y":1080,"x":1920,"u":"https://preview.redd.it/h7ayiozbt7cf1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=f25b54d35a6eaf43451f5d78d2046edf55d974c4"},"id":"h7ayiozbt7cf1"}},"name":"t3_1lx2j1l","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.96,"author_flair_background_color":null,"ups":22,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1tbbr8pu4s","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Resources","can_mod_post":false,"score":22,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://external-preview.redd.it/o-H6rWtrxMkGu6ppy9Z76PzirPIQLGU4dUkkkXovQww.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=6f57464b5bac63e85c04bfc757ab2feccce5637d","edited":1752226612,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"subreddit_type":"public","created":1752226420,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://preview.redd.it/h7ayiozbt7cf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f25b54d35a6eaf43451f5d78d2046edf55d974c4\\"&gt;https://preview.redd.it/h7ayiozbt7cf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f25b54d35a6eaf43451f5d78d2046edf55d974c4&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Hey there,&lt;/p&gt;\\n\\n&lt;p&gt;in the last few weeks, I&amp;#39;ve spent a lot of time learning about Local LLMs but always noticed one glaringly obvious, missing thing: &lt;em&gt;good&lt;/em&gt; tools to run LLM benchmarks on (in terms of output quality, not talking about speed here!) in order to decide which LLM is best suited for a given task.&lt;/p&gt;\\n\\n&lt;p&gt;Thus, I&amp;#39;ve built a small Python tool to run the new and often-discussed HLE benchmark on local Ollama models. Grok 4 just passed the 40% milestone, but how far can our local models go?&lt;/p&gt;\\n\\n&lt;p&gt;My tool supports:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;both &lt;strong&gt;vision-based&lt;/strong&gt; and &lt;strong&gt;text-only&lt;/strong&gt; prompting&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;automatic judging&lt;/strong&gt; by a third-party model not involved in answering the exam questions&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;question randomization&lt;/strong&gt; and only testing for a small subset of HLE&lt;/li&gt;\\n&lt;li&gt;export of the results to &lt;strong&gt;machine-readable JSON&lt;/strong&gt;&lt;/li&gt;\\n&lt;li&gt;running &lt;strong&gt;several evaluations&lt;/strong&gt; for different models all in one go&lt;/li&gt;\\n&lt;li&gt;support for &lt;strong&gt;external Ollama instances&lt;/strong&gt; with Bearer Authentication&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;The entire source code is on GitHub! &lt;a href=\\"https://github.com/mags0ft/hle-eval-ollama\\"&gt;https://github.com/mags0ft/hle-eval-ollama&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;To anyone new to HLE (Humanity&amp;#39;s Last Exam)&lt;/strong&gt;, let me give you a quick rundown: The benchmark has been made by the Center for AI Safety and is known to be one of the hardest currently available. Many people believe that once models reach close to 100%, we&amp;#39;re only a few steps away from AGI (sounds more like buzz than actual facts to me, but whatever...)&lt;/p&gt;\\n\\n&lt;p&gt;My project extends the usability of HLE to local Ollama models. It also improves code quality over the provided benchmarking scripts by the HLE authors (because those only provided support for OpenAI API endpoints) due to more documentation and extended formatting efforts.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;d love to get some feedback, so don&amp;#39;t hesitate to comment! Have fun trying it out!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/o-H6rWtrxMkGu6ppy9Z76PzirPIQLGU4dUkkkXovQww.png?auto=webp&amp;s=24f6d099e936365d8417c46a459bc48b783a7ad3","width":1200,"height":600},"resolutions":[{"url":"https://external-preview.redd.it/o-H6rWtrxMkGu6ppy9Z76PzirPIQLGU4dUkkkXovQww.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=84aa3c4f7f239451e4fb6ade8dbdfcfe99152d99","width":108,"height":54},{"url":"https://external-preview.redd.it/o-H6rWtrxMkGu6ppy9Z76PzirPIQLGU4dUkkkXovQww.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=44ea6164b5081b0a55b6d05d21b49f67da5ff416","width":216,"height":108},{"url":"https://external-preview.redd.it/o-H6rWtrxMkGu6ppy9Z76PzirPIQLGU4dUkkkXovQww.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0356d76952e89d5ebe9effdd8ddeaeed050745ef","width":320,"height":160},{"url":"https://external-preview.redd.it/o-H6rWtrxMkGu6ppy9Z76PzirPIQLGU4dUkkkXovQww.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=81757699f7df25dfc08d7353f703eb730b727ec6","width":640,"height":320},{"url":"https://external-preview.redd.it/o-H6rWtrxMkGu6ppy9Z76PzirPIQLGU4dUkkkXovQww.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e280d37320d6a55abf3d1aaf76ad8b45d6c626d3","width":960,"height":480},{"url":"https://external-preview.redd.it/o-H6rWtrxMkGu6ppy9Z76PzirPIQLGU4dUkkkXovQww.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e4890c1abfcb9966afefbae98769a3fe282627bd","width":1080,"height":540}],"variants":{},"id":"o-H6rWtrxMkGu6ppy9Z76PzirPIQLGU4dUkkkXovQww"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"mod_note":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"num_reports":null,"removal_reason":null,"link_flair_background_color":"#ccac2b","id":"1lx2j1l","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"mags0ft","discussion_type":null,"num_comments":15,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lx2j1l/i_built_a_tool_to_run_humanitys_last_exam_on_your/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lx2j1l/i_built_a_tool_to_run_humanitys_last_exam_on_your/","subreddit_subscribers":497503,"created_utc":1752226420,"num_crossposts":1,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"50c36eba-fdca-11ee-9735-92a88d7e3b87","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"50c36eba-fdca-11ee-9735-92a88d7e3b87","distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2khims","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MidAirRunner","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2khdpu","score":1,"author_fullname":"t2_qwhykwm6l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yay!","edited":false,"author_flair_css_class":null,"name":"t1_n2khims","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Ollama"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yay!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lx2j1l","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx2j1l/i_built_a_tool_to_run_humanitys_last_exam_on_your/n2khims/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752248958,"author_flair_text":"Ollama","collapsed":false,"created_utc":1752248958,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2khdpu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mags0ft","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2iugul","score":1,"author_fullname":"t2_1tbbr8pu4s","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Hey there again, please see my [update](https://www.reddit.com/r/LocalLLaMA/comments/1lx2j1l/comment/n2kh0kh/) \\\\- I added OpenAI API support!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2khdpu","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey there again, please see my &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1lx2j1l/comment/n2kh0kh/\\"&gt;update&lt;/a&gt; - I added OpenAI API support!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx2j1l","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx2j1l/i_built_a_tool_to_run_humanitys_last_exam_on_your/n2khdpu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752248920,"author_flair_text":null,"treatment_tags":[],"created_utc":1752248920,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2iugul","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MidAirRunner","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2iu30h","score":8,"author_fullname":"t2_qwhykwm6l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Can't you make it compatible with the OpenAI API specifications? Then it'll work for all backends.","edited":1752229746,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2iugul","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"Ollama"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Can&amp;#39;t you make it compatible with the OpenAI API specifications? Then it&amp;#39;ll work for all backends.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx2j1l","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx2j1l/i_built_a_tool_to_run_humanitys_last_exam_on_your/n2iugul/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752228461,"author_flair_text":"Ollama","treatment_tags":[],"created_utc":1752228461,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2jo0jp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mags0ft","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2iyrsm","score":2,"author_fullname":"t2_1tbbr8pu4s","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I understand. I'll restructure the project to support different backends - an OpenAI API compatible backend and an Ollama backend. The user will be able to pick which one to use and the project will do the rest. This way, it should work just fine for everyone.\\n\\nBefore testing different models, make sure to have thoroughly read the README and pick a good judge model! I'll come back to you as soon as an OpenAI API backend is implemented.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n2jo0jp","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I understand. I&amp;#39;ll restructure the project to support different backends - an OpenAI API compatible backend and an Ollama backend. The user will be able to pick which one to use and the project will do the rest. This way, it should work just fine for everyone.&lt;/p&gt;\\n\\n&lt;p&gt;Before testing different models, make sure to have thoroughly read the README and pick a good judge model! I&amp;#39;ll come back to you as soon as an OpenAI API backend is implemented.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lx2j1l","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx2j1l/i_built_a_tool_to_run_humanitys_last_exam_on_your/n2jo0jp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752240371,"author_flair_text":null,"treatment_tags":[],"created_utc":1752240371,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2kh74l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mags0ft","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2iyrsm","score":2,"author_fullname":"t2_1tbbr8pu4s","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Please see my [update](https://www.reddit.com/r/LocalLLaMA/comments/1lx2j1l/comment/n2kh0kh/) \\\\- I added experimental OpenAI API support and if you'd like, you could help with testing! :)","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n2kh74l","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Please see my &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1lx2j1l/comment/n2kh0kh/\\"&gt;update&lt;/a&gt; - I added experimental OpenAI API support and if you&amp;#39;d like, you could help with testing! :)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lx2j1l","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx2j1l/i_built_a_tool_to_run_humanitys_last_exam_on_your/n2kh74l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752248869,"author_flair_text":null,"treatment_tags":[],"created_utc":1752248869,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2iyrsm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Lissanro","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2ix0cx","score":3,"author_fullname":"t2_fpfao9g","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes, OpenAI compatible API is very important. Ollama is very slow and has many issues, so I usually run using ik\\\\_llama.cpp when need GPU+CPU inference (for example, when using R1 671B IQ4\\\\_K\\\\_M) and TabbyAPI if can fully fit model in VRAM (like Mistral Large 123B 5bpw). Running benchmark at less than half of normal speed due to Ollama inefficiencies will not be practical, but I bookmarked your project and plan to give it a try once OpenAI API compatibility is there. My main interest besides just running the benchmark on a particular model is to compare quantization impact, to choose the best quant. Even though I have some tests of my own, an actual proper benchmark would be even better, so look forward to trying it out!","edited":false,"author_flair_css_class":null,"name":"t1_n2iyrsm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, OpenAI compatible API is very important. Ollama is very slow and has many issues, so I usually run using ik_llama.cpp when need GPU+CPU inference (for example, when using R1 671B IQ4_K_M) and TabbyAPI if can fully fit model in VRAM (like Mistral Large 123B 5bpw). Running benchmark at less than half of normal speed due to Ollama inefficiencies will not be practical, but I bookmarked your project and plan to give it a try once OpenAI API compatibility is there. My main interest besides just running the benchmark on a particular model is to compare quantization impact, to choose the best quant. Even though I have some tests of my own, an actual proper benchmark would be even better, so look forward to trying it out!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lx2j1l","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx2j1l/i_built_a_tool_to_run_humanitys_last_exam_on_your/n2iyrsm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752230644,"author_flair_text":null,"collapsed":false,"created_utc":1752230644,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ix0cx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mags0ft","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2iulp9","score":1,"author_fullname":"t2_1tbbr8pu4s","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Good point, I could just add generic OpenAI compatibility as a secondary method for querying models. That'd add compatibility for LMStudio models as well as many others. I'm probably gonna go with that one. However, I still want to keep Ollama as the primary focus (because the script by the HLE authors already supplies OpenAI compatibility and that'd be too much of reinventing the wheel haha)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ix0cx","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Good point, I could just add generic OpenAI compatibility as a secondary method for querying models. That&amp;#39;d add compatibility for LMStudio models as well as many others. I&amp;#39;m probably gonna go with that one. However, I still want to keep Ollama as the primary focus (because the script by the HLE authors already supplies OpenAI compatibility and that&amp;#39;d be too much of reinventing the wheel haha)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx2j1l","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx2j1l/i_built_a_tool_to_run_humanitys_last_exam_on_your/n2ix0cx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752229767,"author_flair_text":null,"treatment_tags":[],"created_utc":1752229767,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2iulp9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Cool-Chemical-5629","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2iu30h","score":1,"author_fullname":"t2_qz1qjc86","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I meant the actual LM Studio app running locally. Can't you just connect to the OpenAI API server it provides?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2iulp9","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I meant the actual LM Studio app running locally. Can&amp;#39;t you just connect to the OpenAI API server it provides?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx2j1l","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx2j1l/i_built_a_tool_to_run_humanitys_last_exam_on_your/n2iulp9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752228533,"author_flair_text":null,"treatment_tags":[],"created_utc":1752228533,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2iu30h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mags0ft","can_mod_post":false,"created_utc":1752228254,"send_replies":true,"parent_id":"t1_n2it898","score":3,"author_fullname":"t2_1tbbr8pu4s","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Adding LMStudio support is actually a really good idea. Would kind of go against the name of the project, but that shouldn't be a problem. I'm on it! Thanks for the suggestion. Their SDK seems to support everything I need to make it work.\\n\\nUpdate: I implemented it as an experimental feature - more info [here](https://www.reddit.com/r/LocalLLaMA/comments/1lx2j1l/comment/n2kh0kh/)!","edited":1752252207,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2iu30h","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Adding LMStudio support is actually a really good idea. Would kind of go against the name of the project, but that shouldn&amp;#39;t be a problem. I&amp;#39;m on it! Thanks for the suggestion. Their SDK seems to support everything I need to make it work.&lt;/p&gt;\\n\\n&lt;p&gt;Update: I implemented it as an experimental feature - more info &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1lx2j1l/comment/n2kh0kh/\\"&gt;here&lt;/a&gt;!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx2j1l","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx2j1l/i_built_a_tool_to_run_humanitys_last_exam_on_your/n2iu30h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752228254,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2kh0kh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mags0ft","can_mod_post":false,"created_utc":1752248818,"send_replies":true,"parent_id":"t1_n2it898","score":3,"author_fullname":"t2_1tbbr8pu4s","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"**UPDATE**: I now added OpenAI API support which can be used by specifying \`--backend=openai\` and setting the environment variables \`HLE_EVAL_API_KEY\` and \`HLE_EVAL_ENDPOINT\` to their respective values, which allows you to use LMStudio, OpenRouter, Gemini, ChatGPT etc.!\\n\\nAs I just added this, it's still in a development branch and a little experimental (make sure to read the README), but I'll merge it into \`main\` soon after making sure it all works fine.\\n\\nFor now, you can find the OpenAI API compatible version here: [https://github.com/mags0ft/hle-eval-ollama/tree/openai-api-support](https://github.com/mags0ft/hle-eval-ollama/tree/openai-api-support)\\n\\nThanks again for the feedback!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2kh0kh","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;strong&gt;UPDATE&lt;/strong&gt;: I now added OpenAI API support which can be used by specifying &lt;code&gt;--backend=openai&lt;/code&gt; and setting the environment variables &lt;code&gt;HLE_EVAL_API_KEY&lt;/code&gt; and &lt;code&gt;HLE_EVAL_ENDPOINT&lt;/code&gt; to their respective values, which allows you to use LMStudio, OpenRouter, Gemini, ChatGPT etc.!&lt;/p&gt;\\n\\n&lt;p&gt;As I just added this, it&amp;#39;s still in a development branch and a little experimental (make sure to read the README), but I&amp;#39;ll merge it into &lt;code&gt;main&lt;/code&gt; soon after making sure it all works fine.&lt;/p&gt;\\n\\n&lt;p&gt;For now, you can find the OpenAI API compatible version here: &lt;a href=\\"https://github.com/mags0ft/hle-eval-ollama/tree/openai-api-support\\"&gt;https://github.com/mags0ft/hle-eval-ollama/tree/openai-api-support&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Thanks again for the feedback!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx2j1l","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx2j1l/i_built_a_tool_to_run_humanitys_last_exam_on_your/n2kh0kh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752248818,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2l4vhh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HilLiedTroopsDied","can_mod_post":false,"created_utc":1752255503,"send_replies":true,"parent_id":"t1_n2it898","score":0,"author_fullname":"t2_1snfn3ui","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"this hits like \\"no windows .exe no care\\"","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2l4vhh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;this hits like &amp;quot;no windows .exe no care&amp;quot;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx2j1l","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx2j1l/i_built_a_tool_to_run_humanitys_last_exam_on_your/n2l4vhh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752255503,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n2it898","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Cool-Chemical-5629","can_mod_post":false,"created_utc":1752227800,"send_replies":true,"parent_id":"t3_1lx2j1l","score":5,"author_fullname":"t2_qz1qjc86","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This is a great idea, but with no LM Studio support, this will be pretty hard for many users to actually use. Also, it'd be worth having some sort of leaderboard where people could upload their results to share with the rest of us.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2it898","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is a great idea, but with no LM Studio support, this will be pretty hard for many users to actually use. Also, it&amp;#39;d be worth having some sort of leaderboard where people could upload their results to share with the rest of us.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx2j1l/i_built_a_tool_to_run_humanitys_last_exam_on_your/n2it898/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752227800,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lx2j1l","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2jtca4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mags0ft","can_mod_post":false,"created_utc":1752242040,"send_replies":true,"parent_id":"t1_n2jp0ss","score":1,"author_fullname":"t2_1tbbr8pu4s","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Really good question! There isn't one exact answer, but several ones; first of all, as stated in the post, this project is more thoroughly documented and extensible. The next release features the ability to use OpenAI-compatible endpoints as well, so it's just as interoperable as the HLE script. However, it has been modularized into several smaller files and is much better maintainable. The HLE script is minimal, made to work with HLE only and does its job, but isn't viable as an all-rounder for local model benchmarking when it comes to output quality. This project aims to improve that.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2jtca4","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Really good question! There isn&amp;#39;t one exact answer, but several ones; first of all, as stated in the post, this project is more thoroughly documented and extensible. The next release features the ability to use OpenAI-compatible endpoints as well, so it&amp;#39;s just as interoperable as the HLE script. However, it has been modularized into several smaller files and is much better maintainable. The HLE script is minimal, made to work with HLE only and does its job, but isn&amp;#39;t viable as an all-rounder for local model benchmarking when it comes to output quality. This project aims to improve that.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx2j1l","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx2j1l/i_built_a_tool_to_run_humanitys_last_exam_on_your/n2jtca4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752242040,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2jp0ss","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"henfiber","can_mod_post":false,"created_utc":1752240698,"send_replies":true,"parent_id":"t3_1lx2j1l","score":2,"author_fullname":"t2_lw9me25","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Since the HLE authors provide benchmarking scripts for OpenAI-compatible endpoints and Ollama (and all other local inference tools) have an OpenAI-compatible API, what's the benefit of using this?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2jp0ss","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Since the HLE authors provide benchmarking scripts for OpenAI-compatible endpoints and Ollama (and all other local inference tools) have an OpenAI-compatible API, what&amp;#39;s the benefit of using this?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx2j1l/i_built_a_tool_to_run_humanitys_last_exam_on_your/n2jp0ss/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752240698,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lx2j1l","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),n=()=>e.jsx(t,{data:l});export{n as default};
