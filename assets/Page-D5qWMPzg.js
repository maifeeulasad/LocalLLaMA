import{j as e}from"./index-DLSqWzaI.js";import{R as l}from"./RedditPostRenderer-CysRo2D_.js";import"./index-COXiL3Lo.js";const a=JSON.parse('[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hey all,\\nOur hospital has a ~$20,000 budget to build a local system for running health/medical data analytics using LLMs, with occasional vision tasks (via MCP) and fine-tuning.\\n\\nI do currently have a gemma3-med:27b  and Gemma3, Qwen3 on my 5090 test server and performing pretty good \\n\\nWe’re looking for advice on:\\n\\t1.\\tWhat’s the best and largest LLM you’d recommend we can reasonably run and fine-tune within this budget (open-source preferred)? Use cases include medical Q&amp;A, clinical summarization, and structured data analysis.\\n\\t2.\\tWhich GPU setup is optimal? Should we go for multiple RTX 5090s or consider the RTX 6000 Ada/Pro series, depending on model needs?\\n\\nAny input on model + hardware balance would be greatly appreciated! Bonus points for setups that support mixed workloads (text + vision) or are friendly for continuous experimentation.\\n\\nThanks!\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Best LLM (and setup) recommendation for $20k health analytics project (LLM + some vision + fine-tuning)","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lwe0gn","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.6,"author_flair_background_color":null,"subreddit_type":"public","ups":1,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_e4ojre534","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":1,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1752188371,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752157381,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey all,\\nOur hospital has a ~$20,000 budget to build a local system for running health/medical data analytics using LLMs, with occasional vision tasks (via MCP) and fine-tuning.&lt;/p&gt;\\n\\n&lt;p&gt;I do currently have a gemma3-med:27b  and Gemma3, Qwen3 on my 5090 test server and performing pretty good &lt;/p&gt;\\n\\n&lt;p&gt;We’re looking for advice on:\\n    1.  What’s the best and largest LLM you’d recommend we can reasonably run and fine-tune within this budget (open-source preferred)? Use cases include medical Q&amp;amp;A, clinical summarization, and structured data analysis.\\n    2.  Which GPU setup is optimal? Should we go for multiple RTX 5090s or consider the RTX 6000 Ada/Pro series, depending on model needs?&lt;/p&gt;\\n\\n&lt;p&gt;Any input on model + hardware balance would be greatly appreciated! Bonus points for setups that support mixed workloads (text + vision) or are friendly for continuous experimentation.&lt;/p&gt;\\n\\n&lt;p&gt;Thanks!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lwe0gn","is_robot_indexable":true,"num_duplicates":1,"report_reasons":null,"author":"LeastExperience1579","discussion_type":null,"num_comments":10,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lwe0gn/best_llm_and_setup_recommendation_for_20k_health/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lwe0gn/best_llm_and_setup_recommendation_for_20k_health/","subreddit_subscribers":497505,"created_utc":1752157381,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2gedj1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Capable-Ad-7494","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2g8tmp","score":5,"author_fullname":"t2_9so78ol2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Full finetuning a large model like a 32b itself is on the scale of of 1600 dollars for a day of compute with h100’s, and lora finetuning isn’t ideal for implanting knowledge since it is limited in how much it can express. Full Finetuning isn’t great either, since you say your training on books, and that means you will need to preprocess this data, deal with OCR issues, and likely make synthetic user assistant pairs for a post-train environment to ensure it won’t act as a completions model\\n\\nMost people would have you implement RAG with an embedding model + reranker and i agree with them here.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2gedj1","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Full finetuning a large model like a 32b itself is on the scale of of 1600 dollars for a day of compute with h100’s, and lora finetuning isn’t ideal for implanting knowledge since it is limited in how much it can express. Full Finetuning isn’t great either, since you say your training on books, and that means you will need to preprocess this data, deal with OCR issues, and likely make synthetic user assistant pairs for a post-train environment to ensure it won’t act as a completions model&lt;/p&gt;\\n\\n&lt;p&gt;Most people would have you implement RAG with an embedding model + reranker and i agree with them here.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwe0gn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwe0gn/best_llm_and_setup_recommendation_for_20k_health/n2gedj1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752189837,"author_flair_text":null,"treatment_tags":[],"created_utc":1752189837,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2kip46","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LeastExperience1579","can_mod_post":false,"created_utc":1752249290,"send_replies":true,"parent_id":"t1_n2k8grf","score":1,"author_fullname":"t2_e4ojre534","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thank you I’ll do my best to comment here.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n2kip46","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you I’ll do my best to comment here.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lwe0gn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwe0gn/best_llm_and_setup_recommendation_for_20k_health/n2kip46/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752249290,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2k8grf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"__JockY__","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2gk5lx","score":2,"author_fullname":"t2_qf8h7ka8","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I tend to ignore DMs to be honest. Nothing good has ever come of them for me on Reddit. However, if I see one from LeastExperience1579 I’ll try to resist the urge to immediately delete it!","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n2k8grf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I tend to ignore DMs to be honest. Nothing good has ever come of them for me on Reddit. However, if I see one from LeastExperience1579 I’ll try to resist the urge to immediately delete it!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lwe0gn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwe0gn/best_llm_and_setup_recommendation_for_20k_health/n2k8grf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752246445,"author_flair_text":null,"treatment_tags":[],"created_utc":1752246445,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2gk5lx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LeastExperience1579","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2gjg9b","score":1,"author_fullname":"t2_e4ojre534","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thank you a lot !!\\nWe are located in Taiwan and could I DM you for some future questions? Thanks!","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n2gk5lx","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you a lot !!\\nWe are located in Taiwan and could I DM you for some future questions? Thanks!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lwe0gn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwe0gn/best_llm_and_setup_recommendation_for_20k_health/n2gk5lx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752191773,"author_flair_text":null,"treatment_tags":[],"created_utc":1752191773,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2gjg9b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"__JockY__","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2gii4z","score":2,"author_fullname":"t2_qf8h7ka8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"My pleasure, I hope it was useful. \\n\\nMy final piece of advice for you was hard earned the expensive way: always buy more VRAM than you think you’ll need. I would have saved many thousands of dollars had I the power of foresight and taken my later advice earlier, if that makes sense.\\n\\nIt appears you have budget to do it the right way with room for future (as yet undefined) requirements. Do it the right way.","edited":false,"author_flair_css_class":null,"name":"t1_n2gjg9b","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;My pleasure, I hope it was useful. &lt;/p&gt;\\n\\n&lt;p&gt;My final piece of advice for you was hard earned the expensive way: always buy more VRAM than you think you’ll need. I would have saved many thousands of dollars had I the power of foresight and taken my later advice earlier, if that makes sense.&lt;/p&gt;\\n\\n&lt;p&gt;It appears you have budget to do it the right way with room for future (as yet undefined) requirements. Do it the right way.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lwe0gn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwe0gn/best_llm_and_setup_recommendation_for_20k_health/n2gjg9b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752191535,"author_flair_text":null,"collapsed":false,"created_utc":1752191535,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2gii4z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LeastExperience1579","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2ghbqb","score":1,"author_fullname":"t2_e4ojre534","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thanks for the advice.\\n\\nOur current build is 5090 + open web ui RAG. We are mainly using it for editing medical records and would help use to do some diagnoses in the future. Maybe we can do some fine tuning on Llama4 with unsloth.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2gii4z","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for the advice.&lt;/p&gt;\\n\\n&lt;p&gt;Our current build is 5090 + open web ui RAG. We are mainly using it for editing medical records and would help use to do some diagnoses in the future. Maybe we can do some fine tuning on Llama4 with unsloth.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwe0gn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwe0gn/best_llm_and_setup_recommendation_for_20k_health/n2gii4z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752191214,"author_flair_text":null,"treatment_tags":[],"created_utc":1752191214,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ghbqb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"__JockY__","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2g8tmp","score":2,"author_fullname":"t2_qf8h7ka8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I have no knowledge of the medical field, I defer to your experience of LLMs in that context.\\n\\nFor the rest of the corpus of your work a 96GB GPU opens up the possibility of fine-tuning some truly large and capable models. For example, Unsloth can fine tune Llama 3.1 70B using 80GB VRAM. That’s a _monster_ of a model on which to build your research!","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2ghbqb","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have no knowledge of the medical field, I defer to your experience of LLMs in that context.&lt;/p&gt;\\n\\n&lt;p&gt;For the rest of the corpus of your work a 96GB GPU opens up the possibility of fine-tuning some truly large and capable models. For example, Unsloth can fine tune Llama 3.1 70B using 80GB VRAM. That’s a &lt;em&gt;monster&lt;/em&gt; of a model on which to build your research!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwe0gn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwe0gn/best_llm_and_setup_recommendation_for_20k_health/n2ghbqb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752190820,"author_flair_text":null,"treatment_tags":[],"created_utc":1752190820,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2g8tmp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LeastExperience1579","can_mod_post":false,"created_utc":1752188036,"send_replies":true,"parent_id":"t1_n2el2qp","score":1,"author_fullname":"t2_e4ojre534","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks.\\nHow about the LLM on it ?\\nThe best we are currently running on 5090 is gemma3 27b .\\nWhat are some models we can try out for medical content ? Maybe we can fine tune them on our massive medical books.","edited":1752188336,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2g8tmp","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks.\\nHow about the LLM on it ?\\nThe best we are currently running on 5090 is gemma3 27b .\\nWhat are some models we can try out for medical content ? Maybe we can fine tune them on our massive medical books.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwe0gn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwe0gn/best_llm_and_setup_recommendation_for_20k_health/n2g8tmp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752188036,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2el2qp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"__JockY__","can_mod_post":false,"created_utc":1752170418,"send_replies":true,"parent_id":"t3_1lwe0gn","score":4,"author_fullname":"t2_qf8h7ka8","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"$9k on a 96GB Blackwell PRO 6000 and the other $11k on an Epyc 9xx5 DDR5 server to support it.\\n\\nThis way you have future-proofed yourself for a few years and you have a MIG-capable SOTA GPU to run both standard and multi-modal LLMs at blazing speeds.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2el2qp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;$9k on a 96GB Blackwell PRO 6000 and the other $11k on an Epyc 9xx5 DDR5 server to support it.&lt;/p&gt;\\n\\n&lt;p&gt;This way you have future-proofed yourself for a few years and you have a MIG-capable SOTA GPU to run both standard and multi-modal LLMs at blazing speeds.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwe0gn/best_llm_and_setup_recommendation_for_20k_health/n2el2qp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752170418,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwe0gn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2edacl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Shivacious","can_mod_post":false,"created_utc":1752168329,"send_replies":true,"parent_id":"t3_1lwe0gn","score":3,"author_fullname":"t2_chxnc83m9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"cohore only. op cohore, u would like something trained on verified dataset only that is good. it has one of least hallucination rate (ctx rate we also say). happy to help. further u can slide into my dms or ask here","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2edacl","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;cohore only. op cohore, u would like something trained on verified dataset only that is good. it has one of least hallucination rate (ctx rate we also say). happy to help. further u can slide into my dms or ask here&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwe0gn/best_llm_and_setup_recommendation_for_20k_health/n2edacl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752168329,"author_flair_text":"Llama 405B","treatment_tags":[],"link_id":"t3_1lwe0gn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}}]'),r=()=>e.jsx(l,{data:a});export{r as default};
