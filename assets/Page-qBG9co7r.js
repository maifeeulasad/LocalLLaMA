import{j as e}from"./index-cvG704yx.js";import{R as l}from"./RedditPostRenderer-CBthLTAH.js";import"./index-D-GavSZU.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I'm looking to get a total of at least 140 GB RAM/VRAM combined to run Qwen 235B Q4. Current i have 96 GB RAM so next step is to get some cheap VRAM. After some research i found the following options at around 1000$ each:   \\n\\n1. 4x RTX 3060 (48 GB)\\n2. 4x P100 (64 GB)\\n3. 3x P40 (72 GB)\\n4. 3x RX 9060 (48 GB)\\n5. 4x MI50 32GB (128GB)\\n6. 3x RTX 4060 ti/5060 ti (48 GB)\\n\\nEdit: add more suggestion from comments.  \\n\\nWhich GPU do you recommend or is there anything else better? I know 3090 is king here but cost per GB is around double the above GPU. Any suggestion is appreciated.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Cheapest way to stack VRAM in 2025?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1ltamap","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.93,"author_flair_background_color":null,"subreddit_type":"public","ups":199,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_giuiz","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":199,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1751835244,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751832289,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m looking to get a total of at least 140 GB RAM/VRAM combined to run Qwen 235B Q4. Current i have 96 GB RAM so next step is to get some cheap VRAM. After some research i found the following options at around 1000$ each:   &lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;4x RTX 3060 (48 GB)&lt;/li&gt;\\n&lt;li&gt;4x P100 (64 GB)&lt;/li&gt;\\n&lt;li&gt;3x P40 (72 GB)&lt;/li&gt;\\n&lt;li&gt;3x RX 9060 (48 GB)&lt;/li&gt;\\n&lt;li&gt;4x MI50 32GB (128GB)&lt;/li&gt;\\n&lt;li&gt;3x RTX 4060 ti/5060 ti (48 GB)&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;Edit: add more suggestion from comments.  &lt;/p&gt;\\n\\n&lt;p&gt;Which GPU do you recommend or is there anything else better? I know 3090 is king here but cost per GB is around double the above GPU. Any suggestion is appreciated.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1ltamap","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"gnad","discussion_type":null,"num_comments":151,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/","subreddit_subscribers":496034,"created_utc":1751832289,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1rc2p2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Lowkey_LokiSN","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1ow9rr","score":17,"author_fullname":"t2_xg2jtdg74","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Contrary to popular belief, there's a [really good community-driven initiative](https://rdn-id.com) providing driver support for MI50s on Windows.  \\nI use my MI50 to actually play games on Windows and its performance is similar to that (might even say slightly better) of a Radeon VII. I've shared more about it in my recent post.\\n\\nIf Windows driver support is really what's stopping you, think you should be good\\n\\nEdit:  \\nJust to clarify, you need the Chinese version of the card if you plan to enable its display port  \\nOR  \\nIf you already have a CPU/GPU with graphics output, the above drivers is everything you need","edited":1751865478,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1rc2p2","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Contrary to popular belief, there&amp;#39;s a &lt;a href=\\"https://rdn-id.com\\"&gt;really good community-driven initiative&lt;/a&gt; providing driver support for MI50s on Windows.&lt;br/&gt;\\nI use my MI50 to actually play games on Windows and its performance is similar to that (might even say slightly better) of a Radeon VII. I&amp;#39;ve shared more about it in my recent post.&lt;/p&gt;\\n\\n&lt;p&gt;If Windows driver support is really what&amp;#39;s stopping you, think you should be good&lt;/p&gt;\\n\\n&lt;p&gt;Edit:&lt;br/&gt;\\nJust to clarify, you need the Chinese version of the card if you plan to enable its display port&lt;br/&gt;\\nOR&lt;br/&gt;\\nIf you already have a CPU/GPU with graphics output, the above drivers is everything you need&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1rc2p2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751865103,"author_flair_text":null,"treatment_tags":[],"created_utc":1751865103,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":17}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1p2s7w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"HugoCortell","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1p158r","score":9,"author_fullname":"t2_61s8b5gv","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Oh yeah, that too, but there are cheap solutions on ali. They can be 3D printed too. Or just tape a fan to it lol.","edited":false,"author_flair_css_class":null,"name":"t1_n1p2s7w","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Oh yeah, that too, but there are cheap solutions on ali. They can be 3D printed too. Or just tape a fan to it lol.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1ltamap","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1p2s7w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751834659,"author_flair_text":null,"collapsed":false,"created_utc":1751834659,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1r8mj7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rorowhat","can_mod_post":false,"created_utc":1751863407,"send_replies":true,"parent_id":"t1_n1qzc4v","score":1,"author_fullname":"t2_yq51a","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ah yes, my case is not long enough to support that tail. Thanks tho","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n1r8mj7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ah yes, my case is not long enough to support that tail. Thanks tho&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1ltamap","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1r8mj7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751863407,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1qzc4v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1qy3w7","score":4,"author_fullname":"t2_o65i6kx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Watch the video in this listing.\\n\\nhttps://www.alibaba.com/product-detail/New-AMD-MI50-32G-GPU-Accelerated_1601496082485.html","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n1qzc4v","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Watch the video in this listing.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.alibaba.com/product-detail/New-AMD-MI50-32G-GPU-Accelerated_1601496082485.html\\"&gt;https://www.alibaba.com/product-detail/New-AMD-MI50-32G-GPU-Accelerated_1601496082485.html&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1ltamap","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1qzc4v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751859197,"author_flair_text":null,"treatment_tags":[],"created_utc":1751859197,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1qzb1j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1qy3w7","score":1,"author_fullname":"t2_o65i6kx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I can try. Respond back to let me know you can see it. I'll put in another response just in case it gets shadowed. And thus you would never see this post either.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n1qzb1j","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I can try. Respond back to let me know you can see it. I&amp;#39;ll put in another response just in case it gets shadowed. And thus you would never see this post either.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1ltamap","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1qzb1j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751859184,"author_flair_text":null,"treatment_tags":[],"created_utc":1751859184,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1qy3w7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rorowhat","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1paw68","score":1,"author_fullname":"t2_yq51a","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Can you share a link to what you're talking about?","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n1qy3w7","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Can you share a link to what you&amp;#39;re talking about?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1ltamap","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1qy3w7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751858678,"author_flair_text":null,"treatment_tags":[],"created_utc":1751858678,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1ujyma","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FunnyAsparagus1253","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1paw68","score":1,"author_fullname":"t2_i6c8tay3w","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yep those slot fans are the perfect size","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n1ujyma","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yep those slot fans are the perfect size&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1ltamap","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1ujyma/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751911974,"author_flair_text":null,"treatment_tags":[],"created_utc":1751911974,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1paw68","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1p158r","score":7,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Get a $10 PC slot fan, rip off the grill and then shove it into the end of it. Works great. That's what I do with my AMD datacenter GPUs. I think in one of the Mi50 ads, they have a picture of exactly this.","edited":false,"author_flair_css_class":null,"name":"t1_n1paw68","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Get a $10 PC slot fan, rip off the grill and then shove it into the end of it. Works great. That&amp;#39;s what I do with my AMD datacenter GPUs. I think in one of the Mi50 ads, they have a picture of exactly this.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1ltamap","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1paw68/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751837178,"author_flair_text":null,"collapsed":false,"created_utc":1751837178,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1ryiv1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Aphid_red","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1p158r","score":1,"author_fullname":"t2_csn2q","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You could look for a server. The G292-Z20 (I've seen them from $1000-$1500) can house 8 of them and gets you a good CPU, power supplies, etc. Should be compatible with better GPUs too if you want to upgrade at some point.","edited":false,"author_flair_css_class":null,"name":"t1_n1ryiv1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You could look for a server. The G292-Z20 (I&amp;#39;ve seen them from $1000-$1500) can house 8 of them and gets you a good CPU, power supplies, etc. Should be compatible with better GPUs too if you want to upgrade at some point.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1ltamap","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1ryiv1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751877859,"author_flair_text":null,"collapsed":false,"created_utc":1751877859,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1p158r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"rorowhat","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1ozrsi","score":12,"author_fullname":"t2_yq51a","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Also you need a way to keep them cool, as they don't have fans","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1p158r","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Also you need a way to keep them cool, as they don&amp;#39;t have fans&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1p158r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751834164,"author_flair_text":null,"treatment_tags":[],"created_utc":1751834164,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1pk7le","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullstackSensei","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1pdbag","score":1,"author_fullname":"t2_17n3nqtj56","approved_by":null,"mod_note":null,"all_awardings":[],"body":"If we're talking about AMD, you run this risk with driver instability or bugs continuing forever even when the hardware is supposedly supported and receiving updates. Radeon cards are nutritious for these issues in games over the years.\\n\\nFor ML workloads, I'd say it's actually the reverse. The subset of hardware instructions used in ML is quite small compared to 3D rendering or general HPC comoute. That's why Geohot et all have been able to hack their way around the Asmedia Thunderbolt chioset and run compute kernels on Radeon cards over USB 3.\\n\\nIf the cards are working today with existing models, there's a practically zero chance something will break in the future because of driver issues. One exception might be motherboard or CPU upgrade (to an entirely different socket or platform ) down the line, but given the price of these cards, I don't see why someone would do such an upgrade after building their system around those cards.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n1pk7le","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If we&amp;#39;re talking about AMD, you run this risk with driver instability or bugs continuing forever even when the hardware is supposedly supported and receiving updates. Radeon cards are nutritious for these issues in games over the years.&lt;/p&gt;\\n\\n&lt;p&gt;For ML workloads, I&amp;#39;d say it&amp;#39;s actually the reverse. The subset of hardware instructions used in ML is quite small compared to 3D rendering or general HPC comoute. That&amp;#39;s why Geohot et all have been able to hack their way around the Asmedia Thunderbolt chioset and run compute kernels on Radeon cards over USB 3.&lt;/p&gt;\\n\\n&lt;p&gt;If the cards are working today with existing models, there&amp;#39;s a practically zero chance something will break in the future because of driver issues. One exception might be motherboard or CPU upgrade (to an entirely different socket or platform ) down the line, but given the price of these cards, I don&amp;#39;t see why someone would do such an upgrade after building their system around those cards.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1ltamap","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1pk7le/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751840276,"author_flair_text":null,"treatment_tags":[],"created_utc":1751840276,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1pdbag","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HugoCortell","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1pc1ay","score":3,"author_fullname":"t2_61s8b5gv","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That's what I was mostly talking about, from what I've heard the drivers got dropped before all the kinks could be worked out. Less of an optimization issue and more of a \\"some times the card just crashes if you have the calculator open while browsing the web and we can't figure out why\\" (&lt;-- not a real example) problem.\\n\\nThough I assume for ML in specific, optimization drivers do matter too. More efficient handling of instructions, or better compatibility with new architectures can make a difference.","edited":false,"author_flair_css_class":null,"name":"t1_n1pdbag","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s what I was mostly talking about, from what I&amp;#39;ve heard the drivers got dropped before all the kinks could be worked out. Less of an optimization issue and more of a &amp;quot;some times the card just crashes if you have the calculator open while browsing the web and we can&amp;#39;t figure out why&amp;quot; (&amp;lt;-- not a real example) problem.&lt;/p&gt;\\n\\n&lt;p&gt;Though I assume for ML in specific, optimization drivers do matter too. More efficient handling of instructions, or better compatibility with new architectures can make a difference.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1ltamap","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1pdbag/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751837969,"author_flair_text":null,"collapsed":false,"created_utc":1751837969,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1vvtv1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullstackSensei","can_mod_post":false,"created_utc":1751927692,"send_replies":true,"parent_id":"t1_n1vitzn","score":2,"author_fullname":"t2_17n3nqtj56","approved_by":null,"mod_note":null,"all_awardings":[],"body":"How is DLSS relevant to LLM inference? And how does no support for new CUDA lib affect old hardware that doesn't support the hardware features required for said new CUDA lib? And as I said in my comment llama.cpp still supports CUDA 11 three years after it reached EoL. Llama.cpp also has custom CUDA kernels that bring a lot of features like flash attention to older architectures that CUDA libs never supported.\\n\\nLike everything in life, it's a trade-off. If you have a ton of money to spend on GPUs, you can buy half a dozen or more of  the latest and greatest from Nvidia or AMD. But some of us don't, or chose not to spend absurd amounts of money on GPUs.\\n\\nMy eight water cooled P40s, and the entire rig I built around them with 44 cores and 512GB RAM cost less than a single 5090. The 5090 is faster for models that fit in VRAM, but I can run Qwen 3 235B Q5_K_XL with a ton of context entirely in VRAM.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n1vvtv1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How is DLSS relevant to LLM inference? And how does no support for new CUDA lib affect old hardware that doesn&amp;#39;t support the hardware features required for said new CUDA lib? And as I said in my comment llama.cpp still supports CUDA 11 three years after it reached EoL. Llama.cpp also has custom CUDA kernels that bring a lot of features like flash attention to older architectures that CUDA libs never supported.&lt;/p&gt;\\n\\n&lt;p&gt;Like everything in life, it&amp;#39;s a trade-off. If you have a ton of money to spend on GPUs, you can buy half a dozen or more of  the latest and greatest from Nvidia or AMD. But some of us don&amp;#39;t, or chose not to spend absurd amounts of money on GPUs.&lt;/p&gt;\\n\\n&lt;p&gt;My eight water cooled P40s, and the entire rig I built around them with 44 cores and 512GB RAM cost less than a single 5090. The 5090 is faster for models that fit in VRAM, but I can run Qwen 3 235B Q5_K_XL with a ton of context entirely in VRAM.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1ltamap","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1vvtv1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751927692,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1vitzn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"howardhus","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1uufuf","score":0,"author_fullname":"t2_1zmj3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"every driver increases cuda level. old drivers wont support new cuda lib.\\n\\n\\nalso drivers bring nee freatures like\\n\\n\\nhttps://www.reddit.com/r/HuntShowdown/comments/1idoq25/dlss_4_is_here_with_the_nvidias_57216_drivers/\\n\\nfor those not living under a rock :)","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n1vitzn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;every driver increases cuda level. old drivers wont support new cuda lib.&lt;/p&gt;\\n\\n&lt;p&gt;also drivers bring nee freatures like&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.reddit.com/r/HuntShowdown/comments/1idoq25/dlss_4_is_here_with_the_nvidias_57216_drivers/\\"&gt;https://www.reddit.com/r/HuntShowdown/comments/1idoq25/dlss_4_is_here_with_the_nvidias_57216_drivers/&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;for those not living under a rock :)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1ltamap","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1vitzn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751923542,"author_flair_text":null,"treatment_tags":[],"created_utc":1751923542,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n1uufuf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullstackSensei","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1upb96","score":2,"author_fullname":"t2_17n3nqtj56","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Give me a single AI optimization done by Nvidia at the driver or CUDA SDK in the past three or four years. Even Nvidia themselves have announced that Pascal has been \\"feature complete\\" in the CUDA Toolkit for a long time.\\n\\nBut the conversation here isn't about CUDA, it's about AMD and driver support. ROCm support has been shite even with the latest silicon from AMD. That's why most people using AMD GPUs for local inference rely more on vulkan, which brings me back to what I said about new drivers dropping support not making any sense.\\n\\nYou can keep parroting this nonsensical argument about expensive room heaters all you want. Meanwhile, the rest of us will enjoy actually running LLMs using those cards without issues. People like you told me the same over a year and a half ago when I asked if I should get P40s for $100 a pop. Fortunately I knew better and bought them anyway, and they've been running rock solid with llama.cpp without any issues since. CUDA 11 was discontinued 3 years ago, and with yet support for Keplar and Maxwell, yet llama.cpp still supports CUDA 11. If you don't have the cash or can find the 24GB M40 for cheap where you live, dismissing it as an inference option just because CUDA 12 doesn't support it is not very smart, even if it's not as efficient nor as fast as Ampere, Ada, or Blackwell.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n1uufuf","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Give me a single AI optimization done by Nvidia at the driver or CUDA SDK in the past three or four years. Even Nvidia themselves have announced that Pascal has been &amp;quot;feature complete&amp;quot; in the CUDA Toolkit for a long time.&lt;/p&gt;\\n\\n&lt;p&gt;But the conversation here isn&amp;#39;t about CUDA, it&amp;#39;s about AMD and driver support. ROCm support has been shite even with the latest silicon from AMD. That&amp;#39;s why most people using AMD GPUs for local inference rely more on vulkan, which brings me back to what I said about new drivers dropping support not making any sense.&lt;/p&gt;\\n\\n&lt;p&gt;You can keep parroting this nonsensical argument about expensive room heaters all you want. Meanwhile, the rest of us will enjoy actually running LLMs using those cards without issues. People like you told me the same over a year and a half ago when I asked if I should get P40s for $100 a pop. Fortunately I knew better and bought them anyway, and they&amp;#39;ve been running rock solid with llama.cpp without any issues since. CUDA 11 was discontinued 3 years ago, and with yet support for Keplar and Maxwell, yet llama.cpp still supports CUDA 11. If you don&amp;#39;t have the cash or can find the 24GB M40 for cheap where you live, dismissing it as an inference option just because CUDA 12 doesn&amp;#39;t support it is not very smart, even if it&amp;#39;s not as efficient nor as fast as Ampere, Ada, or Blackwell.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1ltamap","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1uufuf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751915366,"author_flair_text":null,"treatment_tags":[],"created_utc":1751915366,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1upb96","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"howardhus","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1pc1ay","score":1,"author_fullname":"t2_1zmj3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"what you say apply to displaying images a bit. somewhat to games and you are fully wrong on AI.\\n\\non ganes you do get new features. nvidia does this a lot.\\n\\nfor AI this is crucial. new drivers bring supprt for libraries.\\n\\npytorch needs a new cuda version. cuda 11 will be dropped soon with cuda13 about to come out.\\nthen you can not run newer ai models anymore.\\n\\ngetting a card with barely working old drivers for AI is buying an expensive room heater","edited":false,"author_flair_css_class":null,"name":"t1_n1upb96","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;what you say apply to displaying images a bit. somewhat to games and you are fully wrong on AI.&lt;/p&gt;\\n\\n&lt;p&gt;on ganes you do get new features. nvidia does this a lot.&lt;/p&gt;\\n\\n&lt;p&gt;for AI this is crucial. new drivers bring supprt for libraries.&lt;/p&gt;\\n\\n&lt;p&gt;pytorch needs a new cuda version. cuda 11 will be dropped soon with cuda13 about to come out.\\nthen you can not run newer ai models anymore.&lt;/p&gt;\\n\\n&lt;p&gt;getting a card with barely working old drivers for AI is buying an expensive room heater&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1ltamap","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1upb96/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751913693,"author_flair_text":null,"collapsed":false,"created_utc":1751913693,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1pc1ay","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FullstackSensei","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1ozrsi","score":9,"author_fullname":"t2_17n3nqtj56","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Driver updates won't make any difference on any platform that's not new. Driver updates bring optimizations when the hardware is still new and engineers are still figuring how to get the best performance out of it. After a while, driver updates bring mainly bug fixes. After some more time, they bring zero changes to that hardware because all optimizations and bugs have already been done.\\n\\nNo new drivers doesn't shorten the lifespan of such a card. I don't know why people think that. The card will only stop being relevant when the compute it provides is no longer competitive with other/newer alternatives at a given price point.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1pc1ay","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Driver updates won&amp;#39;t make any difference on any platform that&amp;#39;s not new. Driver updates bring optimizations when the hardware is still new and engineers are still figuring how to get the best performance out of it. After a while, driver updates bring mainly bug fixes. After some more time, they bring zero changes to that hardware because all optimizations and bugs have already been done.&lt;/p&gt;\\n\\n&lt;p&gt;No new drivers doesn&amp;#39;t shorten the lifespan of such a card. I don&amp;#39;t know why people think that. The card will only stop being relevant when the compute it provides is no longer competitive with other/newer alternatives at a given price point.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1pc1ay/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751837547,"author_flair_text":null,"treatment_tags":[],"created_utc":1751837547,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}}],"before":null}},"user_reports":[],"saved":false,"id":"n1ozrsi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"HugoCortell","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1ow9rr","score":18,"author_fullname":"t2_61s8b5gv","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"From what I've heard, they run great on Linux (and not at all on Windows, if that matters), they also might not be getting any driver updates any time soon and the current drivers are meh (so the useful lifespan might get limited by that).\\n\\nTheir cheap price indicates that the lack of Windows drivers is a big deal for most (me included, to be honest). But if you use Linux or plan on using this as a server or something, it's the best bang for your buck card there is.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1ozrsi","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;From what I&amp;#39;ve heard, they run great on Linux (and not at all on Windows, if that matters), they also might not be getting any driver updates any time soon and the current drivers are meh (so the useful lifespan might get limited by that).&lt;/p&gt;\\n\\n&lt;p&gt;Their cheap price indicates that the lack of Windows drivers is a big deal for most (me included, to be honest). But if you use Linux or plan on using this as a server or something, it&amp;#39;s the best bang for your buck card there is.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1ozrsi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751833745,"author_flair_text":null,"treatment_tags":[],"created_utc":1751833745,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":18}}],"before":null}},"user_reports":[],"saved":false,"id":"n1ow9rr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"gnad","can_mod_post":false,"created_utc":1751832654,"send_replies":true,"parent_id":"t1_n1ovo9u","score":17,"author_fullname":"t2_giuiz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Interesting. Is there any driver difficulty with these?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ow9rr","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Interesting. Is there any driver difficulty with these?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1ow9rr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751832654,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":17}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1rl5bj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"hurrdurrmeh","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1quvwc","score":1,"author_fullname":"t2_i63k7zcc","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Please post details","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n1rl5bj","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Please post details&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1ltamap","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1rl5bj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751869970,"author_flair_text":null,"treatment_tags":[],"created_utc":1751869970,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1vbcsk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"raptorgzus","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1quvwc","score":1,"author_fullname":"t2_1b3hc2o","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I would be interested in watching a build.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n1vbcsk","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I would be interested in watching a build.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1ltamap","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1vbcsk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751920973,"author_flair_text":null,"treatment_tags":[],"created_utc":1751920973,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1quvwc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Threatening-Silence-","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1pn6lh","score":14,"author_fullname":"t2_15wqsifdjf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah my current build is 9x 3090, I'm going to swap over to the MI50 when they arrive, I'll post of course.","edited":false,"author_flair_css_class":null,"name":"t1_n1quvwc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah my current build is 9x 3090, I&amp;#39;m going to swap over to the MI50 when they arrive, I&amp;#39;ll post of course.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1ltamap","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1quvwc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751857355,"author_flair_text":null,"collapsed":false,"created_utc":1751857355,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":14}}],"before":null}},"user_reports":[],"saved":false,"id":"n1pn6lh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Mybrandnewaccount95","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1pcwvy","score":2,"author_fullname":"t2_2r6yfku9","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Have you posted your full build anywhere?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1pn6lh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Have you posted your full build anywhere?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1pn6lh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751841321,"author_flair_text":null,"treatment_tags":[],"created_utc":1751841321,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1rn19l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Threatening-Silence-","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1rl3sc","score":3,"author_fullname":"t2_15wqsifdjf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"https://www.reddit.com/r/LocalLLaMA/s/zzTvvsas3J\\n\\nJust a mid range gaming board. \\n\\nI use eGPUs and I'm adding more.","edited":false,"author_flair_css_class":null,"name":"t1_n1rn19l","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/s/zzTvvsas3J\\"&gt;https://www.reddit.com/r/LocalLLaMA/s/zzTvvsas3J&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Just a mid range gaming board. &lt;/p&gt;\\n\\n&lt;p&gt;I use eGPUs and I&amp;#39;m adding more.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1ltamap","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1rn19l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751871030,"author_flair_text":null,"collapsed":false,"created_utc":1751871030,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n1rl3sc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"hurrdurrmeh","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1pcwvy","score":1,"author_fullname":"t2_i63k7zcc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Dude what motherboard and ram setup do you have that lets you add that many cards??","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1rl3sc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Dude what motherboard and ram setup do you have that lets you add that many cards??&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1rl3sc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751869946,"author_flair_text":null,"treatment_tags":[],"created_utc":1751869946,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1plqtm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"markovianmind","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1pcwvy","score":0,"author_fullname":"t2_2cpmd6zy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"what model do u run.on these","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1plqtm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;what model do u run.on these&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1plqtm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751840810,"author_flair_text":null,"treatment_tags":[],"created_utc":1751840810,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1qjbex","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Conscious-Map6957","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1pq1tq","score":5,"author_fullname":"t2_7p99opl4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That's actually $145 each.","edited":false,"author_flair_css_class":null,"name":"t1_n1qjbex","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s actually $145 each.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1ltamap","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1qjbex/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751852896,"author_flair_text":null,"collapsed":false,"created_utc":1751852896,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1r0app","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Threatening-Silence-","can_mod_post":false,"created_utc":1751859615,"send_replies":true,"parent_id":"t1_n1qzs9i","score":2,"author_fullname":"t2_15wqsifdjf","approved_by":null,"mod_note":null,"all_awardings":[],"body":"That could very well be part of the reason why they're so cheap. No Trump tax here thankfully...","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n1r0app","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That could very well be part of the reason why they&amp;#39;re so cheap. No Trump tax here thankfully...&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1r0app/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751859615,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltamap","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":8,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1qzs9i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"created_utc":1751859393,"send_replies":true,"parent_id":"t1_n1qz90s","score":-1,"author_fullname":"t2_o65i6kx","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ah.... I assumed you are in the US. Here it's ultimately up to the importer, that would be the buyer in this case. The carrier can act on the buyer's behalf. Either way the buyer will get a bill from customs directly or via the carrier if duty and tariffs are due. Until paid, the package is held. And since we have the Trump Tax now. That can be a tidy sum. Even at the current \\"reduced\\" rates. I don't know if used GPUs like this qualify for any exemptions. And of course it depends on what they declare it as. I guess that's why they asked what you wanted.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n1qzs9i","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ah.... I assumed you are in the US. Here it&amp;#39;s ultimately up to the importer, that would be the buyer in this case. The carrier can act on the buyer&amp;#39;s behalf. Either way the buyer will get a bill from customs directly or via the carrier if duty and tariffs are due. Until paid, the package is held. And since we have the Trump Tax now. That can be a tidy sum. Even at the current &amp;quot;reduced&amp;quot; rates. I don&amp;#39;t know if used GPUs like this qualify for any exemptions. And of course it depends on what they declare it as. I guess that&amp;#39;s why they asked what you wanted.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1ltamap","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1qzs9i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751859393,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1qz90s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Threatening-Silence-","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1qvnwe","score":2,"author_fullname":"t2_15wqsifdjf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"They asked me what I'd like the declarations to be etc. Here in the UK the customs are handled by the courier, it shouldn't be too mental though.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n1qz90s","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They asked me what I&amp;#39;d like the declarations to be etc. Here in the UK the customs are handled by the courier, it shouldn&amp;#39;t be too mental though.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1ltamap","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1qz90s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751859160,"author_flair_text":null,"treatment_tags":[],"created_utc":1751859160,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1qvnwe","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1qubyv","score":3,"author_fullname":"t2_o65i6kx","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Sweet. Did they mention anything about duty and tariffs? Are they taking care of that for you or is it up to you once it hits customs.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n1qvnwe","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sweet. Did they mention anything about duty and tariffs? Are they taking care of that for you or is it up to you once it hits customs.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1ltamap","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1qvnwe/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751857671,"author_flair_text":null,"treatment_tags":[],"created_utc":1751857671,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n1qubyv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Threatening-Silence-","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1pq1tq","score":2,"author_fullname":"t2_15wqsifdjf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"They're sending direct to my house.","edited":false,"author_flair_css_class":null,"name":"t1_n1qubyv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They&amp;#39;re sending direct to my house.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1ltamap","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1qubyv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751857128,"author_flair_text":null,"collapsed":false,"created_utc":1751857128,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1pq1tq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1pcwvy","score":0,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Sweet. $160 each is good. Did they deliver to your address or did you have to use a transhipper?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1pq1tq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sweet. $160 each is good. Did they deliver to your address or did you have to use a transhipper?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1pq1tq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751842324,"author_flair_text":null,"treatment_tags":[],"created_utc":1751842324,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n1pcwvy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Threatening-Silence-","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1panr0","score":23,"author_fullname":"t2_15wqsifdjf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"https://preview.redd.it/v1iiar2bpbbf1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=7fd1373ae1c2a2c1c415a70b4b637fe4034450f5\\n\\nThere's my invoice for 11 cards.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1pcwvy","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://preview.redd.it/v1iiar2bpbbf1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7fd1373ae1c2a2c1c415a70b4b637fe4034450f5\\"&gt;https://preview.redd.it/v1iiar2bpbbf1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7fd1373ae1c2a2c1c415a70b4b637fe4034450f5&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;There&amp;#39;s my invoice for 11 cards.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1pcwvy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751837836,"media_metadata":{"v1iiar2bpbbf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":84,"x":108,"u":"https://preview.redd.it/v1iiar2bpbbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=feb98757aa6298f24f2f7a10bc7123a41b038b3c"},{"y":169,"x":216,"u":"https://preview.redd.it/v1iiar2bpbbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=22a4810a194e6469485ce922e9b2775929c4671a"},{"y":250,"x":320,"u":"https://preview.redd.it/v1iiar2bpbbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2ca563cd141a39c8f6e37cd2b030dfc9312bcc1f"},{"y":501,"x":640,"u":"https://preview.redd.it/v1iiar2bpbbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6133062a040dfccdcf091adf1bfe282d9f2c7651"},{"y":752,"x":960,"u":"https://preview.redd.it/v1iiar2bpbbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4e1d0c307a829ff8c3dcb012a68c95abbe2ee61c"},{"y":846,"x":1080,"u":"https://preview.redd.it/v1iiar2bpbbf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a9672b0b0c6c921b7531c07db2437a4562b7cba9"}],"s":{"y":846,"x":1080,"u":"https://preview.redd.it/v1iiar2bpbbf1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=7fd1373ae1c2a2c1c415a70b4b637fe4034450f5"},"id":"v1iiar2bpbbf1"}},"author_flair_text":null,"treatment_tags":[],"created_utc":1751837836,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":23}}],"before":null}},"user_reports":[],"saved":false,"id":"n1panr0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"fallingdowndizzyvr","can_mod_post":false,"created_utc":1751837103,"send_replies":true,"parent_id":"t1_n1ovo9u","score":11,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; 32GB of 1TB/s vram for $120. \\n\\nIt's $120 if you buy more than 500 of them. Then you have to add shipping, duty and any tariffs. Which could make the price a wee bit higher than $120. That's why the listing says \\"**Shipping fee** and delivery date to be negotiated. \\"","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1panr0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;32GB of 1TB/s vram for $120. &lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;It&amp;#39;s $120 if you buy more than 500 of them. Then you have to add shipping, duty and any tariffs. Which could make the price a wee bit higher than $120. That&amp;#39;s why the listing says &amp;quot;&lt;strong&gt;Shipping fee&lt;/strong&gt; and delivery date to be negotiated. &amp;quot;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1panr0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751837103,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1sjtqw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Apprehensive-Mark241","can_mod_post":false,"created_utc":1751889183,"send_replies":true,"parent_id":"t1_n1ovo9u","score":1,"author_fullname":"t2_cgjdztdb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I had a terrible time with vulkan  trying to allocate vram in chunks that are too big.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1sjtqw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I had a terrible time with vulkan  trying to allocate vram in chunks that are too big.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1sjtqw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751889183,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1thlvw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Threatening-Silence-","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1tfsgd","score":1,"author_fullname":"t2_15wqsifdjf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Don't know yet sorry.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1thlvw","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Don&amp;#39;t know yet sorry.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1thlvw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751900780,"author_flair_text":null,"treatment_tags":[],"created_utc":1751900780,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1tfsgd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Bitter-College8786","can_mod_post":false,"created_utc":1751900243,"send_replies":true,"parent_id":"t1_n1ovo9u","score":1,"author_fullname":"t2_vs18iwd3g","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Does it also work well for image generating stuff like Flux?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1tfsgd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Does it also work well for image generating stuff like Flux?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1tfsgd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751900243,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1u6ehx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Threatening-Silence-","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1u1emn","score":1,"author_fullname":"t2_15wqsifdjf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"They should work with the lmstudio Vulcan runner but Windows support is zero unless you flash the vbios. Linux only","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1u6ehx","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They should work with the lmstudio Vulcan runner but Windows support is zero unless you flash the vbios. Linux only&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1u6ehx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751907924,"author_flair_text":null,"treatment_tags":[],"created_utc":1751907924,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1u1emn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Massive-Question-550","can_mod_post":false,"created_utc":1751906506,"send_replies":true,"parent_id":"t1_n1ovo9u","score":1,"author_fullname":"t2_72xxv3wb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Do they play nice with other gpu's like a 3090? Will they work with lm studio or ollama? ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1u1emn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Do they play nice with other gpu&amp;#39;s like a 3090? Will they work with lm studio or ollama? &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1u1emn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751906506,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1v554w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RenewAi","can_mod_post":false,"created_utc":1751918916,"send_replies":true,"parent_id":"t1_n1ovo9u","score":1,"author_fullname":"t2_2ju9di","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Wait whats the catch here. 32gb vram for $130, what am I missing?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1v554w","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Wait whats the catch here. 32gb vram for $130, what am I missing?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1v554w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751918916,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1ovo9u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Threatening-Silence-","can_mod_post":false,"created_utc":1751832466,"send_replies":true,"parent_id":"t3_1ltamap","score":94,"author_fullname":"t2_15wqsifdjf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Load up on MI50s.\\n\\n32GB of 1TB/s vram for $120. Works with Vulcan.\\n\\nhttps://www.alibaba.com/x/B03rEE?ck=pdp\\n\\nHere's a post from a guy who uses them, with benchmarks. \\n\\nhttps://www.reddit.com/r/LocalLLaMA/s/U98WeACokQ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ovo9u","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Load up on MI50s.&lt;/p&gt;\\n\\n&lt;p&gt;32GB of 1TB/s vram for $120. Works with Vulcan.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.alibaba.com/x/B03rEE?ck=pdp\\"&gt;https://www.alibaba.com/x/B03rEE?ck=pdp&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Here&amp;#39;s a post from a guy who uses them, with benchmarks. &lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/s/U98WeACokQ\\"&gt;https://www.reddit.com/r/LocalLLaMA/s/U98WeACokQ&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1ovo9u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751832466,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltamap","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":94}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1pwgng","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"colin_colout","can_mod_post":false,"created_utc":1751844560,"send_replies":true,"parent_id":"t1_n1p73mh","score":10,"author_fullname":"t2_14l4ya","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"If you're not doing training, PCI speed isn't as much as a concern.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1pwgng","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you&amp;#39;re not doing training, PCI speed isn&amp;#39;t as much as a concern.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1pwgng/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751844560,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1p8m9g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Judtoff","can_mod_post":false,"created_utc":1751836454,"send_replies":true,"parent_id":"t1_n1p73mh","score":6,"author_fullname":"t2_oekws","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Agreed, I'm running 3x 3090 on an x99 dual cpu mobo. No issues. I'd suggest the single 3090 over 2x 3060 etc since pcie slots (and lanes) are often the limiting factor. You're spot on.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1p8m9g","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Agreed, I&amp;#39;m running 3x 3090 on an x99 dual cpu mobo. No issues. I&amp;#39;d suggest the single 3090 over 2x 3060 etc since pcie slots (and lanes) are often the limiting factor. You&amp;#39;re spot on.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1p8m9g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751836454,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1sevjf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"epycguy","can_mod_post":false,"created_utc":1751886936,"send_replies":true,"parent_id":"t1_n1p73mh","score":2,"author_fullname":"t2_bwygwjei7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; second hand 3090s price is similar to two new 3060s, it's easier to handle 3090 and it's faster\\n\\nbut what about 2x used 3060s :D","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1sevjf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;second hand 3090s price is similar to two new 3060s, it&amp;#39;s easier to handle 3090 and it&amp;#39;s faster&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;but what about 2x used 3060s :D&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1sevjf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751886936,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1uhvza","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jacek2023","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1u57se","score":2,"author_fullname":"t2_vqgbql9w","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I have multiple computers, on desktop I have i7-13700kf and 5070, I currently use it for training models for Kaggle, later I will run my code on my \\"supercomputer\\"\\n\\nMy supercomputer is [https://www.reddit.com/r/LocalLLaMA/comments/1kooyfx/llamacpp\\\\_benchmarks\\\\_on\\\\_72gb\\\\_vram\\\\_setup\\\\_2x\\\\_3090\\\\_2x/](https://www.reddit.com/r/LocalLLaMA/comments/1kooyfx/llamacpp_benchmarks_on_72gb_vram_setup_2x_3090_2x/)\\n\\nBut I also tested on mobo/cpu from 2008 (not 2018, 2008) because I read lots of fake information here on reddit [https://www.reddit.com/r/LocalLLaMA/comments/1kbnoyj/qwen3\\\\_on\\\\_2008\\\\_motherboard/](https://www.reddit.com/r/LocalLLaMA/comments/1kbnoyj/qwen3_on_2008_motherboard/)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1uhvza","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have multiple computers, on desktop I have i7-13700kf and 5070, I currently use it for training models for Kaggle, later I will run my code on my &amp;quot;supercomputer&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;My supercomputer is &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1kooyfx/llamacpp_benchmarks_on_72gb_vram_setup_2x_3090_2x/\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1kooyfx/llamacpp_benchmarks_on_72gb_vram_setup_2x_3090_2x/&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;But I also tested on mobo/cpu from 2008 (not 2018, 2008) because I read lots of fake information here on reddit &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1kbnoyj/qwen3_on_2008_motherboard/\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1kbnoyj/qwen3_on_2008_motherboard/&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1uhvza/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751911327,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1751911327,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1u57se","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ii_social","can_mod_post":false,"created_utc":1751907583,"send_replies":true,"parent_id":"t1_n1p73mh","score":1,"author_fullname":"t2_tohvxz80x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I have 1x 3090 and 2x 3060, what motherboard and processor do you have?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1u57se","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have 1x 3090 and 2x 3060, what motherboard and processor do you have?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1u57se/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751907583,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1p73mh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"jacek2023","can_mod_post":false,"created_utc":1751835983,"send_replies":true,"parent_id":"t3_1ltamap","score":27,"author_fullname":"t2_vqgbql9w","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I currently have 2x3090+2x3060, will probably buy more 3090 at some point\\n\\n1) Don't trust \\"online experts\\" that you need very expensive mobo to make it work, you just need mobo with 4 PCI-E slots, I recommend x399 but x99 may also work.\\n\\n2) second hand 3090s price is similar to two new 3060s, it's easier to handle 3090 and it's faster\\n\\n3) these old cards may be problematic (drivers, not sure about llama.cpp support)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1p73mh","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I currently have 2x3090+2x3060, will probably buy more 3090 at some point&lt;/p&gt;\\n\\n&lt;p&gt;1) Don&amp;#39;t trust &amp;quot;online experts&amp;quot; that you need very expensive mobo to make it work, you just need mobo with 4 PCI-E slots, I recommend x399 but x99 may also work.&lt;/p&gt;\\n\\n&lt;p&gt;2) second hand 3090s price is similar to two new 3060s, it&amp;#39;s easier to handle 3090 and it&amp;#39;s faster&lt;/p&gt;\\n\\n&lt;p&gt;3) these old cards may be problematic (drivers, not sure about llama.cpp support)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1p73mh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751835983,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1ltamap","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":27}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1pc580","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"UnreasonableEconomy","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1p3ob2","score":4,"author_fullname":"t2_88lwr6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; The bandwidth should be enough.\\n\\nI hope you're right - there's a lot of confusion in the community about how much bandwidth really matters. If it really works that would be great, it would open up a lot of doors for everyone. I would *guess* it's a major bottleneck but it might not be - after all 235B is only a MOE with 22B active...  hard to say.\\n\\nIn this case I imagine moset of the GPUs will be idle...\\n\\n22B @ Q4 ~ 11 gb active if my math is right. if it's got 8 active experts that's around 1.4gb/expert, and you have 128 of them, so you can have, at 48, around (48/(1.4*128)) = 48/176 ~ 27%, about a quarter of the experts loaded in memory. \\n\\nassuming it's random (which it probably isn't through), you'd need to swap out an expected 8.4 GB out for every token. (as opposed to 11 GB if you only had a single 12 gb GPU), so that gives you a speedup hypothetically (assumign the bus bottleneck) of ~ 23%, for spending 4x as much on GPUs. \\n\\nHmm hmm hmm, interesting problem. \\n\\nCaveat, this is only a (not very cafeful) back of the envelope calculation.\\n\\nI ran the rest of the numbers:\\n- 12gb: 6.7% hit rate, 10.5 GB expected miss per token\\n- 48gb: 26% hit rate, 8.2 GB expected miss per token\\n- 64gb: 35% hit rate, 5.2 GB expected miss per token\\n- 72gb: 40% hit rate, 4.7 gb expected miss per token\\n- 128gb: 71% hit rate, 2.3 gb expected miss per token\\n\\nyou will then need to divide that by 1/bandwidth (sorta, it gets complicated with how many loads you can do in parallel) to get an approximate slowdown based on loads.\\n\\napprox transfer speeds per lane\\n-pcie 4: 2gb/s: ~ 0.72 seconds per expert/0.508 per gb per lane\\n-pcie 5: 4gb/s: ~ 0.36 seconds per expert/0.254 per gb per lane\\n\\nif you can split your 16 lanes by 4 and lose nothing with the adapter:\\n\\npcie4\\n- avg: 12gb: 1.3335 seconds per token\\n- avg: 48gb: 1.0414 seconds per token\\n- avg: 64gb: 0.3302 seconds per token\\n- avg: 72gb: 0.5969 seconds per token\\n- avg:  128gb 0.14605 seconds per token\\n\\npcie5\\n- avg: 12gb: 0.66675 seconds per token\\n- avg: 48gb: 0.5207 seconds per token\\n- avg: 64gb: 0.3302 seconds per token\\n- avg: 72gb: 0.29845 seconds per token\\n- avg:  128gb 0.2921 seconds per token\\n\\n\\nOf course there's also the cpu offloading, which makes everything even more complicated...\\n\\nEh! I guess I found out I don't know how to help ya OP! Sorry!\\n\\nedit: added more numbers","edited":1751838547,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1pc580","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;The bandwidth should be enough.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I hope you&amp;#39;re right - there&amp;#39;s a lot of confusion in the community about how much bandwidth really matters. If it really works that would be great, it would open up a lot of doors for everyone. I would &lt;em&gt;guess&lt;/em&gt; it&amp;#39;s a major bottleneck but it might not be - after all 235B is only a MOE with 22B active...  hard to say.&lt;/p&gt;\\n\\n&lt;p&gt;In this case I imagine moset of the GPUs will be idle...&lt;/p&gt;\\n\\n&lt;p&gt;22B @ Q4 ~ 11 gb active if my math is right. if it&amp;#39;s got 8 active experts that&amp;#39;s around 1.4gb/expert, and you have 128 of them, so you can have, at 48, around (48/(1.4*128)) = 48/176 ~ 27%, about a quarter of the experts loaded in memory. &lt;/p&gt;\\n\\n&lt;p&gt;assuming it&amp;#39;s random (which it probably isn&amp;#39;t through), you&amp;#39;d need to swap out an expected 8.4 GB out for every token. (as opposed to 11 GB if you only had a single 12 gb GPU), so that gives you a speedup hypothetically (assumign the bus bottleneck) of ~ 23%, for spending 4x as much on GPUs. &lt;/p&gt;\\n\\n&lt;p&gt;Hmm hmm hmm, interesting problem. &lt;/p&gt;\\n\\n&lt;p&gt;Caveat, this is only a (not very cafeful) back of the envelope calculation.&lt;/p&gt;\\n\\n&lt;p&gt;I ran the rest of the numbers:\\n- 12gb: 6.7% hit rate, 10.5 GB expected miss per token\\n- 48gb: 26% hit rate, 8.2 GB expected miss per token\\n- 64gb: 35% hit rate, 5.2 GB expected miss per token\\n- 72gb: 40% hit rate, 4.7 gb expected miss per token\\n- 128gb: 71% hit rate, 2.3 gb expected miss per token&lt;/p&gt;\\n\\n&lt;p&gt;you will then need to divide that by 1/bandwidth (sorta, it gets complicated with how many loads you can do in parallel) to get an approximate slowdown based on loads.&lt;/p&gt;\\n\\n&lt;p&gt;approx transfer speeds per lane\\n-pcie 4: 2gb/s: ~ 0.72 seconds per expert/0.508 per gb per lane\\n-pcie 5: 4gb/s: ~ 0.36 seconds per expert/0.254 per gb per lane&lt;/p&gt;\\n\\n&lt;p&gt;if you can split your 16 lanes by 4 and lose nothing with the adapter:&lt;/p&gt;\\n\\n&lt;p&gt;pcie4\\n- avg: 12gb: 1.3335 seconds per token\\n- avg: 48gb: 1.0414 seconds per token\\n- avg: 64gb: 0.3302 seconds per token\\n- avg: 72gb: 0.5969 seconds per token\\n- avg:  128gb 0.14605 seconds per token&lt;/p&gt;\\n\\n&lt;p&gt;pcie5\\n- avg: 12gb: 0.66675 seconds per token\\n- avg: 48gb: 0.5207 seconds per token\\n- avg: 64gb: 0.3302 seconds per token\\n- avg: 72gb: 0.29845 seconds per token\\n- avg:  128gb 0.2921 seconds per token&lt;/p&gt;\\n\\n&lt;p&gt;Of course there&amp;#39;s also the cpu offloading, which makes everything even more complicated...&lt;/p&gt;\\n\\n&lt;p&gt;Eh! I guess I found out I don&amp;#39;t know how to help ya OP! Sorry!&lt;/p&gt;\\n\\n&lt;p&gt;edit: added more numbers&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1pc580/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751837583,"author_flair_text":null,"treatment_tags":[],"created_utc":1751837583,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1qfqw8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Technical_Bar_1908","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1p9xx6","score":2,"author_fullname":"t2_f8arhws9t","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Your PC is still quite a weapon tho, congrats \\nThat 96gb of ram is huuuuuuge","edited":false,"author_flair_css_class":null,"name":"t1_n1qfqw8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Your PC is still quite a weapon tho, congrats \\nThat 96gb of ram is huuuuuuge&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1ltamap","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1qfqw8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751851568,"author_flair_text":null,"collapsed":false,"created_utc":1751851568,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1qf3pc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Technical_Bar_1908","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1p9xx6","score":1,"author_fullname":"t2_f8arhws9t","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"With that setup the 3090 x 2 is probably going to be the weapon of choice. On that chipset you can iteratively upgrade to pcie5 components if you want/need to or as it can be afforded and do the GPU upgrade last. But definitely see how you run on the 3090s with pcie4 SSD first","edited":false,"author_flair_css_class":null,"name":"t1_n1qf3pc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;With that setup the 3090 x 2 is probably going to be the weapon of choice. On that chipset you can iteratively upgrade to pcie5 components if you want/need to or as it can be afforded and do the GPU upgrade last. But definitely see how you run on the 3090s with pcie4 SSD first&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1ltamap","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1qf3pc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751851331,"author_flair_text":null,"collapsed":false,"created_utc":1751851331,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1p9xx6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"gnad","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1p99ly","score":3,"author_fullname":"t2_giuiz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Sorry for not clarifying. I am running x870i mobo, 7950x, 2x48gb ram, pcie 4.0 ssd, no gpu.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1p9xx6","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sorry for not clarifying. I am running x870i mobo, 7950x, 2x48gb ram, pcie 4.0 ssd, no gpu.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1p9xx6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751836873,"author_flair_text":null,"treatment_tags":[],"created_utc":1751836873,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n1p99ly","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Technical_Bar_1908","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1p3ob2","score":4,"author_fullname":"t2_f8arhws9t","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I notice you keep being very very vague and cagey about your specific hardware so its very difficult for people to give you a straight answer... \\nIf you have pcie5 motherboard you should stick to the 5000 series cards. Ideally you would have equitable bandwidth between memory types (RAM, VRAM, SSD) and the faster bandwidth may be better than dropping in earlier cards with more VRAM.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1p99ly","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I notice you keep being very very vague and cagey about your specific hardware so its very difficult for people to give you a straight answer... \\nIf you have pcie5 motherboard you should stick to the 5000 series cards. Ideally you would have equitable bandwidth between memory types (RAM, VRAM, SSD) and the faster bandwidth may be better than dropping in earlier cards with more VRAM.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1p99ly/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751836659,"author_flair_text":null,"treatment_tags":[],"created_utc":1751836659,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n1p3ob2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"gnad","can_mod_post":false,"created_utc":1751834929,"send_replies":true,"parent_id":"t1_n1p2inu","score":2,"author_fullname":"t2_giuiz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"My mobo only have 1x pcie x16 but i plan to use a splitter (the mobo support x4x4x4x4 bifurcation). The bandwidth should be enough.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1p3ob2","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;My mobo only have 1x pcie x16 but i plan to use a splitter (the mobo support x4x4x4x4 bifurcation). The bandwidth should be enough.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1p3ob2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751834929,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1p2inu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"UnreasonableEconomy","can_mod_post":false,"created_utc":1751834578,"send_replies":true,"parent_id":"t3_1ltamap","score":9,"author_fullname":"t2_88lwr6","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Just a thought\\n\\n4x 3060 might end up costing nearly as much (or more than) as 2x 3090\\n\\nyou can't disregard the cost of a workstation mobo and cpu required for all the PCIe lanes...\\n\\n2x 3090 you can get away with a consumer 2x8 mobo, *probably* gonna need a threadripper...\\n\\nWhat kind of board do you have currently?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1p2inu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just a thought&lt;/p&gt;\\n\\n&lt;p&gt;4x 3060 might end up costing nearly as much (or more than) as 2x 3090&lt;/p&gt;\\n\\n&lt;p&gt;you can&amp;#39;t disregard the cost of a workstation mobo and cpu required for all the PCIe lanes...&lt;/p&gt;\\n\\n&lt;p&gt;2x 3090 you can get away with a consumer 2x8 mobo, &lt;em&gt;probably&lt;/em&gt; gonna need a threadripper...&lt;/p&gt;\\n\\n&lt;p&gt;What kind of board do you have currently?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1p2inu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751834578,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltamap","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1pp77k","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"a_beautiful_rhind","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1pmxr2","score":10,"author_fullname":"t2_h5utwre7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt;Doesn't have to be practical.\\n\\nSince they're asking for a multi-gpu rig, I assume they want to *use* the models not hail mary a single MoE and then go play minecraft.\\n\\n&gt;but it would probably work better than a stack of ancient cards\\n\\nSounds like something a cloud model enjoyer would say :P","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1pp77k","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Doesn&amp;#39;t have to be practical.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Since they&amp;#39;re asking for a multi-gpu rig, I assume they want to &lt;em&gt;use&lt;/em&gt; the models not hail mary a single MoE and then go play minecraft.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;but it would probably work better than a stack of ancient cards&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Sounds like something a cloud model enjoyer would say :P&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1pp77k/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751842032,"author_flair_text":null,"treatment_tags":[],"created_utc":1751842032,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}}],"before":null}},"user_reports":[],"saved":false,"id":"n1pmxr2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"rnosov","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1pkpw7","score":3,"author_fullname":"t2_18x6fa","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Setup should work with a used 3090 which is within budget. I'm not saying it will work well but it would probably work better than a stack of ancient cards. And lots of RAM and 3090 will be useful for countless other pursuits. To be honest it blows my mind that we can run o1 class reasoning model on less than 1k USD of hardware at all! Doesn't have to be practical.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1pmxr2","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Setup should work with a used 3090 which is within budget. I&amp;#39;m not saying it will work well but it would probably work better than a stack of ancient cards. And lots of RAM and 3090 will be useful for countless other pursuits. To be honest it blows my mind that we can run o1 class reasoning model on less than 1k USD of hardware at all! Doesn&amp;#39;t have to be practical.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1pmxr2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751841233,"author_flair_text":null,"treatment_tags":[],"created_utc":1751841233,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n1pkpw7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1751840455,"send_replies":true,"parent_id":"t1_n1oxqik","score":29,"author_fullname":"t2_h5utwre7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This is horrible advice. 6t/s of 235b ain't all that. Used 4090 is way over his budget.\\n\\nCounting on hybrid inference is a bad bet for the money spent. Let alone with a reasoning model. 48gb is basically the first step out of vramlet territory. \\n\\nAs much as small models have gotten better, they're more of a \\"it's cool I can run this on my already purchased gaming GPU\\" than serious LLMs.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1pkpw7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is horrible advice. 6t/s of 235b ain&amp;#39;t all that. Used 4090 is way over his budget.&lt;/p&gt;\\n\\n&lt;p&gt;Counting on hybrid inference is a bad bet for the money spent. Let alone with a reasoning model. 48gb is basically the first step out of vramlet territory. &lt;/p&gt;\\n\\n&lt;p&gt;As much as small models have gotten better, they&amp;#39;re more of a &amp;quot;it&amp;#39;s cool I can run this on my already purchased gaming GPU&amp;quot; than serious LLMs.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1pkpw7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751840455,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":29}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1p7d2x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"LA_rent_Aficionado","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1p55f4","score":7,"author_fullname":"t2_t8zbiflk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I feel you, after 2-3 days building/tweaking  I gave up with k transformers until it matures a bit","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1p7d2x","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I feel you, after 2-3 days building/tweaking  I gave up with k transformers until it matures a bit&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1p7d2x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751836065,"author_flair_text":null,"treatment_tags":[],"created_utc":1751836065,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1txucf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ciprianveg","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1p55f4","score":1,"author_fullname":"t2_j8fit2p","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ik_llama.cpp and add 3x3090. You will get cca 10t/s with a q4 235b.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1txucf","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ik_llama.cpp and add 3x3090. You will get cca 10t/s with a q4 235b.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1txucf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751905482,"author_flair_text":null,"treatment_tags":[],"created_utc":1751905482,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1p55f4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"panchovix","can_mod_post":false,"created_utc":1751835382,"send_replies":true,"parent_id":"t1_n1oxqik","score":15,"author_fullname":"t2_j1kqr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Man I would give a go to ktransformers (208GB VRAM + 192GB RAM) if it wasn't hard asf to use. I tinkered some days but didn't know how to replicate -ot behavior there.\\n\\nIt is highly factible I have monke brain, but I have used the others backends without issues.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1p55f4","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Man I would give a go to ktransformers (208GB VRAM + 192GB RAM) if it wasn&amp;#39;t hard asf to use. I tinkered some days but didn&amp;#39;t know how to replicate -ot behavior there.&lt;/p&gt;\\n\\n&lt;p&gt;It is highly factible I have monke brain, but I have used the others backends without issues.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1p55f4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751835382,"author_flair_text":"Llama 405B","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":15}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1ps3ea","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"tempetemplar","can_mod_post":false,"created_utc":1751843028,"send_replies":true,"parent_id":"t1_n1oxqik","score":1,"author_fullname":"t2_atvw2aj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Best suggestion here!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ps3ea","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Best suggestion here!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1ps3ea/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751843028,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1tjvez","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"xanduonc","can_mod_post":false,"created_utc":1751901441,"send_replies":true,"parent_id":"t1_n1oxqik","score":1,"author_fullname":"t2_10n3b6gg97","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"3090/4090 is almost idling in this kind of setup, makes more sense to go for cheaper gpu","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1tjvez","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;3090/4090 is almost idling in this kind of setup, makes more sense to go for cheaper gpu&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1tjvez/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751901441,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1q573z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"beijinghouse","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1p2b8v","score":5,"author_fullname":"t2_1osh4bmmz9","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"2 x 64GB sticks (128GB DDR5) now available \\\\~$300 [https://amzn.to/4eyjab3](https://amzn.to/4eyjab3)\\n\\nWith 96GB, the +32GB RAM gain may seem pointless but if you're already in the headspace of paying up to $1000 for similar amounts of VRAM, it's gotta be in the mix as an option to consider if you decide to go down the MoE + Ktransformers (or ik\\\\_llama.cpp) route since it could work right now even with your current mobo.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1q573z","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;2 x 64GB sticks (128GB DDR5) now available ~$300 &lt;a href=\\"https://amzn.to/4eyjab3\\"&gt;https://amzn.to/4eyjab3&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;With 96GB, the +32GB RAM gain may seem pointless but if you&amp;#39;re already in the headspace of paying up to $1000 for similar amounts of VRAM, it&amp;#39;s gotta be in the mix as an option to consider if you decide to go down the MoE + Ktransformers (or ik_llama.cpp) route since it could work right now even with your current mobo.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1q573z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751847705,"author_flair_text":null,"treatment_tags":[],"created_utc":1751847705,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1t7gu5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"eloquentemu","can_mod_post":false,"created_utc":1751897755,"send_replies":true,"parent_id":"t1_n1pzqjc","score":1,"author_fullname":"t2_lpdsy","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah, while the cost can be pretty high (esp if you go big on RAM which I recommend for large MoEs), you do get what you pay for.  It's nice having a solid platform to work with that can run any model tolerably and offers a lot of extensibility, e.g. GPUs or 10+GbE.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n1t7gu5","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, while the cost can be pretty high (esp if you go big on RAM which I recommend for large MoEs), you do get what you pay for.  It&amp;#39;s nice having a solid platform to work with that can run any model tolerably and offers a lot of extensibility, e.g. GPUs or 10+GbE.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1t7gu5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751897755,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltamap","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":8,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1pzqjc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"InternationalNebula7","can_mod_post":false,"created_utc":1751845714,"send_replies":true,"parent_id":"t1_n1pxaze","score":2,"author_fullname":"t2_1pwtr13a","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Wow that's significantly faster than my current setup","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n1pzqjc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Wow that&amp;#39;s significantly faster than my current setup&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1ltamap","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1pzqjc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751845714,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1pxaze","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"eloquentemu","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1pstrd","score":4,"author_fullname":"t2_lpdsy","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This is Epic 9B14 running 48 cores and 12c DDR5-5200.  (It says CUDA but I hid the devices)\\n\\n| model                          |       size |     params | backend    | ngl |            test |                  t/s |\\n| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |\\n| gemma3n E2B Q4_K - Medium      |   3.65 GiB |     4.46 B | CUDA       |  99 |           pp512 |        406.27 ± 0.00 |\\n| gemma3n E2B Q4_K - Medium      |   3.65 GiB |     4.46 B | CUDA       |  99 |           tg128 |         50.68 ± 0.00 |\\n| gemma3n E4B Q4_K - Medium      |   5.15 GiB |     6.87 B | CUDA       |  99 |           pp512 |        254.83 ± 0.00 |\\n| gemma3n E4B Q4_K - Medium      |   5.15 GiB |     6.87 B | CUDA       |  99 |           tg128 |         36.56 ± 0.00 |\\n\\nFWIW a 4090D is about 2.5x faster on every measure, but that's not terribly surprising since a (good) dedicated GPU can't really be beat on a small dense model","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n1pxaze","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is Epic 9B14 running 48 cores and 12c DDR5-5200.  (It says CUDA but I hid the devices)&lt;/p&gt;\\n\\n&lt;table&gt;&lt;thead&gt;\\n&lt;tr&gt;\\n&lt;th&gt;model&lt;/th&gt;\\n&lt;th align=\\"right\\"&gt;size&lt;/th&gt;\\n&lt;th align=\\"right\\"&gt;params&lt;/th&gt;\\n&lt;th&gt;backend&lt;/th&gt;\\n&lt;th align=\\"right\\"&gt;ngl&lt;/th&gt;\\n&lt;th align=\\"right\\"&gt;test&lt;/th&gt;\\n&lt;th align=\\"right\\"&gt;t/s&lt;/th&gt;\\n&lt;/tr&gt;\\n&lt;/thead&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;td&gt;gemma3n E2B Q4_K - Medium&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;3.65 GiB&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;4.46 B&lt;/td&gt;\\n&lt;td&gt;CUDA&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;99&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;pp512&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;406.27 ± 0.00&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td&gt;gemma3n E2B Q4_K - Medium&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;3.65 GiB&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;4.46 B&lt;/td&gt;\\n&lt;td&gt;CUDA&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;99&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;tg128&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;50.68 ± 0.00&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td&gt;gemma3n E4B Q4_K - Medium&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;5.15 GiB&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;6.87 B&lt;/td&gt;\\n&lt;td&gt;CUDA&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;99&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;pp512&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;254.83 ± 0.00&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td&gt;gemma3n E4B Q4_K - Medium&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;5.15 GiB&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;6.87 B&lt;/td&gt;\\n&lt;td&gt;CUDA&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;99&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;tg128&lt;/td&gt;\\n&lt;td align=\\"right\\"&gt;36.56 ± 0.00&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;/tbody&gt;&lt;/table&gt;\\n\\n&lt;p&gt;FWIW a 4090D is about 2.5x faster on every measure, but that&amp;#39;s not terribly surprising since a (good) dedicated GPU can&amp;#39;t really be beat on a small dense model&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1ltamap","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1pxaze/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751844858,"author_flair_text":null,"treatment_tags":[],"created_utc":1751844858,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n1pstrd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"InternationalNebula7","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1ppjze","score":1,"author_fullname":"t2_1pwtr13a","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Would you be able to test Gemma3n:e4b and Gemma3n:e2b? They're smaller models, but I'm currently using an Intel i5 fourth gen and thinking to upgrade the home lab. Goal is low latency for a voice assistant.\\n\\nCurrent   \\n\\\\- Gemma3n:e2b: Response Tokens 7.4 t/s, Prompt Tokens 58 t/s  \\n\\\\- Gemma3n:e4b: Response Tokens 4.7 t/s, Prompt Tokens 10 t/s","edited":1751843679,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n1pstrd","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Would you be able to test Gemma3n:e4b and Gemma3n:e2b? They&amp;#39;re smaller models, but I&amp;#39;m currently using an Intel i5 fourth gen and thinking to upgrade the home lab. Goal is low latency for a voice assistant.&lt;/p&gt;\\n\\n&lt;p&gt;Current&lt;br/&gt;\\n- Gemma3n:e2b: Response Tokens 7.4 t/s, Prompt Tokens 58 t/s&lt;br/&gt;\\n- Gemma3n:e4b: Response Tokens 4.7 t/s, Prompt Tokens 10 t/s&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1ltamap","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1pstrd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751843284,"author_flair_text":null,"treatment_tags":[],"created_utc":1751843284,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1ppjze","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"eloquentemu","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1pjre4","score":3,"author_fullname":"t2_lpdsy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"They're pretty easy to come by around here, but I could run some of you have a specific request","edited":false,"author_flair_css_class":null,"name":"t1_n1ppjze","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They&amp;#39;re pretty easy to come by around here, but I could run some of you have a specific request&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1ltamap","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1ppjze/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751842155,"author_flair_text":null,"collapsed":false,"created_utc":1751842155,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n1pjre4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"InternationalNebula7","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1pb5ng","score":4,"author_fullname":"t2_1pwtr13a","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I would love to see some LLM benchmarks on CPU only &amp; DDR5 setups.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1pjre4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I would love to see some LLM benchmarks on CPU only &amp;amp; DDR5 setups.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1pjre4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751840120,"author_flair_text":null,"treatment_tags":[],"created_utc":1751840120,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n1pb5ng","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FullstackSensei","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1p2b8v","score":5,"author_fullname":"t2_17n3nqtj56","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Change to a workstation or server motherboard with an Epyc processor. You'll get 8 channels of DDR4-3200 and up to 64 cores with a SP3 motherboard. Will be about 4x the memory bandwidth you have with your current motherboard and 5x the number of PCIe lanes (Epyc has 128 lanes). Best part is: motherboard + CPU + 256GB RAM will cost about the same as your current motherboard + CPU + RAM if you're on a DDR5 platform.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1pb5ng","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Change to a workstation or server motherboard with an Epyc processor. You&amp;#39;ll get 8 channels of DDR4-3200 and up to 64 cores with a SP3 motherboard. Will be about 4x the memory bandwidth you have with your current motherboard and 5x the number of PCIe lanes (Epyc has 128 lanes). Best part is: motherboard + CPU + 256GB RAM will cost about the same as your current motherboard + CPU + RAM if you&amp;#39;re on a DDR5 platform.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1pb5ng/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751837262,"author_flair_text":null,"treatment_tags":[],"created_utc":1751837262,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n1p2b8v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"gnad","can_mod_post":false,"created_utc":1751834516,"send_replies":true,"parent_id":"t1_n1oxqik","score":1,"author_fullname":"t2_giuiz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Unfortunately my mobo only have 2 RAM slots. This would require me to change into 4 RAM slots mobo and getting more RAM and a GPU for prompt which probably wont be cheaper.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1p2b8v","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Unfortunately my mobo only have 2 RAM slots. This would require me to change into 4 RAM slots mobo and getting more RAM and a GPU for prompt which probably wont be cheaper.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1p2b8v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751834516,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1oxqik","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"rnosov","can_mod_post":false,"created_utc":1751833115,"send_replies":true,"parent_id":"t3_1ltamap","score":45,"author_fullname":"t2_18x6fa","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think your best bet would be to get single used 3090/4090 and lots of RAM and try your luck with KTransformers or similar. They [report](https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/AMX.md) 6 tok/sec on a \\"consumer PC\\" with 4090 for  Qwen3-235. You can event \\"try before you buy\\" with cloud 3090 instance.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1oxqik","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think your best bet would be to get single used 3090/4090 and lots of RAM and try your luck with KTransformers or similar. They &lt;a href=\\"https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/AMX.md\\"&gt;report&lt;/a&gt; 6 tok/sec on a &amp;quot;consumer PC&amp;quot; with 4090 for  Qwen3-235. You can event &amp;quot;try before you buy&amp;quot; with cloud 3090 instance.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1oxqik/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751833115,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltamap","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":45}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1pb44a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"PawelSalsa","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1p9f1q","score":4,"author_fullname":"t2_12dubk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"5600mt/s, I use two quants 3b and 4b kxl, they are identical in responses but one is 30gig smaller than other, if there is no difference, why use bigger one? Also, my goal was to run deepseek R1 671b and with 192gb of ram and 120gb vram it works as well with 2b quants from unsloth.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1pb44a","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;5600mt/s, I use two quants 3b and 4b kxl, they are identical in responses but one is 30gig smaller than other, if there is no difference, why use bigger one? Also, my goal was to run deepseek R1 671b and with 192gb of ram and 120gb vram it works as well with 2b quants from unsloth.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1pb44a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751837249,"author_flair_text":null,"treatment_tags":[],"created_utc":1751837249,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n1p9f1q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"gnad","can_mod_post":false,"created_utc":1751836707,"send_replies":true,"parent_id":"t1_n1p8rwc","score":2,"author_fullname":"t2_giuiz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What ram speed do you get with running 4 stick?   \\n\\nI can run Qwen 235B Q2 alright with 96gb but i'd like some room to try bigger quants.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1p9f1q","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What ram speed do you get with running 4 stick?   &lt;/p&gt;\\n\\n&lt;p&gt;I can run Qwen 235B Q2 alright with 96gb but i&amp;#39;d like some room to try bigger quants.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1p9f1q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751836707,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1p8rwc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"PawelSalsa","can_mod_post":false,"created_utc":1751836503,"send_replies":true,"parent_id":"t3_1ltamap","score":6,"author_fullname":"t2_12dubk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You can run qwen3 96Gb 3b k_xl from UnSloth I run it personally and it works perfectly even without any GPU. I have proArt x870e with 192Gb Ram. If you have only two ram slots you can buy 2x 64gb ram sticks on Amazon for about 320usd, and run 3b model inside ram only with around 4t/s. Funny fact, even if I load this model to my 5x 3090 fully, the speed is even slower with only 3t/s as oppose to 4t/s inside ram. The reason is 5x 3090 doesn't scale well with home PC, at least with my setup.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1p8rwc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can run qwen3 96Gb 3b k_xl from UnSloth I run it personally and it works perfectly even without any GPU. I have proArt x870e with 192Gb Ram. If you have only two ram slots you can buy 2x 64gb ram sticks on Amazon for about 320usd, and run 3b model inside ram only with around 4t/s. Funny fact, even if I load this model to my 5x 3090 fully, the speed is even slower with only 3t/s as oppose to 4t/s inside ram. The reason is 5x 3090 doesn&amp;#39;t scale well with home PC, at least with my setup.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1p8rwc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751836503,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltamap","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1rbvs4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"__JockY__","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1r4b0h","score":2,"author_fullname":"t2_qf8h7ka8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Four 48GB RTX A6000 Ampere.","edited":false,"author_flair_css_class":null,"name":"t1_n1rbvs4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Four 48GB RTX A6000 Ampere.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1ltamap","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1rbvs4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751865004,"author_flair_text":null,"collapsed":false,"created_utc":1751865004,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1r4b0h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"COBECT","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1qt3h1","score":2,"author_fullname":"t2_1umam7ln","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What GPUs?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1r4b0h","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What GPUs?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1r4b0h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751861384,"author_flair_text":null,"treatment_tags":[],"created_utc":1751861384,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1rbtgx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"__JockY__","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1r9bny","score":1,"author_fullname":"t2_qf8h7ka8","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Lol I agree with you, but the bean-counters seldom do!","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n1rbtgx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Lol I agree with you, but the bean-counters seldom do!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1ltamap","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1rbtgx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751864971,"author_flair_text":null,"treatment_tags":[],"created_utc":1751864971,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1r9bny","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Caffdy","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1r2uhy","score":1,"author_fullname":"t2_ql2vu0wz","approved_by":null,"mod_note":null,"all_awardings":[],"body":"yeah, when corporate data is involved, a local LLM is the way to go. But then we're talking about a business expense — which can and should — get better hardware for that.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n1r9bny","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;yeah, when corporate data is involved, a local LLM is the way to go. But then we&amp;#39;re talking about a business expense — which can and should — get better hardware for that.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1ltamap","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1r9bny/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751863739,"author_flair_text":null,"treatment_tags":[],"created_utc":1751863739,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1r2uhy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"__JockY__","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1qz8zi","score":3,"author_fullname":"t2_qf8h7ka8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Indeed, and an extreme one. \\n\\nI think a more common scenario is that folks are simply wary of breaking corporate policy by either putting company IP into a pool of OpenAI training data, or by using AI-generated code in a production environment when it’s prohibited.","edited":false,"author_flair_css_class":null,"name":"t1_n1r2uhy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Indeed, and an extreme one. &lt;/p&gt;\\n\\n&lt;p&gt;I think a more common scenario is that folks are simply wary of breaking corporate policy by either putting company IP into a pool of OpenAI training data, or by using AI-generated code in a production environment when it’s prohibited.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1ltamap","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1r2uhy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751860726,"author_flair_text":null,"collapsed":false,"created_utc":1751860726,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n1qz8zi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Caffdy","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1qt3h1","score":1,"author_fullname":"t2_ql2vu0wz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"your situation is unusual not gonna lie","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1qz8zi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;your situation is unusual not gonna lie&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1qz8zi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751859159,"author_flair_text":null,"treatment_tags":[],"created_utc":1751859159,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1qt3h1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"__JockY__","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1qsdec","score":3,"author_fullname":"t2_qf8h7ka8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Not all of us can do that. For example, I do my work in an offline environment where internet access is unavailable. \\n\\nThis is why we run a monster rig capable of 60 tokens/sec from Qwen3 235B A22B in 4-bit AWQ with vLLM. Results are close enough to SOTA that we’re very happy.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1qt3h1","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not all of us can do that. For example, I do my work in an offline environment where internet access is unavailable. &lt;/p&gt;\\n\\n&lt;p&gt;This is why we run a monster rig capable of 60 tokens/sec from Qwen3 235B A22B in 4-bit AWQ with vLLM. Results are close enough to SOTA that we’re very happy.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1qt3h1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751856627,"author_flair_text":null,"treatment_tags":[],"created_utc":1751856627,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n1qsdec","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Caffdy","can_mod_post":false,"created_utc":1751856340,"send_replies":true,"parent_id":"t1_n1pofzb","score":1,"author_fullname":"t2_ql2vu0wz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"honestly, for work, people should just use an API from one of the main vendors, eventually we will get vast memory bandwidth cheaply and  efficiently (not guzzling 8-10x GPU monsters)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1qsdec","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;honestly, for work, people should just use an API from one of the main vendors, eventually we will get vast memory bandwidth cheaply and  efficiently (not guzzling 8-10x GPU monsters)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1qsdec/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751856340,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1pofzb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"__JockY__","can_mod_post":false,"created_utc":1751841766,"send_replies":true,"parent_id":"t3_1ltamap","score":5,"author_fullname":"t2_qf8h7ka8","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Offloading a 235B model to RAM, even at Q4, is gonna suck because you’ll be lucky to get 5 tokens/second. \\n\\nIf you’re just into the novelty of getting a SOTA model to run o. Your hardware, great.\\n\\nBut if you want to actually get useful work accomplished, the slow speeds are quickly going to make this an exercise in frustration.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1pofzb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Offloading a 235B model to RAM, even at Q4, is gonna suck because you’ll be lucky to get 5 tokens/second. &lt;/p&gt;\\n\\n&lt;p&gt;If you’re just into the novelty of getting a SOTA model to run o. Your hardware, great.&lt;/p&gt;\\n\\n&lt;p&gt;But if you want to actually get useful work accomplished, the slow speeds are quickly going to make this an exercise in frustration.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1pofzb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751841766,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltamap","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1pqtrg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Ok_Top9254","can_mod_post":false,"created_utc":1751842591,"send_replies":true,"parent_id":"t1_n1peljw","score":5,"author_fullname":"t2_1e4s71l3kv","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"People have been saying that last 2 years... until DDR6, it's still cheap and 3x faster than best dual channel DDR5 kits. And still has better support than similarly aged amd cards.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1pqtrg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;People have been saying that last 2 years... until DDR6, it&amp;#39;s still cheap and 3x faster than best dual channel DDR5 kits. And still has better support than similarly aged amd cards.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1pqtrg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751842591,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1rqovw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Healthy-Nebula-3603","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1rpg13","score":3,"author_fullname":"t2_ogjj6ebj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yep \\n\\nThat's a law. :)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1rqovw","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yep &lt;/p&gt;\\n\\n&lt;p&gt;That&amp;#39;s a law. :)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1rqovw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751873136,"author_flair_text":null,"treatment_tags":[],"created_utc":1751873136,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n1rpg13","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"muxxington","can_mod_post":false,"created_utc":1751872400,"send_replies":true,"parent_id":"t1_n1peljw","score":2,"author_fullname":"t2_1ktdmsvo","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"So one day I will start my box and the P40s will just not work anymore?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1rpg13","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So one day I will start my box and the P40s will just not work anymore?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1rpg13/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751872400,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1peljw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Healthy-Nebula-3603","can_mod_post":false,"created_utc":1751838389,"send_replies":true,"parent_id":"t3_1ltamap","score":12,"author_fullname":"t2_ogjj6ebj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"P40?? \\n\\n  \\nDo not go there ...those cards are extremely obsolete and soon literally nothing will be working on it","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1peljw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;P40?? &lt;/p&gt;\\n\\n&lt;p&gt;Do not go there ...those cards are extremely obsolete and soon literally nothing will be working on it&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1peljw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751838389,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltamap","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1rfwki","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1r8248","score":1,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Check out my threads about it.","edited":false,"author_flair_css_class":null,"name":"t1_n1rfwki","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Check out my threads about it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1ltamap","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1rfwki/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751867087,"author_flair_text":null,"collapsed":false,"created_utc":1751867087,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1r8248","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Zestyclose-Sell-2049","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1pb4za","score":1,"author_fullname":"t2_dcv8n4by","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"How many tokens per second do you get? And what model?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1r8248","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How many tokens per second do you get? And what model?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1r8248/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751863136,"author_flair_text":null,"treatment_tags":[],"created_utc":1751863136,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1pb4za","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1p4j8s","score":3,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; Maybe i can get this and run llm distributed with my current machine.\\n\\nYou can. That's why I got mine. It's easy to run llama.cpp distributed. I already ran 3 machines before I got the X2. It'll be the 4th.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1pb4za","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Maybe i can get this and run llm distributed with my current machine.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;You can. That&amp;#39;s why I got mine. It&amp;#39;s easy to run llama.cpp distributed. I already ran 3 machines before I got the X2. It&amp;#39;ll be the 4th.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1pb4za/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751837256,"author_flair_text":null,"treatment_tags":[],"created_utc":1751837256,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n1p4j8s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"gnad","can_mod_post":false,"created_utc":1751835192,"send_replies":true,"parent_id":"t1_n1p3vad","score":3,"author_fullname":"t2_giuiz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Alone it has 128GB which is not enough for Qwen 235B Q4. Maybe i can get this and run llm distributed with my current machine.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1p4j8s","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Alone it has 128GB which is not enough for Qwen 235B Q4. Maybe i can get this and run llm distributed with my current machine.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1p4j8s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751835192,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n1p3vad","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Exhales_Deeply","can_mod_post":false,"created_utc":1751834988,"send_replies":true,"parent_id":"t3_1ltamap","score":4,"author_fullname":"t2_p5os4ads","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Have you considered something unified?\\n\\n[https://www.amazon.ca/GMKtec-EVO-X2-Computers-LPDDR5X-8000MHz/dp/B0F53MLYQ6](https://www.amazon.ca/GMKtec-EVO-X2-Computers-LPDDR5X-8000MHz/dp/B0F53MLYQ6)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1p3vad","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Have you considered something unified?&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.amazon.ca/GMKtec-EVO-X2-Computers-LPDDR5X-8000MHz/dp/B0F53MLYQ6\\"&gt;https://www.amazon.ca/GMKtec-EVO-X2-Computers-LPDDR5X-8000MHz/dp/B0F53MLYQ6&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1p3vad/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751834988,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltamap","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1t8k3h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"beryugyo619","can_mod_post":false,"created_utc":1751898085,"send_replies":true,"parent_id":"t1_n1pa6h7","score":1,"author_fullname":"t2_v8wruy0k","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"One thing I don't understand about those V340s, why are those cheaper than MI25? I mean, shouldn't they be twice as fast, maybe not in reality but in theory?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1t8k3h","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;One thing I don&amp;#39;t understand about those V340s, why are those cheaper than MI25? I mean, shouldn&amp;#39;t they be twice as fast, maybe not in reality but in theory?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1t8k3h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751898085,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1pa6h7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"created_utc":1751836949,"send_replies":true,"parent_id":"t3_1ltamap","score":3,"author_fullname":"t2_o65i6kx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"10 x v340s will get you 10 x 16GB = 160GB. That costs 10 x $49 = $490. You might be able to negotiate a volume discount.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1pa6h7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;10 x v340s will get you 10 x 16GB = 160GB. That costs 10 x $49 = $490. You might be able to negotiate a volume discount.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1pa6h7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751836949,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltamap","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1qbc7j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"sub_RedditTor","can_mod_post":false,"created_utc":1751849975,"send_replies":true,"parent_id":"t3_1ltamap","score":3,"author_fullname":"t2_oy3c84euj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It's mostly all about memory bandwidth and support ..\\n\\nYes. Mi50 Instinct is the cheapest way but those cards will lack features Nvidia will have and Rocm has dropped support..","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1qbc7j","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s mostly all about memory bandwidth and support ..&lt;/p&gt;\\n\\n&lt;p&gt;Yes. Mi50 Instinct is the cheapest way but those cards will lack features Nvidia will have and Rocm has dropped support..&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1qbc7j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751849975,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltamap","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1pq6c3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"pducharme","can_mod_post":false,"created_utc":1751842368,"send_replies":true,"parent_id":"t3_1ltamap","score":2,"author_fullname":"t2_ertux","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For me, since i'm not in a hurry, i'll wait for the B60 Dual (48GB) and maybe put 2 (depending on price) to get 96GB VRAM.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1pq6c3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For me, since i&amp;#39;m not in a hurry, i&amp;#39;ll wait for the B60 Dual (48GB) and maybe put 2 (depending on price) to get 96GB VRAM.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1pq6c3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751842368,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltamap","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1p1jwh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"gnad","can_mod_post":false,"created_utc":1751834286,"send_replies":true,"parent_id":"t1_n1oyt4a","score":11,"author_fullname":"t2_giuiz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes but cost per gb significantly more than other option","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1p1jwh","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes but cost per gb significantly more than other option&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1p1jwh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751834286,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1p1gcf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"starkruzr","can_mod_post":false,"created_utc":1751834256,"send_replies":true,"parent_id":"t1_n1oyt4a","score":5,"author_fullname":"t2_34g6p","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"this is what I did. you get some extra useful stuff with Blackwell too.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1p1gcf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;this is what I did. you get some extra useful stuff with Blackwell too.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1p1gcf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751834256,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1psh1i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Dry-Influence9","can_mod_post":false,"created_utc":1751843161,"send_replies":true,"parent_id":"t1_n1oyt4a","score":3,"author_fullname":"t2_vep8zxxd","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"16gb of somewhat slow vram and the bandwidth is a very important part of the equation in our use case.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1psh1i","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;16gb of somewhat slow vram and the bandwidth is a very important part of the equation in our use case.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1psh1i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751843161,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n1oyt4a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"avedave","can_mod_post":false,"created_utc":1751833450,"send_replies":true,"parent_id":"t3_1ltamap","score":4,"author_fullname":"t2_2i9tmcrd","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"5060ti has now 16GB and costs around $400","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1oyt4a","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;5060ti has now 16GB and costs around $400&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1oyt4a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751833450,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltamap","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1pcj2g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"_xulion","can_mod_post":false,"created_utc":1751837710,"send_replies":true,"parent_id":"t3_1ltamap","score":2,"author_fullname":"t2_a1dvxm4d","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I’ve been running 235B Q4 without GPU on my dual 6140 server and I can get 4-5 tps. Cheaper than a single 3090 I think?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1pcj2g","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I’ve been running 235B Q4 without GPU on my dual 6140 server and I can get 4-5 tps. Cheaper than a single 3090 I think?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1pcj2g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751837710,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltamap","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1pq6l4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"waka324","can_mod_post":false,"created_utc":1751842370,"send_replies":true,"parent_id":"t3_1ltamap","score":2,"author_fullname":"t2_88nl5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"RTX 8000 is 48gb for ~2000.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1pq6l4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;RTX 8000 is 48gb for ~2000.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1pq6l4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751842370,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltamap","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1p7qne","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Rich_Artist_8327","can_mod_post":false,"created_utc":1751836180,"send_replies":true,"parent_id":"t3_1ltamap","score":2,"author_fullname":"t2_1jk2ep8a52","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"None of them. 2x 7900 xtx","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1p7qne","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;None of them. 2x 7900 xtx&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1p7qne/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751836180,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltamap","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1pmtuy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Secure_Reflection409","can_mod_post":false,"created_utc":1751841194,"send_replies":true,"parent_id":"t1_n1p9yj0","score":1,"author_fullname":"t2_by77ogdhr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"16GB? These are modified ones, I guess?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1pmtuy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;16GB? These are modified ones, I guess?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1pmtuy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751841194,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1rqo91","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"myjunkyard","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1qw641","score":1,"author_fullname":"t2_152btk5e","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"My bad. I meant 4060 16gb.\\n\\n  \\nBut, it's been weeks since I bought one, and now they no longer sell it, only 5060 ti's 16gb at the price! You're right!\\n\\nYeah aussie prices are a little higher due to gst/vat/crap so in USA I expect them to be cheaper.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1rqo91","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;My bad. I meant 4060 16gb.&lt;/p&gt;\\n\\n&lt;p&gt;But, it&amp;#39;s been weeks since I bought one, and now they no longer sell it, only 5060 ti&amp;#39;s 16gb at the price! You&amp;#39;re right!&lt;/p&gt;\\n\\n&lt;p&gt;Yeah aussie prices are a little higher due to gst/vat/crap so in USA I expect them to be cheaper.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1rqo91/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751873125,"author_flair_text":null,"treatment_tags":[],"created_utc":1751873125,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1qw641","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"created_utc":1751857879,"send_replies":true,"parent_id":"t1_n1p9yj0","score":1,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; I can get them for around USD $495 new.\\n\\nFor that price, why not get a 5060ti 16GB? It's cheaper and better.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1qw641","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;I can get them for around USD $495 new.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;For that price, why not get a 5060ti 16GB? It&amp;#39;s cheaper and better.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1qw641/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751857879,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1p9yj0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"myjunkyard","can_mod_post":false,"created_utc":1751836878,"send_replies":true,"parent_id":"t3_1ltamap","score":1,"author_fullname":"t2_152btk5e","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"RTX 3060 16GB units should be available for you? It's niche but here in Australia I can get them for around USD $495 new.\\n\\nThat should get your 64GB for 4x RTX 3060's?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1p9yj0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;RTX 3060 16GB units should be available for you? It&amp;#39;s niche but here in Australia I can get them for around USD $495 new.&lt;/p&gt;\\n\\n&lt;p&gt;That should get your 64GB for 4x RTX 3060&amp;#39;s?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1p9yj0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751836878,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltamap","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1pb3vj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Icy-Clock6930","can_mod_post":false,"created_utc":1751837247,"send_replies":true,"parent_id":"t3_1ltamap","score":1,"author_fullname":"t2_a40ujbv3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What hardware do you actually have?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1pb3vj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What hardware do you actually have?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1pb3vj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751837247,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltamap","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1pggoj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1751839003,"send_replies":true,"parent_id":"t3_1ltamap","score":1,"author_fullname":"t2_h5utwre7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"P40s are on the way out software wise and aren't cheap anymore. Mi50 seem like the best bet from your list. That or waiting for those promised new nvidia with 24gb.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1pggoj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;P40s are on the way out software wise and aren&amp;#39;t cheap anymore. Mi50 seem like the best bet from your list. That or waiting for those promised new nvidia with 24gb.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1pggoj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751839003,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltamap","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1xcggd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"colin_colout","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1syq0j","score":1,"author_fullname":"t2_14l4ya","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I haven't. I run llama.cpp on headless Linux server.","edited":false,"author_flair_css_class":null,"name":"t1_n1xcggd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I haven&amp;#39;t. I run llama.cpp on headless Linux server.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1ltamap","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1xcggd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751945862,"author_flair_text":null,"collapsed":false,"created_utc":1751945862,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1syq0j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"anonim1133","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1sxc0n","score":1,"author_fullname":"t2_66nqq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Have you just installed ollama/lmstudio and it just used 780m as an accelerator?\\n\\nNo additional configuration was needed?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1syq0j","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Have you just installed ollama/lmstudio and it just used 780m as an accelerator?&lt;/p&gt;\\n\\n&lt;p&gt;No additional configuration was needed?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1syq0j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751894889,"author_flair_text":null,"treatment_tags":[],"created_utc":1751894889,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1sxc0n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"colin_colout","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1rrezc","score":1,"author_fullname":"t2_14l4ya","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Depends on your goals. If I keep my context under 4k tokens I get 20-40tk/s generation on qwen3-30b-a3b. Prompt processing isn't great by fine for smaller prompts (~120-200 tk/a depending on context size). It can run quantized qwen3-235b but pp suffers greatly (both pp and generation ~10tk/s in small context situations)\\n\\nKv cache mitigates this somewhat. \\n\\nIt depends on your use case, expectations , and patents.  You can spend tens of thousands on a 80gb vram setup for near real time inference, or you can spend $800 on a single device and run the same model albeit not realtime","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1sxc0n","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Depends on your goals. If I keep my context under 4k tokens I get 20-40tk/s generation on qwen3-30b-a3b. Prompt processing isn&amp;#39;t great by fine for smaller prompts (~120-200 tk/a depending on context size). It can run quantized qwen3-235b but pp suffers greatly (both pp and generation ~10tk/s in small context situations)&lt;/p&gt;\\n\\n&lt;p&gt;Kv cache mitigates this somewhat. &lt;/p&gt;\\n\\n&lt;p&gt;It depends on your use case, expectations , and patents.  You can spend tens of thousands on a 80gb vram setup for near real time inference, or you can spend $800 on a single device and run the same model albeit not realtime&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1sxc0n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751894406,"author_flair_text":null,"treatment_tags":[],"created_utc":1751894406,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1rrezc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"anonim1133","can_mod_post":false,"created_utc":1751873570,"send_replies":true,"parent_id":"t1_n1pya4p","score":1,"author_fullname":"t2_66nqq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Do you need to \\"hack\\" your way into it? I've had that idea, that integrated GPUs are not able to work as \\"AI accelerators\\"?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1rrezc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Do you need to &amp;quot;hack&amp;quot; your way into it? I&amp;#39;ve had that idea, that integrated GPUs are not able to work as &amp;quot;AI accelerators&amp;quot;?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1rrezc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751873570,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1pya4p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"colin_colout","can_mod_post":false,"created_utc":1751845200,"send_replies":true,"parent_id":"t3_1ltamap","score":1,"author_fullname":"t2_14l4ya","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Bear with me... Integrated GPU and max out your memory. \\n\\nI use a 780m mini pc with an 8845hs and 128gb of dual channel 5600mhz RAM. I can use 80gb of graphics memory rocm/vulkan legacy mode (16gb base + 64gb GTT) and 64gb if I use rocm uma mode. \\n\\nIt takes tinkering but it's likely the cheapest way to get 80gb. There's a desktop CPU with that chip too... Just know you'll be bottle necked on RAM speed and often shader throughput as well.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1pya4p","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Bear with me... Integrated GPU and max out your memory. &lt;/p&gt;\\n\\n&lt;p&gt;I use a 780m mini pc with an 8845hs and 128gb of dual channel 5600mhz RAM. I can use 80gb of graphics memory rocm/vulkan legacy mode (16gb base + 64gb GTT) and 64gb if I use rocm uma mode. &lt;/p&gt;\\n\\n&lt;p&gt;It takes tinkering but it&amp;#39;s likely the cheapest way to get 80gb. There&amp;#39;s a desktop CPU with that chip too... Just know you&amp;#39;ll be bottle necked on RAM speed and often shader throughput as well.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1pya4p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751845200,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltamap","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1qh3qw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rubntagme","can_mod_post":false,"created_utc":1751852070,"send_replies":true,"parent_id":"t3_1ltamap","score":1,"author_fullname":"t2_9u03lebo","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"A thread ripper with 3090 and 256 of ram add another 3090 later","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1qh3qw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;A thread ripper with 3090 and 256 of ram add another 3090 later&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1qh3qw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751852070,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltamap","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1r9gzl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Antakux","can_mod_post":false,"created_utc":1751863811,"send_replies":true,"parent_id":"t3_1ltamap","score":1,"author_fullname":"t2_uhlhqc8","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Just hunt for a cheap 3090s\\nI got mine for $550 and then stack up 2 more 3060 you'll get fast 48gb since the 3090 will carry and 3060's are very cheap and easy to find and memory bandwidth is good for the price too","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1r9gzl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just hunt for a cheap 3090s\\nI got mine for $550 and then stack up 2 more 3060 you&amp;#39;ll get fast 48gb since the 3090 will carry and 3060&amp;#39;s are very cheap and easy to find and memory bandwidth is good for the price too&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1r9gzl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751863811,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltamap","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1rsxvu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"unserioustroller","can_mod_post":false,"created_utc":1751874489,"send_replies":true,"parent_id":"t3_1ltamap","score":1,"author_fullname":"t2_1iislbtdvk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"whichever supports nvlink. don't go for consumer nvidia cards","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1rsxvu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;whichever supports nvlink. don&amp;#39;t go for consumer nvidia cards&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1rsxvu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751874489,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltamap","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1syi2g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"beryugyo619","can_mod_post":false,"created_utc":1751894811,"send_replies":true,"parent_id":"t3_1ltamap","score":1,"author_fullname":"t2_v8wruy0k","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"someone should try this FirePro W7100, definitely not going to work but 40GB for $200 lol https://www.ebay.com/itm/197294714883","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1syi2g","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;someone should try this FirePro W7100, definitely not going to work but 40GB for $200 lol &lt;a href=\\"https://www.ebay.com/itm/197294714883\\"&gt;https://www.ebay.com/itm/197294714883&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1syi2g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751894811,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltamap","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1t1kgz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MelodicRecognition7","can_mod_post":false,"created_utc":1751895878,"send_replies":true,"parent_id":"t3_1ltamap","score":1,"author_fullname":"t2_1eex9ug5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"unfortunately you need 140 GB of **V**RAM to get usable speeds, 96 GB RAM will make it painfully slow.\\nI've tried to run it on 96 GB **V**RAM offloading remaining 40 GBs to CPU and got only 7 t/s which is usable only for short one-sentence questions and answers and totally unusable for thinking mode. I think on 96 GB RAM and just 48 **V**RAM it will be like 0.7 t/s","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1t1kgz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;unfortunately you need 140 GB of &lt;strong&gt;V&lt;/strong&gt;RAM to get usable speeds, 96 GB RAM will make it painfully slow.\\nI&amp;#39;ve tried to run it on 96 GB &lt;strong&gt;V&lt;/strong&gt;RAM offloading remaining 40 GBs to CPU and got only 7 t/s which is usable only for short one-sentence questions and answers and totally unusable for thinking mode. I think on 96 GB RAM and just 48 &lt;strong&gt;V&lt;/strong&gt;RAM it will be like 0.7 t/s&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1t1kgz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751895878,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltamap","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1t706b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"space_man_2","can_mod_post":false,"created_utc":1751897615,"send_replies":true,"parent_id":"t3_1ltamap","score":1,"author_fullname":"t2_bcd7h","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Just a thought but the cheapest way is to use large bar, e.g system memory","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1t706b","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just a thought but the cheapest way is to use large bar, e.g system memory&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1t706b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751897615,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltamap","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1ujqp9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"real-joedoe07","can_mod_post":false,"created_utc":1751911905,"send_replies":true,"parent_id":"t3_1ltamap","score":1,"author_fullname":"t2_sm5q963q","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Buy a Mac Studio","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ujqp9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Buy a Mac Studio&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1ujqp9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751911905,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltamap","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1uk853","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kerneldesign","can_mod_post":false,"created_utc":1751912058,"send_replies":true,"parent_id":"t3_1ltamap","score":1,"author_fullname":"t2_ltxie","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Buy a Mac. Less expensive :)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1uk853","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Buy a Mac. Less expensive :)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1uk853/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751912058,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltamap","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1vc15h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"dr_manhattan_br","can_mod_post":false,"created_utc":1751921199,"send_replies":true,"parent_id":"t3_1ltamap","score":1,"author_fullname":"t2_f7pvk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You need to take into account the power consumption and then choose the best option between power consumption and cost. Also performance could be a factor. Newer GPUs will have support for more features like CUDA generation’s or ROCm. From Nvidia side an RTX 4000 gen and RTX 5000 gen will be preferable","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1vc15h","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You need to take into account the power consumption and then choose the best option between power consumption and cost. Also performance could be a factor. Newer GPUs will have support for more features like CUDA generation’s or ROCm. From Nvidia side an RTX 4000 gen and RTX 5000 gen will be preferable&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1vc15h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751921199,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltamap","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1viil7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Radiant_Truth_8743","can_mod_post":false,"created_utc":1751923430,"send_replies":true,"parent_id":"t3_1ltamap","score":1,"author_fullname":"t2_gsxbw4s4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Go for Nvidia A40 card it's slightly above your 1000$ budget but it's worth it","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1viil7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Go for Nvidia A40 card it&amp;#39;s slightly above your 1000$ budget but it&amp;#39;s worth it&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1viil7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751923430,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltamap","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1vm4zv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"YuriyGavrilov","can_mod_post":false,"created_utc":1751924633,"send_replies":true,"parent_id":"t3_1ltamap","score":1,"author_fullname":"t2_ifnv5cqx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Orange ai studio pro :) will be enough just need to sold your 96gb","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1vm4zv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Orange ai studio pro :) will be enough just need to sold your 96gb&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1vm4zv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751924633,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltamap","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1vo0br","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HeavyBolter333","can_mod_post":false,"created_utc":1751925219,"send_replies":true,"parent_id":"t3_1ltamap","score":1,"author_fullname":"t2_gpk67irt8","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"£1k for 48gb Vram with intel B60 dual","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1vo0br","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;£1k for 48gb Vram with intel B60 dual&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1vo0br/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751925219,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltamap","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1vw3bc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Eden1506","can_mod_post":false,"created_utc":1751927777,"send_replies":true,"parent_id":"t3_1ltamap","score":1,"author_fullname":"t2_2ezqqypt","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"https://www.reddit.com/r/LocalLLaMA/s/dMm1NvXjCk","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1vw3bc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/s/dMm1NvXjCk\\"&gt;https://www.reddit.com/r/LocalLLaMA/s/dMm1NvXjCk&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1vw3bc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751927777,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltamap","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1wqjk3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Sorry_Sort6059","can_mod_post":false,"created_utc":1751937914,"send_replies":true,"parent_id":"t3_1ltamap","score":1,"author_fullname":"t2_11pjfap7i9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Not considering modded 2080ti 22GB or 4090 48GB? Those are some sweet options.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1wqjk3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not considering modded 2080ti 22GB or 4090 48GB? Those are some sweet options.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1wqjk3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751937914,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltamap","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1ry03w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"themungbeans","can_mod_post":false,"created_utc":1751877544,"send_replies":true,"parent_id":"t1_n1prxfe","score":3,"author_fullname":"t2_3vo0xl71","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I had been looking at this option too.  They make a nice dual SXM2 with 2x NVLink pcie cards.  The V100's also offer a slightly newer option than the P40/P100.\\n\\nI would avoid the 3060 they have very slow mem bandwidth.  Digital spaceport did a good YT vid on this and it turned me off that as an option fast.  5060Ti still have avg mem bandwidth but will be costly.\\n\\n3090's are still quite expensive at about 750-800USD shipped.  Always hearing about scam's on those xx90 cards too...  But they are the simplest option and probably the fastest in the budget","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ry03w","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I had been looking at this option too.  They make a nice dual SXM2 with 2x NVLink pcie cards.  The V100&amp;#39;s also offer a slightly newer option than the P40/P100.&lt;/p&gt;\\n\\n&lt;p&gt;I would avoid the 3060 they have very slow mem bandwidth.  Digital spaceport did a good YT vid on this and it turned me off that as an option fast.  5060Ti still have avg mem bandwidth but will be costly.&lt;/p&gt;\\n\\n&lt;p&gt;3090&amp;#39;s are still quite expensive at about 750-800USD shipped.  Always hearing about scam&amp;#39;s on those xx90 cards too...  But they are the simplest option and probably the fastest in the budget&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1ry03w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751877544,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1y0slx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"az226","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1xyifi","score":1,"author_fullname":"t2_yamxn","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"PCIe 3 gen switches are pretty cheap if your motherboard can’t bifurcate.\\n\\nThe actual speed is 260 bidirectional. Their theoretical speed never happens in practice. Same thing with flops. Says 125 but I’ve never seen it be higher than maybe 102-103 for “perfect” shapes and closer to 60 for “anti-perfect” ones.","edited":1751958682,"author_flair_css_class":null,"name":"t1_n1y0slx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;PCIe 3 gen switches are pretty cheap if your motherboard can’t bifurcate.&lt;/p&gt;\\n\\n&lt;p&gt;The actual speed is 260 bidirectional. Their theoretical speed never happens in practice. Same thing with flops. Says 125 but I’ve never seen it be higher than maybe 102-103 for “perfect” shapes and closer to 60 for “anti-perfect” ones.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1ltamap","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1y0slx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751957542,"author_flair_text":null,"collapsed":false,"created_utc":1751957542,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1xyifi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"themungbeans","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1uv4ay","score":1,"author_fullname":"t2_3vo0xl71","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yeah they list it as 155GB/s split over 6 links, this is also bidirectional.  Should total roughly 300GB/s between the two SXM2 processors.  Each SXM2 processor will consume a physical x16 slot, electrically they recommend either 8x+8x or 16x+16x or a bifurcated x16.  Bifurcation is usually motherboard specific somewhat common in server.  Pretty low numbers of consumer boards support this.\\n\\nIf PCIe bus lanes are an issue some not too old dual cpu xeon workstation motherboards can be had pretty cheaply and offer 40 lanes per CPU.  Theoretical 10x PCIe x8 lanes (40x2) offered but in reality you would probably get 8 electrically 8x connectors with a physical x16 slot.\\n\\nThe guys running old mining GPU's run on PCIe1.0x4 (1GB/s) say model loading is slow but once loaded inference is not greatly impacted.  I did see some YT vids confirming this fact using the p102-100's\\n\\nThe price for the 16GB V100 SXM2's are crazy reasonable right now. 275USD for a single on PCIe x16 adaptor (inc cooler) or 605USD for the dual nvlink option with 2x 16GB V100, NVlink board and coolers.  Strickly speaking the 3090 is faster you could get 2x V100's for the price so the V100's offer more VRAM.\\n\\n[MachineZer0](https://www.reddit.com/user/MachineZer0/) did a crazy good summary of some of the models of GPU's mentioned in this thread. I think the TitanV and V100 are similar in processor but the V100 has about 200GB/s more mem bandwidth:  \\\\[Link\\\\](https://redd.it/1f6hjwf)","edited":1751956683,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1xyifi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah they list it as 155GB/s split over 6 links, this is also bidirectional.  Should total roughly 300GB/s between the two SXM2 processors.  Each SXM2 processor will consume a physical x16 slot, electrically they recommend either 8x+8x or 16x+16x or a bifurcated x16.  Bifurcation is usually motherboard specific somewhat common in server.  Pretty low numbers of consumer boards support this.&lt;/p&gt;\\n\\n&lt;p&gt;If PCIe bus lanes are an issue some not too old dual cpu xeon workstation motherboards can be had pretty cheaply and offer 40 lanes per CPU.  Theoretical 10x PCIe x8 lanes (40x2) offered but in reality you would probably get 8 electrically 8x connectors with a physical x16 slot.&lt;/p&gt;\\n\\n&lt;p&gt;The guys running old mining GPU&amp;#39;s run on PCIe1.0x4 (1GB/s) say model loading is slow but once loaded inference is not greatly impacted.  I did see some YT vids confirming this fact using the p102-100&amp;#39;s&lt;/p&gt;\\n\\n&lt;p&gt;The price for the 16GB V100 SXM2&amp;#39;s are crazy reasonable right now. 275USD for a single on PCIe x16 adaptor (inc cooler) or 605USD for the dual nvlink option with 2x 16GB V100, NVlink board and coolers.  Strickly speaking the 3090 is faster you could get 2x V100&amp;#39;s for the price so the V100&amp;#39;s offer more VRAM.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.reddit.com/user/MachineZer0/\\"&gt;MachineZer0&lt;/a&gt; did a crazy good summary of some of the models of GPU&amp;#39;s mentioned in this thread. I think the TitanV and V100 are similar in processor but the V100 has about 200GB/s more mem bandwidth:  [Link](&lt;a href=\\"https://redd.it/1f6hjwf\\"&gt;https://redd.it/1f6hjwf&lt;/a&gt;)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1xyifi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751956290,"author_flair_text":null,"treatment_tags":[],"created_utc":1751956290,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1uv4ay","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"az226","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1sms7e","score":1,"author_fullname":"t2_yamxn","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Bandwidth is like 130GB/second, which is way faster than 16GB/sec PCIe 3. This is much faster for sharded models.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1uv4ay","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Bandwidth is like 130GB/second, which is way faster than 16GB/sec PCIe 3. This is much faster for sharded models.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1uv4ay/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751915590,"author_flair_text":null,"treatment_tags":[],"created_utc":1751915590,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1sms7e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"gnad","can_mod_post":false,"created_utc":1751890432,"send_replies":true,"parent_id":"t1_n1prxfe","score":1,"author_fullname":"t2_giuiz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Interesting. How does this compare to Pcie version?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1sms7e","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Interesting. How does this compare to Pcie version?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1sms7e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751890432,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1prxfe","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"az226","can_mod_post":false,"created_utc":1751842970,"send_replies":true,"parent_id":"t3_1ltamap","score":1,"author_fullname":"t2_yamxn","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You can get 2x V100 SXM2 32GB for like $350-400 each and then get a 2 SXM carrier board with Nvlink from China for like $150.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1prxfe","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can get 2x V100 SXM2 32GB for like $350-400 each and then get a 2 SXM carrier board with Nvlink from China for like $150.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1prxfe/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751842970,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltamap","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1p15nx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"panchovix","can_mod_post":false,"created_utc":1751834167,"send_replies":true,"parent_id":"t1_n1oxs5n","score":7,"author_fullname":"t2_j1kqr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Maybe OP wants to train or use diffusion pipelines.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1p15nx","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Maybe OP wants to train or use diffusion pipelines.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1p15nx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751834167,"author_flair_text":"Llama 405B","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1pai02","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok-Bill3318","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1oyzi6","score":-2,"author_fullname":"t2_8csiswjr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Mac unified memory is VRAM. Buy a studio, spec the ram you want. Done. \\n\\nOne device no cables no heat problems and probably similar cost or less than a bunch of recent GPUs.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1pai02","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Mac unified memory is VRAM. Buy a studio, spec the ram you want. Done. &lt;/p&gt;\\n\\n&lt;p&gt;One device no cables no heat problems and probably similar cost or less than a bunch of recent GPUs.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1pai02/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751837052,"author_flair_text":null,"treatment_tags":[],"created_utc":1751837052,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1qwd51","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1qc62p","score":4,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt; What is the cheapest way to get 256gb of VRAM? A Mac Studio. Show me an alternative?\\n\\n2xAMD Max+ 128GB. That's cheaper.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1qwd51","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;What is the cheapest way to get 256gb of VRAM? A Mac Studio. Show me an alternative?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;2xAMD Max+ 128GB. That&amp;#39;s cheaper.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1qwd51/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751857960,"author_flair_text":null,"treatment_tags":[],"created_utc":1751857960,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1ssywl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"beryugyo619","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1qc62p","score":1,"author_fullname":"t2_v8wruy0k","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"16x GRID K1. $30 each on eBay. $480 for 256GB of CXL at home. \\n\\nGRID K1 is absolute garbage but it is 256GB total and it is $2/GB. There's more realistically MI50 16GB at $150 on eBay, MI50(CN) 32GB at $150ish on Chinese markets, those are $5 and $10 per GB. On the other hand, Mac Studio 128GB is $4k ish, 256GB configuration is $6k ish, 512GB is $10k ish. That's $25-30 ish per GB. \\n\\nIt was just an shortest path to &gt;64GB VRAM and instant thirst quencher for software guys with too much disposable income. It was never cheap.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ssywl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;16x GRID K1. $30 each on eBay. $480 for 256GB of CXL at home. &lt;/p&gt;\\n\\n&lt;p&gt;GRID K1 is absolute garbage but it is 256GB total and it is $2/GB. There&amp;#39;s more realistically MI50 16GB at $150 on eBay, MI50(CN) 32GB at $150ish on Chinese markets, those are $5 and $10 per GB. On the other hand, Mac Studio 128GB is $4k ish, 256GB configuration is $6k ish, 512GB is $10k ish. That&amp;#39;s $25-30 ish per GB. &lt;/p&gt;\\n\\n&lt;p&gt;It was just an shortest path to &amp;gt;64GB VRAM and instant thirst quencher for software guys with too much disposable income. It was never cheap.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1ssywl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751892842,"author_flair_text":null,"treatment_tags":[],"created_utc":1751892842,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1qc62p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HelloFollyWeThereYet","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1oyzi6","score":-2,"author_fullname":"t2_9tf3bs9v","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What is the cheapest way to get 256gb of VRAM?  A Mac Studio.  Show me an alternative?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1qc62p","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What is the cheapest way to get 256gb of VRAM?  A Mac Studio.  Show me an alternative?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1qc62p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751850276,"author_flair_text":null,"treatment_tags":[],"created_utc":1751850276,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1oyzi6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"joninco","can_mod_post":false,"created_utc":1751833504,"send_replies":true,"parent_id":"t1_n1oxs5n","score":18,"author_fullname":"t2_8e8y0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"OP said \\"next step is to get some cheap VRAM\\" which would exclude macs.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1oyzi6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;OP said &amp;quot;next step is to get some cheap VRAM&amp;quot; which would exclude macs.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1oyzi6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751833504,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":18}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1p1yr8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ShinyAnkleBalls","can_mod_post":false,"created_utc":1751834410,"send_replies":true,"parent_id":"t1_n1oxs5n","score":5,"author_fullname":"t2_2m3au2xb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Macs are great value/money if you are ONLY going to do LLM inference/training. If you are going to play with TTS, STT, image and video models, you still need GPUs.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1p1yr8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Macs are great value/money if you are ONLY going to do LLM inference/training. If you are going to play with TTS, STT, image and video models, you still need GPUs.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1p1yr8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751834410,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1p17l9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rorowhat","can_mod_post":false,"created_utc":1751834183,"send_replies":true,"parent_id":"t1_n1oxs5n","score":-3,"author_fullname":"t2_yq51a","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Macs are for the birds","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1p17l9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Macs are for the birds&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltamap","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1p17l9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751834183,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-3}}],"before":null}},"user_reports":[],"saved":false,"id":"n1oxs5n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"illkeepthatinmind","can_mod_post":false,"created_utc":1751833130,"send_replies":true,"parent_id":"t3_1ltamap","score":-5,"author_fullname":"t2_5skk0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":true,"body":"Why no mention of Macs with unified RAM?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1oxs5n","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why no mention of Macs with unified RAM?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/n1oxs5n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751833130,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltamap","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-5}}],"before":null}}]`),n=()=>e.jsx(l,{data:a});export{n as default};
