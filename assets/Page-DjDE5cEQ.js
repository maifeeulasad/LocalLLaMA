import{j as e}from"./index-Bu7qcPAU.js";import{R as l}from"./RedditPostRenderer-CbHA7O5q.js";import"./index-BKgbfxhf.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"1. Mistral\\\\_large-Instruct\\n2. Qwen3-235B\\n3. Command-A\\n4. Deepseek-V3\\n5. Deepseek-R1\\n6. Deepseek-R1-0528\\n7. Deepseek-TNG-R1T2-Chimera\\n8. Kimi-K2\\n9. Ernie-4.5-300b\\n10. llama3.1-405B\\n11. llama3.1-Nemotron-Ultra-253b?\\n12. Others?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Which local 100B+ heavy weight models are your favorite and why?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m58695","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.94,"author_flair_background_color":"#bbbdbf","subreddit_type":"public","ups":108,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","is_original_content":false,"author_fullname":"t2_ah13x","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":108,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753067953,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"richtext","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;ol&gt;\\n&lt;li&gt;Mistral_large-Instruct&lt;/li&gt;\\n&lt;li&gt;Qwen3-235B&lt;/li&gt;\\n&lt;li&gt;Command-A&lt;/li&gt;\\n&lt;li&gt;Deepseek-V3&lt;/li&gt;\\n&lt;li&gt;Deepseek-R1&lt;/li&gt;\\n&lt;li&gt;Deepseek-R1-0528&lt;/li&gt;\\n&lt;li&gt;Deepseek-TNG-R1T2-Chimera&lt;/li&gt;\\n&lt;li&gt;Kimi-K2&lt;/li&gt;\\n&lt;li&gt;Ernie-4.5-300b&lt;/li&gt;\\n&lt;li&gt;llama3.1-405B&lt;/li&gt;\\n&lt;li&gt;llama3.1-Nemotron-Ultra-253b?&lt;/li&gt;\\n&lt;li&gt;Others?&lt;/li&gt;\\n&lt;/ol&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":"llama.cpp","treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1m58695","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"segmond","discussion_type":null,"num_comments":103,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":"light","permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/","subreddit_subscribers":502516,"created_utc":1753067953,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ac6iw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"-p-e-w-","can_mod_post":false,"created_utc":1753072291,"send_replies":true,"parent_id":"t1_n4a3sbc","score":50,"author_fullname":"t2_dkgrhaet","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Itâ€™s crazy how that model was released *a full year ago* and Meta simply hasnâ€™t followed up on it. They really dropped the ball.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ac6iw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Itâ€™s crazy how that model was released &lt;em&gt;a full year ago&lt;/em&gt; and Meta simply hasnâ€™t followed up on it. They really dropped the ball.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4ac6iw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753072291,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":50}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4awwbq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Thedudely1","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4ady7o","score":26,"author_fullname":"t2_i305y","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Llama doesn't head every paragraph with an emoji ðŸ˜©","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4awwbq","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Llama doesn&amp;#39;t head every paragraph with an emoji ðŸ˜©&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4awwbq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753083119,"author_flair_text":null,"treatment_tags":[],"created_utc":1753083119,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":26}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4e4f5g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"InsideYork","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4azpvi","score":2,"author_fullname":"t2_12s3hn4y0b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What do you think is a better example? I don't know those movies lol","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4e4f5g","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What do you think is a better example? I don&amp;#39;t know those movies lol&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4e4f5g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753124697,"author_flair_text":null,"treatment_tags":[],"created_utc":1753124697,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4azpvi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"harlekinrains","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4ady7o","score":14,"author_fullname":"t2_4296b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Try Kimi K2 for depth of knowledge and taste.. ;)\\n\\nPrompt: Movies like Bullet to Bejing:\\n\\nKimi K2:\\nhttps://pastebin.com/73shtg5C\\n\\nLlama 3.3 70B:\\nhttps://pastebin.com/QrUWSLDz","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4azpvi","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Try Kimi K2 for depth of knowledge and taste.. ;)&lt;/p&gt;\\n\\n&lt;p&gt;Prompt: Movies like Bullet to Bejing:&lt;/p&gt;\\n\\n&lt;p&gt;Kimi K2:\\n&lt;a href=\\"https://pastebin.com/73shtg5C\\"&gt;https://pastebin.com/73shtg5C&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Llama 3.3 70B:\\n&lt;a href=\\"https://pastebin.com/QrUWSLDz\\"&gt;https://pastebin.com/QrUWSLDz&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4azpvi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753084729,"author_flair_text":null,"treatment_tags":[],"created_utc":1753084729,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":14}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4auw76","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"InsideYork","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4ady7o","score":4,"author_fullname":"t2_12s3hn4y0b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Which size? Qwen3 4b and 8b feels like chatgpt 3.5 with nothink. I use the site for deepseek v3 which response do you mean? What makes it artificial? The system output format?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4auw76","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Which size? Qwen3 4b and 8b feels like chatgpt 3.5 with nothink. I use the site for deepseek v3 which response do you mean? What makes it artificial? The system output format?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4auw76/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753081975,"author_flair_text":null,"treatment_tags":[],"created_utc":1753081975,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4c6e9y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Dry-Judgment4242","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4c25aq","score":2,"author_fullname":"t2_c8epovvbk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Mine runs decently well even at 80k context.\\n\\n\\nBut I'm running a lot of various context injects and other stuff to direct it.\\n\\n\\nNot much other choices to run heh.... Older models are just too dumb even if their prose is good they don't follow instructions like newer models can.","edited":false,"author_flair_css_class":null,"name":"t1_n4c6e9y","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Mine runs decently well even at 80k context.&lt;/p&gt;\\n\\n&lt;p&gt;But I&amp;#39;m running a lot of various context injects and other stuff to direct it.&lt;/p&gt;\\n\\n&lt;p&gt;Not much other choices to run heh.... Older models are just too dumb even if their prose is good they don&amp;#39;t follow instructions like newer models can.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m58695","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4c6e9y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753104588,"author_flair_text":null,"collapsed":false,"created_utc":1753104588,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4c25aq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ForsookComparison","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4beftr","score":3,"author_fullname":"t2_on5es7pe3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Gemma3 gets loopy as soon as you give it even a modest amount of context.\\n\\nIt's the only model that can succeed at RAG via Tool-Calling and then fail at the finish line due to the context that provides.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4c25aq","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Gemma3 gets loopy as soon as you give it even a modest amount of context.&lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s the only model that can succeed at RAG via Tool-Calling and then fail at the finish line due to the context that provides.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4c25aq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753103153,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1753103153,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4bgzjk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"oblio-","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4beftr","score":1,"author_fullname":"t2_9a80o","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Curious about the setup. How does it work, both as configuration and also performance wise.\\n\\nSo you have the model in VRAM and then the rest is offloaded to RAM? Do you happen to know the performance impact of doing that?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4bgzjk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Curious about the setup. How does it work, both as configuration and also performance wise.&lt;/p&gt;\\n\\n&lt;p&gt;So you have the model in VRAM and then the rest is offloaded to RAM? Do you happen to know the performance impact of doing that?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4bgzjk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753094497,"author_flair_text":null,"treatment_tags":[],"created_utc":1753094497,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4beftr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Dry-Judgment4242","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4ady7o","score":3,"author_fullname":"t2_c8epovvbk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Qwen2.5 was incredible.\\nBut since 3.0, they just became unusable for me.\\nNow just using Gemma 3, 27bfor everything even though I got 120gb VRAM + 128gb ram.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4beftr","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen2.5 was incredible.\\nBut since 3.0, they just became unusable for me.\\nNow just using Gemma 3, 27bfor everything even though I got 120gb VRAM + 128gb ram.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4beftr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753093193,"author_flair_text":null,"treatment_tags":[],"created_utc":1753093193,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n4ady7o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AndreVallestero","can_mod_post":false,"created_utc":1753073111,"send_replies":true,"parent_id":"t1_n4a3sbc","score":31,"author_fullname":"t2_1gvigeuo","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Same reason why I still use Llama3.3 70B. The depth of knowledge is top-tier, and the responses seem to be the most natural.\\n\\n\\nQwen and Deepseek responses both feel more artificial imo","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ady7o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Same reason why I still use Llama3.3 70B. The depth of knowledge is top-tier, and the responses seem to be the most natural.&lt;/p&gt;\\n\\n&lt;p&gt;Qwen and Deepseek responses both feel more artificial imo&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4ady7o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753073111,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":31}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4c1jh2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Affectionate-Cap-600","can_mod_post":false,"created_utc":1753102943,"send_replies":true,"parent_id":"t1_n4a3sbc","score":4,"author_fullname":"t2_5oltmr5b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I found nemotron ultra 253B (derived from llama 405B) to be much better in term of intelligence while retaining a lot of the knowledge of the original model","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4c1jh2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I found nemotron ultra 253B (derived from llama 405B) to be much better in term of intelligence while retaining a lot of the knowledge of the original model&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4c1jh2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753102943,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ax86h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"-dysangel-","can_mod_post":false,"created_utc":1753083311,"send_replies":true,"parent_id":"t1_n4a3sbc","score":2,"author_fullname":"t2_12ggykute6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I guess that explains the trade-off of why llama models have such poor raw reasoning ability. In research, the smaller models do really poorly against for example Qwen3 when applying reinforcement learning for reasoning","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ax86h","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I guess that explains the trade-off of why llama models have such poor raw reasoning ability. In research, the smaller models do really poorly against for example Qwen3 when applying reinforcement learning for reasoning&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4ax86h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753083311,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4cohu1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ylsid","can_mod_post":false,"created_utc":1753110174,"send_replies":true,"parent_id":"t1_n4a3sbc","score":1,"author_fullname":"t2_6lmlc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That's interesting. Could you provide an example where it would succeed where DeepSeek would fail?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4cohu1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s interesting. Could you provide an example where it would succeed where DeepSeek would fail?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4cohu1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753110174,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4etafn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Freonr2","can_mod_post":false,"created_utc":1753131793,"send_replies":true,"parent_id":"t1_n4a3sbc","score":1,"author_fullname":"t2_8xi6x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Likewise, Scout is a very solid vision model and extremely fast.  \\n\\nAt least on my VLM vibes it is comparable to Gemma 3 27B but loads faster.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4etafn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Likewise, Scout is a very solid vision model and extremely fast.  &lt;/p&gt;\\n\\n&lt;p&gt;At least on my VLM vibes it is comparable to Gemma 3 27B but loads faster.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4etafn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753131793,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4a3sbc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ForsookComparison","can_mod_post":false,"created_utc":1753068635,"send_replies":true,"parent_id":"t3_1m58695","score":74,"author_fullname":"t2_on5es7pe3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Llama3.1 405B isn't SOTA anymore for intelligence, but it's still SOTA in terms of its ridiculous depth of knowledge. Pretty sure in raw trivia it'd wipe the floor with Deepseek and Kimi.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4a3sbc","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Llama3.1 405B isn&amp;#39;t SOTA anymore for intelligence, but it&amp;#39;s still SOTA in terms of its ridiculous depth of knowledge. Pretty sure in raw trivia it&amp;#39;d wipe the floor with Deepseek and Kimi.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4a3sbc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753068635,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m58695","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":74}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4bclru","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Admirable-Star7088","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4b5o14","score":1,"author_fullname":"t2_qhlcbiy3k","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks for your insights with FP8. Looks like that our quality experiences are similar and, therefore (at least for Qwen3-235b) the \\"issue\\" is mostly the model itself and not the Q4 quant.\\n\\nIn practise, I have so far used dots.llm1 for writing (such as general text composition and story writing) and Q&amp;A. I did test it *briefly* for coding and it looked promising, however since much smaller (and usually faster) models such as Qwen3-30b-A3b and GLM-4 9b/32b fulfills most of my coding needs, I have just stuck with them for this use case.\\n\\nAs for Qwen3-235b, I have actually not used it for anything practical so far, apart from just testing it. I always find myself falling back to dots.llm1, or much smaller models such as GLM-4/Qwen3-30b.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n4bclru","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for your insights with FP8. Looks like that our quality experiences are similar and, therefore (at least for Qwen3-235b) the &amp;quot;issue&amp;quot; is mostly the model itself and not the Q4 quant.&lt;/p&gt;\\n\\n&lt;p&gt;In practise, I have so far used dots.llm1 for writing (such as general text composition and story writing) and Q&amp;amp;A. I did test it &lt;em&gt;briefly&lt;/em&gt; for coding and it looked promising, however since much smaller (and usually faster) models such as Qwen3-30b-A3b and GLM-4 9b/32b fulfills most of my coding needs, I have just stuck with them for this use case.&lt;/p&gt;\\n\\n&lt;p&gt;As for Qwen3-235b, I have actually not used it for anything practical so far, apart from just testing it. I always find myself falling back to dots.llm1, or much smaller models such as GLM-4/Qwen3-30b.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m58695","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4bclru/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753092212,"author_flair_text":null,"treatment_tags":[],"created_utc":1753092212,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4b5o14","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"random-tomato","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4b4bx3","score":3,"author_fullname":"t2_fmd6oq5v6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"(I run Qwen3-235B and dots.llm1 both in FP8)\\n\\nJust adding my two cents: Qwen3-235B doesnâ€™t feel that incredible for its size, but I think it generally makes fewer mistakes than the 32B. Outside of coding/math stuff I don't really like it, it just feels stiff.\\n\\ndots.llm1 feels like the best non-reasoning MoE model Iâ€™ve tried. For its size it's solid for coding and for technical writing its style is actually quite nice.\\n\\nWhat do you usually use them for?","edited":false,"author_flair_css_class":null,"name":"t1_n4b5o14","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;(I run Qwen3-235B and dots.llm1 both in FP8)&lt;/p&gt;\\n\\n&lt;p&gt;Just adding my two cents: Qwen3-235B doesnâ€™t feel that incredible for its size, but I think it generally makes fewer mistakes than the 32B. Outside of coding/math stuff I don&amp;#39;t really like it, it just feels stiff.&lt;/p&gt;\\n\\n&lt;p&gt;dots.llm1 feels like the best non-reasoning MoE model Iâ€™ve tried. For its size it&amp;#39;s solid for coding and for technical writing its style is actually quite nice.&lt;/p&gt;\\n\\n&lt;p&gt;What do you usually use them for?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m58695","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4b5o14/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753088228,"author_flair_text":"llama.cpp","collapsed":false,"created_utc":1753088228,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ek80x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Affectionate-Cap-600","can_mod_post":false,"created_utc":1753129201,"send_replies":true,"parent_id":"t1_n4eer77","score":1,"author_fullname":"t2_5oltmr5b","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"oh I'm sorry I didn't know about that.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ek80x","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;oh I&amp;#39;m sorry I didn&amp;#39;t know about that.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4ek80x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753129201,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m58695","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":9,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4eer77","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Admirable-Star7088","can_mod_post":false,"created_utc":1753127644,"send_replies":true,"parent_id":"t1_n4edyrd","score":2,"author_fullname":"t2_qhlcbiy3k","approved_by":null,"mod_note":null,"all_awardings":[],"body":"He meant the new non-thinking version released just hours ago:\\n\\n[https://www.reddit.com/r/LocalLLaMA/comments/1m5owi8/qwen3235ba22b2507\\\\_released/](https://www.reddit.com/r/LocalLLaMA/comments/1m5owi8/qwen3235ba22b2507_released/)","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n4eer77","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;He meant the new non-thinking version released just hours ago:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1m5owi8/qwen3235ba22b2507_released/\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1m5owi8/qwen3235ba22b2507_released/&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4eer77/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753127644,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m58695","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":8,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4edyrd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Affectionate-Cap-600","can_mod_post":false,"created_utc":1753127420,"send_replies":true,"parent_id":"t1_n4e9aav","score":1,"author_fullname":"t2_5oltmr5b","approved_by":null,"mod_note":null,"all_awardings":[],"body":"~~it is the same model, it has a thinking / non thinking tag that you would add to the systems message or prompt~~\\n\\nmy bad, thanks for the correction! seems that they released another model today (qwen 235B 0527)\\n\\n(anyway, what I said still apply for qwen 235B reasoning MoEs)","edited":1753129416,"gildings":{},"author_flair_css_class":null,"name":"t1_n4edyrd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;del&gt;it is the same model, it has a thinking / non thinking tag that you would add to the systems message or prompt&lt;/del&gt;&lt;/p&gt;\\n\\n&lt;p&gt;my bad, thanks for the correction! seems that they released another model today (qwen 235B 0527)&lt;/p&gt;\\n\\n&lt;p&gt;(anyway, what I said still apply for qwen 235B reasoning MoEs)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m58695","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4edyrd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753127420,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4eelg3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Admirable-Star7088","can_mod_post":false,"created_utc":1753127600,"send_replies":true,"parent_id":"t1_n4e9aav","score":1,"author_fullname":"t2_qhlcbiy3k","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I saw it too, can't wait to try it out! :D","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n4eelg3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I saw it too, can&amp;#39;t wait to try it out! :D&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m58695","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4eelg3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753127600,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4e9aav","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DrVonSinistro","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4bqpee","score":1,"author_fullname":"t2_cy3wb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"With but I can't wait to download and try the new non-reasoning iteration.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n4e9aav","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;With but I can&amp;#39;t wait to download and try the new non-reasoning iteration.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m58695","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4e9aav/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753126077,"author_flair_text":null,"treatment_tags":[],"created_utc":1753126077,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4bqpee","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Admirable-Star7088","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4bp3lm","score":1,"author_fullname":"t2_qhlcbiy3k","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Are you using 235b with or without reasoning?","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n4bqpee","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Are you using 235b with or without reasoning?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m58695","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4bqpee/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753098890,"author_flair_text":null,"treatment_tags":[],"created_utc":1753098890,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4bp3lm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DrVonSinistro","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4b4bx3","score":3,"author_fullname":"t2_cy3wb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"235B vs 32B for me is that 235B will have much more contextual intelligence. Example when translating Chinese, 32B will translate Yuanyuan as the most common way of writing it which is a repetition of 2x Yuan. 235B will realise its a woman's name and that women might write it in a romanticised fashion. Thus will translate it as its often written when it is a name AND it will tell me that I should inform myself how that person write her name to write it the same way etc.\\n\\n32B to refactor and correct code is 95% as good as 235B but 235B is unmatched vs 32B to CREATE code (vibe).","edited":false,"author_flair_css_class":null,"name":"t1_n4bp3lm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;235B vs 32B for me is that 235B will have much more contextual intelligence. Example when translating Chinese, 32B will translate Yuanyuan as the most common way of writing it which is a repetition of 2x Yuan. 235B will realise its a woman&amp;#39;s name and that women might write it in a romanticised fashion. Thus will translate it as its often written when it is a name AND it will tell me that I should inform myself how that person write her name to write it the same way etc.&lt;/p&gt;\\n\\n&lt;p&gt;32B to refactor and correct code is 95% as good as 235B but 235B is unmatched vs 32B to CREATE code (vibe).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m58695","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4bp3lm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753098230,"author_flair_text":null,"collapsed":false,"created_utc":1753098230,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4bxti2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"segmond","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4b4bx3","score":3,"author_fullname":"t2_ah13x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"wow, how did I miss dots.llm?  never heard of it.","edited":false,"author_flair_css_class":null,"name":"t1_n4bxti2","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;wow, how did I miss dots.llm?  never heard of it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m58695","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4bxti2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753101632,"author_flair_text":"llama.cpp","collapsed":false,"created_utc":1753101632,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4e6qdc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"InsideYork","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4b4bx3","score":2,"author_fullname":"t2_12s3hn4y0b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The bigger models don't seem much better until you see the small ones stumble on easy ones randomly and often.","edited":false,"author_flair_css_class":null,"name":"t1_n4e6qdc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The bigger models don&amp;#39;t seem much better until you see the small ones stumble on easy ones randomly and often.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m58695","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4e6qdc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753125347,"author_flair_text":null,"collapsed":false,"created_utc":1753125347,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4b4bx3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Admirable-Star7088","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4ajh77","score":3,"author_fullname":"t2_qhlcbiy3k","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I have only recently begun to use very large models (after upgrading my RAM), and what struck me (in my relatively limited testings so far) is that they don't seem *that very much* more powerful than much smaller models. Maybe quantization makes them dumber, since I usually can only run very large models at a relatively low quant.\\n\\nErnie-4.5-300b (Q3\\\\_K\\\\_XL): The most knowledgeable model I have ever used locally, but apart from that it does not seem extraordinary smart or powerful. Maybe this model takes a huge hit from Q3?\\n\\nQwen3-235b (Q4\\\\_K\\\\_XL) It's overall good, and also pretty knowledgeable, but for general use (apart from coding) it does not seem much more intelligent than Qwen3-32b or even 30b-A3b. Sometimes Qwen3-235b feels even worse than much smaller models. Could again the relatively low quant (Q4) play a huge part here?\\n\\ndots.llm1 (Q6\\\\_K\\\\_XL): This 142b model is in my experience very sensitive to quantization so I run the highest quant my RAM can handle (Q6). This is the overall smartest large model I've used and is probably my favorite for general use.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4b4bx3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have only recently begun to use very large models (after upgrading my RAM), and what struck me (in my relatively limited testings so far) is that they don&amp;#39;t seem &lt;em&gt;that very much&lt;/em&gt; more powerful than much smaller models. Maybe quantization makes them dumber, since I usually can only run very large models at a relatively low quant.&lt;/p&gt;\\n\\n&lt;p&gt;Ernie-4.5-300b (Q3_K_XL): The most knowledgeable model I have ever used locally, but apart from that it does not seem extraordinary smart or powerful. Maybe this model takes a huge hit from Q3?&lt;/p&gt;\\n\\n&lt;p&gt;Qwen3-235b (Q4_K_XL) It&amp;#39;s overall good, and also pretty knowledgeable, but for general use (apart from coding) it does not seem much more intelligent than Qwen3-32b or even 30b-A3b. Sometimes Qwen3-235b feels even worse than much smaller models. Could again the relatively low quant (Q4) play a huge part here?&lt;/p&gt;\\n\\n&lt;p&gt;dots.llm1 (Q6_K_XL): This 142b model is in my experience very sensitive to quantization so I run the highest quant my RAM can handle (Q6). This is the overall smartest large model I&amp;#39;ve used and is probably my favorite for general use.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4b4bx3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753087443,"author_flair_text":null,"treatment_tags":[],"created_utc":1753087443,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n4ajh77","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DrVonSinistro","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4a9la6","score":7,"author_fullname":"t2_cy3wb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"235B Q4 does outsmart 32B Q8 in few things (coding &amp; translation) for me but 32B Q8 does come close with 2-3x more reasoning and follow-up prompts.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4ajh77","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;235B Q4 does outsmart 32B Q8 in few things (coding &amp;amp; translation) for me but 32B Q8 does come close with 2-3x more reasoning and follow-up prompts.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4ajh77/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753075831,"author_flair_text":null,"treatment_tags":[],"created_utc":1753075831,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4be13f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Secure_Reflection409","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4a9la6","score":2,"author_fullname":"t2_by77ogdhr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It doesn't even outsmart Q4KL from my limited testing.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4be13f","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It doesn&amp;#39;t even outsmart Q4KL from my limited testing.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4be13f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753092978,"author_flair_text":null,"treatment_tags":[],"created_utc":1753092978,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4bvxdb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ForsookComparison","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4a9la6","score":0,"author_fullname":"t2_on5es7pe3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes Q2 is smarter and stronger than even full fat 32B, but being Q2, I have caught it making a silly mistake here and there - reliability is down.","edited":1753103262,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4bvxdb","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes Q2 is smarter and stronger than even full fat 32B, but being Q2, I have caught it making a silly mistake here and there - reliability is down.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4bvxdb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753100933,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1753100933,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n4a9la6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"solidsnakeblue","can_mod_post":false,"created_utc":1753071120,"send_replies":true,"parent_id":"t1_n4a39e3","score":9,"author_fullname":"t2_7zh6fslk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Does Q2 outsmart the Qwen3-32B dense model at Q8?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4a9la6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Does Q2 outsmart the Qwen3-32B dense model at Q8?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4a9la6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753071120,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4c7rko","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4c2uaa","score":1,"author_fullname":"t2_h5utwre7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I thought main benefit of L4 was having to shuffle less data due to one expert always activating.. which you could then place on GPU.\\n\\nIn qwen it can be any of the experts within the model, under-used experts aside. Never considered it as dense vs sparse since the experts were per layer.","edited":false,"author_flair_css_class":null,"name":"t1_n4c7rko","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I thought main benefit of L4 was having to shuffle less data due to one expert always activating.. which you could then place on GPU.&lt;/p&gt;\\n\\n&lt;p&gt;In qwen it can be any of the experts within the model, under-used experts aside. Never considered it as dense vs sparse since the experts were per layer.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m58695","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4c7rko/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753105045,"author_flair_text":null,"collapsed":false,"created_utc":1753105045,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ei5pf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Affectionate-Cap-600","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4c2uaa","score":1,"author_fullname":"t2_5oltmr5b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"yeah I never seen that count (btw, that's really interesting, thanks for sharing) but as I remember both llama 4 and qwen alternate dense and MoEs FFN, but llama has an higher MLP  intermediate dim for both the dense and MoE layers, but the higher dim of the MoEs is balanced from the fact that they use 2 active experts instead of the 8 experts used from qwen, and that one of those two experts is always active. also maybe another reason is that llama has a higher vocabulary and a higher hidden dim so maybe that also contributes to the 'always active' parameters count.\\nbtw in qwen the individual MLPs of the MoEs layers are really small, with 1536 as intermediate dimensions. \\n\\nbut I'm just speculating, where did you got those number from?","edited":1753128919,"author_flair_css_class":null,"name":"t1_n4ei5pf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;yeah I never seen that count (btw, that&amp;#39;s really interesting, thanks for sharing) but as I remember both llama 4 and qwen alternate dense and MoEs FFN, but llama has an higher MLP  intermediate dim for both the dense and MoE layers, but the higher dim of the MoEs is balanced from the fact that they use 2 active experts instead of the 8 experts used from qwen, and that one of those two experts is always active. also maybe another reason is that llama has a higher vocabulary and a higher hidden dim so maybe that also contributes to the &amp;#39;always active&amp;#39; parameters count.\\nbtw in qwen the individual MLPs of the MoEs layers are really small, with 1536 as intermediate dimensions. &lt;/p&gt;\\n\\n&lt;p&gt;but I&amp;#39;m just speculating, where did you got those number from?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m58695","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4ei5pf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753128616,"author_flair_text":null,"collapsed":false,"created_utc":1753128616,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4c2uaa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"b3081a","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4bqbly","score":6,"author_fullname":"t2_17n5yh7l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Qwen is 7.8B dense + 14.2B sparse = 22B active, while Llama is 14B dense + 3B sparse = 17B active. In both cases the dense layers could be easily handled by a single dGPU, but having 14.2B active params in sparse layers makes it extremely difficult to offload to a mainstream desktop platform. You'll get something like 5 token/s with Qwen3 experts offloaded to a dual channel desktop CPU or iGPU, and &gt;15 token/s with Llama4.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4c2uaa","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen is 7.8B dense + 14.2B sparse = 22B active, while Llama is 14B dense + 3B sparse = 17B active. In both cases the dense layers could be easily handled by a single dGPU, but having 14.2B active params in sparse layers makes it extremely difficult to offload to a mainstream desktop platform. You&amp;#39;ll get something like 5 token/s with Qwen3 experts offloaded to a dual channel desktop CPU or iGPU, and &amp;gt;15 token/s with Llama4.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4c2uaa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753103392,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1753103392,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4c1s1w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4c0lk6","score":1,"author_fullname":"t2_h5utwre7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes, it is. With a shared expert so it's faster.","edited":false,"author_flair_css_class":null,"name":"t1_n4c1s1w","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, it is. With a shared expert so it&amp;#39;s faster.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m58695","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4c1s1w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753103025,"author_flair_text":null,"collapsed":false,"created_utc":1753103025,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4c0lk6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ForsookComparison","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4bqbly","score":1,"author_fullname":"t2_on5es7pe3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"And isn't Llama4 17B active?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4c0lk6","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;And isn&amp;#39;t Llama4 17B active?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4c0lk6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753102615,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1753102615,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4bqbly","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"a_beautiful_rhind","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4a54wj","score":7,"author_fullname":"t2_h5utwre7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What? Qwen is 22b active.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4bqbly","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What? Qwen is 22b active.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4bqbly/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753098736,"author_flair_text":null,"treatment_tags":[],"created_utc":1753098736,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}}],"before":null}},"user_reports":[],"saved":false,"id":"n4a54wj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"b3081a","can_mod_post":false,"created_utc":1753069204,"send_replies":true,"parent_id":"t1_n4a39e3","score":9,"author_fullname":"t2_17n5yh7l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Llama 4 is extremely sparse in its active MoE weights (3B active in 384B experts) while Qwen3 235B is much harder to handle (14B active in 227B experts) so llama4 would be almost 5x as fast when using a slow but large memory device for experts offload. That's definitely not the same level of speed in terms of inference.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4a54wj","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Llama 4 is extremely sparse in its active MoE weights (3B active in 384B experts) while Qwen3 235B is much harder to handle (14B active in 227B experts) so llama4 would be almost 5x as fast when using a slow but large memory device for experts offload. That&amp;#39;s definitely not the same level of speed in terms of inference.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4a54wj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753069204,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}}],"before":null}},"user_reports":[],"saved":false,"id":"n4a39e3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ForsookComparison","can_mod_post":false,"created_utc":1753068416,"send_replies":true,"parent_id":"t3_1m58695","score":46,"author_fullname":"t2_on5es7pe3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Qwen3-235B-A22B is seriously underrated here.\\n\\nIt inferences as light and fast as the Llama4's but is actually damn good. Ridiculously intelligent and basically anyone here can run Q2 at *some* speed because of the 22B active params.\\n\\nI think it missed its hype cycle because Llama4 stole the *\\"wow lightspeed inference\\"* hype and Qwen3-32B was strong enough that most people weren't as excited to run something that needed to partially load into system memory.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4a39e3","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen3-235B-A22B is seriously underrated here.&lt;/p&gt;\\n\\n&lt;p&gt;It inferences as light and fast as the Llama4&amp;#39;s but is actually damn good. Ridiculously intelligent and basically anyone here can run Q2 at &lt;em&gt;some&lt;/em&gt; speed because of the 22B active params.&lt;/p&gt;\\n\\n&lt;p&gt;I think it missed its hype cycle because Llama4 stole the &lt;em&gt;&amp;quot;wow lightspeed inference&amp;quot;&lt;/em&gt; hype and Qwen3-32B was strong enough that most people weren&amp;#39;t as excited to run something that needed to partially load into system memory.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4a39e3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753068416,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m58695","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":46}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4akekt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Mediocre_Leg_754","can_mod_post":false,"created_utc":1753076309,"send_replies":true,"parent_id":"t1_n4a5daa","score":5,"author_fullname":"t2_z9t0whjda","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"How are you using Kimi K2 and in which editor?Â I would love to understand your workflow.Â ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4akekt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How are you using Kimi K2 and in which editor?Â I would love to understand your workflow.Â &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4akekt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753076309,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4bh4m2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"oblio-","can_mod_post":false,"created_utc":1753094567,"send_replies":true,"parent_id":"t1_n4a5daa","score":5,"author_fullname":"t2_9a80o","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The thing is, if it's wrong or the prompt is missing something, isn't that just too slow? I'm reminded of old huge Java project compilation times ðŸ˜„","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4bh4m2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The thing is, if it&amp;#39;s wrong or the prompt is missing something, isn&amp;#39;t that just too slow? I&amp;#39;m reminded of old huge Java project compilation times ðŸ˜„&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4bh4m2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753094567,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4cwmn2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"nomorebuttsplz","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4bfrz9","score":1,"author_fullname":"t2_syq52","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I have a mac m3u with 512gb unified ram","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4cwmn2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have a mac m3u with 512gb unified ram&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4cwmn2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753112505,"author_flair_text":null,"treatment_tags":[],"created_utc":1753112505,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4bfrz9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Commercial-Celery769","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4ae8gh","score":2,"author_fullname":"t2_zws5yqyow","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"How much VRAM and RAM do you have? Also I wonder if its usable if you have fast raid 0 NVME swap .","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4bfrz9","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How much VRAM and RAM do you have? Also I wonder if its usable if you have fast raid 0 NVME swap .&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4bfrz9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753093878,"author_flair_text":null,"treatment_tags":[],"created_utc":1753093878,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4ae8gh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"nomorebuttsplz","can_mod_post":false,"created_utc":1753073244,"send_replies":true,"parent_id":"t1_n4a5daa","score":6,"author_fullname":"t2_syq52","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Kimi K2 is the first time where I feel any model (including paid) is smart enough that the quality gap between my thoughts and its output is often quite small, or sometimes, it is actually thinking more clearly than me. This qualitiy gap is typically small enough  so that the speed gap between our thoughts (at 12 tokens/s) suddenly seems enormous in a way it hasn't before. Such that when we debate about economics, science, philosophy, etc, unless I take a very long time to craft a response, most of the time I am going to be corrected by it rather than correcting it.  This intelligence-through-speed phenomenon is similar to [what Terrence Tao observed](https://www.reddit.com/r/singularity/comments/1m4zwvt/a_take_from_terrance_tao_about_the_international/) about Openai's IMO gold medal. I could outdo it, but it might take me hours to beat what it can do in seconds.\\n\\nFeels mixed.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ae8gh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Kimi K2 is the first time where I feel any model (including paid) is smart enough that the quality gap between my thoughts and its output is often quite small, or sometimes, it is actually thinking more clearly than me. This qualitiy gap is typically small enough  so that the speed gap between our thoughts (at 12 tokens/s) suddenly seems enormous in a way it hasn&amp;#39;t before. Such that when we debate about economics, science, philosophy, etc, unless I take a very long time to craft a response, most of the time I am going to be corrected by it rather than correcting it.  This intelligence-through-speed phenomenon is similar to &lt;a href=\\"https://www.reddit.com/r/singularity/comments/1m4zwvt/a_take_from_terrance_tao_about_the_international/\\"&gt;what Terrence Tao observed&lt;/a&gt; about Openai&amp;#39;s IMO gold medal. I could outdo it, but it might take me hours to beat what it can do in seconds.&lt;/p&gt;\\n\\n&lt;p&gt;Feels mixed.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4ae8gh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753073244,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4bnzx4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"KeikakuAccelerator","can_mod_post":false,"created_utc":1753097753,"send_replies":true,"parent_id":"t1_n4a5daa","score":2,"author_fullname":"t2_31lt0ag8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What's your setup to run kimi k2??","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4bnzx4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What&amp;#39;s your setup to run kimi k2??&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4bnzx4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753097753,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4copfd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ylsid","can_mod_post":false,"created_utc":1753110235,"send_replies":true,"parent_id":"t1_n4a5daa","score":1,"author_fullname":"t2_6lmlc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm not sure I'd want 1300 lines of AI generated code without a lot of review lol","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4copfd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m not sure I&amp;#39;d want 1300 lines of AI generated code without a lot of review lol&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4copfd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753110235,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4a5daa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"segmond","can_mod_post":false,"created_utc":1753069301,"send_replies":true,"parent_id":"t3_1m58695","score":28,"author_fullname":"t2_ah13x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Right now, I'm loving kimi k2.  It's a fucking beast!  I'm running it at 1.2-3tk/sec, now folks are going to say it's slow, but guess what?  it's faster than 99.9% of programmers.   Give it a coding prompt and at that speed, you end up with 1300 line of code in 90 minutes.    Create good too.\\n\\nPrior to Kimi K2, I have been loving DeepseekV3, slow and steady it is for me.  Quality over quantity of tokens.   I was enjoying R1/0528 but at 5tk, it's painful, but it's my last resort if non reasoning models fail. I'm yet to try Ernie, just finished downloading it and will be giving it a go this week, haven't heard much about it, so I'm not holding out hope that it's going to be great.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4a5daa","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Right now, I&amp;#39;m loving kimi k2.  It&amp;#39;s a fucking beast!  I&amp;#39;m running it at 1.2-3tk/sec, now folks are going to say it&amp;#39;s slow, but guess what?  it&amp;#39;s faster than 99.9% of programmers.   Give it a coding prompt and at that speed, you end up with 1300 line of code in 90 minutes.    Create good too.&lt;/p&gt;\\n\\n&lt;p&gt;Prior to Kimi K2, I have been loving DeepseekV3, slow and steady it is for me.  Quality over quantity of tokens.   I was enjoying R1/0528 but at 5tk, it&amp;#39;s painful, but it&amp;#39;s my last resort if non reasoning models fail. I&amp;#39;m yet to try Ernie, just finished downloading it and will be giving it a go this week, haven&amp;#39;t heard much about it, so I&amp;#39;m not holding out hope that it&amp;#39;s going to be great.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4a5daa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753069301,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m58695","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":28}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"7d1f04e6-4920-11ef-b2e1-2e580594e1a1","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4cqxa7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mentallyburnt","can_mod_post":false,"created_utc":1753110879,"send_replies":true,"parent_id":"t1_n4a3uim","score":3,"author_fullname":"t2_f80sg4lz5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Always glad to see people enjoying Nevoria!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4cqxa7","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 3.1"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Always glad to see people enjoying Nevoria!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4cqxa7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753110879,"author_flair_text":"Llama 3.1","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#93b1ba","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4bu0me","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Mediocre_Leg_754","can_mod_post":false,"created_utc":1753100206,"send_replies":true,"parent_id":"t1_n4bnaha","score":1,"author_fullname":"t2_z9t0whjda","approved_by":null,"mod_note":null,"all_awardings":[],"body":"How is the latency?","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n4bu0me","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How is the latency?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m58695","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4bu0me/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753100206,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4bnaha","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SkyFeistyLlama8","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4b7xtf","score":1,"author_fullname":"t2_1hgbaqgbnq","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Laptop GPU haha","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n4bnaha","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Laptop GPU haha&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m58695","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4bnaha/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753097438,"author_flair_text":null,"treatment_tags":[],"created_utc":1753097438,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4b7xtf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Mediocre_Leg_754","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4aticb","score":1,"author_fullname":"t2_z9t0whjda","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Where do you run inference for these models? The Groq doesn't provide inference except the qwen 2 30b. Speed is of utmost importance to me.Â ","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n4b7xtf","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Where do you run inference for these models? The Groq doesn&amp;#39;t provide inference except the qwen 2 30b. Speed is of utmost importance to me.Â &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m58695","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4b7xtf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753089568,"author_flair_text":null,"treatment_tags":[],"created_utc":1753089568,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4aticb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"SkyFeistyLlama8","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4akt9v","score":5,"author_fullname":"t2_1hgbaqgbnq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I would use Mistral Small, Gemma 27B or Qwen 3 32B instead of Qwen 30B MOE. That model is fast but it's dumb in the weirdest ways. It also has a tendency to ruminate and think itself in circles.\\n\\nI'm coming around to the opinion that MOE models only make sense in the 14B to 72B *active* range. A 3B active MOE like Qwen 3 loses too much to dense models while still requiring the same memory footprint.","edited":false,"author_flair_css_class":null,"name":"t1_n4aticb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I would use Mistral Small, Gemma 27B or Qwen 3 32B instead of Qwen 30B MOE. That model is fast but it&amp;#39;s dumb in the weirdest ways. It also has a tendency to ruminate and think itself in circles.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m coming around to the opinion that MOE models only make sense in the 14B to 72B &lt;em&gt;active&lt;/em&gt; range. A 3B active MOE like Qwen 3 loses too much to dense models while still requiring the same memory footprint.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m58695","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4aticb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753081202,"author_flair_text":null,"collapsed":false,"created_utc":1753081202,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n4akt9v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Mediocre_Leg_754","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4ajfvx","score":1,"author_fullname":"t2_z9t0whjda","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It gets the job done, but sometimes it messes this up. For example, I have asked it to particularly not reply to the input, but sometimes it replies. The intent is always to address the grammar or cleaning up of the transcription.Â ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4akt9v","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It gets the job done, but sometimes it messes this up. For example, I have asked it to particularly not reply to the input, but sometimes it replies. The intent is always to address the grammar or cleaning up of the transcription.Â &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4akt9v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753076523,"author_flair_text":null,"treatment_tags":[],"created_utc":1753076523,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4ajfvx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"nomorebuttsplz","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4aizio","score":0,"author_fullname":"t2_syq52","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"not if it is smart enough to do the job.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4ajfvx","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;not if it is smart enough to do the job.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4ajfvx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753075813,"author_flair_text":null,"treatment_tags":[],"created_utc":1753075813,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n4aizio","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Mediocre_Leg_754","can_mod_post":false,"created_utc":1753075577,"send_replies":true,"parent_id":"t1_n4a3uim","score":2,"author_fullname":"t2_z9t0whjda","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I am using qwen 3 30b for the purpose of transcription manipulation do you have recommendation for a  faster model?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4aizio","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I am using qwen 3 30b for the purpose of transcription manipulation do you have recommendation for a  faster model?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4aizio/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753075577,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4b49b5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"dugavo","can_mod_post":false,"created_utc":1753087400,"send_replies":true,"parent_id":"t1_n4a3uim","score":1,"author_fullname":"t2_1nge67um4h","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"How's DeepSeek 0324 faster than Kimi K2? It has less active parameters","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4b49b5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How&amp;#39;s DeepSeek 0324 faster than Kimi K2? It has less active parameters&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4b49b5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753087400,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4elwyr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"nomorebuttsplz","can_mod_post":false,"created_utc":1753129692,"send_replies":true,"parent_id":"t1_n4eljod","score":2,"author_fullname":"t2_syq52","approved_by":null,"mod_note":null,"all_awardings":[],"body":"You're welcome!","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n4elwyr","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You&amp;#39;re welcome!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4elwyr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753129692,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m58695","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":8,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4eljod","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Turbulent_Pin7635","can_mod_post":false,"created_utc":1753129587,"send_replies":true,"parent_id":"t1_n4e956f","score":2,"author_fullname":"t2_1hra1kibwa","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thx! You are the guy!","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n4eljod","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thx! You are the guy!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m58695","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4eljod/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753129587,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4e956f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"nomorebuttsplz","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4dr5qx","score":2,"author_fullname":"t2_syq52","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I am attaching this link as well since it seems relevant as Kimi uses the same architecture as Deepseek, and in general larger models seem more resilient against quantization:\\n\\n[https://huggingface.co/unsloth/DeepSeek-R1-GGUF/discussions/37](https://huggingface.co/unsloth/DeepSeek-R1-GGUF/discussions/37)\\n\\nIf these patterns hold true, the UD IQ3\\\\_XXS quant should be almost identical in performance to Q\\\\_4 and above","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n4e956f","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I am attaching this link as well since it seems relevant as Kimi uses the same architecture as Deepseek, and in general larger models seem more resilient against quantization:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/unsloth/DeepSeek-R1-GGUF/discussions/37\\"&gt;https://huggingface.co/unsloth/DeepSeek-R1-GGUF/discussions/37&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;If these patterns hold true, the UD IQ3_XXS quant should be almost identical in performance to Q_4 and above&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m58695","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4e956f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753126036,"author_flair_text":null,"treatment_tags":[],"created_utc":1753126036,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4dr5qx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Turbulent_Pin7635","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4dnh7g","score":2,"author_fullname":"t2_1hra1kibwa","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Great! Thanks for the insight!","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n4dr5qx","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Great! Thanks for the insight!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m58695","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4dr5qx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753120958,"author_flair_text":null,"treatment_tags":[],"created_utc":1753120958,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4dnh7g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"nomorebuttsplz","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4dcxr9","score":2,"author_fullname":"t2_syq52","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It has more natural intelligence and knows more about the world. itâ€™s quicker, but thatâ€™s because it doesnâ€™t reason, so for reasoning heavy tasks 0528 will likely give a better output eventually.Â \\n\\nItâ€™s also more sure of itself, not easily persuaded, never glazes. It can argue a point and cite range of experts with pretty fair accuracy, at least accurate enough to be useful.\\n\\nFeels more like a smart human. In my experience non-reasoning models can be more creative, and this thing is pretty damn smart. Like the scores that it gets in general intelligence and math are very close to the original o1. And thatâ€™s without reasoning. And just like deepseek v3 feels as smart as R1, even though itâ€™s outputs may not be as good, this thing feels smarter than pretty much anything Iâ€™ve used, except maybe 03.\\n\\nI very briefly tested the quant against the online official chat and it performed just as well in the solo benchmark. Â This could benefit from more iterations though.","edited":false,"author_flair_css_class":null,"name":"t1_n4dnh7g","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It has more natural intelligence and knows more about the world. itâ€™s quicker, but thatâ€™s because it doesnâ€™t reason, so for reasoning heavy tasks 0528 will likely give a better output eventually.Â &lt;/p&gt;\\n\\n&lt;p&gt;Itâ€™s also more sure of itself, not easily persuaded, never glazes. It can argue a point and cite range of experts with pretty fair accuracy, at least accurate enough to be useful.&lt;/p&gt;\\n\\n&lt;p&gt;Feels more like a smart human. In my experience non-reasoning models can be more creative, and this thing is pretty damn smart. Like the scores that it gets in general intelligence and math are very close to the original o1. And thatâ€™s without reasoning. And just like deepseek v3 feels as smart as R1, even though itâ€™s outputs may not be as good, this thing feels smarter than pretty much anything Iâ€™ve used, except maybe 03.&lt;/p&gt;\\n\\n&lt;p&gt;I very briefly tested the quant against the online official chat and it performed just as well in the solo benchmark. Â This could benefit from more iterations though.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m58695","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4dnh7g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753119956,"author_flair_text":null,"collapsed":false,"created_utc":1753119956,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4dcxr9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Turbulent_Pin7635","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4cx6qy","score":1,"author_fullname":"t2_1hra1kibwa","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"And even at Q3 do you think it is better than a Q4 R1?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4dcxr9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;And even at Q3 do you think it is better than a Q4 R1?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4dcxr9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753117085,"author_flair_text":null,"treatment_tags":[],"created_utc":1753117085,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4cx6qy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"nomorebuttsplz","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4b5nok","score":1,"author_fullname":"t2_syq52","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm using UD-IQ3\\\\_XXS, full model at 3 bit weight.\\n\\nI am thinking that I could go up one to UD-Q3\\\\_K\\\\_XL but I am not sure if it should be better.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4cx6qy","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m using UD-IQ3_XXS, full model at 3 bit weight.&lt;/p&gt;\\n\\n&lt;p&gt;I am thinking that I could go up one to UD-Q3_K_XL but I am not sure if it should be better.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4cx6qy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753112664,"author_flair_text":null,"treatment_tags":[],"created_utc":1753112664,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4b5nok","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Turbulent_Pin7635","can_mod_post":false,"created_utc":1753088222,"send_replies":true,"parent_id":"t1_n4a3uim","score":1,"author_fullname":"t2_1hra1kibwa","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Which version of Kimi K2 are you using? The full model? I also have the m3u. And how is the speed?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4b5nok","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Which version of Kimi K2 are you using? The full model? I also have the m3u. And how is the speed?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4b5nok/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753088222,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4a3uim","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"nomorebuttsplz","can_mod_post":false,"created_utc":1753068661,"send_replies":true,"parent_id":"t3_1m58695","score":34,"author_fullname":"t2_syq52","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Each has their own use so hard to rank but with Mac Studio m3u:\\n\\n1. Kimi K2 - general purpose\\n2. R1 0528 - coding, science, medical\\n3. Qwen 235b - math, long context general purpose\\n4. DS 0324 - general purpose but faster than Kimi\\n5. Qwen 3 30b - fast\\n6. Llama 3.3 70b Nevoria - creative, uncensored, and fast. edit: Whoops 70b\\n7. Maverick  - agentic workflows -- smart enough and fast prefill","edited":1753069915,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4a3uim","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Each has their own use so hard to rank but with Mac Studio m3u:&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;Kimi K2 - general purpose&lt;/li&gt;\\n&lt;li&gt;R1 0528 - coding, science, medical&lt;/li&gt;\\n&lt;li&gt;Qwen 235b - math, long context general purpose&lt;/li&gt;\\n&lt;li&gt;DS 0324 - general purpose but faster than Kimi&lt;/li&gt;\\n&lt;li&gt;Qwen 3 30b - fast&lt;/li&gt;\\n&lt;li&gt;Llama 3.3 70b Nevoria - creative, uncensored, and fast. edit: Whoops 70b&lt;/li&gt;\\n&lt;li&gt;Maverick  - agentic workflows -- smart enough and fast prefill&lt;/li&gt;\\n&lt;/ol&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4a3uim/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753068661,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m58695","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":34}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4c2me3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Affectionate-Cap-600","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4apboj","score":2,"author_fullname":"t2_5oltmr5b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"often the use of min_P solve the issue of Chinese characters for those kind of model's, at least for me","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4c2me3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;often the use of min_P solve the issue of Chinese characters for those kind of model&amp;#39;s, at least for me&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4c2me3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753103317,"author_flair_text":null,"treatment_tags":[],"created_utc":1753103317,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4bpunw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CSEliot","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4apboj","score":1,"author_fullname":"t2_uqwcijxv2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Sorry, could you explain what \\"temperature\\" does to LLMs in this context? You seem to have a good understanding of it otherwise id just Google it. Thanks!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4bpunw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sorry, could you explain what &amp;quot;temperature&amp;quot; does to LLMs in this context? You seem to have a good understanding of it otherwise id just Google it. Thanks!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4bpunw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753098545,"author_flair_text":null,"treatment_tags":[],"created_utc":1753098545,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4apboj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"dreamai87","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4adnp8","score":1,"author_fullname":"t2_q1q7nlxa","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Itâ€™s good for rag for sure. \\nI am using for lot of stuff except code, itâ€™s not good at coding level. \\nOverall amazing model. Though I keep temperature low at 0.3 to 0.4 as it put Chinese language occasionally when temp is at 0.8 or above. \\nIt follows instructions better than qwen 30b","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4apboj","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Itâ€™s good for rag for sure. \\nI am using for lot of stuff except code, itâ€™s not good at coding level. \\nOverall amazing model. Though I keep temperature low at 0.3 to 0.4 as it put Chinese language occasionally when temp is at 0.8 or above. \\nIt follows instructions better than qwen 30b&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4apboj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753078908,"author_flair_text":null,"treatment_tags":[],"created_utc":1753078908,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4adnp8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No_Conversation9561","can_mod_post":false,"created_utc":1753072974,"send_replies":true,"parent_id":"t1_n4aa00d","score":4,"author_fullname":"t2_jqxb4pte","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Does the smaller Ernie-4.5-21b also have same capabilities for RAG?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4adnp8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Does the smaller Ernie-4.5-21b also have same capabilities for RAG?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4adnp8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753072974,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4c2di1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Affectionate-Cap-600","can_mod_post":false,"created_utc":1753103232,"send_replies":true,"parent_id":"t1_n4aa00d","score":1,"author_fullname":"t2_5oltmr5b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt;efficient context handling and semantic understanding.\\n\\nwhat do you mean?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4c2di1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;efficient context handling and semantic understanding.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;what do you mean?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4c2di1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753103232,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4aa00d","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Longjumpingfish0403","can_mod_post":false,"created_utc":1753071302,"send_replies":true,"parent_id":"t3_1m58695","score":14,"author_fullname":"t2_jarttha4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For those interested in RAG capabilities, it's worth noting that Mistral_large-Instruct and Ernie-4.5-300b often excel in retrieval-augmented tasks due to their efficient context handling and semantic understanding. Exploring their integration with your current systems might offer significant improvements in performance and utility.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4aa00d","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For those interested in RAG capabilities, it&amp;#39;s worth noting that Mistral_large-Instruct and Ernie-4.5-300b often excel in retrieval-augmented tasks due to their efficient context handling and semantic understanding. Exploring their integration with your current systems might offer significant improvements in performance and utility.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4aa00d/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753071302,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m58695","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":14}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4c48hy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"-Ellary-","can_mod_post":false,"created_utc":1753103864,"send_replies":true,"parent_id":"t1_n4a3ngh","score":1,"author_fullname":"t2_s4zzntp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Original Command R+ also quite fun to use.  \\nIt is not as smart as Mistral Large 2 2407.  \\nBut it got some ... knowledge.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4c48hy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Original Command R+ also quite fun to use.&lt;br/&gt;\\nIt is not as smart as Mistral Large 2 2407.&lt;br/&gt;\\nBut it got some ... knowledge.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4c48hy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753103864,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4a3ngh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"TheLocalDrummer","can_mod_post":false,"created_utc":1753068579,"send_replies":true,"parent_id":"t3_1m58695","score":16,"author_fullname":"t2_w6l58p741","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Nemotron Ultra and Mistral Large 2407","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4a3ngh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nemotron Ultra and Mistral Large 2407&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4a3ngh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753068579,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m58695","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":16}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ewvet","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Affectionate-Cap-600","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4eewul","score":2,"author_fullname":"t2_5oltmr5b","approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt;Looking at the Nemotron Ultra model card, I remembered why I had passed it up when it was introduced -- it seemed like a lossy downscale of the 405B, and superfluous since I had the actual 405B\\n\\nI also thought that, but I was happily surprised to see that it actually outperformed llama 405B in every occasion I compared them. also I was surprised to see that after the pruning it retained a particular aspect of the 405B that was really useful for me, an incredible fluency in Italian (much better than every other open model around).\\n\\nAlso I read the papers (there's one for the NAS, one for the FFNfusion and one about the training recipe)from nvidia about its training, the Neural Architecture Search they used is much more 'advanced' than other pruning strategies I've seen.\\n\\n\\nbtw you have a lot of ram in your homelab servers lol. i use those models going with the cheapest provider for each one... I though about putting together something with some old xeon, a lot of ram and a cheap mig, but since I'm not really worried about privacy, I'm having trouble justifying the price. also there are providers that have quite honests ToS and retention policy, so I'm quite happy with the current situation","edited":1753133781,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n4ewvet","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Looking at the Nemotron Ultra model card, I remembered why I had passed it up when it was introduced -- it seemed like a lossy downscale of the 405B, and superfluous since I had the actual 405B&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I also thought that, but I was happily surprised to see that it actually outperformed llama 405B in every occasion I compared them. also I was surprised to see that after the pruning it retained a particular aspect of the 405B that was really useful for me, an incredible fluency in Italian (much better than every other open model around).&lt;/p&gt;\\n\\n&lt;p&gt;Also I read the papers (there&amp;#39;s one for the NAS, one for the FFNfusion and one about the training recipe)from nvidia about its training, the Neural Architecture Search they used is much more &amp;#39;advanced&amp;#39; than other pruning strategies I&amp;#39;ve seen.&lt;/p&gt;\\n\\n&lt;p&gt;btw you have a lot of ram in your homelab servers lol. i use those models going with the cheapest provider for each one... I though about putting together something with some old xeon, a lot of ram and a cheap mig, but since I&amp;#39;m not really worried about privacy, I&amp;#39;m having trouble justifying the price. also there are providers that have quite honests ToS and retention policy, so I&amp;#39;m quite happy with the current situation&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m58695","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4ewvet/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753132866,"author_flair_text":null,"treatment_tags":[],"created_utc":1753132866,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4eewul","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ttkciar","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4dp9a9","score":2,"author_fullname":"t2_cpegz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks for the tips :-)\\n\\nLooking at the Nemotron Ultra model card, I remembered why I had passed it up when it was introduced -- it seemed like a lossy downscale of the 405B, and superfluous since I had the actual 405B.  That may have been a mistake, though, since I'm not actually using the 405B in practice, but rather the 70B.\\n\\nNemotron Ultra should infer at about twice the speed of Tulu3-405B, which might be less painful on my existing hardware, and is more likely to fit in faster hardware's memory when I upgrade.  Thus it seems worth checking out.\\n\\n&gt; may I ask what hardware are you using to run those models? and what speeds do you get with tulu 405B?\\n\\nI'm running the large models via pure CPU inference on old Xeon systems with llama.cpp.  I have three Dell T7910 each with two Xeon E5-2660v3, one T7910 with two Xeon E5-2680v3, and one Supermicro CSE-829U with two Xeon E5-2690v4.  Each has 256GB of DDR4-2133 on eight memory channels.\\n\\nOn the dual E5-2660v3 systems I am getting about 0.15 tokens per second with Tulu3-405B (Q4_K_M), which is less than what it \\"should\\" given its aggregate memory bandwidth.  I suspect the interprocessor communication fabric is getting saturated and posing a bottleneck.  Fiddling with numactl and llama-cli's \`--numa\` options only made it slower, unfortunately.\\n\\nThese are not the best systems for inference, but they are what I already had for running GEANT4 and Rocstar simulations (for which they are quite adequate).\\n\\nFor using smaller models, I have an MI60 (32GB) and V340 (16GB).  When MI210 have come down in price quite a bit, I plan on picking up a few of those, which should improve my homelab's inference capabilities rather a lot.  I also hope to use them for fine-tuning and continued pretraining.","edited":1753127989,"author_flair_css_class":null,"name":"t1_n4eewul","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for the tips :-)&lt;/p&gt;\\n\\n&lt;p&gt;Looking at the Nemotron Ultra model card, I remembered why I had passed it up when it was introduced -- it seemed like a lossy downscale of the 405B, and superfluous since I had the actual 405B.  That may have been a mistake, though, since I&amp;#39;m not actually using the 405B in practice, but rather the 70B.&lt;/p&gt;\\n\\n&lt;p&gt;Nemotron Ultra should infer at about twice the speed of Tulu3-405B, which might be less painful on my existing hardware, and is more likely to fit in faster hardware&amp;#39;s memory when I upgrade.  Thus it seems worth checking out.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;may I ask what hardware are you using to run those models? and what speeds do you get with tulu 405B?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I&amp;#39;m running the large models via pure CPU inference on old Xeon systems with llama.cpp.  I have three Dell T7910 each with two Xeon E5-2660v3, one T7910 with two Xeon E5-2680v3, and one Supermicro CSE-829U with two Xeon E5-2690v4.  Each has 256GB of DDR4-2133 on eight memory channels.&lt;/p&gt;\\n\\n&lt;p&gt;On the dual E5-2660v3 systems I am getting about 0.15 tokens per second with Tulu3-405B (Q4_K_M), which is less than what it &amp;quot;should&amp;quot; given its aggregate memory bandwidth.  I suspect the interprocessor communication fabric is getting saturated and posing a bottleneck.  Fiddling with numactl and llama-cli&amp;#39;s &lt;code&gt;--numa&lt;/code&gt; options only made it slower, unfortunately.&lt;/p&gt;\\n\\n&lt;p&gt;These are not the best systems for inference, but they are what I already had for running GEANT4 and Rocstar simulations (for which they are quite adequate).&lt;/p&gt;\\n\\n&lt;p&gt;For using smaller models, I have an MI60 (32GB) and V340 (16GB).  When MI210 have come down in price quite a bit, I plan on picking up a few of those, which should improve my homelab&amp;#39;s inference capabilities rather a lot.  I also hope to use them for fine-tuning and continued pretraining.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m58695","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4eewul/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753127689,"author_flair_text":"llama.cpp","collapsed":false,"created_utc":1753127689,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4dp9a9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Affectionate-Cap-600","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4d6gs3","score":3,"author_fullname":"t2_5oltmr5b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"may I ask what hardware are you using to run those models? and what speeds do you get with tulu 405B?\\n\\nalso, just a reminder, if you try nemotron you should not add any instructions in the system message, and use the system role just to add \\"detailed reasoning on\\" or \\"... off\\". \\nadding anything else to the system message resulted in really bad performance for me (they explain that in their HF model card) \\n\\nAnyway, it follow instruction really well, and in my pipeline I had no issue switching from model that use the system message to this model, just including what I would add to the system in the user message, structuring it as \\"# Instructuons: ... \\\\n\\\\n # Prompt: ...\\"\\n\\nfor me it did an amazing job following instructions about structured output, without any grammar constraints or \\"json mode\\", and we talk about 5-10K tokens of structured json","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4dp9a9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;may I ask what hardware are you using to run those models? and what speeds do you get with tulu 405B?&lt;/p&gt;\\n\\n&lt;p&gt;also, just a reminder, if you try nemotron you should not add any instructions in the system message, and use the system role just to add &amp;quot;detailed reasoning on&amp;quot; or &amp;quot;... off&amp;quot;. \\nadding anything else to the system message resulted in really bad performance for me (they explain that in their HF model card) &lt;/p&gt;\\n\\n&lt;p&gt;Anyway, it follow instruction really well, and in my pipeline I had no issue switching from model that use the system message to this model, just including what I would add to the system in the user message, structuring it as &amp;quot;# Instructuons: ... \\\\n\\\\n # Prompt: ...&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;for me it did an amazing job following instructions about structured output, without any grammar constraints or &amp;quot;json mode&amp;quot;, and we talk about 5-10K tokens of structured json&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4dp9a9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753120441,"author_flair_text":null,"treatment_tags":[],"created_utc":1753120441,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n4d6gs3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ttkciar","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4c2wu3","score":1,"author_fullname":"t2_cpegz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I have not tried Nemotron Ultra, but will add it to the download queue and give it a shot.  Thank you for the suggestion!","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4d6gs3","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have not tried Nemotron Ultra, but will add it to the download queue and give it a shot.  Thank you for the suggestion!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4d6gs3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753115293,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1753115293,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4c2wu3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Affectionate-Cap-600","can_mod_post":false,"created_utc":1753103416,"send_replies":true,"parent_id":"t1_n4a4j3d","score":1,"author_fullname":"t2_5oltmr5b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt;I've been quite impressed with Tulu3-405B (deep STEM retrain of Llama3.1-405B).\\n\\nhow does it compare with nemotron ultra (another modrl derivate from llama 405B)? i haven't tested tulu3 405B, I'll give it  a try!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4c2wu3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;I&amp;#39;ve been quite impressed with Tulu3-405B (deep STEM retrain of Llama3.1-405B).&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;how does it compare with nemotron ultra (another modrl derivate from llama 405B)? i haven&amp;#39;t tested tulu3 405B, I&amp;#39;ll give it  a try!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4c2wu3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753103416,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4a4j3d","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ttkciar","can_mod_post":false,"created_utc":1753068949,"send_replies":true,"parent_id":"t3_1m58695","score":11,"author_fullname":"t2_cpegz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I've been quite impressed with Tulu3-405B (deep STEM retrain of Llama3.1-405B).  It is knowledgeable, nuanced, understands implication, and capable of conjecture if pressed to it.  My usual use-case is to provide it excerpts from two or more journal publications about nuclear physics, and my personal notes which relate to those publications, and pose a list of questions (and then go to bed, and let it infer its answers while I sleep, because pure CPU inference is sloooow).\\n\\nUnfortunately I lack the hardware to use it as frequently as I would like.  Tulu3-70B is nice too, but there's a definite competence gap.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4a4j3d","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve been quite impressed with Tulu3-405B (deep STEM retrain of Llama3.1-405B).  It is knowledgeable, nuanced, understands implication, and capable of conjecture if pressed to it.  My usual use-case is to provide it excerpts from two or more journal publications about nuclear physics, and my personal notes which relate to those publications, and pose a list of questions (and then go to bed, and let it infer its answers while I sleep, because pure CPU inference is sloooow).&lt;/p&gt;\\n\\n&lt;p&gt;Unfortunately I lack the hardware to use it as frequently as I would like.  Tulu3-70B is nice too, but there&amp;#39;s a definite competence gap.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4a4j3d/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753068949,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m58695","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4dqlzl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mrtime777","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4abmd0","score":1,"author_fullname":"t2_37pn0768","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You can try to use something like this: [https://pastebin.com/v6MrsKQ4](https://pastebin.com/v6MrsKQ4)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4dqlzl","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can try to use something like this: &lt;a href=\\"https://pastebin.com/v6MrsKQ4\\"&gt;https://pastebin.com/v6MrsKQ4&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4dqlzl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753120806,"author_flair_text":null,"treatment_tags":[],"created_utc":1753120806,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4abmd0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"nomorebuttsplz","can_mod_post":false,"created_utc":1753072033,"send_replies":true,"parent_id":"t1_n4a475s","score":8,"author_fullname":"t2_syq52","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I love how Kimi k2 doesn't glaze. What system prompts do you use to improve it?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4abmd0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I love how Kimi k2 doesn&amp;#39;t glaze. What system prompts do you use to improve it?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4abmd0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753072033,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}}],"before":null}},"user_reports":[],"saved":false,"id":"n4a475s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"mrtime777","can_mod_post":false,"created_utc":1753068808,"send_replies":true,"parent_id":"t3_1m58695","score":4,"author_fullname":"t2_37pn0768","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Deepseek-R1-0528 and Kimi-K2. But Kimi-K2 responds like an introvert not interested in conversation (if without any system prompts), it's a bit annoying","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4a475s","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Deepseek-R1-0528 and Kimi-K2. But Kimi-K2 responds like an introvert not interested in conversation (if without any system prompts), it&amp;#39;s a bit annoying&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4a475s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753068808,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m58695","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4a6xc1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok-Pattern9779","can_mod_post":false,"created_utc":1753069961,"send_replies":true,"parent_id":"t3_1m58695","score":3,"author_fullname":"t2_a29pmyxj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"K2 is ideal for Rust and Go, thanks to Groq's rapid token generation. That's why it's now my go-to tool.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4a6xc1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;K2 is ideal for Rust and Go, thanks to Groq&amp;#39;s rapid token generation. That&amp;#39;s why it&amp;#39;s now my go-to tool.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4a6xc1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753069961,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m58695","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4al9cy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Entubulated","can_mod_post":false,"created_utc":1753076754,"send_replies":true,"parent_id":"t3_1m58695","score":3,"author_fullname":"t2_1opxde6hyq","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Current hardware constraints being what they are, I'm not using large dense models for now.\\n\\nKimi K2 is just too damn big.\\n\\nDeepSeek is also too big, but ... from those, Chimera is the best bet for me as it outdoes v3-0324 without spending 20k tokens arguing with itself over trivial bullshit like R1 sometimes does. At not so fast output rates, that matters.\\n\\nNot made the time to play with Ernie, and it's slower than Chimera on my system due to that 47b active parameters. Likely not going to do anything more with it until after next new system build, whenever the hell that happens.\\n\\nQwen3 is much more accessible, though it really does lack the depth of random knowledge that DeepSeek has.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4al9cy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Current hardware constraints being what they are, I&amp;#39;m not using large dense models for now.&lt;/p&gt;\\n\\n&lt;p&gt;Kimi K2 is just too damn big.&lt;/p&gt;\\n\\n&lt;p&gt;DeepSeek is also too big, but ... from those, Chimera is the best bet for me as it outdoes v3-0324 without spending 20k tokens arguing with itself over trivial bullshit like R1 sometimes does. At not so fast output rates, that matters.&lt;/p&gt;\\n\\n&lt;p&gt;Not made the time to play with Ernie, and it&amp;#39;s slower than Chimera on my system due to that 47b active parameters. Likely not going to do anything more with it until after next new system build, whenever the hell that happens.&lt;/p&gt;\\n\\n&lt;p&gt;Qwen3 is much more accessible, though it really does lack the depth of random knowledge that DeepSeek has.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4al9cy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753076754,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m58695","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4axpmb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"-dysangel-","can_mod_post":false,"created_utc":1753083586,"send_replies":true,"parent_id":"t3_1m58695","score":3,"author_fullname":"t2_12ggykute6","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Deepseek-R1-0528 is my favourite large model, because it performs coherently even when quantised down to 250GB (Unsloth Q2\\\\_K), which means TTFT isn't quite as bad as the larger quants.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4axpmb","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Deepseek-R1-0528 is my favourite large model, because it performs coherently even when quantised down to 250GB (Unsloth Q2_K), which means TTFT isn&amp;#39;t quite as bad as the larger quants.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4axpmb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753083586,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m58695","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4c9kus","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Jawzper","can_mod_post":false,"created_utc":1753105633,"send_replies":true,"parent_id":"t3_1m58695","score":3,"author_fullname":"t2_gebwv","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I have 64gb of RAM and 24gb of VRAM. Is it feasible for me to use any of these larger models without making them stupid from too much quantizing? Use case would be converting chapter drafts into fully fledged mockups, ie. long-form storytelling/prose. I haven't tried anything over 70b yet.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4c9kus","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have 64gb of RAM and 24gb of VRAM. Is it feasible for me to use any of these larger models without making them stupid from too much quantizing? Use case would be converting chapter drafts into fully fledged mockups, ie. long-form storytelling/prose. I haven&amp;#39;t tried anything over 70b yet.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4c9kus/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753105633,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m58695","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4a6bpf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Ravenpest","can_mod_post":false,"created_utc":1753069707,"send_replies":true,"parent_id":"t3_1m58695","score":4,"author_fullname":"t2_21lhc1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I still have a soft spot for older models. I happen to be using Venus 103b 1.1 sometimes, narrow use cases, but it has some strengths which I enjoy. Same with Lumikabra 0.4 (merge of the old Mistral Large, Magnum and other stuff) again because its great for very specific use cases. R1 \\\\\\\\ R1-0528 as main tools for everything but that shouldn't be a surprise. Disappointed by Qwen 235b which I think is ass and if you can run R1 there's no reason nor need for it. K2 got some things right up my alley but I still return to R1 no question. Frankly I still prefer Command R+ to Command A as of style and expression, I cant wait to have the compute to get some LoRA done","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4a6bpf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I still have a soft spot for older models. I happen to be using Venus 103b 1.1 sometimes, narrow use cases, but it has some strengths which I enjoy. Same with Lumikabra 0.4 (merge of the old Mistral Large, Magnum and other stuff) again because its great for very specific use cases. R1 \\\\ R1-0528 as main tools for everything but that shouldn&amp;#39;t be a surprise. Disappointed by Qwen 235b which I think is ass and if you can run R1 there&amp;#39;s no reason nor need for it. K2 got some things right up my alley but I still return to R1 no question. Frankly I still prefer Command R+ to Command A as of style and expression, I cant wait to have the compute to get some LoRA done&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4a6bpf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753069707,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m58695","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4c36j1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Affectionate-Cap-600","can_mod_post":false,"created_utc":1753103508,"send_replies":true,"parent_id":"t1_n4awuzw","score":1,"author_fullname":"t2_5oltmr5b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"there are many providers that host those open weights large models at a really low price... nothing local, but it is useful","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4c36j1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;there are many providers that host those open weights large models at a really low price... nothing local, but it is useful&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4c36j1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753103508,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4awuzw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kzoltan","can_mod_post":false,"created_utc":1753083098,"send_replies":true,"parent_id":"t3_1m58695","score":2,"author_fullname":"t2_b1uu5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It would be useful if people posted how they run these models. Iâ€™m sure llama 405b is great, butâ€¦","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4awuzw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It would be useful if people posted how they run these models. Iâ€™m sure llama 405b is great, butâ€¦&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4awuzw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753083098,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m58695","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4bfvs0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"True_Requirement_891","can_mod_post":false,"created_utc":1753093931,"send_replies":true,"parent_id":"t1_n4azhi0","score":0,"author_fullname":"t2_yfi9sqrzf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Maverick just fails to understand even a little bit complex stuff...\\n\\nHow do you manage to use it for data manip","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4bfvs0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Maverick just fails to understand even a little bit complex stuff...&lt;/p&gt;\\n\\n&lt;p&gt;How do you manage to use it for data manip&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m58695","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4bfvs0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753093931,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n4azhi0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jzn21","can_mod_post":false,"created_utc":1753084594,"send_replies":true,"parent_id":"t3_1m58695","score":2,"author_fullname":"t2_2oh4fbzf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I use Llama 4 Maverick for data manipulation because most others fail or are slow. Really an underrated model IMO","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4azhi0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I use Llama 4 Maverick for data manipulation because most others fail or are slow. Really an underrated model IMO&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4azhi0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753084594,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m58695","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"d2642412-d9ce-11ed-ae30-32b11309f5bd","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4a6jqz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ortegaalfredo","can_mod_post":false,"created_utc":1753069801,"send_replies":true,"parent_id":"t3_1m58695","score":3,"author_fullname":"t2_g177e","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Qwen3-235B is the only one you can run reasonably fast and it's smart enough to be useful. Most other require an unpractical or too expensive amount of GPUs.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4a6jqz","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Alpaca"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen3-235B is the only one you can run reasonably fast and it&amp;#39;s smart enough to be useful. Most other require an unpractical or too expensive amount of GPUs.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4a6jqz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753069801,"author_flair_text":"Alpaca","treatment_tags":[],"link_id":"t3_1m58695","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":"#bd9e9e","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4bh8oi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"PigletImpossible1384","can_mod_post":false,"created_utc":1753094623,"send_replies":true,"parent_id":"t3_1m58695","score":1,"author_fullname":"t2_q1njd8z3k","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"OpenBuddy-R10528DistillQwen-72B-Preview1","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4bh8oi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;OpenBuddy-R10528DistillQwen-72B-Preview1&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4bh8oi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753094623,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m58695","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4c271k","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Affectionate-Cap-600","can_mod_post":false,"created_utc":1753103170,"send_replies":true,"parent_id":"t3_1m58695","score":1,"author_fullname":"t2_5oltmr5b","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I really like nemotron ultra 253B. \\nI'm not saying it is the smartest model...I recent wrote why in other comment here on LocalLLaMA.. I'll avoid the copy paste, that's the comment: https://www.reddit.com/r/LocalLLaMA/s/yQBfF5I7nL","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4c271k","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I really like nemotron ultra 253B. \\nI&amp;#39;m not saying it is the smartest model...I recent wrote why in other comment here on LocalLLaMA.. I&amp;#39;ll avoid the copy paste, that&amp;#39;s the comment: &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/s/yQBfF5I7nL\\"&gt;https://www.reddit.com/r/LocalLLaMA/s/yQBfF5I7nL&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4c271k/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753103170,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m58695","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4clj8z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"_supert_","can_mod_post":false,"created_utc":1753109307,"send_replies":true,"parent_id":"t3_1m58695","score":1,"author_fullname":"t2_dwj10","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I use a merge of finetunes of Mistral Large 123. I am really impressed with Kimi K2 but it's too slow for regular use on my hardware.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4clj8z","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I use a merge of finetunes of Mistral Large 123. I am really impressed with Kimi K2 but it&amp;#39;s too slow for regular use on my hardware.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4clj8z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753109307,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m58695","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4d8h8f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Initial-Swan6385","can_mod_post":false,"created_utc":1753115860,"send_replies":true,"parent_id":"t3_1m58695","score":1,"author_fullname":"t2_1niuwd9khu","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"WizardLM was my favorite for a long time","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4d8h8f","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;WizardLM was my favorite for a long time&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4d8h8f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753115860,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m58695","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4d9fox","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"allenasm","can_mod_post":false,"created_utc":1753116128,"send_replies":true,"parent_id":"t3_1m58695","score":1,"author_fullname":"t2_fouwt","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"llama-4 maverick 128e.  229gb on my mac m3 512","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4d9fox","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;llama-4 maverick 128e.  229gb on my mac m3 512&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4d9fox/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753116128,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m58695","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4efees","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"leonken56","can_mod_post":false,"created_utc":1753127827,"send_replies":true,"parent_id":"t3_1m58695","score":1,"author_fullname":"t2_br4io","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"how do you run heavy weight models?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4efees","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;how do you run heavy weight models?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4efees/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753127827,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m58695","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4fi5ta","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Weary-Wing-6806","can_mod_post":false,"created_utc":1753139673,"send_replies":true,"parent_id":"t3_1m58695","score":1,"author_fullname":"t2_1t2xvghrcr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Deepseek R1-0528 still feels like the cleanest signal. You're trading speed for brains. Deepseek R1-0528 isn't flashy or fast, but it thinks better than most IMO.... more accurate, more thoughtful, fewer dumb mistakes.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4fi5ta","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Deepseek R1-0528 still feels like the cleanest signal. You&amp;#39;re trading speed for brains. Deepseek R1-0528 isn&amp;#39;t flashy or fast, but it thinks better than most IMO.... more accurate, more thoughtful, fewer dumb mistakes.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4fi5ta/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753139673,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m58695","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4a5qd6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Business-Weekend-537","can_mod_post":false,"created_utc":1753069456,"send_replies":true,"parent_id":"t3_1m58695","score":1,"author_fullname":"t2_rkb6qbej1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Can anyone let me know what the input/output window size of the referenced models are?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4a5qd6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Can anyone let me know what the input/output window size of the referenced models are?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4a5qd6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753069456,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m58695","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4bplt7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1753098444,"send_replies":true,"parent_id":"t3_1m58695","score":1,"author_fullname":"t2_h5utwre7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Pixtral-Large is mistral-large but with vision. Not a lot of choices in that tier. If I had to pick a favorite right now, that's what it would be.\\n\\nI still like command-A and even command-r. Latter is showing it's age. \\n\\nThe deepseeks run on my machine, but they are on the slow side so I'd still rather use them on API when it's freely available. Kimi has to have smaller quants so it's worse in this regard. Not likely to use it locally.\\n\\nQwen-235b is an odd case. It takes a bunch of resources and it's relatively smart, but it just doesn't have much non-stem knowledge. People slept on the smoothie version, I'd love to try the EXL3 weights of that but nobody made them. \\n\\nWas waiting on ernie to make it to ik_llama as a lighter deepseek. Have not heard good things from those that got to try it. Probably going to be a huge disappointment. Keeping hope alive on that one.\\n\\nLlama 405b and by extension, the 235b, are too big and not suited for hybrid inference nor would they fit in my vram. The nous version of the former was great when it was free on openrouter.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4bplt7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Pixtral-Large is mistral-large but with vision. Not a lot of choices in that tier. If I had to pick a favorite right now, that&amp;#39;s what it would be.&lt;/p&gt;\\n\\n&lt;p&gt;I still like command-A and even command-r. Latter is showing it&amp;#39;s age. &lt;/p&gt;\\n\\n&lt;p&gt;The deepseeks run on my machine, but they are on the slow side so I&amp;#39;d still rather use them on API when it&amp;#39;s freely available. Kimi has to have smaller quants so it&amp;#39;s worse in this regard. Not likely to use it locally.&lt;/p&gt;\\n\\n&lt;p&gt;Qwen-235b is an odd case. It takes a bunch of resources and it&amp;#39;s relatively smart, but it just doesn&amp;#39;t have much non-stem knowledge. People slept on the smoothie version, I&amp;#39;d love to try the EXL3 weights of that but nobody made them. &lt;/p&gt;\\n\\n&lt;p&gt;Was waiting on ernie to make it to ik_llama as a lighter deepseek. Have not heard good things from those that got to try it. Probably going to be a huge disappointment. Keeping hope alive on that one.&lt;/p&gt;\\n\\n&lt;p&gt;Llama 405b and by extension, the 235b, are too big and not suited for hybrid inference nor would they fit in my vram. The nous version of the former was great when it was free on openrouter.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4bplt7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753098444,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m58695","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4a6xmy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Business-Weekend-537","can_mod_post":false,"created_utc":1753069965,"send_replies":true,"parent_id":"t3_1m58695","score":0,"author_fullname":"t2_rkb6qbej1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Also of the oneâ€™s referenced does anyone know which are best for usage with RAG?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4a6xmy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Also of the oneâ€™s referenced does anyone know which are best for usage with RAG?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/n4a6xmy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753069965,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m58695","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}}]`),n=()=>e.jsx(l,{data:a});export{n as default};
