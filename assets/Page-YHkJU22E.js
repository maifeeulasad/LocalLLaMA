import{j as e}from"./index-BpC9hjVs.js";import{R as t}from"./RedditPostRenderer-BEo6AnSR.js";import"./index-DwkJHX1_.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hey all!  \\nI've just released my [qwen3-rs](vscode-file://vscode-app/snap/code/198/usr/share/code/resources/app/out/vs/code/electron-sandbox/workbench/workbench.html), a Rust project for running and exporting Qwen3 models (Qwen3-0.6B, 4B, 8B, DeepSeek-R1-0528-Qwen3-8B, etc) with minimal dependencies and no Python required.\\n\\n* **Educational:** Core algorithms are reimplemented from scratch for learning and transparency.\\n* **CLI tools:** Export HuggingFace Qwen3 models to a custom binary format, then run inference (on CPU)\\n* **Modular:** Clean separation between export, inference, and CLI.\\n* **Safety:** Some unsafe code is used, mostly to work with memory mapping files (helpful to lower memory requirements on export/inference)\\n* **Future plans:** I would be curious to see how to extend it to support:\\n   * fine-tuning of a small models\\n   * optimize inference performance (e.g. matmul operations)\\n   * WASM build to run inference in a browser\\n\\nBasically, I used [qwen3.c](https://github.com/adriancable/qwen3.c) as a reference implementation translated from C/Python to Rust with a help of commercial LLMs (mostly Claude Sonnet 4). Please note that my primary goal is self learning in this field, so some inaccuracies can be definitely there.\\n\\nGitHub: [https://github.com/reinterpretcat/qwen3-rs](vscode-file://vscode-app/snap/code/198/usr/share/code/resources/app/out/vs/code/electron-sandbox/workbench/workbench.html)","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"[Rust] qwen3-rs: Educational Qwen3 Architecture Inference (No Python, Minimal Deps)","link_flair_richtext":[{"e":"text","t":"Other"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1ly7sb0","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.93,"author_flair_background_color":null,"subreddit_type":"public","ups":31,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_fkwrm","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Other","can_mod_post":false,"score":31,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1752345715,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey all!&lt;br/&gt;\\nI&amp;#39;ve just released my [qwen3-rs](vscode-file://vscode-app/snap/code/198/usr/share/code/resources/app/out/vs/code/electron-sandbox/workbench/workbench.html), a Rust project for running and exporting Qwen3 models (Qwen3-0.6B, 4B, 8B, DeepSeek-R1-0528-Qwen3-8B, etc) with minimal dependencies and no Python required.&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;strong&gt;Educational:&lt;/strong&gt; Core algorithms are reimplemented from scratch for learning and transparency.&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;CLI tools:&lt;/strong&gt; Export HuggingFace Qwen3 models to a custom binary format, then run inference (on CPU)&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Modular:&lt;/strong&gt; Clean separation between export, inference, and CLI.&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Safety:&lt;/strong&gt; Some unsafe code is used, mostly to work with memory mapping files (helpful to lower memory requirements on export/inference)&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Future plans:&lt;/strong&gt; I would be curious to see how to extend it to support:\\n\\n&lt;ul&gt;\\n&lt;li&gt;fine-tuning of a small models&lt;/li&gt;\\n&lt;li&gt;optimize inference performance (e.g. matmul operations)&lt;/li&gt;\\n&lt;li&gt;WASM build to run inference in a browser&lt;/li&gt;\\n&lt;/ul&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Basically, I used &lt;a href=\\"https://github.com/adriancable/qwen3.c\\"&gt;qwen3.c&lt;/a&gt; as a reference implementation translated from C/Python to Rust with a help of commercial LLMs (mostly Claude Sonnet 4). Please note that my primary goal is self learning in this field, so some inaccuracies can be definitely there.&lt;/p&gt;\\n\\n&lt;p&gt;GitHub: [&lt;a href=\\"https://github.com/reinterpretcat/qwen3-rs%5D(vscode-file://vscode-app/snap/code/198/usr/share/code/resources/app/out/vs/code/electron-sandbox/workbench/workbench.html)\\"&gt;https://github.com/reinterpretcat/qwen3-rs](vscode-file://vscode-app/snap/code/198/usr/share/code/resources/app/out/vs/code/electron-sandbox/workbench/workbench.html)&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/LxoqZs4q3Osj78IVaTXSrgUKqNHcOujrOF1Tg6_GYA4.jpeg?auto=webp&amp;s=f64a6eef9fb25bb8dece4a00b49169cb6de85df2","width":640,"height":640},"resolutions":[{"url":"https://external-preview.redd.it/LxoqZs4q3Osj78IVaTXSrgUKqNHcOujrOF1Tg6_GYA4.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b6a4a1ab699ce9984d57b0696bdd1f873de9e614","width":108,"height":108},{"url":"https://external-preview.redd.it/LxoqZs4q3Osj78IVaTXSrgUKqNHcOujrOF1Tg6_GYA4.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=80ccbeb83c907fb5b897374c139c51e76825ec00","width":216,"height":216},{"url":"https://external-preview.redd.it/LxoqZs4q3Osj78IVaTXSrgUKqNHcOujrOF1Tg6_GYA4.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f5ad415a9157f412849b8def8bc5c576f5d41217","width":320,"height":320},{"url":"https://external-preview.redd.it/LxoqZs4q3Osj78IVaTXSrgUKqNHcOujrOF1Tg6_GYA4.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3ac79d8600937790d6301fdd4917b87eabf6336a","width":640,"height":640}],"variants":{},"id":"LxoqZs4q3Osj78IVaTXSrgUKqNHcOujrOF1Tg6_GYA4"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"7a7848d2-bf8e-11ed-8c2f-765d15199f78","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#94e044","id":"1ly7sb0","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"eis_kalt","discussion_type":null,"num_comments":7,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1ly7sb0/rust_qwen3rs_educational_qwen3_architecture/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1ly7sb0/rust_qwen3rs_educational_qwen3_architecture/","subreddit_subscribers":498345,"created_utc":1752345715,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2t04e0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Languages_Learner","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2sjel8","score":1,"author_fullname":"t2_v9x8tm7u","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks for mentioning high ram consumption. I asked Gemini to fix that issue in [export.py](http://export.py) from original qwen3.c repo. Fixed [export.py](http://export.py) was converting qwen3-4b model for almost 30 minutes on my laptop with 16 gb ram but managed to create quantatized q8\\\\_0 model file. Unfortunately, qwen3.exe terminated silently while trying to load it. So i temporarily declined idea of reducing ram consumption because Gemini needed multiple attempts to make modified [export.py](http://export.py) be fully compatible with qwen3.exe. And each attempt to complete conversion process successfully would require waiting for 30 min which is unacceptable. I will try your rust converter.","edited":1752361680,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2t04e0","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for mentioning high ram consumption. I asked Gemini to fix that issue in &lt;a href=\\"http://export.py\\"&gt;export.py&lt;/a&gt; from original qwen3.c repo. Fixed &lt;a href=\\"http://export.py\\"&gt;export.py&lt;/a&gt; was converting qwen3-4b model for almost 30 minutes on my laptop with 16 gb ram but managed to create quantatized q8_0 model file. Unfortunately, qwen3.exe terminated silently while trying to load it. So i temporarily declined idea of reducing ram consumption because Gemini needed multiple attempts to make modified &lt;a href=\\"http://export.py\\"&gt;export.py&lt;/a&gt; be fully compatible with qwen3.exe. And each attempt to complete conversion process successfully would require waiting for 30 min which is unacceptable. I will try your rust converter.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ly7sb0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ly7sb0/rust_qwen3rs_educational_qwen3_architecture/n2t04e0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752360565,"author_flair_text":null,"treatment_tags":[],"created_utc":1752360565,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2sjel8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"eis_kalt","can_mod_post":false,"created_utc":1752354848,"send_replies":true,"parent_id":"t1_n2s3xwl","score":2,"author_fullname":"t2_fkwrm","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Cool! Have you tried to port also [export.py](http://export.py) script with all essential dependencies, e.g. for chat template, tokenizer generation? I found that it is quite hungry to RAM and 32GB is not enough to process Qwen3-4B model. So, I ported it to Rust as well (qwen3-export crate) using memory mapping files through memmap2 crate to address this.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2sjel8","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Cool! Have you tried to port also &lt;a href=\\"http://export.py\\"&gt;export.py&lt;/a&gt; script with all essential dependencies, e.g. for chat template, tokenizer generation? I found that it is quite hungry to RAM and 32GB is not enough to process Qwen3-4B model. So, I ported it to Rust as well (qwen3-export crate) using memory mapping files through memmap2 crate to address this.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ly7sb0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ly7sb0/rust_qwen3rs_educational_qwen3_architecture/n2sjel8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752354848,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2s3xwl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Languages_Learner","can_mod_post":false,"created_utc":1752349884,"send_replies":true,"parent_id":"t3_1ly7sb0","score":3,"author_fullname":"t2_v9x8tm7u","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Recently my favourite way to spend free time was chatting with Gemini 2.5 Pro to force it properly converting qwen3.c to some programming languages:\\n\\n[JohnClaw/qwen3.vb: VB.NET-port of qwen3.c](https://github.com/JohnClaw/qwen3.vb)\\n\\n[JohnClaw/qwen3.cs: C#-port of qwen3.c](https://github.com/JohnClaw/qwen3.cs)\\n\\n[JohnClaw/qwen3.go: Go-port of qwen3.c](https://github.com/JohnClaw/qwen3.go)\\n\\n[JohnClaw/qwen3.java: Java-port of qwen3.c](https://github.com/JohnClaw/qwen3.java)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2s3xwl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Recently my favourite way to spend free time was chatting with Gemini 2.5 Pro to force it properly converting qwen3.c to some programming languages:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/JohnClaw/qwen3.vb\\"&gt;JohnClaw/qwen3.vb: VB.NET-port of qwen3.c&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/JohnClaw/qwen3.cs\\"&gt;JohnClaw/qwen3.cs: C#-port of qwen3.c&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/JohnClaw/qwen3.go\\"&gt;JohnClaw/qwen3.go: Go-port of qwen3.c&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/JohnClaw/qwen3.java\\"&gt;JohnClaw/qwen3.java: Java-port of qwen3.c&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ly7sb0/rust_qwen3rs_educational_qwen3_architecture/n2s3xwl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752349884,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ly7sb0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2rwtv4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BlackSoulAVE","can_mod_post":false,"created_utc":1752347558,"send_replies":true,"parent_id":"t3_1ly7sb0","score":2,"author_fullname":"t2_vjhlc","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I just started working on something similar but for Molmo. I’m not converting to another PL but nice to see other people are reverse engineering as well to learn. \\n\\nCurrently working on rewriting their Trainer class to its barebones so I can understand what’s going on.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2rwtv4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I just started working on something similar but for Molmo. I’m not converting to another PL but nice to see other people are reverse engineering as well to learn. &lt;/p&gt;\\n\\n&lt;p&gt;Currently working on rewriting their Trainer class to its barebones so I can understand what’s going on.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ly7sb0/rust_qwen3rs_educational_qwen3_architecture/n2rwtv4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752347558,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ly7sb0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2v71oe","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"datbackup","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2v6lq7","score":3,"author_fullname":"t2_ielo6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I think these types of projects are time well spent. I  had a fantasy the other day about implementing custom cuda kernels… will I ever do it? I don’t know but it would definitely be cool to try","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2v71oe","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think these types of projects are time well spent. I  had a fantasy the other day about implementing custom cuda kernels… will I ever do it? I don’t know but it would definitely be cool to try&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ly7sb0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ly7sb0/rust_qwen3rs_educational_qwen3_architecture/n2v71oe/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752394855,"author_flair_text":null,"treatment_tags":[],"created_utc":1752394855,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2v6lq7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"eis_kalt","can_mod_post":false,"created_utc":1752394592,"send_replies":true,"parent_id":"t1_n2tnkf2","score":3,"author_fullname":"t2_fkwrm","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"not so far: still learning some basics","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2v6lq7","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;not so far: still learning some basics&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ly7sb0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ly7sb0/rust_qwen3rs_educational_qwen3_architecture/n2v6lq7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752394592,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2tnkf2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"datbackup","can_mod_post":false,"created_utc":1752369076,"send_replies":true,"parent_id":"t3_1ly7sb0","score":2,"author_fullname":"t2_ielo6","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Any plans to support the qwen3 moe models?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2tnkf2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Any plans to support the qwen3 moe models?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ly7sb0/rust_qwen3rs_educational_qwen3_architecture/n2tnkf2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752369076,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ly7sb0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),o=()=>e.jsx(t,{data:l});export{o as default};
