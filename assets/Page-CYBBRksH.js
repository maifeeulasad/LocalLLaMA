import{j as e}from"./index-sMC9-RRY.js";import{R as l}from"./RedditPostRenderer-pY-fX8U9.js";import"./index-8OT1L0zU.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hello Community,   \\nI've recently started to in local LLMs with my desire to build a local AI that I can use to automate some of my work and fulfill some personal projects of mine.  \\nSo far I tried models via LM Studio and integrate it with VS Code via Continue plugin, but discovered that I cant use it as agent that way. So currently I configured ollama and I have deepseek and llama models available and I'm trying to integrate it with OpenHands, but its not recognizing the model. Anyway. This is to provide some background to where I currently am\\n\\nTo my understanding I need something like OpenHands where the model will act like an agent and will have premissions to browser internet, modify files on my PC, create and execute python scripts, correct? \\n\\nMy ask is if someone can provide me some guidance on what sort of software I need to use to accomplish this. My goal is to have a chat interface to communicate with model and not via Python and integrate it with VS Code for example to build the whole project on its own following my instructions.  \\n\\n\\nThank you in advance.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Assistance for beginner in local LLM","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lmvv5e","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.75,"author_flair_background_color":null,"subreddit_type":"public","ups":2,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_6lei9t8i","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":2,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751140684,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hello Community,&lt;br/&gt;\\nI&amp;#39;ve recently started to in local LLMs with my desire to build a local AI that I can use to automate some of my work and fulfill some personal projects of mine.&lt;br/&gt;\\nSo far I tried models via LM Studio and integrate it with VS Code via Continue plugin, but discovered that I cant use it as agent that way. So currently I configured ollama and I have deepseek and llama models available and I&amp;#39;m trying to integrate it with OpenHands, but its not recognizing the model. Anyway. This is to provide some background to where I currently am&lt;/p&gt;\\n\\n&lt;p&gt;To my understanding I need something like OpenHands where the model will act like an agent and will have premissions to browser internet, modify files on my PC, create and execute python scripts, correct? &lt;/p&gt;\\n\\n&lt;p&gt;My ask is if someone can provide me some guidance on what sort of software I need to use to accomplish this. My goal is to have a chat interface to communicate with model and not via Python and integrate it with VS Code for example to build the whole project on its own following my instructions.  &lt;/p&gt;\\n\\n&lt;p&gt;Thank you in advance.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lmvv5e","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"JunkismyFunk","discussion_type":null,"num_comments":3,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lmvv5e/assistance_for_beginner_in_local_llm/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lmvv5e/assistance_for_beginner_in_local_llm/","subreddit_subscribers":492625,"created_utc":1751140684,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0axjdp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Wild_Requirement8902","can_mod_post":false,"created_utc":1751144428,"send_replies":true,"parent_id":"t3_1lmvv5e","score":0,"author_fullname":"t2_98yqzbqx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"try the new realease of jan and enable mcp tools","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0axjdp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;try the new realease of jan and enable mcp tools&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmvv5e/assistance_for_beginner_in_local_llm/n0axjdp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751144428,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmvv5e","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0b6225","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ArsNeph","can_mod_post":false,"created_utc":1751147305,"send_replies":true,"parent_id":"t3_1lmvv5e","score":1,"author_fullname":"t2_vt0xkv60d","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You're probably looking for Cline or Roo Code, which are both VScode extensions, and do a good job. To make it agentic, you simply have to give it permission to execute all tasks without oversight. Browsing internet requires a bit of configuration. In all honesty, if you're running Llama 3.1 8B and Deepseek Distill Llama 8B, you're going to have a VERY hard time accomplishing anything. First of all, those models are not at all suited to agentic coding, or coding in general. Try the Qwen 3 series, 8B, 14B, and 30B MoE. Secondly, with Ollama, the default context length is worthless, so you have to manually create a Modelfile, that says FROM &lt;modelname&gt; PARAMETER num\\\\_ctx &lt;context length&gt;, according to the documentation, and then run the Ollama create command, just to change the context length. This is a pain, and terrible idea. I'd suggest running vanilla llama.cpp or koboldCPP or something with more control. \\n\\nFinally, I want to let you know that local models below 30B aren't great for any coding tasks except for basic syntax and fill in the middle. If you want real agentic behavior, Qwen 3 32B is your best bet for local, if you can run it. That said, there are amazing open source coding models, like Qwen 3 235B and Deepseek V3/R1, but they are nearly impossible to run locally. For coding, the big models are in a different league than small ones, so you may just be better off going to OpenRouter, getting an API key, plugging it in to Cline/Roo Code, and picking Deepseek for cost efficient coding, or Claude 4 Sonnet/Gemini 2.5 Pro for very high quality.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0b6225","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You&amp;#39;re probably looking for Cline or Roo Code, which are both VScode extensions, and do a good job. To make it agentic, you simply have to give it permission to execute all tasks without oversight. Browsing internet requires a bit of configuration. In all honesty, if you&amp;#39;re running Llama 3.1 8B and Deepseek Distill Llama 8B, you&amp;#39;re going to have a VERY hard time accomplishing anything. First of all, those models are not at all suited to agentic coding, or coding in general. Try the Qwen 3 series, 8B, 14B, and 30B MoE. Secondly, with Ollama, the default context length is worthless, so you have to manually create a Modelfile, that says FROM &amp;lt;modelname&amp;gt; PARAMETER num_ctx &amp;lt;context length&amp;gt;, according to the documentation, and then run the Ollama create command, just to change the context length. This is a pain, and terrible idea. I&amp;#39;d suggest running vanilla llama.cpp or koboldCPP or something with more control. &lt;/p&gt;\\n\\n&lt;p&gt;Finally, I want to let you know that local models below 30B aren&amp;#39;t great for any coding tasks except for basic syntax and fill in the middle. If you want real agentic behavior, Qwen 3 32B is your best bet for local, if you can run it. That said, there are amazing open source coding models, like Qwen 3 235B and Deepseek V3/R1, but they are nearly impossible to run locally. For coding, the big models are in a different league than small ones, so you may just be better off going to OpenRouter, getting an API key, plugging it in to Cline/Roo Code, and picking Deepseek for cost efficient coding, or Claude 4 Sonnet/Gemini 2.5 Pro for very high quality.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmvv5e/assistance_for_beginner_in_local_llm/n0b6225/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751147305,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmvv5e","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0b8y4v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jefrederickson","can_mod_post":false,"created_utc":1751148306,"send_replies":true,"parent_id":"t3_1lmvv5e","score":1,"author_fullname":"t2_4l62v97","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Continue is great for VSCode. Also playing around with GPT4All which makes RAG w local docs easy.  Def check out LangGraph","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0b8y4v","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Continue is great for VSCode. Also playing around with GPT4All which makes RAG w local docs easy.  Def check out LangGraph&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmvv5e/assistance_for_beginner_in_local_llm/n0b8y4v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751148306,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmvv5e","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),i=()=>e.jsx(l,{data:t});export{i as default};
