import{j as e}from"./index-BOnf-UhU.js";import{R as t}from"./RedditPostRenderer-Ce39qICS.js";import"./index-CDase6VD.js";const l=JSON.parse('[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"NextCoder - a Microsoft Collection","link_flair_richtext":[{"e":"text","t":"New Model"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":75,"top_awarded_type":null,"hide_score":false,"name":"t3_1lurzqf","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.97,"author_flair_background_color":null,"ups":127,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_kwl47","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"New Model","can_mod_post":false,"score":127,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://external-preview.redd.it/7_jhFTazab6GMtEoANxssbRBy-NQcSp84SYt3Tyoa40.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=ed1bc9d878af5f352b8a651230b4f7a0d0b174d3","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"link","content_categories":null,"is_self":false,"subreddit_type":"public","created":1751989504,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"huggingface.co","allow_live_comments":false,"selftext_html":null,"likes":null,"suggested_sort":null,"banned_at_utc":null,"url_overridden_by_dest":"https://huggingface.co/collections/microsoft/nextcoder-6815ee6bfcf4e42f20d45028","view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/7_jhFTazab6GMtEoANxssbRBy-NQcSp84SYt3Tyoa40.png?auto=webp&amp;s=b3e4bf852dfe93ae28541ec1034617b2146a59e3","width":1200,"height":648},"resolutions":[{"url":"https://external-preview.redd.it/7_jhFTazab6GMtEoANxssbRBy-NQcSp84SYt3Tyoa40.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b1f0986f5db53e6006b251423459f34eeb980baa","width":108,"height":58},{"url":"https://external-preview.redd.it/7_jhFTazab6GMtEoANxssbRBy-NQcSp84SYt3Tyoa40.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ad42050cffda7472ad56d93a001cc80af264f584","width":216,"height":116},{"url":"https://external-preview.redd.it/7_jhFTazab6GMtEoANxssbRBy-NQcSp84SYt3Tyoa40.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4c6335a5606b421a0be05dfdf7dd7443a82970f2","width":320,"height":172},{"url":"https://external-preview.redd.it/7_jhFTazab6GMtEoANxssbRBy-NQcSp84SYt3Tyoa40.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6b2179fc5426163403bc73a148e1730509944514","width":640,"height":345},{"url":"https://external-preview.redd.it/7_jhFTazab6GMtEoANxssbRBy-NQcSp84SYt3Tyoa40.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5164df9aad5c7a0bf16ffca26d00e9d92758d8a8","width":960,"height":518},{"url":"https://external-preview.redd.it/7_jhFTazab6GMtEoANxssbRBy-NQcSp84SYt3Tyoa40.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d4a39aefb821cd32c2e1b663ff9657c968f65f31","width":1080,"height":583}],"variants":{},"id":"7_jhFTazab6GMtEoANxssbRBy-NQcSp84SYt3Tyoa40"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ced98442-f5d3-11ed-b657-66d3b15490c6","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"mod_note":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"num_reports":null,"removal_reason":null,"link_flair_background_color":"#ffb000","id":"1lurzqf","is_robot_indexable":true,"num_duplicates":2,"report_reasons":null,"author":"Dark_Fire_12","discussion_type":null,"num_comments":27,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lurzqf/nextcoder_a_microsoft_collection/","stickied":false,"url":"https://huggingface.co/collections/microsoft/nextcoder-6815ee6bfcf4e42f20d45028","subreddit_subscribers":497025,"created_utc":1751989504,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n228kh1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"kevin_1994","can_mod_post":false,"send_replies":true,"parent_id":"t1_n20v6kj","score":5,"author_fullname":"t2_o015g","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"How does this compare to Qwen3 32b?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n228kh1","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How does this compare to Qwen3 32b?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"link_id":"t3_1lurzqf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lurzqf/nextcoder_a_microsoft_collection/n228kh1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752010252,"author_flair_text":null,"treatment_tags":[],"created_utc":1752010252,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n20v6kj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Ulterior-Motive_","can_mod_post":false,"created_utc":1751996750,"send_replies":true,"parent_id":"t1_n205gu2","score":17,"author_fullname":"t2_127atw4awd","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Interesting how QwenCoder-2.5 32B holds up, old as it is at this point","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n20v6kj","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Interesting how QwenCoder-2.5 32B holds up, old as it is at this point&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lurzqf","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lurzqf/nextcoder_a_microsoft_collection/n20v6kj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751996750,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":17}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n23vfnn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"GreatBigJerk","can_mod_post":false,"created_utc":1752029505,"send_replies":true,"parent_id":"t1_n205gu2","score":9,"author_fullname":"t2_650y4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Those benchmarks are sus. The 32B model would be around Gemini\'s level based on the aider benchmark.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n23vfnn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Those benchmarks are sus. The 32B model would be around Gemini&amp;#39;s level based on the aider benchmark.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lurzqf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lurzqf/nextcoder_a_microsoft_collection/n23vfnn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752029505,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}}],"before":null}},"user_reports":[],"saved":false,"id":"n205gu2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Dark_Fire_12","can_mod_post":false,"created_utc":1751989577,"send_replies":true,"parent_id":"t3_1lurzqf","score":24,"author_fullname":"t2_kwl47","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Evaluation and Performance\\n\\nhttps://preview.redd.it/wrjjp61i8obf1.png?width=515&amp;format=png&amp;auto=webp&amp;s=2e3904cb9579dbb6e86fe3b6c92662ce9748b8c1\\n\\n*Comparison of base QwenCoder-2.5 models of different sizes and their SELEKT-enhanced versions across three code editing benchmarks.*","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n205gu2","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Evaluation and Performance&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/wrjjp61i8obf1.png?width=515&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2e3904cb9579dbb6e86fe3b6c92662ce9748b8c1\\"&gt;https://preview.redd.it/wrjjp61i8obf1.png?width=515&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2e3904cb9579dbb6e86fe3b6c92662ce9748b8c1&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;em&gt;Comparison of base QwenCoder-2.5 models of different sizes and their SELEKT-enhanced versions across three code editing benchmarks.&lt;/em&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lurzqf/nextcoder_a_microsoft_collection/n205gu2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751989577,"media_metadata":{"wrjjp61i8obf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":150,"x":108,"u":"https://preview.redd.it/wrjjp61i8obf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=048300b5d80e5ba7ac253823a22f912080e3d735"},{"y":301,"x":216,"u":"https://preview.redd.it/wrjjp61i8obf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e7f7a1d61559ba7ac9d1325870abb18afe098d18"},{"y":446,"x":320,"u":"https://preview.redd.it/wrjjp61i8obf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8202f7f236f8f41a8ed870a5748368f87dbd6734"}],"s":{"y":718,"x":515,"u":"https://preview.redd.it/wrjjp61i8obf1.png?width=515&amp;format=png&amp;auto=webp&amp;s=2e3904cb9579dbb6e86fe3b6c92662ce9748b8c1"},"id":"wrjjp61i8obf1"}},"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lurzqf","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":24}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n20ax3c","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"pokemonplayer2001","can_mod_post":false,"created_utc":1751991130,"send_replies":true,"parent_id":"t1_n209j1m","score":6,"author_fullname":"t2_11qjf3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ya, small projects or edits.\\n\\nCould be a nice use case.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n20ax3c","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ya, small projects or edits.&lt;/p&gt;\\n\\n&lt;p&gt;Could be a nice use case.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lurzqf","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lurzqf/nextcoder_a_microsoft_collection/n20ax3c/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751991130,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n20k9pc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Kooshi_Govno","can_mod_post":false,"created_utc":1751993772,"send_replies":true,"parent_id":"t1_n209j1m","score":3,"author_fullname":"t2_7kg5p","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It can likely be extended to 128k with yarn and still be good, like the original coder","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n20k9pc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It can likely be extended to 128k with yarn and still be good, like the original coder&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lurzqf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lurzqf/nextcoder_a_microsoft_collection/n20k9pc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751993772,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n209j1m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"VegaKH","can_mod_post":false,"created_utc":1751990732,"send_replies":true,"parent_id":"t3_1lurzqf","score":15,"author_fullname":"t2_11wjla","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This may be the first useful model MS has ever released. Looks pretty good for the size. But 32K context limits it to small projects.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n209j1m","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This may be the first useful model MS has ever released. Looks pretty good for the size. But 32K context limits it to small projects.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lurzqf/nextcoder_a_microsoft_collection/n209j1m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751990732,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lurzqf","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":15}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n20m068","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Kooshi_Govno","can_mod_post":false,"created_utc":1751994246,"send_replies":true,"parent_id":"t1_n20gn8f","score":6,"author_fullname":"t2_7kg5p","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Aider Polyglot is the bench to watch there, and Microsoft was clearly focused on improving performance there specifically. After a quick search, it seems like it specifically tests 6: https://github.com/Aider-AI/polyglot-benchmark\\n\\nBut being Microsoft, I would bet on C# and Typescript performance improving a lot as well.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n20m068","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Aider Polyglot is the bench to watch there, and Microsoft was clearly focused on improving performance there specifically. After a quick search, it seems like it specifically tests 6: &lt;a href=\\"https://github.com/Aider-AI/polyglot-benchmark\\"&gt;https://github.com/Aider-AI/polyglot-benchmark&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;But being Microsoft, I would bet on C# and Typescript performance improving a lot as well.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lurzqf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lurzqf/nextcoder_a_microsoft_collection/n20m068/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751994246,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2132tc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Creative-Size2658","can_mod_post":false,"send_replies":true,"parent_id":"t1_n20mpum","score":1,"author_fullname":"t2_1f3xb4r4ae","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; I would also recommend checking out other Coder finetunes. Openhands has been the best I\'ve personally tested so far.\\n\\nI\'ve tested OpenHands when MistralAI released Devstral, but I couldn\'t make myself to the whole thing in a browser. The whole git management was truly impressive though, but I want a simpler UX/UI so for my web related development I\'m sticking to Qwen3 32B and 30B in Zed.\\n\\nI\'m waiting for Qwen3-coder and Xcode 26 new built-in agent features to give it a try. I hope I won\'t be disappointed!","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2132tc","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;I would also recommend checking out other Coder finetunes. Openhands has been the best I&amp;#39;ve personally tested so far.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I&amp;#39;ve tested OpenHands when MistralAI released Devstral, but I couldn&amp;#39;t make myself to the whole thing in a browser. The whole git management was truly impressive though, but I want a simpler UX/UI so for my web related development I&amp;#39;m sticking to Qwen3 32B and 30B in Zed.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m waiting for Qwen3-coder and Xcode 26 new built-in agent features to give it a try. I hope I won&amp;#39;t be disappointed!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lurzqf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lurzqf/nextcoder_a_microsoft_collection/n2132tc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751998867,"author_flair_text":null,"treatment_tags":[],"created_utc":1751998867,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n20mpum","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Kooshi_Govno","can_mod_post":false,"created_utc":1751994440,"send_replies":true,"parent_id":"t1_n20gn8f","score":2,"author_fullname":"t2_7kg5p","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I would also recommend checking out other Coder finetunes. Openhands has been the best I\'ve personally tested so far. There was also one that specifically focused on web UI that was posted a couple weeks ago.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n20mpum","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I would also recommend checking out other Coder finetunes. Openhands has been the best I&amp;#39;ve personally tested so far. There was also one that specifically focused on web UI that was posted a couple weeks ago.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lurzqf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lurzqf/nextcoder_a_microsoft_collection/n20mpum/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751994440,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n20gn8f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Creative-Size2658","can_mod_post":false,"created_utc":1751992752,"send_replies":true,"parent_id":"t3_1lurzqf","score":6,"author_fullname":"t2_1f3xb4r4ae","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Do we have any information regarding the programming languages that were tested?\\n\\nI started using Qwen2.5 Coder instead of Codestral because it was so much better at Swift/SwiftUI, while significantly worse at web development.\\n\\nThanks!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n20gn8f","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Do we have any information regarding the programming languages that were tested?&lt;/p&gt;\\n\\n&lt;p&gt;I started using Qwen2.5 Coder instead of Codestral because it was so much better at Swift/SwiftUI, while significantly worse at web development.&lt;/p&gt;\\n\\n&lt;p&gt;Thanks!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lurzqf/nextcoder_a_microsoft_collection/n20gn8f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751992752,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lurzqf","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n20pom3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"LocoMod","can_mod_post":false,"created_utc":1751995250,"send_replies":true,"parent_id":"t3_1lurzqf","score":5,"author_fullname":"t2_6uuoq","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"32B GGUFs: https://huggingface.co/gabriellarson/NextCoder-32B-GGUF","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n20pom3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;32B GGUFs: &lt;a href=\\"https://huggingface.co/gabriellarson/NextCoder-32B-GGUF\\"&gt;https://huggingface.co/gabriellarson/NextCoder-32B-GGUF&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lurzqf/nextcoder_a_microsoft_collection/n20pom3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751995250,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lurzqf","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n209710","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"TheRealMasonMac","can_mod_post":false,"created_utc":1751990636,"send_replies":true,"parent_id":"t3_1lurzqf","score":14,"author_fullname":"t2_101haj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Based on the paper, here is the description of the **SeleKT** algorithm.\\n\\n### Overview and Purpose\\n\\nSeleKT, which stands for \\"Selective Knowledge Transfer,\\" is a novel model adaptation algorithm designed to fine-tune code language models (LMs) for specific tasks like code editing without losing the general abilities (e.g., code generation, instruction following) acquired during pre-training. It aims to prevent \\"catastrophic forgetting\\" by selectively and dynamically updating only the most important model weights for the new task.\\n\\n### Core Problem and Motivation\\n\\nThe paper identifies two key challenges in adapting pre-trained LMs:\\n1.  **Lack of high-quality fine-tuning data** for diverse code edits.\\n2.  **Catastrophic forgetting**, where fine-tuning on a specific task degrades the model\'s general, pre-learned abilities.\\n\\nExisting parameter-efficient fine-tuning (PEFT) methods like LoRA often select which parameters to update *a priori* (before training begins) and keep them fixed. The authors of this paper argue that the parameters needing updates should be **continuously re-assessed** during the fine-tuning process based on the training loss.\\n\\nThe robust adaptation problem is formally stated as minimizing the training loss `L(θ)` subject to the constraint that the updated model weights `θ` remain close to the original base model weights `θ_base`, specifically by limiting the number of changed parameters (L0-norm):\\n\\n`arg min L(θ)  s.t.  ||θ - θ_base||₀ ≤ c`\\n\\n### Key Insights and Mechanism\\n\\nSeleKT is built on two main insights:\\n\\n1.  **Dense Gradients:** To identify the most important parameters, the algorithm first performs a standard full fine-tuning step, updating all model parameters. This allows it to compute \\"dense gradients\\" that determine the optimal direction of change for the entire model to minimize the training loss on the code-editing data.\\n\\n2.  **Sparse Projection:** After identifying the direction of change, the algorithm performs a \\"sparse projection.\\" It computes a \\"task vector\\" (`τ = θ - θ_base`), which represents the changes made to the weights. It then identifies the `top-k` parameters with the largest magnitude of change in this vector and applies updates *only* to this small subset. All other parameters are reset to their original values from the base model. This step ensures the fine-tuned model stays close to the base model, avoiding overfitting.\\n\\n### The Algorithm (SeleKT: Selective Knowledge Transfer)\\n\\nThe algorithm is presented formally in **Algorithm 1**. It is parameterized by:\\n*   **Sparsity (α):** The fraction of total model parameters to be updated.\\n*   **Periodicity (M):** How often (in terms of training steps) the sparse projection step is performed.\\n\\nThe steps are as follows:\\n\\n**Require:** Base LM weights `θ_base`, training data `D`, epochs `E`, periodicity `M`, sparsity `α`.\\n**Ensure:** Final fine-tuned weights `θ_FT`.\\n\\n1.  Initialize `θ ← θ_base`.\\n2.  For each epoch `e` from 1 to `E`:\\n3.  For each minibatch `D[s]` in the training data:\\n4.  Update the model weights by taking a standard training step with dense gradients: `θ ← TrainStep(θ, D[s])`.\\n5.  **Periodically perform the projection:** If the current step `s` is a multiple of `M`:\\n6.  Compute the task vector: `τ ← θ - θ_base`.\\n7.  Select the top `α * N` parameters (where N is the total number of parameters) by creating a mask `γ` that is 1 for the top parameters in `τ` (by magnitude) and 0 otherwise.\\n8.  Project the updates onto the base model: `θ ← θ_base + γ ◦ τ` (where `◦` is element-wise multiplication). This applies the changes only to the selected sparse set of weights.\\n9.  End if.\\n10. End for (minibatch).\\n11. End for (epoch).\\n12. Return `θ` as `θ_FT`.\\n\\nThis process of periodically re-assessing which weights to update, based on their magnitude of change during full fine-tuning, is the key differentiator of SeleKT from other sparse adaptation methods.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n209710","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Based on the paper, here is the description of the &lt;strong&gt;SeleKT&lt;/strong&gt; algorithm.&lt;/p&gt;\\n\\n&lt;h3&gt;Overview and Purpose&lt;/h3&gt;\\n\\n&lt;p&gt;SeleKT, which stands for &amp;quot;Selective Knowledge Transfer,&amp;quot; is a novel model adaptation algorithm designed to fine-tune code language models (LMs) for specific tasks like code editing without losing the general abilities (e.g., code generation, instruction following) acquired during pre-training. It aims to prevent &amp;quot;catastrophic forgetting&amp;quot; by selectively and dynamically updating only the most important model weights for the new task.&lt;/p&gt;\\n\\n&lt;h3&gt;Core Problem and Motivation&lt;/h3&gt;\\n\\n&lt;p&gt;The paper identifies two key challenges in adapting pre-trained LMs:\\n1.  &lt;strong&gt;Lack of high-quality fine-tuning data&lt;/strong&gt; for diverse code edits.\\n2.  &lt;strong&gt;Catastrophic forgetting&lt;/strong&gt;, where fine-tuning on a specific task degrades the model&amp;#39;s general, pre-learned abilities.&lt;/p&gt;\\n\\n&lt;p&gt;Existing parameter-efficient fine-tuning (PEFT) methods like LoRA often select which parameters to update &lt;em&gt;a priori&lt;/em&gt; (before training begins) and keep them fixed. The authors of this paper argue that the parameters needing updates should be &lt;strong&gt;continuously re-assessed&lt;/strong&gt; during the fine-tuning process based on the training loss.&lt;/p&gt;\\n\\n&lt;p&gt;The robust adaptation problem is formally stated as minimizing the training loss &lt;code&gt;L(θ)&lt;/code&gt; subject to the constraint that the updated model weights &lt;code&gt;θ&lt;/code&gt; remain close to the original base model weights &lt;code&gt;θ_base&lt;/code&gt;, specifically by limiting the number of changed parameters (L0-norm):&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;arg min L(θ)  s.t.  ||θ - θ_base||₀ ≤ c&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;h3&gt;Key Insights and Mechanism&lt;/h3&gt;\\n\\n&lt;p&gt;SeleKT is built on two main insights:&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Dense Gradients:&lt;/strong&gt; To identify the most important parameters, the algorithm first performs a standard full fine-tuning step, updating all model parameters. This allows it to compute &amp;quot;dense gradients&amp;quot; that determine the optimal direction of change for the entire model to minimize the training loss on the code-editing data.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Sparse Projection:&lt;/strong&gt; After identifying the direction of change, the algorithm performs a &amp;quot;sparse projection.&amp;quot; It computes a &amp;quot;task vector&amp;quot; (&lt;code&gt;τ = θ - θ_base&lt;/code&gt;), which represents the changes made to the weights. It then identifies the &lt;code&gt;top-k&lt;/code&gt; parameters with the largest magnitude of change in this vector and applies updates &lt;em&gt;only&lt;/em&gt; to this small subset. All other parameters are reset to their original values from the base model. This step ensures the fine-tuned model stays close to the base model, avoiding overfitting.&lt;/p&gt;&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;h3&gt;The Algorithm (SeleKT: Selective Knowledge Transfer)&lt;/h3&gt;\\n\\n&lt;p&gt;The algorithm is presented formally in &lt;strong&gt;Algorithm 1&lt;/strong&gt;. It is parameterized by:\\n*   &lt;strong&gt;Sparsity (α):&lt;/strong&gt; The fraction of total model parameters to be updated.\\n*   &lt;strong&gt;Periodicity (M):&lt;/strong&gt; How often (in terms of training steps) the sparse projection step is performed.&lt;/p&gt;\\n\\n&lt;p&gt;The steps are as follows:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Require:&lt;/strong&gt; Base LM weights &lt;code&gt;θ_base&lt;/code&gt;, training data &lt;code&gt;D&lt;/code&gt;, epochs &lt;code&gt;E&lt;/code&gt;, periodicity &lt;code&gt;M&lt;/code&gt;, sparsity &lt;code&gt;α&lt;/code&gt;.\\n&lt;strong&gt;Ensure:&lt;/strong&gt; Final fine-tuned weights &lt;code&gt;θ_FT&lt;/code&gt;.&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt; Initialize &lt;code&gt;θ ← θ_base&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt; For each epoch &lt;code&gt;e&lt;/code&gt; from 1 to &lt;code&gt;E&lt;/code&gt;:&lt;/li&gt;\\n&lt;li&gt; For each minibatch &lt;code&gt;D[s]&lt;/code&gt; in the training data:&lt;/li&gt;\\n&lt;li&gt; Update the model weights by taking a standard training step with dense gradients: &lt;code&gt;θ ← TrainStep(θ, D[s])&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt; &lt;strong&gt;Periodically perform the projection:&lt;/strong&gt; If the current step &lt;code&gt;s&lt;/code&gt; is a multiple of &lt;code&gt;M&lt;/code&gt;:&lt;/li&gt;\\n&lt;li&gt; Compute the task vector: &lt;code&gt;τ ← θ - θ_base&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt; Select the top &lt;code&gt;α * N&lt;/code&gt; parameters (where N is the total number of parameters) by creating a mask &lt;code&gt;γ&lt;/code&gt; that is 1 for the top parameters in &lt;code&gt;τ&lt;/code&gt; (by magnitude) and 0 otherwise.&lt;/li&gt;\\n&lt;li&gt; Project the updates onto the base model: &lt;code&gt;θ ← θ_base + γ ◦ τ&lt;/code&gt; (where &lt;code&gt;◦&lt;/code&gt; is element-wise multiplication). This applies the changes only to the selected sparse set of weights.&lt;/li&gt;\\n&lt;li&gt; End if.&lt;/li&gt;\\n&lt;li&gt;End for (minibatch).&lt;/li&gt;\\n&lt;li&gt;End for (epoch).&lt;/li&gt;\\n&lt;li&gt;Return &lt;code&gt;θ&lt;/code&gt; as &lt;code&gt;θ_FT&lt;/code&gt;.&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;This process of periodically re-assessing which weights to update, based on their magnitude of change during full fine-tuning, is the key differentiator of SeleKT from other sparse adaptation methods.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lurzqf/nextcoder_a_microsoft_collection/n209710/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751990636,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lurzqf","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":14}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n25nqe9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Hurricane31337","can_mod_post":false,"send_replies":true,"parent_id":"t1_n25n9n4","score":2,"author_fullname":"t2_13pf1m","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah, but as a WinForms, WPF or .NET MAUI developer that doesn’t help at all. 🤷‍♂️","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n25nqe9","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, but as a WinForms, WPF or .NET MAUI developer that doesn’t help at all. 🤷‍♂️&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lurzqf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lurzqf/nextcoder_a_microsoft_collection/n25nqe9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752060703,"author_flair_text":null,"treatment_tags":[],"created_utc":1752060703,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n25n9n4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"thirteen-bit","can_mod_post":false,"created_utc":1752060510,"send_replies":true,"parent_id":"t1_n20ycmp","score":1,"author_fullname":"t2_9l12dgc5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"At least I see a C, C++ and Go code in a dataset:\\n\\nhttps://huggingface.co/datasets/microsoft/NextCoderDataset#data-distribution","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n25n9n4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;At least I see a C, C++ and Go code in a dataset:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/datasets/microsoft/NextCoderDataset#data-distribution\\"&gt;https://huggingface.co/datasets/microsoft/NextCoderDataset#data-distribution&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lurzqf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lurzqf/nextcoder_a_microsoft_collection/n25n9n4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752060510,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n20ycmp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Hurricane31337","can_mod_post":false,"created_utc":1751997616,"send_replies":true,"parent_id":"t3_1lurzqf","score":9,"author_fullname":"t2_13pf1m","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It’s a really nice release, but I don’t know why they didn’t include C#, VB.NET or TypeScript in their dataset. These languages are from Microsoft themselves after all and you’d think they want to push those (at least C#).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n20ycmp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It’s a really nice release, but I don’t know why they didn’t include C#, VB.NET or TypeScript in their dataset. These languages are from Microsoft themselves after all and you’d think they want to push those (at least C#).&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lurzqf/nextcoder_a_microsoft_collection/n20ycmp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751997616,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lurzqf","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n20biw6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Ok_Needleworker_5247","can_mod_post":false,"created_utc":1751991303,"send_replies":true,"parent_id":"t3_1lurzqf","score":3,"author_fullname":"t2_1gmprv51a1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Interesting release timing. Do these models have practical enhancements for everyday programming tasks, or are they more aimed at complex code edits that typical devs might not encounter often? Would love to see some real-world use cases.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n20biw6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Interesting release timing. Do these models have practical enhancements for everyday programming tasks, or are they more aimed at complex code edits that typical devs might not encounter often? Would love to see some real-world use cases.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lurzqf/nextcoder_a_microsoft_collection/n20biw6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751991303,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lurzqf","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n20g3jk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"indicava","can_mod_post":false,"created_utc":1751992596,"send_replies":true,"parent_id":"t3_1lurzqf","score":3,"author_fullname":"t2_4dvff","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"One of the big advantages of PEFT (LoRA) fine tuning is that it significantly reduces the compute (especially VRAM) needed for fine tuning. \\n\\nIf I understand correctly, this algorithm always performs a full parameter fine tune in each step, so resource wise we would still need the same compute as for a full parameter fine tune?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n20g3jk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;One of the big advantages of PEFT (LoRA) fine tuning is that it significantly reduces the compute (especially VRAM) needed for fine tuning. &lt;/p&gt;\\n\\n&lt;p&gt;If I understand correctly, this algorithm always performs a full parameter fine tune in each step, so resource wise we would still need the same compute as for a full parameter fine tune?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lurzqf/nextcoder_a_microsoft_collection/n20g3jk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751992596,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lurzqf","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n20x3ac","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"jedisct1","can_mod_post":false,"created_utc":1751997277,"send_replies":true,"parent_id":"t3_1lurzqf","score":3,"author_fullname":"t2_7p6tw","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"MLX files:\\n- https://huggingface.co/jedisct1/NextCoder-32B-mlx\\n- https://huggingface.co/jedisct1/NextCoder-14B-mlx\\n- https://huggingface.co/jedisct1/NextCoder-8B-mlx","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n20x3ac","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;MLX files:\\n- &lt;a href=\\"https://huggingface.co/jedisct1/NextCoder-32B-mlx\\"&gt;https://huggingface.co/jedisct1/NextCoder-32B-mlx&lt;/a&gt;\\n- &lt;a href=\\"https://huggingface.co/jedisct1/NextCoder-14B-mlx\\"&gt;https://huggingface.co/jedisct1/NextCoder-14B-mlx&lt;/a&gt;\\n- &lt;a href=\\"https://huggingface.co/jedisct1/NextCoder-8B-mlx\\"&gt;https://huggingface.co/jedisct1/NextCoder-8B-mlx&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lurzqf/nextcoder_a_microsoft_collection/n20x3ac/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751997277,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lurzqf","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n23nw6m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"HilLiedTroopsDied","can_mod_post":false,"created_utc":1752026858,"send_replies":true,"parent_id":"t3_1lurzqf","score":2,"author_fullname":"t2_1snfn3ui","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Does this handle Fill in the middle like qwen2.5-coder does?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n23nw6m","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Does this handle Fill in the middle like qwen2.5-coder does?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lurzqf/nextcoder_a_microsoft_collection/n23nw6m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752026858,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lurzqf","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n207eo6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Dark_Fire_12","can_mod_post":false,"created_utc":1751990125,"send_replies":true,"parent_id":"t1_n206yj0","score":14,"author_fullname":"t2_kwl47","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That was empty at the time, the models came out today.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n207eo6","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That was empty at the time, the models came out today.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lurzqf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lurzqf/nextcoder_a_microsoft_collection/n207eo6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751990125,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":14}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n20j3es","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Languages_Learner","can_mod_post":false,"created_utc":1751993444,"send_replies":true,"parent_id":"t1_n206yj0","score":3,"author_fullname":"t2_v9x8tm7u","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Here are some ggufs:\\n\\n[NikolayKozloff/NextCoder-7B-Q8\\\\_0-GGUF · Hugging Face](https://huggingface.co/NikolayKozloff/NextCoder-7B-Q8_0-GGUF)\\n\\n[NikolayKozloff/NextCoder-14B-Q5\\\\_K\\\\_S-GGUF · Hugging Face](https://huggingface.co/NikolayKozloff/NextCoder-14B-Q5_K_S-GGUF)","edited":1751994677,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n20j3es","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Here are some ggufs:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/NikolayKozloff/NextCoder-7B-Q8_0-GGUF\\"&gt;NikolayKozloff/NextCoder-7B-Q8_0-GGUF · Hugging Face&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/NikolayKozloff/NextCoder-14B-Q5_K_S-GGUF\\"&gt;NikolayKozloff/NextCoder-14B-Q5_K_S-GGUF · Hugging Face&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lurzqf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lurzqf/nextcoder_a_microsoft_collection/n20j3es/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751993444,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n206yj0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AdamDhahabi","can_mod_post":false,"created_utc":1751989996,"send_replies":true,"parent_id":"t3_1lurzqf","score":3,"author_fullname":"t2_x5lnbc2","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Discussed two months ago. I can\'t even find gguf for it. [https://www.reddit.com/r/LocalLLaMA/comments/1kdy8ia/microsoft\\\\_is\\\\_cooking\\\\_coding\\\\_models\\\\_nextcoder/](https://www.reddit.com/r/LocalLLaMA/comments/1kdy8ia/microsoft_is_cooking_coding_models_nextcoder/)","edited":1751990460,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n206yj0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Discussed two months ago. I can&amp;#39;t even find gguf for it. &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1kdy8ia/microsoft_is_cooking_coding_models_nextcoder/\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1kdy8ia/microsoft_is_cooking_coding_models_nextcoder/&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lurzqf/nextcoder_a_microsoft_collection/n206yj0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751989996,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lurzqf","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n207kx0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Dark_Fire_12","can_mod_post":false,"created_utc":1751990174,"send_replies":true,"parent_id":"t3_1lurzqf","score":2,"author_fullname":"t2_kwl47","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Paper Link: [https://www.microsoft.com/en-us/research/publication/nextcoder-robust-adaptation-of-code-lms-to-diverse-code-edits/](https://www.microsoft.com/en-us/research/publication/nextcoder-robust-adaptation-of-code-lms-to-diverse-code-edits/)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n207kx0","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Paper Link: &lt;a href=\\"https://www.microsoft.com/en-us/research/publication/nextcoder-robust-adaptation-of-code-lms-to-diverse-code-edits/\\"&gt;https://www.microsoft.com/en-us/research/publication/nextcoder-robust-adaptation-of-code-lms-to-diverse-code-edits/&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lurzqf/nextcoder_a_microsoft_collection/n207kx0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751990174,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lurzqf","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n24ghpu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"deleteme123","can_mod_post":false,"created_utc":1752038253,"send_replies":true,"parent_id":"t3_1lurzqf","score":1,"author_fullname":"t2_3c2kr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"NextCoder14B seems like the sweet spot?\\n\\nAnyone running this on a Macbook M series? stats?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n24ghpu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;NextCoder14B seems like the sweet spot?&lt;/p&gt;\\n\\n&lt;p&gt;Anyone running this on a Macbook M series? stats?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lurzqf/nextcoder_a_microsoft_collection/n24ghpu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752038253,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lurzqf","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]'),n=()=>e.jsx(t,{data:l});export{n as default};
