import{j as e}from"./index-BgwOAK4-.js";import{R as l}from"./RedditPostRenderer-BOBjDTFu.js";import"./index-BL22wVg5.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I have an article to instruct those models to rewrite in a different style without missing information, Qwen3-32B did an excellent job, it keeps the meaning but almost rewrite everything.\\n\\nQwen3-14B,8B tend to miss some information but acceptable\\n\\nQwen3-4B miss 50% of information\\n\\nMistral 3.2, on the other hand does not miss anything but almost copied the original with minor changes.\\n\\nGemma3-27: almost a true copy, just stupid\\n\\nStructured data generation: Another test is to extract Json from raw html, Qweb3-4b fakes data and all others performs well.\\n\\nArticle classification: long messy reddit posts with simple prompt to classify if the post is looking for help, Qwen3-8,14,32 all made it 100% correct, Qwen3-4b mostly correct, Mistral and Gemma always make some mistakes to classify.\\n\\nOverall, I should say 8b is the best one to do such tasks especially for long articles, the model consumes less vRam allows more vRam allocated to KV Cache\\n\\nJust my small and simple test today, hope it helps if someone is looking for this use case.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"My simple test: Qwen3-32b &gt; Qwen3-14B ≈ DS Qwen3-8 ≳ Qwen3-4B &gt; Mistral 3.2 24B &gt; Gemma3-27b-it,","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m1ylw0","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.83,"author_flair_background_color":null,"subreddit_type":"public","ups":59,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_7sgltzo8","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":59,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752727938,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I have an article to instruct those models to rewrite in a different style without missing information, Qwen3-32B did an excellent job, it keeps the meaning but almost rewrite everything.&lt;/p&gt;\\n\\n&lt;p&gt;Qwen3-14B,8B tend to miss some information but acceptable&lt;/p&gt;\\n\\n&lt;p&gt;Qwen3-4B miss 50% of information&lt;/p&gt;\\n\\n&lt;p&gt;Mistral 3.2, on the other hand does not miss anything but almost copied the original with minor changes.&lt;/p&gt;\\n\\n&lt;p&gt;Gemma3-27: almost a true copy, just stupid&lt;/p&gt;\\n\\n&lt;p&gt;Structured data generation: Another test is to extract Json from raw html, Qweb3-4b fakes data and all others performs well.&lt;/p&gt;\\n\\n&lt;p&gt;Article classification: long messy reddit posts with simple prompt to classify if the post is looking for help, Qwen3-8,14,32 all made it 100% correct, Qwen3-4b mostly correct, Mistral and Gemma always make some mistakes to classify.&lt;/p&gt;\\n\\n&lt;p&gt;Overall, I should say 8b is the best one to do such tasks especially for long articles, the model consumes less vRam allows more vRam allocated to KV Cache&lt;/p&gt;\\n\\n&lt;p&gt;Just my small and simple test today, hope it helps if someone is looking for this use case.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1m1ylw0","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"BestLeonNA","discussion_type":null,"num_comments":51,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/","subreddit_subscribers":500897,"created_utc":1752727938,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3mqlpi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"YearZero","can_mod_post":false,"created_utc":1752760326,"send_replies":true,"parent_id":"t1_n3min1u","score":3,"author_fullname":"t2_4kpsn","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"And translator! It seems that it tends to lose details in long contexts more than Qwen3 32b, however.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3mqlpi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;And translator! It seems that it tends to lose details in long contexts more than Qwen3 32b, however.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m1ylw0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3mqlpi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752760326,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n3min1u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ParaboloidalCrest","can_mod_post":false,"created_utc":1752757843,"send_replies":true,"parent_id":"t3_1m1ylw0","score":14,"author_fullname":"t2_nc2u4f7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This is purely anecdotal. Gemma in my experience is the best writer out there, but this as well is anecdotal.","edited":1752758128,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3min1u","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is purely anecdotal. Gemma in my experience is the best writer out there, but this as well is anecdotal.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3min1u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752757843,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m1ylw0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":14}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3l9dil","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"jacek2023","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3l5voa","score":6,"author_fullname":"t2_vqgbql9w","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"In machine learning, there is the concept of \\"training data\\" and \\"test data.\\" You train your model on the training dataset, but you validate it on a separate test dataset. If you validate on the same data you trained on, the model's performance will be misleading. Similarly, you can't design a prompt specifically to work well with Qwen and then complain that Mistral doesn't handle it the same way.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3l9dil","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;In machine learning, there is the concept of &amp;quot;training data&amp;quot; and &amp;quot;test data.&amp;quot; You train your model on the training dataset, but you validate it on a separate test dataset. If you validate on the same data you trained on, the model&amp;#39;s performance will be misleading. Similarly, you can&amp;#39;t design a prompt specifically to work well with Qwen and then complain that Mistral doesn&amp;#39;t handle it the same way.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m1ylw0","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3l9dil/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752736713,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752736713,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n3l5voa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BestLeonNA","can_mod_post":false,"created_utc":1752734857,"send_replies":true,"parent_id":"t1_n3l3bd2","score":1,"author_fullname":"t2_7sgltzo8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I just switched to GPT to refine my prompt, Mistral has a lot of improvement comparing the refined prompt from Claude. I think the Claude style prompt (looks more technical) works better with Qwen's thinking mode. But Gemma3.... still fall behind, not much difference between the many versions of prompt","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3l5voa","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I just switched to GPT to refine my prompt, Mistral has a lot of improvement comparing the refined prompt from Claude. I think the Claude style prompt (looks more technical) works better with Qwen&amp;#39;s thinking mode. But Gemma3.... still fall behind, not much difference between the many versions of prompt&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m1ylw0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3l5voa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752734857,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3l3bd2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"jacek2023","can_mod_post":false,"created_utc":1752733528,"send_replies":true,"parent_id":"t3_1m1ylw0","score":23,"author_fullname":"t2_vqgbql9w","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"There is a strong possibility that your test is overfitted to Qwen, leading to poor performance on other models.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3l3bd2","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There is a strong possibility that your test is overfitted to Qwen, leading to poor performance on other models.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3l3bd2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752733528,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m1ylw0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":23}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3lenw5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No_Efficiency_1144","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3l6cmz","score":9,"author_fullname":"t2_1nkj9l14b0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Prompt differences are so huge it can make LLMs feel totally different","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3lenw5","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Prompt differences are so huge it can make LLMs feel totally different&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m1ylw0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3lenw5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752739687,"author_flair_text":null,"treatment_tags":[],"created_utc":1752739687,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}}],"before":null}},"user_reports":[],"saved":false,"id":"n3l6cmz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"BestLeonNA","can_mod_post":false,"created_utc":1752735102,"send_replies":true,"parent_id":"t1_n3ktzs8","score":5,"author_fullname":"t2_7sgltzo8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes, Mistral improves a lot by completely changing the prompt","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3l6cmz","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, Mistral improves a lot by completely changing the prompt&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m1ylw0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3l6cmz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752735102,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"7d921414-5177-11ee-b947-e27b363b98d5","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3qzvsm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BestLeonNA","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3lp479","score":1,"author_fullname":"t2_7sgltzo8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ok, I will try Magistral to see, and yes, there are quite a bit of reasoning for this task. I'd like to use Mistral/Magistral comparing to Qwen3 since they has more training data on English","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3qzvsm","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ok, I will try Magistral to see, and yes, there are quite a bit of reasoning for this task. I&amp;#39;d like to use Mistral/Magistral comparing to Qwen3 since they has more training data on English&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m1ylw0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3qzvsm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752806940,"author_flair_text":null,"treatment_tags":[],"created_utc":1752806940,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3lp479","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ReturningTarzan","can_mod_post":false,"created_utc":1752745667,"send_replies":true,"parent_id":"t1_n3ktzs8","score":2,"author_fullname":"t2_4dru3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Also important to optimize sampling settings for each model. Temperature doesn't mean the same thing to Qwen as it does to Mistral or to Gemma, so if one of them is hallucinating more than you'd expect, it might just need a lower temperature. As for reasoning, maybe Magistral is a better candidate than Mistral when comparing against Qwen3.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3lp479","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"ExLlama Developer"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Also important to optimize sampling settings for each model. Temperature doesn&amp;#39;t mean the same thing to Qwen as it does to Mistral or to Gemma, so if one of them is hallucinating more than you&amp;#39;d expect, it might just need a lower temperature. As for reasoning, maybe Magistral is a better candidate than Mistral when comparing against Qwen3.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m1ylw0","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3lp479/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752745667,"author_flair_text":"ExLlama Developer","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#5a74cc","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ktzs8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Equivalent_Cut_5845","can_mod_post":false,"created_utc":1752728881,"send_replies":true,"parent_id":"t3_1m1ylw0","score":25,"author_fullname":"t2_1oy2v7xti6","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I mean it might just be the problem with your prompt, or for whatever reason thinking models are super suited to your tasks.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ktzs8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I mean it might just be the problem with your prompt, or for whatever reason thinking models are super suited to your tasks.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3ktzs8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752728881,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m1ylw0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":25}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"50c36eba-fdca-11ee-9735-92a88d7e3b87","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3lrhn9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ayylmaonade","can_mod_post":false,"created_utc":1752746927,"send_replies":true,"parent_id":"t3_1m1ylw0","score":5,"author_fullname":"t2_oqajf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Could you share the prompt? I recently switched to Qwen3-30B-A3B MoE after daily-driving Qwen3-14B and I'd like to compare.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3lrhn9","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Ollama"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Could you share the prompt? I recently switched to Qwen3-30B-A3B MoE after daily-driving Qwen3-14B and I&amp;#39;d like to compare.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3lrhn9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752746927,"author_flair_text":"Ollama","treatment_tags":[],"link_id":"t3_1m1ylw0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3kyn31","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BestLeonNA","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3ky1zv","score":3,"author_fullname":"t2_7sgltzo8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yes, it's possible, I'm trying some completely different prompt structures see if anything changes","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3kyn31","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, it&amp;#39;s possible, I&amp;#39;m trying some completely different prompt structures see if anything changes&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m1ylw0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3kyn31/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752731173,"author_flair_text":null,"treatment_tags":[],"created_utc":1752731173,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ky1zv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"myvirtualrealitymask","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3kvled","score":4,"author_fullname":"t2_tfqbspezw","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Still, it's entirely up to your prompt and sampling parameters.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3ky1zv","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Still, it&amp;#39;s entirely up to your prompt and sampling parameters.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m1ylw0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3ky1zv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752730882,"author_flair_text":null,"treatment_tags":[],"created_utc":1752730882,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n3kvled","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BestLeonNA","can_mod_post":false,"created_utc":1752729652,"send_replies":true,"parent_id":"t1_n3ku9zd","score":1,"author_fullname":"t2_7sgltzo8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It's not writing, it's re-style an existing article.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3kvled","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s not writing, it&amp;#39;s re-style an existing article.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m1ylw0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3kvled/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752729652,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ku9zd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"myvirtualrealitymask","can_mod_post":false,"created_utc":1752729017,"send_replies":true,"parent_id":"t3_1m1ylw0","score":11,"author_fullname":"t2_tfqbspezw","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This is entirely prompt based. Mistral 3.2 works amazingly on writing tasks.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ku9zd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is entirely prompt based. Mistral 3.2 works amazingly on writing tasks.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3ku9zd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752729017,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m1ylw0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3m8vcp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3lo5kt","score":2,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"27b is pretty good at math, surprisingly so. Not coding but math and to lesser extent science.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3m8vcp","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;27b is pretty good at math, surprisingly so. Not coding but math and to lesser extent science.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m1ylw0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3m8vcp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752754495,"author_flair_text":null,"treatment_tags":[],"created_utc":1752754495,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3lo5kt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"PurpleUpbeat2820","can_mod_post":false,"created_utc":1752745143,"send_replies":true,"parent_id":"t1_n3l6954","score":5,"author_fullname":"t2_7xnuxw8f","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It doesn't feel stupider to me but the gap between 4b and 27b does feel surprisingly small. I think 27b does produce higher quality language than 4b. As I've found them to be far below other models (particularly qwen) when it comes to technical knowledge I only use them to summarize texts.\\n\\nSomething I find particularly irritating about the gemma models is that they are always ludicrously positive and waste lots of tokens writing things like \\"What an absolutely fantastic question!\\". I find that intensely irritating. Qwen doesn't tend to do this and, in particular, when specifically instructed to use neutral language and avoid emotive writing it does so very well whereas gemma is always ludicrously positive. This makes gemma useless for preprocessing before using an embedding model, for example, because the embedded vector ends up mostly conveying gemma ludicrous sentiments and not the semantic meaning of the document.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3lo5kt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It doesn&amp;#39;t feel stupider to me but the gap between 4b and 27b does feel surprisingly small. I think 27b does produce higher quality language than 4b. As I&amp;#39;ve found them to be far below other models (particularly qwen) when it comes to technical knowledge I only use them to summarize texts.&lt;/p&gt;\\n\\n&lt;p&gt;Something I find particularly irritating about the gemma models is that they are always ludicrously positive and waste lots of tokens writing things like &amp;quot;What an absolutely fantastic question!&amp;quot;. I find that intensely irritating. Qwen doesn&amp;#39;t tend to do this and, in particular, when specifically instructed to use neutral language and avoid emotive writing it does so very well whereas gemma is always ludicrously positive. This makes gemma useless for preprocessing before using an embedding model, for example, because the embedded vector ends up mostly conveying gemma ludicrous sentiments and not the semantic meaning of the document.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m1ylw0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3lo5kt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752745143,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3li3rp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"martinerous","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3l6r3g","score":2,"author_fullname":"t2_5tp54ey","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Just speculating here. Gemma usually is good at following prompts and examples, and maybe this time it \\"shoots itself in a foot\\" by sticking to the original text too much and is unable to deviate from it enough.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3li3rp","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just speculating here. Gemma usually is good at following prompts and examples, and maybe this time it &amp;quot;shoots itself in a foot&amp;quot; by sticking to the original text too much and is unable to deviate from it enough.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m1ylw0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3li3rp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752741676,"author_flair_text":null,"treatment_tags":[],"created_utc":1752741676,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3l9n7f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"terminoid_","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3l826p","score":5,"author_fullname":"t2_1iu07dnz2i","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"you've really bungled your settings or something then","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3l9n7f","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;you&amp;#39;ve really bungled your settings or something then&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m1ylw0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3l9n7f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752736861,"author_flair_text":null,"treatment_tags":[],"created_utc":1752736861,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n3l826p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"beryugyo619","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3l6r3g","score":1,"author_fullname":"t2_v8wruy0k","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I mean, literally gemma3-4b feels way smarter than its own 27b sibling. It's weird.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3l826p","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I mean, literally gemma3-4b feels way smarter than its own 27b sibling. It&amp;#39;s weird.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m1ylw0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3l826p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752735997,"author_flair_text":null,"treatment_tags":[],"created_utc":1752735997,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3l6r3g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BestLeonNA","can_mod_post":false,"created_utc":1752735308,"send_replies":true,"parent_id":"t1_n3l6954","score":1,"author_fullname":"t2_7sgltzo8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I don't know, with the same prompt testing all these model together, Gemma3 is always the worst, for example I asked to remove names and change to user and assistant (it's a conversation between customer and sales with real names), all other models can correctly identify which one is user role which one is assistant and change the  conversation accordingly, but Gemma .... just leave the original names there","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3l6r3g","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t know, with the same prompt testing all these model together, Gemma3 is always the worst, for example I asked to remove names and change to user and assistant (it&amp;#39;s a conversation between customer and sales with real names), all other models can correctly identify which one is user role which one is assistant and change the  conversation accordingly, but Gemma .... just leave the original names there&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m1ylw0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3l6r3g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752735308,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3m8xj8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3luhop","score":3,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"no it is not.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3m8xj8","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;no it is not.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m1ylw0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3m8xj8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752754518,"author_flair_text":null,"treatment_tags":[],"created_utc":1752754518,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n3luhop","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Thomas-Lore","can_mod_post":false,"created_utc":1752748437,"send_replies":true,"parent_id":"t1_n3l6954","score":-3,"author_fullname":"t2_5hobp6m4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"4B is a reasoning model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3luhop","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;4B is a reasoning model.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m1ylw0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3luhop/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752748437,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-3}}],"before":null}},"user_reports":[],"saved":false,"id":"n3l6954","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"beryugyo619","can_mod_post":false,"created_utc":1752735051,"send_replies":true,"parent_id":"t3_1m1ylw0","score":3,"author_fullname":"t2_v8wruy0k","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Is something wrong with Gemma3 27b? It feels stupider than 4b.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3l6954","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Is something wrong with Gemma3 27b? It feels stupider than 4b.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3l6954/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752735051,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m1ylw0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3msok1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AltruisticList6000","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3mhp8j","score":3,"author_fullname":"t2_hnjq9xn4a","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Hmm I hope Cydonia will help it, maybe I will try it out. The last Cydonias I used were the ones based on 2409 and 2501. 2501 Cydonia had the same problems as the official 2501 Mistral, so repetitive, broken responses, infinity generations. \\n\\nBefore that I used to use the 22b 2409 based Cydonia until I realized the base/official 22b 2409 model is better, smarter and less repetitive than Cydonia. In fact I find the official 22b 2409 the best at RP out of all models I tried. It seem to be capable of doing an infinite amount of character behaviour whereas newer Mistrals always have some kind of generic slop behaviour or description integrated and Cydonia tends to default to its specific, similarly-acting characters too. And the offical 22b 2409 is absolutely wild and creative at NSFW too for some reason. I feel like it's a forgotten gem for creative writing and RP.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3msok1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hmm I hope Cydonia will help it, maybe I will try it out. The last Cydonias I used were the ones based on 2409 and 2501. 2501 Cydonia had the same problems as the official 2501 Mistral, so repetitive, broken responses, infinity generations. &lt;/p&gt;\\n\\n&lt;p&gt;Before that I used to use the 22b 2409 based Cydonia until I realized the base/official 22b 2409 model is better, smarter and less repetitive than Cydonia. In fact I find the official 22b 2409 the best at RP out of all models I tried. It seem to be capable of doing an infinite amount of character behaviour whereas newer Mistrals always have some kind of generic slop behaviour or description integrated and Cydonia tends to default to its specific, similarly-acting characters too. And the offical 22b 2409 is absolutely wild and creative at NSFW too for some reason. I feel like it&amp;#39;s a forgotten gem for creative writing and RP.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m1ylw0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3msok1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752760947,"author_flair_text":null,"treatment_tags":[],"created_utc":1752760947,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3q0h45","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"IrisColt","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3mhp8j","score":1,"author_fullname":"t2_c2f558x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Reluctantly, I’ll admit that Cydonia is an absolute powerhouse of a model for general creative writing tasks.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3q0h45","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Reluctantly, I’ll admit that Cydonia is an absolute powerhouse of a model for general creative writing tasks.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m1ylw0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3q0h45/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752794297,"author_flair_text":null,"treatment_tags":[],"created_utc":1752794297,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3mhp8j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kaisurniwurer","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3llxz0","score":3,"author_fullname":"t2_qafso","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I did notice that it does tend to repeat itself quite hardcore, but I have hopes for the new Cydonia if it arrives, since I like it's writing more than 3.1 and what's most important it handles longer context very well.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3mhp8j","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I did notice that it does tend to repeat itself quite hardcore, but I have hopes for the new Cydonia if it arrives, since I like it&amp;#39;s writing more than 3.1 and what&amp;#39;s most important it handles longer context very well.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m1ylw0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3mhp8j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752757534,"author_flair_text":null,"treatment_tags":[],"created_utc":1752757534,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n3llxz0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AltruisticList6000","can_mod_post":false,"created_utc":1752743884,"send_replies":true,"parent_id":"t1_n3ld3xk","score":3,"author_fullname":"t2_hnjq9xn4a","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Idk they said they reduced repetations of mistral 3.2 but I noticed it to be even more repetitive than 3.1 which was already more repetitive than the older 22b. Although 3.2 had less infinite generations just like they claimed so that's good. And it seems way overfit on the generic AI style with the em dashes. It literally copy pastes its own replies making it unusuable for RP or creative writing. For that I still like 22b 2409 it's behaving way better and not \\"overfit\\" on this new style of AI writing and slop. Oh and the 22b doesn't have infinite generation problem at all.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3llxz0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Idk they said they reduced repetations of mistral 3.2 but I noticed it to be even more repetitive than 3.1 which was already more repetitive than the older 22b. Although 3.2 had less infinite generations just like they claimed so that&amp;#39;s good. And it seems way overfit on the generic AI style with the em dashes. It literally copy pastes its own replies making it unusuable for RP or creative writing. For that I still like 22b 2409 it&amp;#39;s behaving way better and not &amp;quot;overfit&amp;quot; on this new style of AI writing and slop. Oh and the 22b doesn&amp;#39;t have infinite generation problem at all.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m1ylw0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3llxz0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752743884,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3r0cu5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BestLeonNA","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3lqxpy","score":2,"author_fullname":"t2_7sgltzo8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Interesting, it's quite a consistent finding as my test","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3r0cu5","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Interesting, it&amp;#39;s quite a consistent finding as my test&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m1ylw0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3r0cu5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752807122,"author_flair_text":null,"treatment_tags":[],"created_utc":1752807122,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3lqxpy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"PurpleUpbeat2820","can_mod_post":false,"created_utc":1752746636,"send_replies":true,"parent_id":"t1_n3ld3xk","score":2,"author_fullname":"t2_7xnuxw8f","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; I asked model for an answer (just basic chat) three times, then fed it those answers in an injection, then asked again. I was using mistral for it and it often repeated one of the answers. Just by adding \\"do not use the answers verbatim\\" made it generate a new one.\\n\\nI have a handy script called \`another\` that does something similar. You give it a list of things of the same kind and it uses an LLM to generate another thing of the same kind:\\n\\n    % echo \\"Wales\\\\nMauritius\\" | another\\n    Seychelles\\n    % echo \\"Antarctica\\\\nEurope\\" | another\\n    North America\\n    % echo \\"Springboks\\\\nAll Blacks\\" | another\\n    Wallabies","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3lqxpy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;I asked model for an answer (just basic chat) three times, then fed it those answers in an injection, then asked again. I was using mistral for it and it often repeated one of the answers. Just by adding &amp;quot;do not use the answers verbatim&amp;quot; made it generate a new one.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I have a handy script called &lt;code&gt;another&lt;/code&gt; that does something similar. You give it a list of things of the same kind and it uses an LLM to generate another thing of the same kind:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;% echo &amp;quot;Wales\\\\nMauritius&amp;quot; | another\\nSeychelles\\n% echo &amp;quot;Antarctica\\\\nEurope&amp;quot; | another\\nNorth America\\n% echo &amp;quot;Springboks\\\\nAll Blacks&amp;quot; | another\\nWallabies\\n&lt;/code&gt;&lt;/pre&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m1ylw0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3lqxpy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752746636,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ld3xk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kaisurniwurer","can_mod_post":false,"created_utc":1752738809,"send_replies":true,"parent_id":"t3_1m1ylw0","score":3,"author_fullname":"t2_qafso","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I did a somewhat similar experiment, thought I wasn't testing models but rather a system.\\n\\nI asked model for an answer (just basic chat) three times, then fed it those answers in an injection, then asked again. I was using mistral for it and it often repeated one of the answers. Just by adding \\"do not use the answers verbatim\\" made it generate a new one.\\n\\nMaybe overly stiff still, but it might not be a bad thing actually, having such strong prompt adherence. Though the natural comprehensions would be to write a new one anyway, so not sure.\\n\\nAnyway, interesting findings. Thanks for sharing.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ld3xk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I did a somewhat similar experiment, thought I wasn&amp;#39;t testing models but rather a system.&lt;/p&gt;\\n\\n&lt;p&gt;I asked model for an answer (just basic chat) three times, then fed it those answers in an injection, then asked again. I was using mistral for it and it often repeated one of the answers. Just by adding &amp;quot;do not use the answers verbatim&amp;quot; made it generate a new one.&lt;/p&gt;\\n\\n&lt;p&gt;Maybe overly stiff still, but it might not be a bad thing actually, having such strong prompt adherence. Though the natural comprehensions would be to write a new one anyway, so not sure.&lt;/p&gt;\\n\\n&lt;p&gt;Anyway, interesting findings. Thanks for sharing.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3ld3xk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752738809,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m1ylw0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3sx05s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FalseMap1582","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3ovblx","score":1,"author_fullname":"t2_14u3g9s5kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Q6\\\\_K feels like the sweet spot to me. But the difference from Q4\\\\_K\\\\_M or Q8 is subtle","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3sx05s","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Q6_K feels like the sweet spot to me. But the difference from Q4_K_M or Q8 is subtle&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m1ylw0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3sx05s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752840666,"author_flair_text":null,"treatment_tags":[],"created_utc":1752840666,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ovblx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CBW1255","can_mod_post":false,"created_utc":1752781843,"send_replies":true,"parent_id":"t1_n3oos0w","score":1,"author_fullname":"t2_uprpjkzls","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"what quant?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ovblx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;what quant?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m1ylw0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3ovblx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752781843,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3oos0w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FalseMap1582","can_mod_post":false,"created_utc":1752779946,"send_replies":true,"parent_id":"t3_1m1ylw0","score":2,"author_fullname":"t2_14u3g9s5kx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Qwen3-32b has been the best local model to me. It is the only one I trust enough to handle some simple coding tasks with Aider. I do find Gemma 3 a little bit better in portuguese, though","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3oos0w","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen3-32b has been the best local model to me. It is the only one I trust enough to handle some simple coding tasks with Aider. I do find Gemma 3 a little bit better in portuguese, though&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3oos0w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752779946,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m1ylw0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3l8i9j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"PavelPivovarov","can_mod_post":false,"created_utc":1752736238,"send_replies":true,"parent_id":"t1_n3l1cq4","score":5,"author_fullname":"t2_7ea11vu","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That's my go-to model, but I wouldn't say it's better than 32b. It's surprisingly good though.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3l8i9j","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s my go-to model, but I wouldn&amp;#39;t say it&amp;#39;s better than 32b. It&amp;#39;s surprisingly good though.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m1ylw0","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3l8i9j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752736238,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ll5hj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AltruisticList6000","can_mod_post":false,"created_utc":1752743430,"send_replies":true,"parent_id":"t1_n3l1cq4","score":4,"author_fullname":"t2_hnjq9xn4a","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"In my experience it was at the level of 14b (sometimes less precise) and since it spilled over RAM from my VRAM, it was about the same speed as 14b in VRAM fully. And it definitely wasn't at the level of 32b. 32b was way better at following small details in the prompt and tasks well ahead of both 14b and 30b.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ll5hj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;In my experience it was at the level of 14b (sometimes less precise) and since it spilled over RAM from my VRAM, it was about the same speed as 14b in VRAM fully. And it definitely wasn&amp;#39;t at the level of 32b. 32b was way better at following small details in the prompt and tasks well ahead of both 14b and 30b.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m1ylw0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3ll5hj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752743430,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3l5lw1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BestLeonNA","can_mod_post":false,"created_utc":1752734716,"send_replies":true,"parent_id":"t1_n3l1cq4","score":3,"author_fullname":"t2_7sgltzo8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"No, I heard mixed opinions on it, but haven't tried myself.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3l5lw1","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No, I heard mixed opinions on it, but haven&amp;#39;t tried myself.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m1ylw0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3l5lw1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752734716,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3m8oma","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1752754427,"send_replies":true,"parent_id":"t1_n3l1cq4","score":4,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"32b is _massively_ better than 30b. I use 30b as my main coding assistant model cause it stupidly fast, but it is not even comparable to 14b, let alone 32b.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3m8oma","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;32b is &lt;em&gt;massively&lt;/em&gt; better than 30b. I use 30b as my main coding assistant model cause it stupidly fast, but it is not even comparable to 14b, let alone 32b.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m1ylw0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3m8oma/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752754427,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n3l1cq4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"asifitwasantani","can_mod_post":false,"created_utc":1752732523,"send_replies":true,"parent_id":"t3_1m1ylw0","score":4,"author_fullname":"t2_1og3ywhm","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Have you tried qwen3:30b-a3b ? I feel it's even better than the 32b..","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3l1cq4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Have you tried qwen3:30b-a3b ? I feel it&amp;#39;s even better than the 32b..&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3l1cq4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752732523,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m1ylw0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3m41ml","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Nearby_Ad6249","can_mod_post":false,"created_utc":1752752661,"send_replies":true,"parent_id":"t3_1m1ylw0","score":2,"author_fullname":"t2_1tk58xwute","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"qwen3:32b at full BF16 model size is phenomenal. Much better than Q8 for difficult questions, albeit at 0.3 t/s on a 128GB M3 Macbook.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3m41ml","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;qwen3:32b at full BF16 model size is phenomenal. Much better than Q8 for difficult questions, albeit at 0.3 t/s on a 128GB M3 Macbook.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3m41ml/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752752661,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m1ylw0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3l8yex","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"custodiam99","can_mod_post":false,"created_utc":1752736480,"send_replies":true,"parent_id":"t3_1m1ylw0","score":1,"author_fullname":"t2_nqnhgqqf5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Can you try Qwen3 32b q4 too?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3l8yex","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Can you try Qwen3 32b q4 too?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3l8yex/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752736480,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m1ylw0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3lxeie","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Flashy_Management962","can_mod_post":false,"created_utc":1752749801,"send_replies":true,"parent_id":"t1_n3lopjj","score":3,"author_fullname":"t2_n9dnke1h","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"use llama cpp server with llama swap, there you can set exactly where the layers and the kv cache of the model goes. It most likely has something to do with automatic kv cache allocation which was always a trouble with ollama","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3lxeie","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;use llama cpp server with llama swap, there you can set exactly where the layers and the kv cache of the model goes. It most likely has something to do with automatic kv cache allocation which was always a trouble with ollama&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m1ylw0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3lxeie/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752749801,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n3lopjj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Rich_Artist_8327","can_mod_post":false,"created_utc":1752745444,"send_replies":true,"parent_id":"t3_1m1ylw0","score":1,"author_fullname":"t2_1jk2ep8a52","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I have 24GB 7900 xtx and cant run Mistral3.2 15GB model on fully in GPU VRAM it always takes total 26GB and loads only 14GB to vram and rest to RAM. Anyone else? Latest Ollama","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3lopjj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have 24GB 7900 xtx and cant run Mistral3.2 15GB model on fully in GPU VRAM it always takes total 26GB and loads only 14GB to vram and rest to RAM. Anyone else? Latest Ollama&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3lopjj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752745444,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m1ylw0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3r0sv3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BestLeonNA","can_mod_post":false,"created_utc":1752807295,"send_replies":true,"parent_id":"t1_n3n1bz2","score":1,"author_fullname":"t2_7sgltzo8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes, almost, I set 64k which is the max I can fit into my vRam, my prompt (including the original text) is around 30k and I asked the model to return same length","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3r0sv3","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, almost, I set 64k which is the max I can fit into my vRam, my prompt (including the original text) is around 30k and I asked the model to return same length&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m1ylw0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3r0sv3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752807295,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3n1bz2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"philiplrussell","can_mod_post":false,"created_utc":1752763421,"send_replies":true,"parent_id":"t3_1m1ylw0","score":1,"author_fullname":"t2_2ermd0ja","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"How long is your prompt? Does it max out the context windows?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3n1bz2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How long is your prompt? Does it max out the context windows?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3n1bz2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752763421,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m1ylw0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3pzgot","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"lemon07r","can_mod_post":false,"created_utc":1752793960,"send_replies":true,"parent_id":"t3_1m1ylw0","score":1,"author_fullname":"t2_i697e","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Try the DS qwen 3 8B slerp merge. Should do better, uses the superior qwen tokenizer, and has all the special tokens and what not for tool usage. [https://huggingface.co/lemon07r/Qwen3-R1-SLERP-Q3T-8B](https://huggingface.co/lemon07r/Qwen3-R1-SLERP-Q3T-8B)\\n\\n30A3B should be good too, more or less slightly above 14b.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3pzgot","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Try the DS qwen 3 8B slerp merge. Should do better, uses the superior qwen tokenizer, and has all the special tokens and what not for tool usage. &lt;a href=\\"https://huggingface.co/lemon07r/Qwen3-R1-SLERP-Q3T-8B\\"&gt;https://huggingface.co/lemon07r/Qwen3-R1-SLERP-Q3T-8B&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;30A3B should be good too, more or less slightly above 14b.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3pzgot/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752793960,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m1ylw0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3rl51u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GregoryfromtheHood","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3r0yi7","score":1,"author_fullname":"t2_g0qor","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"In English. And it's part of a fiction writing workflow where it generates the story piece by piece. It'll get instructions for various rewrite tasks like making it longer or shorter, or making sure to include a particular event or theme if it was missed in the initial write, or rewriting it completely to fit better within the context around it. Along with this, a lot of other context is given, like where it is up to in the story and an outline of the whole story and things that have happened so far etc.\\n\\nThis amount of context I have found can confuse a lot of Models. Actually all models other than Gemma that I've tried so far have issues somewhere along the way in the process, but Gemma 3 12b and 27b consistently perform the tasks well and make good use of the large context without getting confused.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3rl51u","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;In English. And it&amp;#39;s part of a fiction writing workflow where it generates the story piece by piece. It&amp;#39;ll get instructions for various rewrite tasks like making it longer or shorter, or making sure to include a particular event or theme if it was missed in the initial write, or rewriting it completely to fit better within the context around it. Along with this, a lot of other context is given, like where it is up to in the story and an outline of the whole story and things that have happened so far etc.&lt;/p&gt;\\n\\n&lt;p&gt;This amount of context I have found can confuse a lot of Models. Actually all models other than Gemma that I&amp;#39;ve tried so far have issues somewhere along the way in the process, but Gemma 3 12b and 27b consistently perform the tasks well and make good use of the large context without getting confused.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m1ylw0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3rl51u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752816095,"author_flair_text":null,"treatment_tags":[],"created_utc":1752816095,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3r0yi7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BestLeonNA","can_mod_post":false,"created_utc":1752807355,"send_replies":true,"parent_id":"t1_n3q6flf","score":1,"author_fullname":"t2_7sgltzo8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Could you share what type of rewriting and in what language?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3r0yi7","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Could you share what type of rewriting and in what language?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m1ylw0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3r0yi7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752807355,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3q6flf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GregoryfromtheHood","can_mod_post":false,"created_utc":1752796292,"send_replies":true,"parent_id":"t3_1m1ylw0","score":1,"author_fullname":"t2_g0qor","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"In my tests and workflows, which specifically do drive rewrites of large pieces of text, Gemma3-27b significantly outperforms Qwen3-32b which cannot follow the instructions well enough a lot of the time.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3q6flf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;In my tests and workflows, which specifically do drive rewrites of large pieces of text, Gemma3-27b significantly outperforms Qwen3-32b which cannot follow the instructions well enough a lot of the time.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3q6flf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752796292,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m1ylw0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ktyea","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok_Cow1976","can_mod_post":false,"created_utc":1752728863,"send_replies":true,"parent_id":"t3_1m1ylw0","score":0,"author_fullname":"t2_3pwbsmdr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thanks for sharing!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ktyea","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for sharing!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1ylw0/my_simple_test_qwen332b_qwen314b_ds_qwen38/n3ktyea/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752728863,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m1ylw0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}}]`),s=()=>e.jsx(l,{data:t});export{s as default};
