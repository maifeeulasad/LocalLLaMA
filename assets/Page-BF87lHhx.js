import{j as e}from"./index-C_z07ZVC.js";import{R as l}from"./RedditPostRenderer-DPnSR41P.js";import"./index-DKzOAewW.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Should you deploy LLMs locally on smartphones?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":55,"top_awarded_type":null,"hide_score":false,"name":"t3_1lpjebh","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.5,"author_flair_background_color":null,"ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_6wtkrxlj","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://external-preview.redd.it/jN8OMK4SXNJMNzn8XJGs2HRNzTx9RdEKl-fqkD8vaOY.jpeg?width=140&amp;height=55&amp;crop=140:55,smart&amp;auto=webp&amp;s=8aad0739be99781f0c85c57714646509c2629ca3","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"link","content_categories":null,"is_self":false,"subreddit_type":"public","created":1751420018,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"medium.com","allow_live_comments":false,"selftext_html":null,"likes":null,"suggested_sort":null,"banned_at_utc":null,"url_overridden_by_dest":"https://medium.com/@ndubuakuhenry/should-you-deploy-llms-locally-on-smartphones-0151f6217fce","view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/jN8OMK4SXNJMNzn8XJGs2HRNzTx9RdEKl-fqkD8vaOY.jpeg?auto=webp&amp;s=8173fa1c747233cb9ade560c4377d446486de232","width":1200,"height":479},"resolutions":[{"url":"https://external-preview.redd.it/jN8OMK4SXNJMNzn8XJGs2HRNzTx9RdEKl-fqkD8vaOY.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=efc1750ab2c7f276e22e81e5c7eaa4ff8121752b","width":108,"height":43},{"url":"https://external-preview.redd.it/jN8OMK4SXNJMNzn8XJGs2HRNzTx9RdEKl-fqkD8vaOY.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3908b31ac59abfb903904f163cc87cb7b9c794af","width":216,"height":86},{"url":"https://external-preview.redd.it/jN8OMK4SXNJMNzn8XJGs2HRNzTx9RdEKl-fqkD8vaOY.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ad8ac265fec0c4c425177d14158a6dee0912691f","width":320,"height":127},{"url":"https://external-preview.redd.it/jN8OMK4SXNJMNzn8XJGs2HRNzTx9RdEKl-fqkD8vaOY.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8b770fe72693afba20eb2e44ac6f8b28f0577b0e","width":640,"height":255},{"url":"https://external-preview.redd.it/jN8OMK4SXNJMNzn8XJGs2HRNzTx9RdEKl-fqkD8vaOY.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d608bd593443ef2d7221ab49e60b4c8a7b55b02f","width":960,"height":383},{"url":"https://external-preview.redd.it/jN8OMK4SXNJMNzn8XJGs2HRNzTx9RdEKl-fqkD8vaOY.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=227600535ee6bce58f0ad471ddbe6a740ff94299","width":1080,"height":431}],"variants":{},"id":"jN8OMK4SXNJMNzn8XJGs2HRNzTx9RdEKl-fqkD8vaOY"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"mod_note":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"num_reports":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lpjebh","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Henrie_the_dreamer","discussion_type":null,"num_comments":8,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lpjebh/should_you_deploy_llms_locally_on_smartphones/","stickied":false,"url":"https://medium.com/@ndubuakuhenry/should-you-deploy-llms-locally-on-smartphones-0151f6217fce","subreddit_subscribers":493457,"created_utc":1751420018,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0vk8v2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"National_Meeting_749","can_mod_post":false,"created_utc":1751424315,"send_replies":true,"parent_id":"t1_n0vcc6w","score":6,"author_fullname":"t2_drm5tg5d","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah, I'll pop in a Qwen 3 4b when it's okay to make mistakes as long as you make them *fast*","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0vk8v2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, I&amp;#39;ll pop in a Qwen 3 4b when it&amp;#39;s okay to make mistakes as long as you make them &lt;em&gt;fast&lt;/em&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpjebh","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpjebh/should_you_deploy_llms_locally_on_smartphones/n0vk8v2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751424315,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0w0y99","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Henrie_the_dreamer","can_mod_post":false,"created_utc":1751431181,"send_replies":true,"parent_id":"t1_n0vcc6w","score":1,"author_fullname":"t2_6wtkrxlj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You make really good points, but here are a few follow ups:\\n\\n1) What makes models like Claude very good is the pre and post processing, as well as workflows built on-top of the model. In the framework, we found this to really make a change and abstracting that away from users.\\n\\n2) Gemma 3n allows mix-and-match, we are working on techniques to smoothly vary the number of parameters to load and use based on the device, hence the efforts in the benchmarking.\\n\\n3) We are working on higher-level APIs like (Classify, Embed, ReRank, etc.), where we autotune mix-and-match to select the safe number of parameters needed to perform each task for each device and phone usage level, to avoid battery drain.\\n\\nWhat do you think? Honestly, I posted this to get as many brutal feedback as possible, keep em coming!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0w0y99","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You make really good points, but here are a few follow ups:&lt;/p&gt;\\n\\n&lt;p&gt;1) What makes models like Claude very good is the pre and post processing, as well as workflows built on-top of the model. In the framework, we found this to really make a change and abstracting that away from users.&lt;/p&gt;\\n\\n&lt;p&gt;2) Gemma 3n allows mix-and-match, we are working on techniques to smoothly vary the number of parameters to load and use based on the device, hence the efforts in the benchmarking.&lt;/p&gt;\\n\\n&lt;p&gt;3) We are working on higher-level APIs like (Classify, Embed, ReRank, etc.), where we autotune mix-and-match to select the safe number of parameters needed to perform each task for each device and phone usage level, to avoid battery drain.&lt;/p&gt;\\n\\n&lt;p&gt;What do you think? Honestly, I posted this to get as many brutal feedback as possible, keep em coming!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpjebh","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpjebh/should_you_deploy_llms_locally_on_smartphones/n0w0y99/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751431181,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0vcc6w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"offlinesir","can_mod_post":false,"created_utc":1751421533,"send_replies":true,"parent_id":"t3_1lpjebh","score":4,"author_fullname":"t2_jn5ft2le","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I would argue in the future, maybe, but not now. Most consumers now don't have a phone that can run a local model. Yes, the iPhone 15 pro and onwards support their local model, along with Gemini nano support on pixel 8a or above or Samsung s24 or above. But that's not the majority of smartphone users. Yes, on other devices that don't support their companies model, you can run an LLM. But it will be slow, generate heat+use battery, and the trade offs won't be worth it.\\n\\nAlso, I don't think local models are really there yet. They can do simple things, but they are more prone to mistakes. Gemma 3n is an example of a path forward, though. I know you show the chatbot arena score of Gemma 3n being so close to Claude, but come on, we both know that Claude is the better model by a long shot. It's not all about the benchmarks.\\n\\nThe biggest issue is that when you develop an app, you should try to develop for everyone. Make sure it runs on every device. The thing is, not every device in user's hands is able to run a local LLM.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0vcc6w","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I would argue in the future, maybe, but not now. Most consumers now don&amp;#39;t have a phone that can run a local model. Yes, the iPhone 15 pro and onwards support their local model, along with Gemini nano support on pixel 8a or above or Samsung s24 or above. But that&amp;#39;s not the majority of smartphone users. Yes, on other devices that don&amp;#39;t support their companies model, you can run an LLM. But it will be slow, generate heat+use battery, and the trade offs won&amp;#39;t be worth it.&lt;/p&gt;\\n\\n&lt;p&gt;Also, I don&amp;#39;t think local models are really there yet. They can do simple things, but they are more prone to mistakes. Gemma 3n is an example of a path forward, though. I know you show the chatbot arena score of Gemma 3n being so close to Claude, but come on, we both know that Claude is the better model by a long shot. It&amp;#39;s not all about the benchmarks.&lt;/p&gt;\\n\\n&lt;p&gt;The biggest issue is that when you develop an app, you should try to develop for everyone. Make sure it runs on every device. The thing is, not every device in user&amp;#39;s hands is able to run a local LLM.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpjebh/should_you_deploy_llms_locally_on_smartphones/n0vcc6w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751421533,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpjebh","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0vkjua","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok_Warning2146","can_mod_post":false,"created_utc":1751424427,"send_replies":true,"parent_id":"t3_1lpjebh","score":3,"author_fullname":"t2_s6sfw4yy","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Can be useful in survival/camping/in-flight situation","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0vkjua","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Can be useful in survival/camping/in-flight situation&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpjebh/should_you_deploy_llms_locally_on_smartphones/n0vkjua/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751424427,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpjebh","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0vdua5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"xoexohexox","can_mod_post":false,"created_utc":1751422048,"send_replies":true,"parent_id":"t3_1lpjebh","score":1,"author_fullname":"t2_323db","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Sure in the winter, warm those hands up","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0vdua5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sure in the winter, warm those hands up&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpjebh/should_you_deploy_llms_locally_on_smartphones/n0vdua5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751422048,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpjebh","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0vy6qa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mtmttuan","can_mod_post":false,"created_utc":1751429921,"send_replies":true,"parent_id":"t3_1lpjebh","score":1,"author_fullname":"t2_6mjqz0at","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Laptop/desktop local deployment isn't even that good, and now we are talking about deployment on phones?\\n\\nI believe that *should* be the *future*, but it's like a few more years at least.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0vy6qa","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Laptop/desktop local deployment isn&amp;#39;t even that good, and now we are talking about deployment on phones?&lt;/p&gt;\\n\\n&lt;p&gt;I believe that &lt;em&gt;should&lt;/em&gt; be the &lt;em&gt;future&lt;/em&gt;, but it&amp;#39;s like a few more years at least.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpjebh/should_you_deploy_llms_locally_on_smartphones/n0vy6qa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751429921,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpjebh","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0vzz09","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ttkciar","can_mod_post":false,"created_utc":1751430722,"send_replies":true,"parent_id":"t3_1lpjebh","score":1,"author_fullname":"t2_cpegz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"That makes no sense to me.  It's better to run one's LLM on a burly server at home, with oodles of memory and amps of wall current, and interface with it from the phone over the network.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0vzz09","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That makes no sense to me.  It&amp;#39;s better to run one&amp;#39;s LLM on a burly server at home, with oodles of memory and amps of wall current, and interface with it from the phone over the network.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpjebh/should_you_deploy_llms_locally_on_smartphones/n0vzz09/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751430722,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lpjebh","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0w3djo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"android369","can_mod_post":false,"created_utc":1751432317,"send_replies":true,"parent_id":"t3_1lpjebh","score":1,"author_fullname":"t2_uqxziybm","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For general coversation yes i would do that, to keep the data private.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0w3djo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For general coversation yes i would do that, to keep the data private.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpjebh/should_you_deploy_llms_locally_on_smartphones/n0w3djo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751432317,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpjebh","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),s=()=>e.jsx(l,{data:a});export{s as default};
