import{j as e}from"./index-xfnGEtuL.js";import{R as t}from"./RedditPostRenderer-DAZRIZZK.js";import"./index-BaNn5-RR.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"**TL;DR: Model performance is non-uniform across context lengths due to \\"Context Rot\\", including state-of-the-art GPT-4.1, Claude 4, Gemini 2.5, and Qwen3 models.**\\n\\nResearch reveals that LLMs (large language models) experience significant performance *\\"degradation\\"* as input context length increases, even on simple tasks. Testing 18 models across various scenarios, including needle-in-haystack retrieval, conversational QA, and text replication, shows that performance drops are non-uniform and model-specific. \\n\\nKey findings include: Lower similarity between questions and answers accelerates degradation, distractors have amplified negative effects at longer contexts, haystack structure matters more than semantic similarity, and even basic text copying becomes unreliable at scale. \\n\\nThe study challenges assumptions about long-context capabilities and emphasizes the importance of context engineering for reliable LLM performance.\\n\\n\\n\\n\\\\[Report\\\\]: [https://research.trychroma.com/context-rot](https://research.trychroma.com/context-rot)  \\n  \\n\\\\[Youtube\\\\]: [https://www.youtube.com/watch?v=TUjQuC4ugak](https://www.youtube.com/watch?v=TUjQuC4ugak)\\n\\n\\\\[Open-source Codebase\\\\]: [https://github.com/chroma-core/context-rot](https://github.com/chroma-core/context-rot)","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Context Rot: How Increasing Input Tokens Impacts LLM Performance","link_flair_richtext":[{"e":"text","t":"News"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":98,"top_awarded_type":null,"hide_score":false,"name":"t3_1m4fs2t","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.89,"author_flair_background_color":null,"ups":241,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_yy8p0c7c1","secure_media":null,"is_reddit_media_domain":true,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"News","can_mod_post":false,"score":241,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://b.thumbs.redditmedia.com/wGem7sWlt1ED82Y7hUTJQ6HH2tjfP5P1zc7zT6yKMuk.jpg","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"image","content_categories":null,"is_self":false,"subreddit_type":"public","created":1752985024,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"i.redd.it","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;strong&gt;TL;DR: Model performance is non-uniform across context lengths due to &amp;quot;Context Rot&amp;quot;, including state-of-the-art GPT-4.1, Claude 4, Gemini 2.5, and Qwen3 models.&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Research reveals that LLMs (large language models) experience significant performance &lt;em&gt;&amp;quot;degradation&amp;quot;&lt;/em&gt; as input context length increases, even on simple tasks. Testing 18 models across various scenarios, including needle-in-haystack retrieval, conversational QA, and text replication, shows that performance drops are non-uniform and model-specific. &lt;/p&gt;\\n\\n&lt;p&gt;Key findings include: Lower similarity between questions and answers accelerates degradation, distractors have amplified negative effects at longer contexts, haystack structure matters more than semantic similarity, and even basic text copying becomes unreliable at scale. &lt;/p&gt;\\n\\n&lt;p&gt;The study challenges assumptions about long-context capabilities and emphasizes the importance of context engineering for reliable LLM performance.&lt;/p&gt;\\n\\n&lt;p&gt;[Report]: &lt;a href=\\"https://research.trychroma.com/context-rot\\"&gt;https://research.trychroma.com/context-rot&lt;/a&gt;  &lt;/p&gt;\\n\\n&lt;p&gt;[Youtube]: &lt;a href=\\"https://www.youtube.com/watch?v=TUjQuC4ugak\\"&gt;https://www.youtube.com/watch?v=TUjQuC4ugak&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;[Open-source Codebase]: &lt;a href=\\"https://github.com/chroma-core/context-rot\\"&gt;https://github.com/chroma-core/context-rot&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"url_overridden_by_dest":"https://i.redd.it/x8dkgvkifydf1.jpeg","view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://preview.redd.it/x8dkgvkifydf1.jpeg?auto=webp&amp;s=57e6704431196e832d50047c69e8f10dd900d161","width":1092,"height":772},"resolutions":[{"url":"https://preview.redd.it/x8dkgvkifydf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ee39ea66e1d0b5d1b3ad61e605a8d4691605af43","width":108,"height":76},{"url":"https://preview.redd.it/x8dkgvkifydf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=87595ecc8d8b55119d8efc2a38feef7aa5fde66c","width":216,"height":152},{"url":"https://preview.redd.it/x8dkgvkifydf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=927a3e4557a49d849c3ac05d42f641c1f03f7732","width":320,"height":226},{"url":"https://preview.redd.it/x8dkgvkifydf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=102a7ef47ffcfc42af3f68c707719a67b3a06693","width":640,"height":452},{"url":"https://preview.redd.it/x8dkgvkifydf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=33eeb66844f42832a4c787326572da82b1cf74bc","width":960,"height":678},{"url":"https://preview.redd.it/x8dkgvkifydf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6fa5529d85f97775c2e8e72d4a8ac1dfa8303ef5","width":1080,"height":763}],"variants":{},"id":"zjBGOwnOK5FNwOOG8xwPRLfKqGJmpMn1rW-t7NovVG8"}],"enabled":true},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"mod_note":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"num_reports":null,"removal_reason":null,"link_flair_background_color":"#cc3600","id":"1m4fs2t","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"5h3r_10ck","discussion_type":null,"num_comments":37,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/","stickied":false,"url":"https://i.redd.it/x8dkgvkifydf1.jpeg","subreddit_subscribers":502516,"created_utc":1752985024,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n44mfob","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"JShelbyJ","can_mod_post":false,"send_replies":true,"parent_id":"t1_n44l5hu","score":35,"author_fullname":"t2_6182hcch","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"They paid money to quantify the effect. It’s a better ad than spamming your inbox.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n44mfob","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They paid money to quantify the effect. It’s a better ad than spamming your inbox.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4fs2t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/n44mfob/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752995215,"author_flair_text":null,"treatment_tags":[],"created_utc":1752995215,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":35}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n45b2iq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No_Afternoon_4260","can_mod_post":false,"send_replies":true,"parent_id":"t1_n44l5hu","score":5,"author_fullname":"t2_cj9kap4bx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Research.trychroma.com lol","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n45b2iq","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Research.trychroma.com lol&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4fs2t","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/n45b2iq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753009410,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1753009410,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4cjqpb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jeffreyhuber","can_mod_post":false,"send_replies":true,"parent_id":"t1_n44l5hu","score":1,"author_fullname":"t2_aua9g","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"ads make money - this was a money pit (one that we were spend for the community)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4cjqpb","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;ads make money - this was a money pit (one that we were spend for the community)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4fs2t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/n4cjqpb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753108786,"author_flair_text":null,"treatment_tags":[],"created_utc":1753108786,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n44l5hu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"and_human","can_mod_post":false,"created_utc":1752994515,"send_replies":true,"parent_id":"t1_n44c1sg","score":49,"author_fullname":"t2_7x9tlie1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It’s an ad.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44l5hu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It’s an ad.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4fs2t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/n44l5hu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752994515,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":49}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n467sx8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"claythearc","can_mod_post":false,"send_replies":true,"parent_id":"t1_n46488p","score":7,"author_fullname":"t2_65rk0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; lack of context makes models dumb \\n\\nThis is true, but there’s a point where it hurts like a bell curve. On SOTA models that seems to be in the 30-40k range, based on benchmarks on the very tiny ones like llama 8b it can be like 1k tokens. \\n\\nThere are arguments that benchmarks don’t necessarily reflect reality but I think needle in the haystack is pretty relevant because data extraction is something a lot of people do like hr chat bots or api doc bots etc. \\n\\nNoLiMa (from adobe) has the best graphs to illustrate it, imo https://github.com/adobe-research/NoLiMa","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n467sx8","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;lack of context makes models dumb &lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;This is true, but there’s a point where it hurts like a bell curve. On SOTA models that seems to be in the 30-40k range, based on benchmarks on the very tiny ones like llama 8b it can be like 1k tokens. &lt;/p&gt;\\n\\n&lt;p&gt;There are arguments that benchmarks don’t necessarily reflect reality but I think needle in the haystack is pretty relevant because data extraction is something a lot of people do like hr chat bots or api doc bots etc. &lt;/p&gt;\\n\\n&lt;p&gt;NoLiMa (from adobe) has the best graphs to illustrate it, imo &lt;a href=\\"https://github.com/adobe-research/NoLiMa\\"&gt;https://github.com/adobe-research/NoLiMa&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4fs2t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/n467sx8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753022374,"author_flair_text":null,"treatment_tags":[],"created_utc":1753022374,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}}],"before":null}},"user_reports":[],"saved":false,"id":"n46488p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"BFGsuno","can_mod_post":false,"created_utc":1753021201,"send_replies":true,"parent_id":"t1_n44c1sg","score":-7,"author_fullname":"t2_1pu42hfw1u","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; I feel like this has been known for years at this point\\n\\nNo, it was the other ways around. Lack of context would make model dumb.\\n\\nAnd imho i question this research. I am prompting since 2022 and context ALWAYS improve generated outputs because it focuses model on specific tasks.\\n\\nEvery time I see study like this I always think of 70% statistic when it comes to papers published. Aka 70% of papers are bogus and can't be repeated.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n46488p","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;I feel like this has been known for years at this point&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;No, it was the other ways around. Lack of context would make model dumb.&lt;/p&gt;\\n\\n&lt;p&gt;And imho i question this research. I am prompting since 2022 and context ALWAYS improve generated outputs because it focuses model on specific tasks.&lt;/p&gt;\\n\\n&lt;p&gt;Every time I see study like this I always think of 70% statistic when it comes to papers published. Aka 70% of papers are bogus and can&amp;#39;t be repeated.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4fs2t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/n46488p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753021201,"author_flair_text":null,"treatment_tags":[],"collapsed":true,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-7}}],"before":null}},"user_reports":[],"saved":false,"id":"n44c1sg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"claythearc","can_mod_post":false,"created_utc":1752989685,"send_replies":true,"parent_id":"t3_1m4fs2t","score":148,"author_fullname":"t2_65rk0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I feel like this has been known for years at this point - between benchmarks like NoLima, LV-Eval, and long bench it’s been pretty well documented - especially on the micro models we self host here their usable context can be like 10k or less tokens despite a 128k “limit”","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44c1sg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I feel like this has been known for years at this point - between benchmarks like NoLima, LV-Eval, and long bench it’s been pretty well documented - especially on the micro models we self host here their usable context can be like 10k or less tokens despite a 128k “limit”&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/n44c1sg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752989685,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4fs2t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":148}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n44mjzy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"SnooStories2143","can_mod_post":false,"created_utc":1752995282,"send_replies":true,"parent_id":"t1_n44iqyi","score":12,"author_fullname":"t2_40hv58f0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"https://arxiv.org/abs/2402.14848","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44mjzy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://arxiv.org/abs/2402.14848\\"&gt;https://arxiv.org/abs/2402.14848&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4fs2t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/n44mjzy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752995282,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}}],"before":null}},"user_reports":[],"saved":false,"id":"n44iqyi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Fun-Purple-7737","can_mod_post":false,"created_utc":1752993205,"send_replies":true,"parent_id":"t3_1m4fs2t","score":44,"author_fullname":"t2_8lm8q1xe","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"cool. but this is known for years already [https://arxiv.org/abs/2404.02060](https://arxiv.org/abs/2404.02060), [https://arxiv.org/abs/2502.05167](https://arxiv.org/abs/2502.05167)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44iqyi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;cool. but this is known for years already &lt;a href=\\"https://arxiv.org/abs/2404.02060\\"&gt;https://arxiv.org/abs/2404.02060&lt;/a&gt;, &lt;a href=\\"https://arxiv.org/abs/2502.05167\\"&gt;https://arxiv.org/abs/2502.05167&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/n44iqyi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752993205,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4fs2t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":44}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n45r28h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AppealSame4367","can_mod_post":false,"created_utc":1753016519,"send_replies":true,"parent_id":"t1_n44hlbw","score":11,"author_fullname":"t2_sxud8ccv4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The root problem is math: Exponentially more connections or even more than exponential the more interconnected data you have.\\n\\nMight be solvable with smart approximations for now. Or quantum computing later on (superposition?, quantum entanglement? no clue honestly)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n45r28h","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The root problem is math: Exponentially more connections or even more than exponential the more interconnected data you have.&lt;/p&gt;\\n\\n&lt;p&gt;Might be solvable with smart approximations for now. Or quantum computing later on (superposition?, quantum entanglement? no clue honestly)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4fs2t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/n45r28h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753016519,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}}],"before":null}},"user_reports":[],"saved":false,"id":"n44hlbw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"masc98","can_mod_post":false,"created_utc":1752992575,"send_replies":true,"parent_id":"t3_1m4fs2t","score":31,"author_fullname":"t2_12nt66","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"the root problem, taking aside architectural limits, is data mixture. the fact that 90% of documents are in the 2k tokens length, would explain the rot behaviour. language modeling is not magic ffs, if u have an out of distribution input, the model is gonna underperform. simple as that.\\n\\nnowadays with commercial llms the sweet spot is still around ~30k tokens. over that, I start a new chat. at least from my tests.\\n\\nif we re talking about doc embeddings, then there s no way you can compress a 100k tokens doc in one 3072 feature vector. not today, 2025-07. and this is not about context rot. this is about compression/expressability ratio","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44hlbw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;the root problem, taking aside architectural limits, is data mixture. the fact that 90% of documents are in the 2k tokens length, would explain the rot behaviour. language modeling is not magic ffs, if u have an out of distribution input, the model is gonna underperform. simple as that.&lt;/p&gt;\\n\\n&lt;p&gt;nowadays with commercial llms the sweet spot is still around ~30k tokens. over that, I start a new chat. at least from my tests.&lt;/p&gt;\\n\\n&lt;p&gt;if we re talking about doc embeddings, then there s no way you can compress a 100k tokens doc in one 3072 feature vector. not today, 2025-07. and this is not about context rot. this is about compression/expressability ratio&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/n44hlbw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752992575,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4fs2t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":31}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4g2rif","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Beautiful-Essay1945","can_mod_post":false,"send_replies":true,"parent_id":"t1_n448yqe","score":1,"author_fullname":"t2_14bv8c06dm","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I have tried, but it's not enough. To complete the task, I simply switched models, ensuring enough information from the previous part was carried over to complete the larger objective. \\n\\nIt's like I've created small, specialized employees, each picking up from where the previous model left off. \\n\\nHowever, I can foresee this happening with the latest AI agent developments from OpenAI, where they will soon manage context size more effectively. \\n\\nCurrently, models aren't yet equipped with the ability to effectively utilize other models and act as a 'boss' overseeing them.\\n\\nI'm not a tech guy, to be honest, I come from a commerce background. so its hard for me make something good enough to show to this community","edited":false,"author_flair_css_class":null,"name":"t1_n4g2rif","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have tried, but it&amp;#39;s not enough. To complete the task, I simply switched models, ensuring enough information from the previous part was carried over to complete the larger objective. &lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s like I&amp;#39;ve created small, specialized employees, each picking up from where the previous model left off. &lt;/p&gt;\\n\\n&lt;p&gt;However, I can foresee this happening with the latest AI agent developments from OpenAI, where they will soon manage context size more effectively. &lt;/p&gt;\\n\\n&lt;p&gt;Currently, models aren&amp;#39;t yet equipped with the ability to effectively utilize other models and act as a &amp;#39;boss&amp;#39; overseeing them.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m not a tech guy, to be honest, I come from a commerce background. so its hard for me make something good enough to show to this community&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m4fs2t","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/n4g2rif/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753146693,"author_flair_text":null,"collapsed":false,"created_utc":1753146693,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n448yqe","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"simracerman","can_mod_post":false,"send_replies":true,"parent_id":"t1_n447syk","score":3,"author_fullname":"t2_vbzgnic","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Wow! We’d be grateful to have that done locally if you can.\\n\\nMake a post when you have something to test.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n448yqe","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Wow! We’d be grateful to have that done locally if you can.&lt;/p&gt;\\n\\n&lt;p&gt;Make a post when you have something to test.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4fs2t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/n448yqe/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752988170,"author_flair_text":null,"treatment_tags":[],"created_utc":1752988170,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n447syk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Beautiful-Essay1945","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4473bp","score":8,"author_fullname":"t2_14bv8c06dm","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"this is possible, I can somewhere achieve this with mcps  like memory and sequential thinking and few more... with a good prompt\\n\\nMore like the grok 4 heavy was doing! with multiple agents...\\n\\nThat's a good suggestion, let me give a shot","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n447syk","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;this is possible, I can somewhere achieve this with mcps  like memory and sequential thinking and few more... with a good prompt&lt;/p&gt;\\n\\n&lt;p&gt;More like the grok 4 heavy was doing! with multiple agents...&lt;/p&gt;\\n\\n&lt;p&gt;That&amp;#39;s a good suggestion, let me give a shot&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4fs2t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/n447syk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752987594,"author_flair_text":null,"treatment_tags":[],"created_utc":1752987594,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}}],"before":null}},"user_reports":[],"saved":false,"id":"n4473bp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"simracerman","can_mod_post":false,"created_utc":1752987258,"send_replies":true,"parent_id":"t1_n445kwb","score":22,"author_fullname":"t2_vbzgnic","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The lowest size for the task. With each task you get to decide when the quality degrades, then you back off.\\n\\nUntil we figure out how to run agents that monitor the LLMs output like a supervisor and dynamically run multiple short iterations on the same prompt before producing the final response, we won’t have a sweet spot.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4473bp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The lowest size for the task. With each task you get to decide when the quality degrades, then you back off.&lt;/p&gt;\\n\\n&lt;p&gt;Until we figure out how to run agents that monitor the LLMs output like a supervisor and dynamically run multiple short iterations on the same prompt before producing the final response, we won’t have a sweet spot.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4fs2t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/n4473bp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752987258,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":22}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n46i1lk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Willdudes","can_mod_post":false,"send_replies":true,"parent_id":"t1_n447r19","score":3,"author_fullname":"t2_14v7k3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The model determines a lot, it is why I like https://fiction.live/stories/Fiction-liveBench-Feb-21-2025/oQdzQvKHw8JyXbN87\\n\\nIt shows you how quickly some models drop off.\\n\\nThe best you can do is build evaluation for the specific tasks with different contexts lengths and do a large number of runs to see where your drop-off is regarding context.  ","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n46i1lk","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The model determines a lot, it is why I like &lt;a href=\\"https://fiction.live/stories/Fiction-liveBench-Feb-21-2025/oQdzQvKHw8JyXbN87\\"&gt;https://fiction.live/stories/Fiction-liveBench-Feb-21-2025/oQdzQvKHw8JyXbN87&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;It shows you how quickly some models drop off.&lt;/p&gt;\\n\\n&lt;p&gt;The best you can do is build evaluation for the specific tasks with different contexts lengths and do a large number of runs to see where your drop-off is regarding context.  &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4fs2t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/n46i1lk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753025589,"author_flair_text":null,"treatment_tags":[],"created_utc":1753025589,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n447r19","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"5h3r_10ck","can_mod_post":false,"created_utc":1752987569,"send_replies":true,"parent_id":"t1_n445kwb","score":2,"author_fullname":"t2_yy8p0c7c1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Umm, I don't think there is a single *\\"sweet spot\\"* context length that applies universally. The report says that it’s highly dependent on your **(a) specific task, (b) the model in use, and (c) the nature of your input**.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n447r19","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Umm, I don&amp;#39;t think there is a single &lt;em&gt;&amp;quot;sweet spot&amp;quot;&lt;/em&gt; context length that applies universally. The report says that it’s highly dependent on your &lt;strong&gt;(a) specific task, (b) the model in use, and (c) the nature of your input&lt;/strong&gt;.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4fs2t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/n447r19/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752987569,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n45qcyp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"g15mouse","can_mod_post":false,"created_utc":1753016249,"send_replies":true,"parent_id":"t1_n445kwb","score":1,"author_fullname":"t2_l2p5p","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"12","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n45qcyp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;12&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4fs2t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/n45qcyp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753016249,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n44cbz6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Yes_but_I_think","can_mod_post":false,"send_replies":true,"parent_id":"t1_n446run","score":-1,"author_fullname":"t2_rea1qh6m","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Not advertisement if true.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n44cbz6","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not advertisement if true.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4fs2t","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/n44cbz6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752989827,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752989827,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n446run","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"this-just_in","can_mod_post":false,"created_utc":1752987105,"send_replies":true,"parent_id":"t1_n445kwb","score":-2,"author_fullname":"t2_kdmu4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Chroma.\\n\\nI jest, but clearly the undertone here is that there are all sorts of performance degradation in the real world with long context (context stuffing) such as distractors, model limitations, etc.  So I would guess the authors believe Chroma, a vector database often used for RAG, would be a great way to reduce that context length, stuffing only important tokens, negating the problems you would see otherwise.\\n\\nI would have been interested to see their experiment augmented with RAG using Chroma.  I would read the follow up.","edited":1752987633,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n446run","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Chroma.&lt;/p&gt;\\n\\n&lt;p&gt;I jest, but clearly the undertone here is that there are all sorts of performance degradation in the real world with long context (context stuffing) such as distractors, model limitations, etc.  So I would guess the authors believe Chroma, a vector database often used for RAG, would be a great way to reduce that context length, stuffing only important tokens, negating the problems you would see otherwise.&lt;/p&gt;\\n\\n&lt;p&gt;I would have been interested to see their experiment augmented with RAG using Chroma.  I would read the follow up.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4fs2t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/n446run/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752987105,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n44a7xm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DorphinPack","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4470jg","score":7,"author_fullname":"t2_zebuyjw9s","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It’s model and problem dependent","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n44a7xm","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It’s model and problem dependent&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4fs2t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/n44a7xm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752988783,"author_flair_text":null,"treatment_tags":[],"created_utc":1752988783,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}}],"before":null}},"user_reports":[],"saved":false,"id":"n4470jg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ThinkExtension2328","can_mod_post":false,"created_utc":1752987222,"send_replies":true,"parent_id":"t1_n445kwb","score":-3,"author_fullname":"t2_8eneodlk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"8100 for local stuff iv noticed , but it depends. Its all a wild balancing act.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4470jg","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;8100 for local stuff iv noticed , but it depends. Its all a wild balancing act.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4fs2t","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/n4470jg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752987222,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-3}}],"before":null}},"user_reports":[],"saved":false,"id":"n445kwb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Beautiful-Essay1945","can_mod_post":false,"created_utc":1752986545,"send_replies":true,"parent_id":"t3_1m4fs2t","score":13,"author_fullname":"t2_14bv8c06dm","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"what's the sweet spot then?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n445kwb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;what&amp;#39;s the sweet spot then?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/n445kwb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752986545,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4fs2t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":13}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n452e4g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"karaposu","can_mod_post":false,"created_utc":1753004494,"send_replies":true,"parent_id":"t1_n44hbg8","score":17,"author_fullname":"t2_qzy7otr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"fading attention is better","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n452e4g","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;fading attention is better&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4fs2t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/n452e4g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753004494,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":17}}],"before":null}},"user_reports":[],"saved":false,"id":"n44hbg8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AbyssianOne","can_mod_post":false,"created_utc":1752992427,"send_replies":true,"parent_id":"t3_1m4fs2t","score":30,"author_fullname":"t2_1651c3kskq","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"\\"Context rot\\" sensationalized name for finite attention.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44hbg8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&amp;quot;Context rot&amp;quot; sensationalized name for finite attention.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/n44hbg8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752992427,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4fs2t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":30}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n451rqu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Final_Wheel_7486","can_mod_post":false,"created_utc":1753004123,"send_replies":true,"parent_id":"t3_1m4fs2t","score":5,"author_fullname":"t2_cyrs5dhp","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This is just my two cents, so take it with a grain of salt, but I could imagine the following:\\n\\n\\nDuring training, after the model has learned how to complete text and how to predict the most probable next tokens (pretreating), instruction fine-tuning is done.\\n\\n\\nI believe that, maybe, the datasets used by huge companies or even those available on Hugging Face for instruction fine-tuning are simply not diverse enough in terms of context length in order to properly tell these models how to handle said long context.\\n\\n\\nLooking at the Alpaca dataset for example, one can see that most example conversations are only pretty short and they will never really satisfy the context length of the model. Thus, I could imagine that the model never really knows how to diversify and how to handle very long context.\\n\\n\\nThis is further amplified due to the fact that there are probably way more short conversations in such instruction fine-tune datasets than really long conversations - but there should be a uniform number of both of those in order to prevent this behavior.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n451rqu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is just my two cents, so take it with a grain of salt, but I could imagine the following:&lt;/p&gt;\\n\\n&lt;p&gt;During training, after the model has learned how to complete text and how to predict the most probable next tokens (pretreating), instruction fine-tuning is done.&lt;/p&gt;\\n\\n&lt;p&gt;I believe that, maybe, the datasets used by huge companies or even those available on Hugging Face for instruction fine-tuning are simply not diverse enough in terms of context length in order to properly tell these models how to handle said long context.&lt;/p&gt;\\n\\n&lt;p&gt;Looking at the Alpaca dataset for example, one can see that most example conversations are only pretty short and they will never really satisfy the context length of the model. Thus, I could imagine that the model never really knows how to diversify and how to handle very long context.&lt;/p&gt;\\n\\n&lt;p&gt;This is further amplified due to the fact that there are probably way more short conversations in such instruction fine-tune datasets than really long conversations - but there should be a uniform number of both of those in order to prevent this behavior.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/n451rqu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753004123,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4fs2t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"50c36eba-fdca-11ee-9735-92a88d7e3b87","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n44xruh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"besmin","can_mod_post":false,"created_utc":1753001722,"send_replies":true,"parent_id":"t3_1m4fs2t","score":3,"author_fullname":"t2_zcfv0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Remember those long system prompts that were supposed to help guide the model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44xruh","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Ollama"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Remember those long system prompts that were supposed to help guide the model.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/n44xruh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753001722,"author_flair_text":"Ollama","treatment_tags":[],"link_id":"t3_1m4fs2t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n469h4l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ParaboloidalCrest","can_mod_post":false,"created_utc":1753022913,"send_replies":true,"parent_id":"t3_1m4fs2t","score":2,"author_fullname":"t2_nc2u4f7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"As a Reasonably Intelligent Human Agent I can hardly hold a ten-digit telephone number in my context window before writing it down.","edited":1753023183,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n469h4l","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;As a Reasonably Intelligent Human Agent I can hardly hold a ten-digit telephone number in my context window before writing it down.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/n469h4l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753022913,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4fs2t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4b5swh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Mart-McUH","can_mod_post":false,"created_utc":1753088307,"send_replies":true,"parent_id":"t1_n45n14q","score":1,"author_fullname":"t2_q3eqbw2b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"No, it is not just about rubbish input. The longer the text, the harder to understand (also for humans) and inconsistencies generated by LLM then happen very fast (contradicting what happened before).\\n\\nDoes not mean you can't have efficient 300k token, depends on task (eg needle in haystack usually works).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4b5swh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No, it is not just about rubbish input. The longer the text, the harder to understand (also for humans) and inconsistencies generated by LLM then happen very fast (contradicting what happened before).&lt;/p&gt;\\n\\n&lt;p&gt;Does not mean you can&amp;#39;t have efficient 300k token, depends on task (eg needle in haystack usually works).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4fs2t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/n4b5swh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753088307,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n45n14q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Robert__Sinclair","can_mod_post":false,"created_utc":1753014940,"send_replies":true,"parent_id":"t3_1m4fs2t","score":3,"author_fullname":"t2_120m02qa8a","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"this is true only if you chat with the model or if you add \\"rubbish\\" to the context. I had successful prompts of OVER 300K tokens! It depends on how the context is organized and the quality of the content, **not** the size.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n45n14q","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;this is true only if you chat with the model or if you add &amp;quot;rubbish&amp;quot; to the context. I had successful prompts of OVER 300K tokens! It depends on how the context is organized and the quality of the content, &lt;strong&gt;not&lt;/strong&gt; the size.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/n45n14q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753014940,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4fs2t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ax03y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Aphid_red","can_mod_post":false,"created_utc":1753083180,"send_replies":true,"parent_id":"t3_1m4fs2t","score":1,"author_fullname":"t2_csn2q","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The question I have is not whether there's only limited long-term capability, but whether 'having more context' also impacts the model's performance on the most recent context. After all, with more input, the 'answer' is also just plainly more difficult to get right. For humans, the performance falls off too. \\n\\nDoes a model which has a 50K input perform markedly worse on tasks about the last 2K than one that just got the relevant 2K input?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ax03y","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The question I have is not whether there&amp;#39;s only limited long-term capability, but whether &amp;#39;having more context&amp;#39; also impacts the model&amp;#39;s performance on the most recent context. After all, with more input, the &amp;#39;answer&amp;#39; is also just plainly more difficult to get right. For humans, the performance falls off too. &lt;/p&gt;\\n\\n&lt;p&gt;Does a model which has a 50K input perform markedly worse on tasks about the last 2K than one that just got the relevant 2K input?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/n4ax03y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753083180,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4fs2t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n45car5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1753010046,"send_replies":true,"parent_id":"t3_1m4fs2t","score":1,"author_fullname":"t2_uz37qfx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"read the paper, it is interesting. Especially interesting is the task of having like a sequence of 100 \\"apple\\" words, with one word replaced with \\"apples\\". A simple request to copy verbatim the sequence already causes errors. What is interesting, Gemini 2.5 pro is performing worst compared to the other models.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n45car5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;read the paper, it is interesting. Especially interesting is the task of having like a sequence of 100 &amp;quot;apple&amp;quot; words, with one word replaced with &amp;quot;apples&amp;quot;. A simple request to copy verbatim the sequence already causes errors. What is interesting, Gemini 2.5 pro is performing worst compared to the other models.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/n45car5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753010046,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4fs2t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n45si0v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"evilbarron2","can_mod_post":false,"created_utc":1753017061,"send_replies":true,"parent_id":"t3_1m4fs2t","score":1,"author_fullname":"t2_gr2fr79s1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"There seem to be a lot of amateurs dismissing this as “someone already said this before”, which they appear to believe somehow negates this issue? I don’t understand that take, seems stupid.\\n\\nMore relevant: prompts from chat interfaces - and presumably IDEs like Copilot or Cursor - inject a bunch of stuff into prompts like tool definitions, chat history, RAG context, Internal instructions, metadata, and who knows what else. If LLMs are this sensitive to inputs, all this additional content *must* be impacting responses, right?\\n\\nIf we have an NLP system that requires highly structured inputs for optimal functioning, do we really have an NLP system?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n45si0v","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There seem to be a lot of amateurs dismissing this as “someone already said this before”, which they appear to believe somehow negates this issue? I don’t understand that take, seems stupid.&lt;/p&gt;\\n\\n&lt;p&gt;More relevant: prompts from chat interfaces - and presumably IDEs like Copilot or Cursor - inject a bunch of stuff into prompts like tool definitions, chat history, RAG context, Internal instructions, metadata, and who knows what else. If LLMs are this sensitive to inputs, all this additional content &lt;em&gt;must&lt;/em&gt; be impacting responses, right?&lt;/p&gt;\\n\\n&lt;p&gt;If we have an NLP system that requires highly structured inputs for optimal functioning, do we really have an NLP system?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/n45si0v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753017061,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4fs2t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n49j343","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Vancha","can_mod_post":false,"created_utc":1753060692,"send_replies":true,"parent_id":"t1_n45dgfo","score":1,"author_fullname":"t2_5htqe","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"[https://www.youtube.com/watch?v=Yqz3h6RF_7I](https://www.youtube.com/watch?v=Yqz3h6RF_7I)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n49j343","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://www.youtube.com/watch?v=Yqz3h6RF_7I\\"&gt;https://www.youtube.com/watch?v=Yqz3h6RF_7I&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4fs2t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/n49j343/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753060692,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n45dgfo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Significantik","can_mod_post":false,"created_utc":1753010632,"send_replies":true,"parent_id":"t3_1m4fs2t","score":0,"author_fullname":"t2_bivsfri8w","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Rot? To undergo biological decay?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n45dgfo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Rot? To undergo biological decay?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/n45dgfo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753010632,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4fs2t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n46kq5o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"VoidAlchemy","can_mod_post":false,"created_utc":1753026398,"send_replies":true,"parent_id":"t3_1m4fs2t","score":0,"author_fullname":"t2_n321yfw5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yeah just because the model says it supports 128k it doesn't mean you should try to use it all. It cracks me up seeing people vibe coding with a 15k system prompt not including their actual code💀","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n46kq5o","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah just because the model says it supports 128k it doesn&amp;#39;t mean you should try to use it all. It cracks me up seeing people vibe coding with a 15k system prompt not including their actual code💀&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/n46kq5o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753026398,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m4fs2t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n45qatz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppealSame4367","can_mod_post":false,"created_utc":1753016226,"send_replies":true,"parent_id":"t3_1m4fs2t","score":0,"author_fullname":"t2_sxud8ccv4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Much context, too much compute, data get fuzzy. Wow\\n\\nI love it when i can skip reading and watching something","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n45qatz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Much context, too much compute, data get fuzzy. Wow&lt;/p&gt;\\n\\n&lt;p&gt;I love it when i can skip reading and watching something&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4fs2t/context_rot_how_increasing_input_tokens_impacts/n45qatz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753016226,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4fs2t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}}]`),n=()=>e.jsx(t,{data:l});export{n as default};
