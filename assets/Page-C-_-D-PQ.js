import{j as e}from"./index-BOnf-UhU.js";import{R as t}from"./RedditPostRenderer-Ce39qICS.js";import"./index-CDase6VD.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"When dealing with PDFs that have complicated layouts, like multi-level subheadings, multi-column formats, or tables that stretch across pages, I've found that just extracting the content cleanly is half the battle. Lately, I’ve been using OCRFlux at the front of the pipeline. Most of what I work with are academic papers and technical documents, which are rarely straightforward. The layouts are dense: nested headings, columns, tables broken across pages, all the usual suspects. OCRFlux can stitch paragraphs and tables back together well and keep the overall reading flow intact.\\n\\n\\n\\nMy pipeline goes something like this: PDF in → OCRFlux processes and outputs either plain text or structured JSON → a quick cleanup step (fixing breaks, filtering noise) → chunk and pass to the local LLM, sometimes with retrieval if the document is long.\\n\\n\\n\\nOne thing I’m still figuring out is the best way to handle tables. Sometimes raw text is fine, but in more complex cases I’ve had better luck converting them to markdown or reformatting as Q&amp;A pairs. There’s also a bit of a balance between cleaning things up and preserving enough layout cues for the model to stay grounded.\\n\\n\\n\\nCurious to hear how others are approaching this. Do you preprocess layouts explicitly? Are you running OCR and the LLM in separate steps or bundling them together? And what’s been working (or not) for you when it comes to PDFs that don’t play nice with linear reading order?\\n\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"How to combine local OCR with LLM for document Q&amp;A?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m279pe","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.88,"author_flair_background_color":null,"subreddit_type":"public","ups":12,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1gtbynfrne","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":12,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752758148,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;When dealing with PDFs that have complicated layouts, like multi-level subheadings, multi-column formats, or tables that stretch across pages, I&amp;#39;ve found that just extracting the content cleanly is half the battle. Lately, I’ve been using OCRFlux at the front of the pipeline. Most of what I work with are academic papers and technical documents, which are rarely straightforward. The layouts are dense: nested headings, columns, tables broken across pages, all the usual suspects. OCRFlux can stitch paragraphs and tables back together well and keep the overall reading flow intact.&lt;/p&gt;\\n\\n&lt;p&gt;My pipeline goes something like this: PDF in → OCRFlux processes and outputs either plain text or structured JSON → a quick cleanup step (fixing breaks, filtering noise) → chunk and pass to the local LLM, sometimes with retrieval if the document is long.&lt;/p&gt;\\n\\n&lt;p&gt;One thing I’m still figuring out is the best way to handle tables. Sometimes raw text is fine, but in more complex cases I’ve had better luck converting them to markdown or reformatting as Q&amp;amp;A pairs. There’s also a bit of a balance between cleaning things up and preserving enough layout cues for the model to stay grounded.&lt;/p&gt;\\n\\n&lt;p&gt;Curious to hear how others are approaching this. Do you preprocess layouts explicitly? Are you running OCR and the LLM in separate steps or bundling them together? And what’s been working (or not) for you when it comes to PDFs that don’t play nice with linear reading order?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1m279pe","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Feisty-Jury-7011","discussion_type":null,"num_comments":3,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m279pe/how_to_combine_local_ocr_with_llm_for_document_qa/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m279pe/how_to_combine_local_ocr_with_llm_for_document_qa/","subreddit_subscribers":500897,"created_utc":1752758148,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3mnwer","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"nerdlord420","can_mod_post":false,"created_utc":1752759502,"send_replies":true,"parent_id":"t3_1m279pe","score":2,"author_fullname":"t2_a6s61","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"We've had most luck with RolmOCR, we just pass extracted pages from PDFs in base64 format to vLLM and it gives us pretty accurate markdown formatted text. Then you can put them in whatever you'd like for document Q&amp;A. We like LightRAG but I think OpenWebUI recently made some RAG improvements.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3mnwer","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;We&amp;#39;ve had most luck with RolmOCR, we just pass extracted pages from PDFs in base64 format to vLLM and it gives us pretty accurate markdown formatted text. Then you can put them in whatever you&amp;#39;d like for document Q&amp;amp;A. We like LightRAG but I think OpenWebUI recently made some RAG improvements.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m279pe/how_to_combine_local_ocr_with_llm_for_document_qa/n3mnwer/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752759502,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m279pe","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3n9vmx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"caetydid","can_mod_post":false,"created_utc":1752765807,"send_replies":true,"parent_id":"t3_1m279pe","score":1,"author_fullname":"t2_2b6dk0nt","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I use Mistral Small 3.2 to extract the structure and content of medical documents given multipage scans in PDF files. You can get JSON output with a python script, a single rtx 3090/4090/5090 and one instance of llm-server.\\n\\nThe secret ingredient is in the prompting where you\\"ll have to define the output format, and specify the expected input. I have tested to combine traditional OCR with vision LLMs but I havent seen any benefit.\\n\\nNB: Mistral Small accepts up to ten consecutive pages, but I stick to single page processing. Processing in overlaps could be useful for you to identify elements spread across page boundaries. I still have to see another LLM or combined approach to match the accuracy of Mistral for my particular task.","edited":1752766034,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3n9vmx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I use Mistral Small 3.2 to extract the structure and content of medical documents given multipage scans in PDF files. You can get JSON output with a python script, a single rtx 3090/4090/5090 and one instance of llm-server.&lt;/p&gt;\\n\\n&lt;p&gt;The secret ingredient is in the prompting where you&amp;quot;ll have to define the output format, and specify the expected input. I have tested to combine traditional OCR with vision LLMs but I havent seen any benefit.&lt;/p&gt;\\n\\n&lt;p&gt;NB: Mistral Small accepts up to ten consecutive pages, but I stick to single page processing. Processing in overlaps could be useful for you to identify elements spread across page boundaries. I still have to see another LLM or combined approach to match the accuracy of Mistral for my particular task.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m279pe/how_to_combine_local_ocr_with_llm_for_document_qa/n3n9vmx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752765807,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m279pe","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3qeyb1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Available_Driver6406","can_mod_post":false,"created_utc":1752799311,"send_replies":true,"parent_id":"t3_1m279pe","score":1,"author_fullname":"t2_mxdkomgg","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Try this:\\n\\nhttps://github.com/docling-project/docling\\n\\nhttps://huggingface.co/ds4sd/SmolDocling-256M-preview","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3qeyb1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Try this:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/docling-project/docling\\"&gt;https://github.com/docling-project/docling&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/ds4sd/SmolDocling-256M-preview\\"&gt;https://huggingface.co/ds4sd/SmolDocling-256M-preview&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m279pe/how_to_combine_local_ocr_with_llm_for_document_qa/n3qeyb1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752799311,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m279pe","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),s=()=>e.jsx(t,{data:l});export{s as default};
