import{j as e}from"./index-BpC9hjVs.js";import{R as l}from"./RedditPostRenderer-BEo6AnSR.js";import"./index-DwkJHX1_.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Interesting finding. SOTA throughputs for Coder LLMs, 10x speed up over frontier models.\\n\\nPlayground: [https://chat.inceptionlabs.ai/](https://chat.inceptionlabs.ai/)\\n\\nAPI: [https://platform.inceptionlabs.ai/](https://platform.inceptionlabs.ai/)\\n\\nPaper says:\\n\\nWe present Mercury, a new generation of commercial-scale large language models (LLMs) based on diffusion. In this report, we detail Mercury Coder, our first set of diffusion LLMs designed for coding applications. Currently, Mercury Coder comes in two sizes: Mini and Small. These models set a new state-of-the-art on the speed-quality frontier. Based on independent evaluations conducted by Artificial Analysis, Mercury Coder Mini and Mercury Coder Small achieve state-of-the-art throughputs of 1109 tokens/sec and 737 tokens/sec, respectively, on NVIDIA H100 GPUs and outperform speed-optimized frontier models by up to 10x on average while maintaining comparable quality. We discuss additional results on a variety of code benchmarks spanning multiple languages and use-cases as well as real-world validation by developers on Copilot Arena, where the model currently ranks second on quality and is the fastest model overall. We also release a public API atÂ [this https URL](https://platform.inceptionlabs.ai/)Â and free playground atÂ [this https URL](https://chat.inceptionlabs.ai/)\\n\\n[https://arxiv.org/abs/2506.17298](https://arxiv.org/abs/2506.17298)","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Mercury: Ultra-Fast Language Models Based on Diffusion","link_flair_richtext":[{"e":"text","t":"Resources"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lueziv","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.53,"author_flair_background_color":null,"subreddit_type":"public","ups":1,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1qc1ybvg79","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Resources","can_mod_post":false,"score":1,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751947104,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Interesting finding. SOTA throughputs for Coder LLMs, 10x speed up over frontier models.&lt;/p&gt;\\n\\n&lt;p&gt;Playground: &lt;a href=\\"https://chat.inceptionlabs.ai/\\"&gt;https://chat.inceptionlabs.ai/&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;API: &lt;a href=\\"https://platform.inceptionlabs.ai/\\"&gt;https://platform.inceptionlabs.ai/&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Paper says:&lt;/p&gt;\\n\\n&lt;p&gt;We present Mercury, a new generation of commercial-scale large language models (LLMs) based on diffusion. In this report, we detail Mercury Coder, our first set of diffusion LLMs designed for coding applications. Currently, Mercury Coder comes in two sizes: Mini and Small. These models set a new state-of-the-art on the speed-quality frontier. Based on independent evaluations conducted by Artificial Analysis, Mercury Coder Mini and Mercury Coder Small achieve state-of-the-art throughputs of 1109 tokens/sec and 737 tokens/sec, respectively, on NVIDIA H100 GPUs and outperform speed-optimized frontier models by up to 10x on average while maintaining comparable quality. We discuss additional results on a variety of code benchmarks spanning multiple languages and use-cases as well as real-world validation by developers on Copilot Arena, where the model currently ranks second on quality and is the fastest model overall. We also release a public API atÂ &lt;a href=\\"https://platform.inceptionlabs.ai/\\"&gt;this https URL&lt;/a&gt;Â and free playground atÂ &lt;a href=\\"https://chat.inceptionlabs.ai/\\"&gt;this https URL&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://arxiv.org/abs/2506.17298\\"&gt;https://arxiv.org/abs/2506.17298&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#ccac2b","id":"1lueziv","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Happy_Percentage_384","discussion_type":null,"num_comments":7,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lueziv/mercury_ultrafast_language_models_based_on/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lueziv/mercury_ultrafast_language_models_based_on/","subreddit_subscribers":496034,"created_utc":1751947104,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1xi8id","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Kooshi_Govno","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1xi2o7","score":4,"author_fullname":"t2_7kg5p","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"diffusion is really promising for speed. I can't wait to see what comes out over the next couple years","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1xi8id","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;diffusion is really promising for speed. I can&amp;#39;t wait to see what comes out over the next couple years&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lueziv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lueziv/mercury_ultrafast_language_models_based_on/n1xi8id/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751948280,"author_flair_text":null,"treatment_tags":[],"created_utc":1751948280,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n1xi2o7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Happy_Percentage_384","can_mod_post":false,"created_utc":1751948207,"send_replies":true,"parent_id":"t1_n1xhbq3","score":3,"author_fullname":"t2_1qc1ybvg79","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"yeah, hope we ll see stronger open alternatives soon","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1xi2o7","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;yeah, hope we ll see stronger open alternatives soon&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lueziv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lueziv/mercury_ultrafast_language_models_based_on/n1xi2o7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751948207,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n1xhbq3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Kooshi_Govno","can_mod_post":false,"created_utc":1751947880,"send_replies":true,"parent_id":"t3_1lueziv","score":19,"author_fullname":"t2_7kg5p","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"mildly interesting, but closed source and can't even one shot a Tetris clone. ðŸ‘Ž","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1xhbq3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;mildly interesting, but closed source and can&amp;#39;t even one shot a Tetris clone. ðŸ‘Ž&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lueziv/mercury_ultrafast_language_models_based_on/n1xhbq3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751947880,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lueziv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":19}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"609bf7d4-01f3-11f0-9760-5611c8333bee","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1xi2oh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"You_Wen_AzzHu","can_mod_post":false,"created_utc":1751948207,"send_replies":true,"parent_id":"t3_1lueziv","score":10,"author_fullname":"t2_p4oxcufl","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Good concept, but not interested unless itâ€™s open source. It doesnâ€™t even come close to any of the closed-source competitors.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1xi2oh","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"exllama"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Good concept, but not interested unless itâ€™s open source. It doesnâ€™t even come close to any of the closed-source competitors.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lueziv/mercury_ultrafast_language_models_based_on/n1xi2oh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751948207,"author_flair_text":"exllama","treatment_tags":[],"link_id":"t3_1lueziv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1xhhlu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Repulsive_Educator61","can_mod_post":false,"created_utc":1751947951,"send_replies":true,"parent_id":"t3_1lueziv","score":5,"author_fullname":"t2_8qh69vps","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"is this local? ( cz localLlama)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1xhhlu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;is this local? ( cz localLlama)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lueziv/mercury_ultrafast_language_models_based_on/n1xhhlu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751947951,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lueziv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"body":"Speaking totally with layman knowledge. If it's diffusion based, can we set a soft token limit like resolution? Train controlnets? Use text2text for style improvement or translation? Increase step count to improve output?","subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1xtefi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"trajo123","can_mod_post":false,"created_utc":1751953580,"send_replies":true,"parent_id":"t1_n1xo7vk","score":3,"author_fullname":"t2_3ofwm7j3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"In principle yes. Imo, the main advantage of diffusion over autoregressive models is the powerful conditioning mechanism.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1xtefi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;In principle yes. Imo, the main advantage of diffusion over autoregressive models is the powerful conditioning mechanism.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lueziv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lueziv/mercury_ultrafast_language_models_based_on/n1xtefi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751953580,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n1xo7vk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"xadiant","can_mod_post":false,"created_utc":1751951015,"send_replies":true,"parent_id":"t3_1lueziv","score":3,"author_fullname":"t2_omgp6","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"author_cakeday":true,"edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1xo7vk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Speaking totally with layman knowledge. If it&amp;#39;s diffusion based, can we set a soft token limit like resolution? Train controlnets? Use text2text for style improvement or translation? Increase step count to improve output?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lueziv/mercury_ultrafast_language_models_based_on/n1xo7vk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751951015,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lueziv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}}]`),s=()=>e.jsx(l,{data:a});export{s as default};
