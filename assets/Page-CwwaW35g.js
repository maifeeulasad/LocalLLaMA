import{j as e}from"./index-F0NXdzZX.js";import{R as l}from"./RedditPostRenderer-CoEZB1dt.js";import"./index-DrtravXm.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I'm feeding millions of documents consisting of 2-10 sentences each into Llama-3.1-8B-Instruct using pyTorch and asking for a &lt;25 token summary. On a single H200, I'm averaging around 30-40/toks/sec.\\n\\nMy batch size is 128, which leads to VRAM utilization topping out at around 101GB (out of 141 GB). If I increase it much further, I start to get OOM exceptions. I'm using flash attention-2.\\n\\nThis is only about 2.5x faster than a RTX-4090 (using a smaller batch size and 4-bit), which is surprising. Can anyone recommend further tuning tips? Thank you.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"What kind of throughput can I expect with Llama 3.1 on a H200?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lure0g","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.83,"author_flair_background_color":null,"subreddit_type":"public","ups":4,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_7suvt","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":4,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751988104,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m feeding millions of documents consisting of 2-10 sentences each into Llama-3.1-8B-Instruct using pyTorch and asking for a &amp;lt;25 token summary. On a single H200, I&amp;#39;m averaging around 30-40/toks/sec.&lt;/p&gt;\\n\\n&lt;p&gt;My batch size is 128, which leads to VRAM utilization topping out at around 101GB (out of 141 GB). If I increase it much further, I start to get OOM exceptions. I&amp;#39;m using flash attention-2.&lt;/p&gt;\\n\\n&lt;p&gt;This is only about 2.5x faster than a RTX-4090 (using a smaller batch size and 4-bit), which is surprising. Can anyone recommend further tuning tips? Thank you.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lure0g","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"big_like_a_pickle","discussion_type":null,"num_comments":9,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lure0g/what_kind_of_throughput_can_i_expect_with_llama/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lure0g/what_kind_of_throughput_can_i_expect_with_llama/","subreddit_subscribers":496593,"created_utc":1751988104,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n20e0sq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Theio666","can_mod_post":false,"created_utc":1751992012,"send_replies":true,"parent_id":"t1_n201mwu","score":3,"author_fullname":"t2_ikhuo","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"qwen 2.5 7b in fp8 vLMM on 4070 tis gets to around 1k tokens/second in batch, so 30-40 is something going VERY wrong.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n20e0sq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;qwen 2.5 7b in fp8 vLMM on 4070 tis gets to around 1k tokens/second in batch, so 30-40 is something going VERY wrong.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lure0g","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lure0g/what_kind_of_throughput_can_i_expect_with_llama/n20e0sq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751992012,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n20el34","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Theio666","can_mod_post":false,"send_replies":true,"parent_id":"t1_n204vby","score":4,"author_fullname":"t2_ikhuo","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Still too slow, qwen 7b in fp8 gets to 1k tps on 4070tis, so server level H200 should easily reach several thousands tps on 7b model with proper setup.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n20el34","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Still too slow, qwen 7b in fp8 gets to 1k tps on 4070tis, so server level H200 should easily reach several thousands tps on 7b model with proper setup.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lure0g","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lure0g/what_kind_of_throughput_can_i_expect_with_llama/n20el34/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751992168,"author_flair_text":null,"treatment_tags":[],"created_utc":1751992168,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n204vby","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Most-Trainer-8876","can_mod_post":false,"created_utc":1751989410,"send_replies":true,"parent_id":"t1_n201mwu","score":2,"author_fullname":"t2_g0cs2h1x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"He is probably running full precision model","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n204vby","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;He is probably running full precision model&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lure0g","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lure0g/what_kind_of_throughput_can_i_expect_with_llama/n204vby/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751989410,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n26e2qg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"tomz17","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2414tr","score":1,"author_fullname":"t2_1mhx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"No prob... but FYI, with multiple simultaneous requests you should still be hitting thousands of T/S generation based on the model + hardware you described.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n26e2qg","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No prob... but FYI, with multiple simultaneous requests you should still be hitting thousands of T/S generation based on the model + hardware you described.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lure0g","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lure0g/what_kind_of_throughput_can_i_expect_with_llama/n26e2qg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752069707,"author_flair_text":null,"treatment_tags":[],"created_utc":1752069707,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2414tr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"big_like_a_pickle","can_mod_post":false,"created_utc":1752031652,"send_replies":true,"parent_id":"t1_n201mwu","score":2,"author_fullname":"t2_7suvt","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks for the VLLM tip. I've been able to get it up to 320/toks/sec using VLLM. Not sure how I fubared my other code so badly, but I'm looking into it.\\n\\nMy actual LLM throughput may be quite a bit higher, but my instrumentation also wraps a LoRA-tuned DeBERTa job that removes domain-specific boilerplate before summarization.","edited":1752031851,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2414tr","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for the VLLM tip. I&amp;#39;ve been able to get it up to 320/toks/sec using VLLM. Not sure how I fubared my other code so badly, but I&amp;#39;m looking into it.&lt;/p&gt;\\n\\n&lt;p&gt;My actual LLM throughput may be quite a bit higher, but my instrumentation also wraps a LoRA-tuned DeBERTa job that removes domain-specific boilerplate before summarization.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lure0g","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lure0g/what_kind_of_throughput_can_i_expect_with_llama/n2414tr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752031652,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n201mwu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"tomz17","can_mod_post":false,"created_utc":1751988490,"send_replies":true,"parent_id":"t3_1lure0g","score":10,"author_fullname":"t2_1mhx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt;I'm averaging around 30-40/toks/sec.\\n\\nI'm getting about double that on a single instance running on a single 3090 @ 250watts running llama.cpp...   so you are doing something VERY wrong\\n\\nIMHO, set up something like VLLM, and then feed your batched requests in using the API.  You should be hitting thousands of t/s on an 8B model with batched inference and an H200.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n201mwu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;I&amp;#39;m averaging around 30-40/toks/sec.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I&amp;#39;m getting about double that on a single instance running on a single 3090 @ 250watts running llama.cpp...   so you are doing something VERY wrong&lt;/p&gt;\\n\\n&lt;p&gt;IMHO, set up something like VLLM, and then feed your batched requests in using the API.  You should be hitting thousands of t/s on an 8B model with batched inference and an H200.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lure0g/what_kind_of_throughput_can_i_expect_with_llama/n201mwu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751988490,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lure0g","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n20279l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"____vladrad","can_mod_post":false,"created_utc":1751988652,"send_replies":true,"parent_id":"t3_1lure0g","score":1,"author_fullname":"t2_u6i8a0ay","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I wonder how it would look with two h100 running vllm using tp set to two? You’ll get a much higher throughput and batch size for the same ish price.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n20279l","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I wonder how it would look with two h100 running vllm using tp set to two? You’ll get a much higher throughput and batch size for the same ish price.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lure0g/what_kind_of_throughput_can_i_expect_with_llama/n20279l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751988652,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lure0g","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n20sz4s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Hot_Turnip_3309","can_mod_post":false,"created_utc":1751996144,"send_replies":true,"parent_id":"t3_1lure0g","score":1,"author_fullname":"t2_161yq4x22a","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"you should be running vllm or sglang","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n20sz4s","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;you should be running vllm or sglang&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lure0g/what_kind_of_throughput_can_i_expect_with_llama/n20sz4s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751996144,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lure0g","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"609bf7d4-01f3-11f0-9760-5611c8333bee","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n20tuct","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"You_Wen_AzzHu","can_mod_post":false,"created_utc":1751996383,"send_replies":true,"parent_id":"t3_1lure0g","score":1,"author_fullname":"t2_p4oxcufl","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Your metrics are off. Please set it up properly.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n20tuct","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"exllama"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Your metrics are off. Please set it up properly.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lure0g/what_kind_of_throughput_can_i_expect_with_llama/n20tuct/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751996383,"author_flair_text":"exllama","treatment_tags":[],"link_id":"t3_1lure0g","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(l,{data:t});export{n as default};
