import{j as e}from"./index-xfnGEtuL.js";import{R as l}from"./RedditPostRenderer-DAZRIZZK.js";import"./index-BaNn5-RR.js";const t=JSON.parse('[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hey everyone\\n\\nRunning MoE models on my machine, I\'m constantly frustrated working with \\\\`--overide-tensor\\\\` regexes in llama.cpp. They\'re hard to maintain, break easily, and are unreadable \\n\\nI built a little cli tool which builds these \\\\`--override-tensor\\\\` arguments automatically for your architecture.\\n\\nOn my machine (Xeon e5 2699v3, 128GB DDR4, 2x3090, 1x3060) this runs Qwen3 235B Q4XL at 5.5 tok/s\\n\\n    #!/bin/bash\\n    \\n    export CUDA_VISIBLE_DEVICES=2,0,1\\n    \\n    # Generate tensor overrides\\n    TENSOR_OVERRIDES=$(gguf-tensor-overrider -g https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q4_K_XL/Qwen3-235B-A22B-UD-Q4_K_XL-00001-of-00003.gguf -c 32000 --gpu-percentage 0.85)\\n    \\n    # Build command with tensor overrides\\n    CMD=\\"/home/kevin/llama.cpp/build/bin/llama-cli \\\\\\n      -hf unsloth/Qwen3-235B-A22B-GGUF:Q4_K_XL \\\\\\n      -c 32000 \\\\\\n      -fa \\\\\\n      -sm row \\\\\\n      $TENSOR_OVERRIDES\\"\\n    \\n    # Execute command directly (no pipe)\\n    eval \\"$CMD\\"\\n\\nResults:\\n\\n    &gt; hey there\\n    &lt;think&gt;\\n    Okay, the user just said \\"hey there\\". That\'s pretty casual. I should respond in a friendly and welcoming way. Maybe ask how they\'re doing and offer help. Let me keep it simple and approachable.\\n    \\n    I need to make sure the response is open-ended so they feel comfortable to ask anything. Avoid any technical jargon. Just a warm greeting and an offer to assist with whatever they need. Yeah, that should work.\\n    &lt;/think&gt;\\n    \\n    Hello! How can I assist you today? ðŸ˜Š\\n    \\n    &gt;\\n    llama_perf_sampler_print:    sampling time =      15.58 ms /   114 runs   (    0.14 ms per token,  7318.01 tokens per second)\\n    llama_perf_context_print:        load time =  152623.89 ms\\n    llama_perf_context_print: prompt eval time =    1918.59 ms /    10 tokens (  191.86 ms per token,     5.21 tokens per second)\\n    llama_perf_context_print:        eval time =   18799.44 ms /   103 runs   (  182.52 ms per token,     5.48 tokens per second)\\n    llama_perf_context_print:       total time =   30823.94 ms /   113 tokens\\n\\nThese commands should also work with ik\\\\_llama.cpp. 5.5 tok/s is about what I was getting before with ik\\\\_llama.cpp.\\n\\nHere is the link to the repository: [https://github.com/k-koehler/gguf-tensor-overrider](https://github.com/k-koehler/gguf-tensor-overrider/tree/main)\\n\\nHopefully some of your find this useful!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"I built a cli tool to automatically figure out tensor overrides in llama.cpp","link_flair_richtext":[{"e":"text","t":"Resources"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lpmx00","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.97,"author_flair_background_color":null,"subreddit_type":"public","ups":39,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_o015g","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Resources","can_mod_post":false,"score":39,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":true,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751431108,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey everyone&lt;/p&gt;\\n\\n&lt;p&gt;Running MoE models on my machine, I&amp;#39;m constantly frustrated working with `--overide-tensor` regexes in llama.cpp. They&amp;#39;re hard to maintain, break easily, and are unreadable &lt;/p&gt;\\n\\n&lt;p&gt;I built a little cli tool which builds these `--override-tensor` arguments automatically for your architecture.&lt;/p&gt;\\n\\n&lt;p&gt;On my machine (Xeon e5 2699v3, 128GB DDR4, 2x3090, 1x3060) this runs Qwen3 235B Q4XL at 5.5 tok/s&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;#!/bin/bash\\n\\nexport CUDA_VISIBLE_DEVICES=2,0,1\\n\\n# Generate tensor overrides\\nTENSOR_OVERRIDES=$(gguf-tensor-overrider -g https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/UD-Q4_K_XL/Qwen3-235B-A22B-UD-Q4_K_XL-00001-of-00003.gguf -c 32000 --gpu-percentage 0.85)\\n\\n# Build command with tensor overrides\\nCMD=&amp;quot;/home/kevin/llama.cpp/build/bin/llama-cli \\\\\\n  -hf unsloth/Qwen3-235B-A22B-GGUF:Q4_K_XL \\\\\\n  -c 32000 \\\\\\n  -fa \\\\\\n  -sm row \\\\\\n  $TENSOR_OVERRIDES&amp;quot;\\n\\n# Execute command directly (no pipe)\\neval &amp;quot;$CMD&amp;quot;\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;Results:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;&amp;gt; hey there\\n&amp;lt;think&amp;gt;\\nOkay, the user just said &amp;quot;hey there&amp;quot;. That&amp;#39;s pretty casual. I should respond in a friendly and welcoming way. Maybe ask how they&amp;#39;re doing and offer help. Let me keep it simple and approachable.\\n\\nI need to make sure the response is open-ended so they feel comfortable to ask anything. Avoid any technical jargon. Just a warm greeting and an offer to assist with whatever they need. Yeah, that should work.\\n&amp;lt;/think&amp;gt;\\n\\nHello! How can I assist you today? ðŸ˜Š\\n\\n&amp;gt;\\nllama_perf_sampler_print:    sampling time =      15.58 ms /   114 runs   (    0.14 ms per token,  7318.01 tokens per second)\\nllama_perf_context_print:        load time =  152623.89 ms\\nllama_perf_context_print: prompt eval time =    1918.59 ms /    10 tokens (  191.86 ms per token,     5.21 tokens per second)\\nllama_perf_context_print:        eval time =   18799.44 ms /   103 runs   (  182.52 ms per token,     5.48 tokens per second)\\nllama_perf_context_print:       total time =   30823.94 ms /   113 tokens\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;These commands should also work with ik_llama.cpp. 5.5 tok/s is about what I was getting before with ik_llama.cpp.&lt;/p&gt;\\n\\n&lt;p&gt;Here is the link to the repository: &lt;a href=\\"https://github.com/k-koehler/gguf-tensor-overrider/tree/main\\"&gt;https://github.com/k-koehler/gguf-tensor-overrider&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Hopefully some of your find this useful!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#ccac2b","id":"1lpmx00","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"kevin_1994","discussion_type":null,"num_comments":11,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lpmx00/i_built_a_cli_tool_to_automatically_figure_out/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lpmx00/i_built_a_cli_tool_to_automatically_figure_out/","subreddit_subscribers":494001,"created_utc":1751431108,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0wjobe","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"kevin_1994","can_mod_post":false,"created_utc":1751440972,"send_replies":true,"parent_id":"t1_n0wizmh","score":3,"author_fullname":"t2_o015g","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes! By default it uses all gpus in priority by size. So for example, if you have a 30 gb model with a 3090 and 3060, it should try to split it as 24gb on the 3090 and 6gb on the 3060. You can change this behavior with `--granular-gpu-percentage` flag","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0wjobe","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes! By default it uses all gpus in priority by size. So for example, if you have a 30 gb model with a 3090 and 3060, it should try to split it as 24gb on the 3090 and 6gb on the 3060. You can change this behavior with &lt;code&gt;--granular-gpu-percentage&lt;/code&gt; flag&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"link_id":"t3_1lpmx00","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpmx00/i_built_a_cli_tool_to_automatically_figure_out/n0wjobe/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751440972,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0wizmh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"jacek2023","can_mod_post":false,"created_utc":1751440575,"send_replies":true,"parent_id":"t3_1lpmx00","score":3,"author_fullname":"t2_vqgbql9w","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"does it work with multiple GPUs?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0wizmh","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;does it work with multiple GPUs?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpmx00/i_built_a_cli_tool_to_automatically_figure_out/n0wizmh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751440575,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lpmx00","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0xfdul","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Accomplished_Mode170","can_mod_post":false,"created_utc":1751457667,"send_replies":true,"parent_id":"t3_1lpmx00","score":1,"author_fullname":"t2_4hfmiefj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Any interest in allowing for a target KV corpus to shape which activations and experts are targeted? ðŸ“Š","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0xfdul","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Any interest in allowing for a target KV corpus to shape which activations and experts are targeted? ðŸ“Š&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpmx00/i_built_a_cli_tool_to_automatically_figure_out/n0xfdul/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751457667,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpmx00","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0xgdxi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DeProgrammer99","can_mod_post":false,"created_utc":1751458062,"send_replies":true,"parent_id":"t3_1lpmx00","score":2,"author_fullname":"t2_w4j8t","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Nice. I just added a feature to [GGUFDump](https://github.com/dpmm99/GGUFDump) the other day to just try to **list** the tensors in a reasonable GPU-offloading priority order, but this is more immediately practical for use.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0xgdxi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nice. I just added a feature to &lt;a href=\\"https://github.com/dpmm99/GGUFDump\\"&gt;GGUFDump&lt;/a&gt; the other day to just try to &lt;strong&gt;list&lt;/strong&gt; the tensors in a reasonable GPU-offloading priority order, but this is more immediately practical for use.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpmx00/i_built_a_cli_tool_to_automatically_figure_out/n0xgdxi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751458062,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpmx00","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ybcnu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"kevin_1994","can_mod_post":false,"created_utc":1751468124,"send_replies":true,"parent_id":"t1_n0y87vq","score":1,"author_fullname":"t2_o015g","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Of course!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ybcnu","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Of course!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"link_id":"t3_1lpmx00","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpmx00/i_built_a_cli_tool_to_automatically_figure_out/n0ybcnu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751468124,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0y87vq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"LA_rent_Aficionado","can_mod_post":false,"created_utc":1751467229,"send_replies":true,"parent_id":"t3_1lpmx00","score":1,"author_fullname":"t2_t8zbiflk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"very cool!\\n\\nI didn\'t see a license file, would you be open to me incorporating your workflow into my llama.cpp launcher?\\n\\n[https://github.com/thad0ctor/llama-server-launcher](https://github.com/thad0ctor/llama-server-launcher)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0y87vq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;very cool!&lt;/p&gt;\\n\\n&lt;p&gt;I didn&amp;#39;t see a license file, would you be open to me incorporating your workflow into my llama.cpp launcher?&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/thad0ctor/llama-server-launcher\\"&gt;https://github.com/thad0ctor/llama-server-launcher&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpmx00/i_built_a_cli_tool_to_automatically_figure_out/n0y87vq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751467229,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpmx00","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0zf87p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"kevin_1994","can_mod_post":false,"created_utc":1751479342,"send_replies":true,"parent_id":"t1_n0ytpvm","score":1,"author_fullname":"t2_o015g","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You shoudn\'t have to redownload any models. The command just spits out a bunch of `--override-tensor \\"&lt;block&gt;=&lt;device&gt;\\"` arguments","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0zf87p","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You shoudn&amp;#39;t have to redownload any models. The command just spits out a bunch of &lt;code&gt;--override-tensor &amp;quot;&amp;lt;block&amp;gt;=&amp;lt;device&amp;gt;&amp;quot;&lt;/code&gt; arguments&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"link_id":"t3_1lpmx00","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpmx00/i_built_a_cli_tool_to_automatically_figure_out/n0zf87p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751479342,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ytpvm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MatterMean5176","can_mod_post":false,"created_utc":1751473278,"send_replies":true,"parent_id":"t3_1lpmx00","score":1,"author_fullname":"t2_1ju039btvf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Awesome, I need something like this. Does this require redownloading the models for it to work or can it be used on models already downloaded? Sorry if that\'s a dumb question.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ytpvm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Awesome, I need something like this. Does this require redownloading the models for it to work or can it be used on models already downloaded? Sorry if that&amp;#39;s a dumb question.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpmx00/i_built_a_cli_tool_to_automatically_figure_out/n0ytpvm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751473278,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpmx00","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n13g8v9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ttkciar","can_mod_post":false,"created_utc":1751533172,"send_replies":true,"parent_id":"t1_n0wn5au","score":0,"author_fullname":"t2_cpegz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Any sane system will have bash in /bin.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n13g8v9","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Any sane system will have bash in /bin.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpmx00","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpmx00/i_built_a_cli_tool_to_automatically_figure_out/n13g8v9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751533172,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n0wn5au","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Black-Mack","can_mod_post":false,"created_utc":1751443027,"send_replies":true,"parent_id":"t3_1lpmx00","score":0,"author_fullname":"t2_1s8rdwqx9a","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Please, replace\\n```\\n#!/bin/bash\\n```\\nWith\\n```\\n#!/usr/bin/env bash\\n```\\n[More info here](https://stackoverflow.com/questions/16365130/what-is-the-difference-between-usr-bin-env-bash-and-usr-bin-bash)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0wn5au","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Please, replace\\n```&lt;/p&gt;\\n\\n&lt;h1&gt;!/bin/bash&lt;/h1&gt;\\n\\n&lt;p&gt;&lt;code&gt;\\nWith\\n&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;h1&gt;!/usr/bin/env bash&lt;/h1&gt;\\n\\n&lt;p&gt;```\\n&lt;a href=\\"https://stackoverflow.com/questions/16365130/what-is-the-difference-between-usr-bin-env-bash-and-usr-bin-bash\\"&gt;More info here&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpmx00/i_built_a_cli_tool_to_automatically_figure_out/n0wn5au/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751443027,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpmx00","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}}]'),n=()=>e.jsx(l,{data:t});export{n as default};
