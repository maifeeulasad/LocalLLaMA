import{j as e}from"./index-BlGsFJYy.js";import{R as l}from"./RedditPostRenderer-B6uvq_Zl.js";import"./index-DDvVtNwD.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"A couple days ago I made a post sharing my experiment training an LLM on only 1800's London text. That post got more attention than I expected and some people have been checking it out on GitHub. So I just wanted to share an update on this project. I trained a second version using 500 books, legal documents, journals, etc. I also expanded the time period to 1800-1875 instead of 1800-1850. This model is now able to produce semi-coherent sentences with almost no modern references. It's no where near an LLM right now, more like a sentence generator but I'm having a lot of fun doing this and gonna keep scaling up. Many people have been giving me good feedback/advice so thank you ! I'm a bit busy right now but once I find the time I will push everything to GitHub.\\n\\n[Output and Hallucinations, Prompt: \\\\\\\\\\"In the autumn of 1847,\\\\\\\\\\"](https://preview.redd.it/kxh4l1irzidf1.png?width=922&amp;format=png&amp;auto=webp&amp;s=ee6ba605bf41a988010fd1245efe5f8dd895c4e1)\\n\\n[https://github.com/haykgrigo3/TimeCapsuleLLM/tree/main](https://github.com/haykgrigo3/TimeCapsuleLLM/tree/main)","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Training an LLM only on books from the 1800's - Update","link_flair_richtext":[{"e":"text","t":"Post of the day  "},{"a":":X:","u":"https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X","e":"emoji"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":78,"top_awarded_type":null,"hide_score":false,"media_metadata":{"kxh4l1irzidf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":60,"x":108,"u":"https://preview.redd.it/kxh4l1irzidf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3b9a8dbe0db48f3f9f13ce09ee7f734995521da4"},{"y":121,"x":216,"u":"https://preview.redd.it/kxh4l1irzidf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=358964afd0e836c177c625133b74a649c00050f2"},{"y":179,"x":320,"u":"https://preview.redd.it/kxh4l1irzidf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c0298b789b46335d12a79fb33b9995e0c7931397"},{"y":359,"x":640,"u":"https://preview.redd.it/kxh4l1irzidf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2c29f72d4c0dc749bf952f48a9d4d8ad2e388da8"}],"s":{"y":518,"x":922,"u":"https://preview.redd.it/kxh4l1irzidf1.png?width=922&amp;format=png&amp;auto=webp&amp;s=ee6ba605bf41a988010fd1245efe5f8dd895c4e1"},"id":"kxh4l1irzidf1"}},"name":"t3_1m2nvpn","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.96,"author_flair_background_color":"transparent","subreddit_type":"public","ups":282,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":"c07aa42e-51fe-11f0-afcc-462aad931709","is_original_content":false,"author_fullname":"t2_1ink6kzg93","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Post of the day  :X:","can_mod_post":false,"score":282,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://b.thumbs.redditmedia.com/nsMpO5S0s6t0aJGmgRVTbKS-Fsyr-akDtUyycEROI9U.jpg","edited":false,"author_flair_css_class":null,"author_flair_richtext":[{"a":":X:","u":"https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X","e":"emoji"}],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752797939,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"richtext","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;A couple days ago I made a post sharing my experiment training an LLM on only 1800&amp;#39;s London text. That post got more attention than I expected and some people have been checking it out on GitHub. So I just wanted to share an update on this project. I trained a second version using 500 books, legal documents, journals, etc. I also expanded the time period to 1800-1875 instead of 1800-1850. This model is now able to produce semi-coherent sentences with almost no modern references. It&amp;#39;s no where near an LLM right now, more like a sentence generator but I&amp;#39;m having a lot of fun doing this and gonna keep scaling up. Many people have been giving me good feedback/advice so thank you ! I&amp;#39;m a bit busy right now but once I find the time I will push everything to GitHub.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/kxh4l1irzidf1.png?width=922&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ee6ba605bf41a988010fd1245efe5f8dd895c4e1\\"&gt;Output and Hallucinations, Prompt: \\\\&amp;quot;In the autumn of 1847,\\\\&amp;quot;&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/haykgrigo3/TimeCapsuleLLM/tree/main\\"&gt;https://github.com/haykgrigo3/TimeCapsuleLLM/tree/main&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5563f7e6-52bf-11f0-a755-7266d77e32bb","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":":X:","treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#58a7a4","id":"1m2nvpn","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Remarkable-Trick-177","discussion_type":null,"num_comments":52,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":"dark","permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/","subreddit_subscribers":501527,"created_utc":1752797939,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3scitp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"IrisColt","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3s4tuy","score":4,"author_fullname":"t2_c2f558x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I am also interested on this, by the way, thanks!!!","edited":false,"author_flair_css_class":null,"name":"t1_n3scitp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I am also interested on this, by the way, thanks!!!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m2nvpn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3scitp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752831064,"author_flair_text":null,"collapsed":false,"created_utc":1752831064,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3xxmd1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"minpeter2","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3s4tuy","score":2,"author_fullname":"t2_1fc9cbovwe","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"[https://github.com/minpeter/tiny-ko](https://github.com/minpeter/tiny-ko)\\n\\nI'm still working on it, but I'm writing some code to pretrain a model on llama architecture. Hope it helps.","edited":false,"author_flair_css_class":null,"name":"t1_n3xxmd1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://github.com/minpeter/tiny-ko\\"&gt;https://github.com/minpeter/tiny-ko&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m still working on it, but I&amp;#39;m writing some code to pretrain a model on llama architecture. Hope it helps.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m2nvpn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3xxmd1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752899370,"author_flair_text":null,"collapsed":false,"created_utc":1752899370,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3she7c","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"schlammsuhler","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3s4tuy","score":3,"author_fullname":"t2_cx7q6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"After loading the model you can overwrite the tensors with random or zero. I have done it when doubling layercount of a model. Sonnet was very helpful. My colab is a mess but if you cant figure it out i can share","edited":false,"author_flair_css_class":null,"name":"t1_n3she7c","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;After loading the model you can overwrite the tensors with random or zero. I have done it when doubling layercount of a model. Sonnet was very helpful. My colab is a mess but if you cant figure it out i can share&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m2nvpn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3she7c/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752833716,"author_flair_text":null,"collapsed":false,"created_utc":1752833716,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n3s4tuy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"smartsometimes","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3rmaiw","score":14,"author_fullname":"t2_hrgfe","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"But pretraining the full LLM from scratch?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3s4tuy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;But pretraining the full LLM from scratch?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2nvpn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3s4tuy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752826644,"author_flair_text":null,"treatment_tags":[],"created_utc":1752826644,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":14}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3tpv9e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"bralynn2222","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3sjowq","score":0,"author_fullname":"t2_769j0jzd","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That’s entirely dictated on data choice","edited":false,"author_flair_css_class":null,"name":"t1_n3tpv9e","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That’s entirely dictated on data choice&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m2nvpn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3tpv9e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752849862,"author_flair_text":null,"collapsed":false,"created_utc":1752849862,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n3sjowq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"masc98","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3rmaiw","score":11,"author_fullname":"t2_12nt66","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"if it goes on the finetuning path, that s not gonna be a timecapsule model anymore.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3sjowq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;if it goes on the finetuning path, that s not gonna be a timecapsule model anymore.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2nvpn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3sjowq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752834904,"author_flair_text":null,"treatment_tags":[],"created_utc":1752834904,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}}],"before":null}},"user_reports":[],"saved":false,"id":"n3rmaiw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"bralynn2222","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3qtoge","score":31,"author_fullname":"t2_769j0jzd","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Unsloth has a google colab for anything you’ll need SFT , CPT , DPO , GRPO or full model retraining","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3rmaiw","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Unsloth has a google colab for anything you’ll need SFT , CPT , DPO , GRPO or full model retraining&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2nvpn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3rmaiw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752816668,"author_flair_text":null,"treatment_tags":[],"created_utc":1752816668,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":31}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3upeh9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Expensive-Apricot-25","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3qtoge","score":3,"author_fullname":"t2_idqkwio0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"look up the meta llama 3 &amp; llama 3.1 research papers.\\n\\nthey extensively go over everything. there is enough information there to fully recreate all llama3 models from scratch.\\n\\nthey also did extensive research on scaling laws for training and hyper parameters using super small models, so it will probably also already tell you the optimal parameters for a model of this scale upfront.\\n\\ni doubt there are any tutorials like nano-GPT, but if you already built nano-GPT from scratch, its no t that big of a step up, just has more modern components. That being said, you can find youtube videos going over the research paper at a high level which is probably useful.\\n\\nthe entire pytorch model architecture implementation can be found here i believe (or at least within this repository): [https://github.com/meta-llama/llama3/blob/main/llama/model.py](https://github.com/meta-llama/llama3/blob/main/llama/model.py)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3upeh9","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;look up the meta llama 3 &amp;amp; llama 3.1 research papers.&lt;/p&gt;\\n\\n&lt;p&gt;they extensively go over everything. there is enough information there to fully recreate all llama3 models from scratch.&lt;/p&gt;\\n\\n&lt;p&gt;they also did extensive research on scaling laws for training and hyper parameters using super small models, so it will probably also already tell you the optimal parameters for a model of this scale upfront.&lt;/p&gt;\\n\\n&lt;p&gt;i doubt there are any tutorials like nano-GPT, but if you already built nano-GPT from scratch, its no t that big of a step up, just has more modern components. That being said, you can find youtube videos going over the research paper at a high level which is probably useful.&lt;/p&gt;\\n\\n&lt;p&gt;the entire pytorch model architecture implementation can be found here i believe (or at least within this repository): &lt;a href=\\"https://github.com/meta-llama/llama3/blob/main/llama/model.py\\"&gt;https://github.com/meta-llama/llama3/blob/main/llama/model.py&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2nvpn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3upeh9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752859851,"author_flair_text":null,"treatment_tags":[],"created_utc":1752859851,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3sq4cv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SpacemanCraig3","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3qtoge","score":2,"author_fullname":"t2_13jvln","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Why would you need that?\\n\\nThey publish, and if you're not competent then have o3 do the implementation.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3sq4cv","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why would you need that?&lt;/p&gt;\\n\\n&lt;p&gt;They publish, and if you&amp;#39;re not competent then have o3 do the implementation.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2nvpn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3sq4cv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752837878,"author_flair_text":null,"treatment_tags":[],"created_utc":1752837878,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ttgy8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"citaman","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3qtoge","score":3,"author_fullname":"t2_7zubl1l8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Go look at the implementation in the transformer library (couple dependencies but with some time you can understand the logic)  \\n  \\n[**Transformers Llama Model**](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py)\\n\\n[Llama-3 8b config](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/blob/main/config.json)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3ttgy8","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Go look at the implementation in the transformer library (couple dependencies but with some time you can understand the logic)  &lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py\\"&gt;&lt;strong&gt;Transformers Llama Model&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/blob/main/config.json\\"&gt;Llama-3 8b config&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2nvpn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3ttgy8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752850874,"author_flair_text":null,"treatment_tags":[],"created_utc":1752850874,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n3qtoge","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"mtomas7","can_mod_post":false,"created_utc":1752804614,"send_replies":true,"parent_id":"t1_n3qnr6e","score":36,"author_fullname":"t2_gct10","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Do you have a link to some good tutorials on training Llama3 from scratch?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3qtoge","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Do you have a link to some good tutorials on training Llama3 from scratch?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2nvpn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3qtoge/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752804614,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":36}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n40zb2u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"f86_pilot","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3ztkxd","score":1,"author_fullname":"t2_y8hw453","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Oh yeah if you just scale down an existing acthiecture that would work and would likely perform better than the existing 100M version.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n40zb2u","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Oh yeah if you just scale down an existing acthiecture that would work and would likely perform better than the existing 100M version.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m2nvpn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n40zb2u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752945788,"author_flair_text":null,"treatment_tags":[],"created_utc":1752945788,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ztkxd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Expensive-Apricot-25","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3xqrx9","score":2,"author_fullname":"t2_idqkwio0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thats why you make the model smaller following the scaling laws meta published, like I said a few times already.\\n\\nEven if you didn’t do that, like I said earlier, in deep learning there is this thing called the double decent curve, even if the model can memorize the entire data set, it will still outperform a smaller model (as long as num param != num data points exactly). You would just simply use early stopping to stop training the model when it starts to overfit.\\n\\nA larger model would perform better, even if it is able to memorize the entire data set.","edited":false,"author_flair_css_class":null,"name":"t1_n3ztkxd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thats why you make the model smaller following the scaling laws meta published, like I said a few times already.&lt;/p&gt;\\n\\n&lt;p&gt;Even if you didn’t do that, like I said earlier, in deep learning there is this thing called the double decent curve, even if the model can memorize the entire data set, it will still outperform a smaller model (as long as num param != num data points exactly). You would just simply use early stopping to stop training the model when it starts to overfit.&lt;/p&gt;\\n\\n&lt;p&gt;A larger model would perform better, even if it is able to memorize the entire data set.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m2nvpn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3ztkxd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752932644,"author_flair_text":null,"collapsed":false,"created_utc":1752932644,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3xqrx9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"f86_pilot","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3unw1l","score":1,"author_fullname":"t2_y8hw453","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"llama3 would definitly be more effective if it was a \\"normal\\" sized training dataset than this 100M model, but with only between 500-1000 books of training token content, it would perform worse on this task and make overfitting and output worse. More paramaters can more easily memorize more of the training set reducing generalization.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3xqrx9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;llama3 would definitly be more effective if it was a &amp;quot;normal&amp;quot; sized training dataset than this 100M model, but with only between 500-1000 books of training token content, it would perform worse on this task and make overfitting and output worse. More paramaters can more easily memorize more of the training set reducing generalization.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2nvpn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3xqrx9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752896343,"author_flair_text":null,"treatment_tags":[],"created_utc":1752896343,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3unw1l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Expensive-Apricot-25","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3t0py1","score":4,"author_fullname":"t2_idqkwio0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"not talking about fine tuning, sorry, i thought that would be more obvious since that would defeat the purpose... i'm talking about implementing llama3 yourself, or training an existing architecture implementation from scratch - so you don't have to do any extra work yourself.\\n\\nYou don't need to make the model as big as the official llama3 models, you can just follow the scaling laws they already research (and already tested different values extensively for smaller models of this size)\\n\\nEven if that weren't the case, in deep learning there is the double decent curve, so a massively over parameterized model will actually perform and generalize slightly better.\\n\\nAgain, nanoGPT is a toy model based on GPT-2, and lacks many modern features in production LLMs, you would get better results using a more modern architecture like llama3. I only proposed llama3 because of the EXTENSIVE research and documentation meta released.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3unw1l","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;not talking about fine tuning, sorry, i thought that would be more obvious since that would defeat the purpose... i&amp;#39;m talking about implementing llama3 yourself, or training an existing architecture implementation from scratch - so you don&amp;#39;t have to do any extra work yourself.&lt;/p&gt;\\n\\n&lt;p&gt;You don&amp;#39;t need to make the model as big as the official llama3 models, you can just follow the scaling laws they already research (and already tested different values extensively for smaller models of this size)&lt;/p&gt;\\n\\n&lt;p&gt;Even if that weren&amp;#39;t the case, in deep learning there is the double decent curve, so a massively over parameterized model will actually perform and generalize slightly better.&lt;/p&gt;\\n\\n&lt;p&gt;Again, nanoGPT is a toy model based on GPT-2, and lacks many modern features in production LLMs, you would get better results using a more modern architecture like llama3. I only proposed llama3 because of the EXTENSIVE research and documentation meta released.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2nvpn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3unw1l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752859429,"author_flair_text":null,"treatment_tags":[],"created_utc":1752859429,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3znjcy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mitchins-au","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3t0py1","score":1,"author_fullname":"t2_4hjtgq5u","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Maybe something like SmolLM is more applicable? Why not just make it 1850-1900.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3znjcy","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Maybe something like SmolLM is more applicable? Why not just make it 1850-1900.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2nvpn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3znjcy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752930468,"author_flair_text":null,"treatment_tags":[],"created_utc":1752930468,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3t0py1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"omegaindebt","can_mod_post":false,"created_utc":1752842031,"send_replies":true,"parent_id":"t1_n3qnr6e","score":7,"author_fullname":"t2_2god87yf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The experiment that they said they were doing was trying to get a model trained on just texts from 1800-1850 (1875 now)\\n\\nIf you're saying that they should copy the Llama 3 architecture, then they probably can't because that requires a lot of data. it would be akin to having a bucket to carry a cup of water. Nigh useless and unnecessarily finicky to work with.\\n\\nIf you're saying to fine-tune using an existing llama 3 model, then that would destroy the concept of a time capsule model.\\n\\nYes, they can alter the existing llama 3 model architecture and shrink it by an order of magnitude, but at that point it will be akin to a nanoGPT architecture.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3t0py1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The experiment that they said they were doing was trying to get a model trained on just texts from 1800-1850 (1875 now)&lt;/p&gt;\\n\\n&lt;p&gt;If you&amp;#39;re saying that they should copy the Llama 3 architecture, then they probably can&amp;#39;t because that requires a lot of data. it would be akin to having a bucket to carry a cup of water. Nigh useless and unnecessarily finicky to work with.&lt;/p&gt;\\n\\n&lt;p&gt;If you&amp;#39;re saying to fine-tune using an existing llama 3 model, then that would destroy the concept of a time capsule model.&lt;/p&gt;\\n\\n&lt;p&gt;Yes, they can alter the existing llama 3 model architecture and shrink it by an order of magnitude, but at that point it will be akin to a nanoGPT architecture.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2nvpn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3t0py1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752842031,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n415xuu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Expensive-Apricot-25","can_mod_post":false,"send_replies":true,"parent_id":"t1_n40ywhq","score":1,"author_fullname":"t2_idqkwio0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"no, you are thinking about \\"grokking\\", which is an entirely different phenomenon.\\n\\nand grokking isn't a universal phenomenon, it was later discovered to be an artifact of a vanishing gradient problem within the type of model.\\n\\nthe double decent curve says that as you increase parameters, the validation loss decreases up to a point where num params == test points where it starts to go up again, but if you keep increasing the parameters, the validation loss goes down again, and it goes even lower.","edited":false,"author_flair_css_class":null,"name":"t1_n415xuu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;no, you are thinking about &amp;quot;grokking&amp;quot;, which is an entirely different phenomenon.&lt;/p&gt;\\n\\n&lt;p&gt;and grokking isn&amp;#39;t a universal phenomenon, it was later discovered to be an artifact of a vanishing gradient problem within the type of model.&lt;/p&gt;\\n\\n&lt;p&gt;the double decent curve says that as you increase parameters, the validation loss decreases up to a point where num params == test points where it starts to go up again, but if you keep increasing the parameters, the validation loss goes down again, and it goes even lower.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m2nvpn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n415xuu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752947791,"author_flair_text":null,"collapsed":false,"created_utc":1752947791,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n40ywhq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"f86_pilot","can_mod_post":false,"send_replies":true,"parent_id":"t1_n40923r","score":1,"author_fullname":"t2_y8hw453","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Actualy that would help, allowing you to fit more paramaters per training token, but I am still not sure if it would be able to get to 1B stable given this small training dataset. Generaly for effective LLM training around 20 token's per paramameter is needed. However, if I assume this dataset is 1,000 books, and each book is 100,000 tokens, that gives us 100M training tokens, or for the 100M model 1 token per paramater. But, using this method might allow us to scale to 200M which would yield 0.5 token's per paramater, but scaling to 1B would yield just 0.1 which is too low.\\n\\n  \\nAlso, the double decent risk curve is actually delibretly training further past the point where overfitting and interpoliation starts to happen, but if you keep training it and ajust regularization hyperparamaters, then you can actualy start to decrease the test loss by continuing to train on the same data. This is the opposite to early stopping.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n40ywhq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Actualy that would help, allowing you to fit more paramaters per training token, but I am still not sure if it would be able to get to 1B stable given this small training dataset. Generaly for effective LLM training around 20 token&amp;#39;s per paramameter is needed. However, if I assume this dataset is 1,000 books, and each book is 100,000 tokens, that gives us 100M training tokens, or for the 100M model 1 token per paramater. But, using this method might allow us to scale to 200M which would yield 0.5 token&amp;#39;s per paramater, but scaling to 1B would yield just 0.1 which is too low.&lt;/p&gt;\\n\\n&lt;p&gt;Also, the double decent risk curve is actually delibretly training further past the point where overfitting and interpoliation starts to happen, but if you keep training it and ajust regularization hyperparamaters, then you can actualy start to decrease the test loss by continuing to train on the same data. This is the opposite to early stopping.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2nvpn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n40ywhq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752945664,"author_flair_text":null,"treatment_tags":[],"created_utc":1752945664,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n40923r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Expensive-Apricot-25","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3xq6ix","score":2,"author_fullname":"t2_idqkwio0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"double decent curve. \\n\\nif you stop training the model right before it overfits (early stopping), it will outperform a smaller model, even if it is able to memorize the entire dataset (if you continued training), and it will generalize better.\\n\\nthis is a fairly well understood concept, and it works better if you use regularization techniques that are already standard in the first place.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n40923r","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;double decent curve. &lt;/p&gt;\\n\\n&lt;p&gt;if you stop training the model right before it overfits (early stopping), it will outperform a smaller model, even if it is able to memorize the entire dataset (if you continued training), and it will generalize better.&lt;/p&gt;\\n\\n&lt;p&gt;this is a fairly well understood concept, and it works better if you use regularization techniques that are already standard in the first place.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2nvpn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n40923r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752937649,"author_flair_text":null,"treatment_tags":[],"created_utc":1752937649,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3xq6ix","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"f86_pilot","can_mod_post":false,"created_utc":1752896086,"send_replies":true,"parent_id":"t1_n3qnr6e","score":2,"author_fullname":"t2_y8hw453","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Using an existing llama 3 achtiecture would actualy perform worse, becouse of the tiny training data quantity. 500 books is a tiny amount, and the 100M paramater version is likely already overfitting. a 1B paramater version would only overfit more and result in worse generability, (but it might appear more readable, becouse its directly memorizing its training data). In fact this model currently may itself be overfiting.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3xq6ix","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Using an existing llama 3 achtiecture would actualy perform worse, becouse of the tiny training data quantity. 500 books is a tiny amount, and the 100M paramater version is likely already overfitting. a 1B paramater version would only overfit more and result in worse generability, (but it might appear more readable, becouse its directly memorizing its training data). In fact this model currently may itself be overfiting.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2nvpn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3xq6ix/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752896086,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3qnr6e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Expensive-Apricot-25","can_mod_post":false,"created_utc":1752802458,"send_replies":true,"parent_id":"t3_1m2nvpn","score":109,"author_fullname":"t2_idqkwio0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"u should just use the llama3 architecture.\\n\\nthere are already plenty of implementations, its a modern LLM architecture with all the bells and whistles of full blown LLMs, and should give u much better performance than the simplified nanoGPT.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3qnr6e","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;u should just use the llama3 architecture.&lt;/p&gt;\\n\\n&lt;p&gt;there are already plenty of implementations, its a modern LLM architecture with all the bells and whistles of full blown LLMs, and should give u much better performance than the simplified nanoGPT.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3qnr6e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752802458,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2nvpn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":109}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3tuh25","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"alamacra","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3tlxud","score":4,"author_fullname":"t2_o5fyioqm","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"There definitely were a lot of books written in the day, however they, of course, are not all digitised. It should be possible to do, after all, humans of the day built early cars, powerplants, cameras and battleships, and saw through complex projects of various types, be it city or infrastructure planning, such as countrywide rail and telegraph networks. All of the info concerning this must surely have been recorded.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3tuh25","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There definitely were a lot of books written in the day, however they, of course, are not all digitised. It should be possible to do, after all, humans of the day built early cars, powerplants, cameras and battleships, and saw through complex projects of various types, be it city or infrastructure planning, such as countrywide rail and telegraph networks. All of the info concerning this must surely have been recorded.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2nvpn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3tuh25/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752851157,"author_flair_text":null,"treatment_tags":[],"created_utc":1752851157,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n3tlxud","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ExactSeaworthiness34","can_mod_post":false,"created_utc":1752848747,"send_replies":true,"parent_id":"t1_n3qkc3r","score":8,"author_fullname":"t2_ddhvy03s","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"There's simply not enough data to become that smart. Data from that time periods is several orders of magnitude lower than today's available data","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3tlxud","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There&amp;#39;s simply not enough data to become that smart. Data from that time periods is several orders of magnitude lower than today&amp;#39;s available data&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2nvpn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3tlxud/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752848747,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3snmgm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Salty-Garage7777","can_mod_post":false,"created_utc":1752836776,"send_replies":true,"parent_id":"t1_n3qkc3r","score":6,"author_fullname":"t2_14m2ycs468","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Then it'd be best to give it all written data in all languages to a certain date. 😉","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3snmgm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Then it&amp;#39;d be best to give it all written data in all languages to a certain date. 😉&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2nvpn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3snmgm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752836776,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3rjy0c","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Paradigmind","can_mod_post":false,"created_utc":1752815515,"send_replies":true,"parent_id":"t1_n3qkc3r","score":4,"author_fullname":"t2_6ste18zta","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Provide it also every information of the Doomsday Clock.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3rjy0c","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Provide it also every information of the Doomsday Clock.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2nvpn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3rjy0c/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752815515,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n3qkc3r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"silenceimpaired","can_mod_post":false,"created_utc":1752801214,"send_replies":true,"parent_id":"t3_1m2nvpn","score":72,"author_fullname":"t2_dissgzyl","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Be interesting to see how well AI can predict the future. What would it think happens 100 years from training data. Just indicate the time has passed. Or better yet have it list most notable events and scientific discoveries from 1850 to 1975 by decade. :)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3qkc3r","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Be interesting to see how well AI can predict the future. What would it think happens 100 years from training data. Just indicate the time has passed. Or better yet have it list most notable events and scientific discoveries from 1850 to 1975 by decade. :)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3qkc3r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752801214,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2nvpn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":72}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3qxyek","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ready_to_fuck_yeahh","can_mod_post":false,"created_utc":1752806208,"send_replies":true,"parent_id":"t3_1m2nvpn","score":63,"author_fullname":"t2_j4lnwvga2","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"https://preview.redd.it/kfqracqrojdf1.jpeg?width=1179&amp;format=pjpg&amp;auto=webp&amp;s=7a3de350327eec528433d96b36520f2c4dd18da3","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3qxyek","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://preview.redd.it/kfqracqrojdf1.jpeg?width=1179&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=7a3de350327eec528433d96b36520f2c4dd18da3\\"&gt;https://preview.redd.it/kfqracqrojdf1.jpeg?width=1179&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=7a3de350327eec528433d96b36520f2c4dd18da3&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3qxyek/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752806208,"media_metadata":{"kfqracqrojdf1":{"status":"valid","e":"Image","m":"image/jpeg","p":[{"y":160,"x":108,"u":"https://preview.redd.it/kfqracqrojdf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1ab7b77bce268c70e32426bb1c9e648c7375410a"},{"y":320,"x":216,"u":"https://preview.redd.it/kfqracqrojdf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=86784546d582f1171f505aa87d92bf65ce158617"},{"y":474,"x":320,"u":"https://preview.redd.it/kfqracqrojdf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=87df99701d1367e5778efa19b48d42ffeb883c02"},{"y":948,"x":640,"u":"https://preview.redd.it/kfqracqrojdf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b10a8ff793fb26ba82c333aa98e05cb5e646f6f2"},{"y":1423,"x":960,"u":"https://preview.redd.it/kfqracqrojdf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7214c83541f74977d70161762f05306414809a5b"},{"y":1601,"x":1080,"u":"https://preview.redd.it/kfqracqrojdf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0f3968f6b8f48c4b21e6fb8a51908e6ec5fb5bfc"}],"s":{"y":1748,"x":1179,"u":"https://preview.redd.it/kfqracqrojdf1.jpeg?width=1179&amp;format=pjpg&amp;auto=webp&amp;s=7a3de350327eec528433d96b36520f2c4dd18da3"},"id":"kfqracqrojdf1"}},"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2nvpn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":63}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3qziiu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"keyser1884","can_mod_post":false,"created_utc":1752806799,"send_replies":true,"parent_id":"t3_1m2nvpn","score":19,"author_fullname":"t2_h0yti","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You have my full approbation. May your model be as sharp as a cutlass and twice as well-tempered.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3qziiu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You have my full approbation. May your model be as sharp as a cutlass and twice as well-tempered.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3qziiu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752806799,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2nvpn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":19}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3qpavd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Strange_Test7665","can_mod_post":false,"created_utc":1752803024,"send_replies":true,"parent_id":"t3_1m2nvpn","score":13,"author_fullname":"t2_t0zjq9mi","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"this is a really cool project.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3qpavd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;this is a really cool project.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3qpavd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752803024,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2nvpn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":13}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ubsw7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mtomas7","can_mod_post":false,"created_utc":1752856041,"send_replies":true,"parent_id":"t1_n3rojjc","score":2,"author_fullname":"t2_gct10","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Looks like a great resource!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ubsw7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Looks like a great resource!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2nvpn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3ubsw7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752856041,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3rojjc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Beautiful-Maybe-7473","can_mod_post":false,"created_utc":1752817802,"send_replies":true,"parent_id":"t3_1m2nvpn","score":11,"author_fullname":"t2_b1zxundg","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Probably a larger corpus would help? e.g. https://github.com/mimno/ota ?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3rojjc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Probably a larger corpus would help? e.g. &lt;a href=\\"https://github.com/mimno/ota\\"&gt;https://github.com/mimno/ota&lt;/a&gt; ?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3rojjc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752817802,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2nvpn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3qmmgr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No_Efficiency_1144","can_mod_post":false,"created_utc":1752802043,"send_replies":true,"parent_id":"t3_1m2nvpn","score":10,"author_fullname":"t2_1nkj9l14b0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I really love it\\n\\n\\nI have read a lot of novels from this time/place and the following words jumped out at me as having the authentic vibe:\\n\\n\\nextensive, ground (instead of “land”), probable, Governor-General, disposed, subsistence, colonies, inhabitants, not less than,","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3qmmgr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I really love it&lt;/p&gt;\\n\\n&lt;p&gt;I have read a lot of novels from this time/place and the following words jumped out at me as having the authentic vibe:&lt;/p&gt;\\n\\n&lt;p&gt;extensive, ground (instead of “land”), probable, Governor-General, disposed, subsistence, colonies, inhabitants, not less than,&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3qmmgr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752802043,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2nvpn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3rucgo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AFAIX","can_mod_post":false,"created_utc":1752820835,"send_replies":true,"parent_id":"t3_1m2nvpn","score":10,"author_fullname":"t2_8i8xi","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Should train it on letters from that period, would be cool to have a letter writing model that outputs two pages worth of text every time","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3rucgo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Should train it on letters from that period, would be cool to have a letter writing model that outputs two pages worth of text every time&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3rucgo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752820835,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2nvpn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3rk6qj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Blizado","can_mod_post":false,"created_utc":1752815633,"send_replies":true,"parent_id":"t3_1m2nvpn","score":7,"author_fullname":"t2_j0e2r","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think your approach would be also good for medieval roleplays, where newer knowledge inside the LLM would be also not the best. But maybe there is simply a not enough training material problem to make a good enough model out of it. Maybe with selected synthetic training material.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3rk6qj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think your approach would be also good for medieval roleplays, where newer knowledge inside the LLM would be also not the best. But maybe there is simply a not enough training material problem to make a good enough model out of it. Maybe with selected synthetic training material.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3rk6qj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752815633,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2nvpn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3rsppi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3qycvp","score":11,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"4060 is the closest (288 Gb/sec) in bandwidth to the Babbage Engine among all modern videocards.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3rsppi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;4060 is the closest (288 Gb/sec) in bandwidth to the Babbage Engine among all modern videocards.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2nvpn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3rsppi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752819966,"author_flair_text":null,"treatment_tags":[],"created_utc":1752819966,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}}],"before":null}},"user_reports":[],"saved":false,"id":"n3qycvp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ready_to_fuck_yeahh","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3quuv7","score":9,"author_fullname":"t2_j4lnwvga2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Got it, checked his github: GPU: Geforce rtx 4060 CPU: i5-13400F Ram: 16GB DDR5.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3qycvp","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Got it, checked his github: GPU: Geforce rtx 4060 CPU: i5-13400F Ram: 16GB DDR5.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2nvpn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3qycvp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752806360,"author_flair_text":null,"treatment_tags":[],"created_utc":1752806360,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3r8a6x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"lordlestar","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3quuv7","score":7,"author_fullname":"t2_g0eyh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Using an Ada Lovelace nvidia card","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3r8a6x","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Using an Ada Lovelace nvidia card&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2nvpn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3r8a6x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752810301,"author_flair_text":null,"treatment_tags":[],"created_utc":1752810301,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}}],"before":null}},"user_reports":[],"saved":false,"id":"n3quuv7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"BestUsernameLeft","can_mod_post":false,"created_utc":1752805049,"send_replies":true,"parent_id":"t1_n3qr5pz","score":46,"author_fullname":"t2_uk9jh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Probably the Babbage Engine, for period authenticity.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3quuv7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Probably the Babbage Engine, for period authenticity.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2nvpn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3quuv7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752805049,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":46}}],"before":null}},"user_reports":[],"saved":false,"id":"n3qr5pz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ready_to_fuck_yeahh","can_mod_post":false,"created_utc":1752803702,"send_replies":true,"parent_id":"t3_1m2nvpn","score":4,"author_fullname":"t2_j4lnwvga2","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What's the hardware?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3qr5pz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What&amp;#39;s the hardware?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3qr5pz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752803702,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2nvpn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3sd4sn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"R1skM4tr1x","can_mod_post":false,"created_utc":1752831416,"send_replies":true,"parent_id":"t3_1m2nvpn","score":5,"author_fullname":"t2_8k23emmh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The fifty,000 is an interesting and logical mistake, cool project!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3sd4sn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The fifty,000 is an interesting and logical mistake, cool project!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3sd4sn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752831416,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2nvpn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3s31x6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Patentsmatter","can_mod_post":false,"created_utc":1752825633,"send_replies":true,"parent_id":"t3_1m2nvpn","score":4,"author_fullname":"t2_cocl8roo","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt; There was a large and extensive supply of ground from the North of England.\\n\\nWhy use drugs any longer? Sentences like these, and meditation, allow to reach alien spheres of consciousness.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3s31x6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;There was a large and extensive supply of ground from the North of England.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Why use drugs any longer? Sentences like these, and meditation, allow to reach alien spheres of consciousness.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3s31x6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752825633,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2nvpn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ul9si","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Marans","can_mod_post":false,"created_utc":1752858706,"send_replies":true,"parent_id":"t1_n3tyfjz","score":2,"author_fullname":"t2_iwt0i","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Those books still have copyright. Can't reasonably get the book. earliest you can go currently is probably 1930, because all of that are copyright free (like Winnie puh and steamboat Willy, that's why there are now Horrorfilms with them)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ul9si","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Those books still have copyright. Can&amp;#39;t reasonably get the book. earliest you can go currently is probably 1930, because all of that are copyright free (like Winnie puh and steamboat Willy, that&amp;#39;s why there are now Horrorfilms with them)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2nvpn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3ul9si/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752858706,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3tyfjz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"whatstheprobability","can_mod_post":false,"created_utc":1752852269,"send_replies":true,"parent_id":"t3_1m2nvpn","score":3,"author_fullname":"t2_a3rta2","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If it turns out there isn't enough data for 1875, it would still be interesting to do this for more recent years. Even something like 50 years ago in 1975 would be interesting.  Has anyone already done this?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3tyfjz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If it turns out there isn&amp;#39;t enough data for 1875, it would still be interesting to do this for more recent years. Even something like 50 years ago in 1975 would be interesting.  Has anyone already done this?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3tyfjz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752852269,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2nvpn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3vi5ff","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"nivvis","can_mod_post":false,"created_utc":1752868225,"send_replies":true,"parent_id":"t3_1m2nvpn","score":3,"author_fullname":"t2_39blx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I’ve OCR’ed ~30k pages of the 1910 Encyclopedia Britannica if you’ve any interest in trying it out. A bit past your time period.\\n\\nCool idea — was thinking about doing the same with this encyclopedia.\\n\\nEdit: just picked up a physical, digital copy of ~1875 that may be of use.\\n\\nFound an interesting older project that cites 5b tokens — I don’t see them releasing the dataset anywhere. Maybe you can distill data from it though in order to aid in pretraining?\\nhttps://github.com/Living-with-machines/histLM/","edited":1752869208,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3vi5ff","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I’ve OCR’ed ~30k pages of the 1910 Encyclopedia Britannica if you’ve any interest in trying it out. A bit past your time period.&lt;/p&gt;\\n\\n&lt;p&gt;Cool idea — was thinking about doing the same with this encyclopedia.&lt;/p&gt;\\n\\n&lt;p&gt;Edit: just picked up a physical, digital copy of ~1875 that may be of use.&lt;/p&gt;\\n\\n&lt;p&gt;Found an interesting older project that cites 5b tokens — I don’t see them releasing the dataset anywhere. Maybe you can distill data from it though in order to aid in pretraining?\\n&lt;a href=\\"https://github.com/Living-with-machines/histLM/\\"&gt;https://github.com/Living-with-machines/histLM/&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3vi5ff/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752868225,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2nvpn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3r9fzw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"offlinesir","can_mod_post":false,"created_utc":1752810783,"send_replies":true,"parent_id":"t3_1m2nvpn","score":4,"author_fullname":"t2_jn5ft2le","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"GGUF when? /s","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3r9fzw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;GGUF when? /s&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3r9fzw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752810783,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2nvpn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3rx7z0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Pristine_Pick823","can_mod_post":false,"created_utc":1752822374,"send_replies":true,"parent_id":"t3_1m2nvpn","score":2,"author_fullname":"t2_ap8rfz5j","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Awesome project! Keep it up and thanks for the update.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3rx7z0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Awesome project! Keep it up and thanks for the update.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3rx7z0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752822374,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2nvpn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3sf61t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jetaudio","can_mod_post":false,"created_utc":1752832530,"send_replies":true,"parent_id":"t3_1m2nvpn","score":2,"author_fullname":"t2_5plbh7ia","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Maybe you can overfit the model on the dataset. And use bigger model too.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3sf61t","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Maybe you can overfit the model on the dataset. And use bigger model too.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3sf61t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752832530,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2nvpn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3sraic","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ArcaneThoughts","can_mod_post":false,"created_utc":1752838382,"send_replies":true,"parent_id":"t3_1m2nvpn","score":2,"author_fullname":"t2_hgivzvub","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Would be interesting to compare against a modern LLM (around 1b maybe) and also against the same one fine tuned with the same dataset.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3sraic","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Would be interesting to compare against a modern LLM (around 1b maybe) and also against the same one fine tuned with the same dataset.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3sraic/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752838382,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2nvpn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3syyuy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DaoDeDickinson","can_mod_post":false,"created_utc":1752841395,"send_replies":true,"parent_id":"t3_1m2nvpn","score":2,"author_fullname":"t2_dujyi","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Can anyone develop an AI that could learn the game of Obligationes and then teach it to someone else and play it with someone? Can anyone develop a child that could learn the game of Obligationes and then teach it to someone else and play it with someone?\\n\\n... sigh","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3syyuy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Can anyone develop an AI that could learn the game of Obligationes and then teach it to someone else and play it with someone? Can anyone develop a child that could learn the game of Obligationes and then teach it to someone else and play it with someone?&lt;/p&gt;\\n\\n&lt;p&gt;... sigh&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3syyuy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752841395,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2nvpn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3u6zuo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mtomas7","can_mod_post":false,"created_utc":1752854665,"send_replies":true,"parent_id":"t3_1m2nvpn","score":2,"author_fullname":"t2_gct10","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"BTW, this is a similar project, but the author is using a finetuning option instead: [https://www.reddit.com/r/LocalLLaMA/comments/1m1s7w9/regency\\\\_bewildered\\\\_is\\\\_a\\\\_stylistic\\\\_persona\\\\_imprint/](https://www.reddit.com/r/LocalLLaMA/comments/1m1s7w9/regency_bewildered_is_a_stylistic_persona_imprint/)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3u6zuo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;BTW, this is a similar project, but the author is using a finetuning option instead: &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1m1s7w9/regency_bewildered_is_a_stylistic_persona_imprint/\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1m1s7w9/regency_bewildered_is_a_stylistic_persona_imprint/&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3u6zuo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752854665,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2nvpn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"2ff18162-05ce-11ee-aa52-6a828e39b56c","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3uanob","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CosmosisQ","can_mod_post":false,"created_utc":1752855713,"send_replies":true,"parent_id":"t3_1m2nvpn","score":1,"author_fullname":"t2_derpe","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt; It's no where near an LLM right now, more like a sentence generator [...]\\n\\nIt might not be an LLM, but with its 16 million parameters, it is certainly a rather Large Model of Language!\\n\\n(An LML, if you will.)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3uanob","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Orca"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;It&amp;#39;s no where near an LLM right now, more like a sentence generator [...]&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;It might not be an LLM, but with its 16 million parameters, it is certainly a rather Large Model of Language!&lt;/p&gt;\\n\\n&lt;p&gt;(An LML, if you will.)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2nvpn/training_an_llm_only_on_books_from_the_1800s/n3uanob/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752855713,"author_flair_text":"Orca","treatment_tags":[],"link_id":"t3_1m2nvpn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),o=()=>e.jsx(l,{data:a});export{o as default};
