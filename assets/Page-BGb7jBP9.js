import{j as e}from"./index-C_z07ZVC.js";import{R as t}from"./RedditPostRenderer-DPnSR41P.js";import"./index-DKzOAewW.js";const a=JSON.parse('[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Need help, I am running a series of full fine-tuning on Llama 2 7B hf with unsloth. For some time, it was working just fine, and then this happened. I didn\'t notice until after the training was completed. I was sure of the training script because I had previously executed it with a slightly different setting (I modified how many checkpoints to save), and it was running with no problem at all. I ran all the trainings on the same GPU card, RTX A6000.\\n\\nRun A\\n\\n[Run A](https://preview.redd.it/1jvncyciv1bf1.png?width=1801&amp;format=png&amp;auto=webp&amp;s=c52ffc1ddf585adfb0690dbffaac99b58017d7c8)\\n\\nRun B\\n\\n[Run B](https://preview.redd.it/5myd8r0lv1bf1.png?width=1808&amp;format=png&amp;auto=webp&amp;s=74cda16524182268aa8c358f7e55744774ecd97e)\\n\\nOn some other models (this one with Gemma), after some time with the same script it returns this error:  \\n`/tmp/torchinductor_user/ey/cey6r66b2emihdiuktnmimfzgbacyvafuvx2vlr4kpbmybs2o63r.py:45: unknown: block: [0,0,0], thread: [5,0,0] Assertion \\\\`index out of bounds: 0 &lt;= tmp8 &lt; ks0\\\\` failed.`\\n\\nI suppose that can be what caused the grad norm to become 0 in the llama model? Currently, I have no other clue outside of this.\\n\\nHere are the parameters that I am using:\\n\\n                per_device_train_batch_size = 1,\\n                gradient_accumulation_steps = 16,\\n                learning_rate = 5e-5,\\n                lr_scheduler_type = \\"linear\\",\\n                embedding_learning_rate = 1e-5,\\n                warmup_ratio = 0.1,\\n                epochs = 1,\\n                fp16 = not is_bfloat16_supported(),\\n                bf16 = is_bfloat16_supported(),\\n                optim = \\"adamw_8bit\\",\\n                weight_decay = 0.01,\\n                seed = 3407,\\n                logging_steps = 1,\\n                report_to = \\"wandb\\",\\n                output_dir = output_path,\\n                save_strategy=\\"steps\\",\\n                save_steps=total_steps // 10,\\n                save_total_limit=11,\\n                save_safetensors=True,\\n\\nThe difference between run A and run B is the number of layers trained. I am training multiple models with each different number of unfrozen layers. So for some reason, the ones with high trainable parameter counts always fail this way. How can I debug this and what might\'ve caused this? Any suggestions/helps would be greatly appreciated! Thank you","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Why do grad norm sink to 0 (at least I think) randomly during unsloth full finetuning?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":42,"top_awarded_type":null,"hide_score":false,"media_metadata":{"5myd8r0lv1bf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":32,"x":108,"u":"https://preview.redd.it/5myd8r0lv1bf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1d144ade08b2c96f586214413c11297979ec8d4d"},{"y":65,"x":216,"u":"https://preview.redd.it/5myd8r0lv1bf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8bfda56b2d1181a124b4a1dd8749cd0962a070fb"},{"y":97,"x":320,"u":"https://preview.redd.it/5myd8r0lv1bf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6c3a7f9a8335b8c1a04f2da73b9dd10be57dddb2"},{"y":195,"x":640,"u":"https://preview.redd.it/5myd8r0lv1bf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=422d6eb34940d5d70a88cf024964863027609079"},{"y":292,"x":960,"u":"https://preview.redd.it/5myd8r0lv1bf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2c87aaefafa376baf550f1c602538410113e2eff"},{"y":329,"x":1080,"u":"https://preview.redd.it/5myd8r0lv1bf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7a6f69250b0e96031240201ba666fb27ad9f4f22"}],"s":{"y":551,"x":1808,"u":"https://preview.redd.it/5myd8r0lv1bf1.png?width=1808&amp;format=png&amp;auto=webp&amp;s=74cda16524182268aa8c358f7e55744774ecd97e"},"id":"5myd8r0lv1bf1"},"1jvncyciv1bf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":32,"x":108,"u":"https://preview.redd.it/1jvncyciv1bf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7ddc391d0cb5045ec9b8a02bbabdc84c80f399bb"},{"y":65,"x":216,"u":"https://preview.redd.it/1jvncyciv1bf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e5ee500470b397366b447b7782723b17a9523f7f"},{"y":97,"x":320,"u":"https://preview.redd.it/1jvncyciv1bf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=23533cd9aae59ca64b2508d127fea3b8ed93cd79"},{"y":195,"x":640,"u":"https://preview.redd.it/1jvncyciv1bf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=61320140882f8e401cdf9b01be3a1c9099a81435"},{"y":293,"x":960,"u":"https://preview.redd.it/1jvncyciv1bf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=94333bdb4b621043395fab513635aac40e82a221"},{"y":329,"x":1080,"u":"https://preview.redd.it/1jvncyciv1bf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c232f09941d21e18e262241ac45390eada31dc2b"}],"s":{"y":550,"x":1801,"u":"https://preview.redd.it/1jvncyciv1bf1.png?width=1801&amp;format=png&amp;auto=webp&amp;s=c52ffc1ddf585adfb0690dbffaac99b58017d7c8"},"id":"1jvncyciv1bf1"}},"name":"t3_1ls91w3","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.67,"author_flair_background_color":null,"subreddit_type":"public","ups":1,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1hv8pwe507","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":1,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://a.thumbs.redditmedia.com/-WaLKKKHOW63KWzApJwmrCQ0HG8GbQsikPkjj1sphR8.jpg","edited":1751722984,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751719489,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Need help, I am running a series of full fine-tuning on Llama 2 7B hf with unsloth. For some time, it was working just fine, and then this happened. I didn&amp;#39;t notice until after the training was completed. I was sure of the training script because I had previously executed it with a slightly different setting (I modified how many checkpoints to save), and it was running with no problem at all. I ran all the trainings on the same GPU card, RTX A6000.&lt;/p&gt;\\n\\n&lt;p&gt;Run A&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/1jvncyciv1bf1.png?width=1801&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c52ffc1ddf585adfb0690dbffaac99b58017d7c8\\"&gt;Run A&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Run B&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/5myd8r0lv1bf1.png?width=1808&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=74cda16524182268aa8c358f7e55744774ecd97e\\"&gt;Run B&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;On some other models (this one with Gemma), after some time with the same script it returns this error:&lt;br/&gt;\\n&lt;code&gt;/tmp/torchinductor_user/ey/cey6r66b2emihdiuktnmimfzgbacyvafuvx2vlr4kpbmybs2o63r.py:45: unknown: block: [0,0,0], thread: [5,0,0] Assertion \\\\&lt;/code&gt;index out of bounds: 0 &amp;lt;= tmp8 &amp;lt; ks0` failed.`&lt;/p&gt;\\n\\n&lt;p&gt;I suppose that can be what caused the grad norm to become 0 in the llama model? Currently, I have no other clue outside of this.&lt;/p&gt;\\n\\n&lt;p&gt;Here are the parameters that I am using:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;            per_device_train_batch_size = 1,\\n            gradient_accumulation_steps = 16,\\n            learning_rate = 5e-5,\\n            lr_scheduler_type = &amp;quot;linear&amp;quot;,\\n            embedding_learning_rate = 1e-5,\\n            warmup_ratio = 0.1,\\n            epochs = 1,\\n            fp16 = not is_bfloat16_supported(),\\n            bf16 = is_bfloat16_supported(),\\n            optim = &amp;quot;adamw_8bit&amp;quot;,\\n            weight_decay = 0.01,\\n            seed = 3407,\\n            logging_steps = 1,\\n            report_to = &amp;quot;wandb&amp;quot;,\\n            output_dir = output_path,\\n            save_strategy=&amp;quot;steps&amp;quot;,\\n            save_steps=total_steps // 10,\\n            save_total_limit=11,\\n            save_safetensors=True,\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;The difference between run A and run B is the number of layers trained. I am training multiple models with each different number of unfrozen layers. So for some reason, the ones with high trainable parameter counts always fail this way. How can I debug this and what might&amp;#39;ve caused this? Any suggestions/helps would be greatly appreciated! Thank you&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1ls91w3","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Old-Acanthisitta-574","discussion_type":null,"num_comments":0,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1ls91w3/why_do_grad_norm_sink_to_0_at_least_i_think/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1ls91w3/why_do_grad_norm_sink_to_0_at_least_i_think/","subreddit_subscribers":494897,"created_utc":1751719489,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[],"before":null}}]'),s=()=>e.jsx(t,{data:a});export{s as default};
