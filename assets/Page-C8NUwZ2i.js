import{j as e}from"./index-F0NXdzZX.js";import{R as t}from"./RedditPostRenderer-CoEZB1dt.js";import"./index-DrtravXm.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I've worked about 7 years in software development companies, and it's \\"easy\\" to be a software/backend/web developer because we use tools/frameworks/libs that are mature and battle-tested.\\n\\nProblem with Django? Update it, the bug was probably fixed ages ago.\\n\\nWith LLMs it's an absolute clusterfuck. You just bought an RTX 5090? Boom, you have to recompile everything to make it work with SM_120. And I'm skipping the hellish Ubuntu installation part with cursed headers just to get it running in degraded mode.\\n\\nExample from last week: vLLM implemented Dual Chunked Attention for Qwen 7B/14B 1M, THE ONLY (open weight) model that seriously handles long context.\\n\\n1. Unmerged bugfix that makes it **UNUSABLE** https://github.com/vllm-project/vllm/pull/19084\\n2. FP8 wasn't working, I had to make the PR myself https://github.com/vllm-project/vllm/pull/19420  \\n3. Some guy broke Dual Chunk attention because of CUDA kernel and division by zero, had to write another PR https://github.com/vllm-project/vllm/pull/20488\\n\\nHoly shit, I spend more time at the office hammering away at libraries than actually working on the project that's supposed to use these libraries.\\n\\nAm I going crazy or do you guys also notice this is a COMPLETE SHITSHOW????\\n\\nAnd I'm not even talking about the nightmare of having to use virtualized GPUs with NVIDIA GRID drivers that you can't download yourself and that EXPLODE at the slightest conflict: \\n\\n\`driver versions &lt;----&gt; torch version &lt;-----&gt; vLLM version\`\\n\\n\\n\\nIt's driving me insane.\\n\\nI don't understand how Ggerganov can keep working on llama.cpp every single day with no break and not turn INSANE.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Anyone else feel like working with LLM libs is like navigating a minefield ?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lrjy15","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.92,"author_flair_background_color":"#bbbdbf","subreddit_type":"public","ups":131,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","is_original_content":false,"author_fullname":"t2_152zyn72n4","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":131,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1751638923,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"richtext","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve worked about 7 years in software development companies, and it&amp;#39;s &amp;quot;easy&amp;quot; to be a software/backend/web developer because we use tools/frameworks/libs that are mature and battle-tested.&lt;/p&gt;\\n\\n&lt;p&gt;Problem with Django? Update it, the bug was probably fixed ages ago.&lt;/p&gt;\\n\\n&lt;p&gt;With LLMs it&amp;#39;s an absolute clusterfuck. You just bought an RTX 5090? Boom, you have to recompile everything to make it work with SM_120. And I&amp;#39;m skipping the hellish Ubuntu installation part with cursed headers just to get it running in degraded mode.&lt;/p&gt;\\n\\n&lt;p&gt;Example from last week: vLLM implemented Dual Chunked Attention for Qwen 7B/14B 1M, THE ONLY (open weight) model that seriously handles long context.&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;Unmerged bugfix that makes it &lt;strong&gt;UNUSABLE&lt;/strong&gt; &lt;a href=\\"https://github.com/vllm-project/vllm/pull/19084\\"&gt;https://github.com/vllm-project/vllm/pull/19084&lt;/a&gt;&lt;/li&gt;\\n&lt;li&gt;FP8 wasn&amp;#39;t working, I had to make the PR myself &lt;a href=\\"https://github.com/vllm-project/vllm/pull/19420\\"&gt;https://github.com/vllm-project/vllm/pull/19420&lt;/a&gt;&lt;br/&gt;&lt;/li&gt;\\n&lt;li&gt;Some guy broke Dual Chunk attention because of CUDA kernel and division by zero, had to write another PR &lt;a href=\\"https://github.com/vllm-project/vllm/pull/20488\\"&gt;https://github.com/vllm-project/vllm/pull/20488&lt;/a&gt;&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;Holy shit, I spend more time at the office hammering away at libraries than actually working on the project that&amp;#39;s supposed to use these libraries.&lt;/p&gt;\\n\\n&lt;p&gt;Am I going crazy or do you guys also notice this is a COMPLETE SHITSHOW????&lt;/p&gt;\\n\\n&lt;p&gt;And I&amp;#39;m not even talking about the nightmare of having to use virtualized GPUs with NVIDIA GRID drivers that you can&amp;#39;t download yourself and that EXPLODE at the slightest conflict: &lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;driver versions &amp;lt;----&amp;gt; torch version &amp;lt;-----&amp;gt; vLLM version&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s driving me insane.&lt;/p&gt;\\n\\n&lt;p&gt;I don&amp;#39;t understand how Ggerganov can keep working on llama.cpp every single day with no break and not turn INSANE.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/L1dGEw1UUf0GHnpmf5uDpElgs8er0s6PxenyNFO_HEs.png?auto=webp&amp;s=d5a316e649c7301ee83fc9d79f029748b4b9c2ac","width":1200,"height":600},"resolutions":[{"url":"https://external-preview.redd.it/L1dGEw1UUf0GHnpmf5uDpElgs8er0s6PxenyNFO_HEs.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4129a80937cfe58e8199cc33db87b61d13901e85","width":108,"height":54},{"url":"https://external-preview.redd.it/L1dGEw1UUf0GHnpmf5uDpElgs8er0s6PxenyNFO_HEs.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ebcf0f7824053e06fbffb388aac640d0a5b5594d","width":216,"height":108},{"url":"https://external-preview.redd.it/L1dGEw1UUf0GHnpmf5uDpElgs8er0s6PxenyNFO_HEs.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fa4d4757f190695ea42fdb147abc8832a8641249","width":320,"height":160},{"url":"https://external-preview.redd.it/L1dGEw1UUf0GHnpmf5uDpElgs8er0s6PxenyNFO_HEs.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=92338135c3a069a62034d2d9464d3910f7b05699","width":640,"height":320},{"url":"https://external-preview.redd.it/L1dGEw1UUf0GHnpmf5uDpElgs8er0s6PxenyNFO_HEs.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8e9ef1e665e533340d9acd1ec25e7f92fdbf8648","width":960,"height":480},{"url":"https://external-preview.redd.it/L1dGEw1UUf0GHnpmf5uDpElgs8er0s6PxenyNFO_HEs.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a4a3e58f7cad83a6b34458387ae70bc5ad29c244","width":1080,"height":540}],"variants":{},"id":"L1dGEw1UUf0GHnpmf5uDpElgs8er0s6PxenyNFO_HEs"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":"llama.cpp","treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lrjy15","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"LinkSea8324","discussion_type":null,"num_comments":42,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":"light","permalink":"/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/","subreddit_subscribers":494987,"created_utc":1751638923,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"total_awards_received":0,"approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"ups":42,"removal_reason":null,"link_id":"t3_1lrjy15","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"total_awards_received":0,"approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"ups":5,"removal_reason":null,"link_id":"t3_1lrjy15","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1egnmt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"TheTerrasque","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1e6hu9","score":0,"author_fullname":"t2_9uv8v","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt; It's not fool-proof. I don't remember which library / framework it was (maybe LlamaFactory?), but I've hit the issue where I've used a commit hash and it still broke (probably due to some force-push or rebase?).\\n\\nAny rebase or force-push would change the commit hash. And no, it's not foolproof still. Usually it's because a dependency isn't exactly version locked or *they* do shenanigans like that. *Edit: Or the commit doesn't exist any more for some reason*\\n\\n&gt; But I'm curious, where did you see this?\\n\\nI don't remember specifics, but I've seen it enough times to go *\\"aye, that happens now and then. Target the git commit or have an offline package / copy in own repo\\"*","edited":1751678486,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1egnmt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;It&amp;#39;s not fool-proof. I don&amp;#39;t remember which library / framework it was (maybe LlamaFactory?), but I&amp;#39;ve hit the issue where I&amp;#39;ve used a commit hash and it still broke (probably due to some force-push or rebase?).&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Any rebase or force-push would change the commit hash. And no, it&amp;#39;s not foolproof still. Usually it&amp;#39;s because a dependency isn&amp;#39;t exactly version locked or &lt;em&gt;they&lt;/em&gt; do shenanigans like that. &lt;em&gt;Edit: Or the commit doesn&amp;#39;t exist any more for some reason&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;But I&amp;#39;m curious, where did you see this?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I don&amp;#39;t remember specifics, but I&amp;#39;ve seen it enough times to go &lt;em&gt;&amp;quot;aye, that happens now and then. Target the git commit or have an offline package / copy in own repo&amp;quot;&lt;/em&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrjy15","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/n1egnmt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751678268,"author_flair_text":null,"treatment_tags":[],"created_utc":1751678268,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n1e6hu9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"DELETED","no_follow":false,"author":"[deleted]","can_mod_post":false,"created_utc":1751673999,"send_replies":true,"parent_id":"t1_n1e26mc","score":5,"approved_by":null,"report_reasons":null,"all_awardings":[],"subreddit_id":"t5_81eyvm","body":"[deleted]","edited":false,"author_flair_css_class":null,"downs":0,"is_submitter":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;[deleted]&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"associated_award":null,"stickied":false,"subreddit_type":"public","can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/n1e6hu9/","num_reports":null,"locked":false,"name":"t1_n1e6hu9","created":1751673999,"subreddit":"LocalLLaMA","author_flair_text":null,"treatment_tags":[],"collapsed":true,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"","collapsed_because_crowd_control":null,"mod_reports":[],"mod_note":null,"distinguished":null}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1eh17q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"TheTerrasque","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1eg5fw","score":0,"author_fullname":"t2_9uv8v","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I mean, yes it'll break if they remove that commit from the git repo, but then you're on your way down the river of shit with no paddle already. At least you *know* now that it happened and any and all assumptions are null and void","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1eh17q","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I mean, yes it&amp;#39;ll break if they remove that commit from the git repo, but then you&amp;#39;re on your way down the river of shit with no paddle already. At least you &lt;em&gt;know&lt;/em&gt; now that it happened and any and all assumptions are null and void&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrjy15","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/n1eh17q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751678427,"author_flair_text":null,"treatment_tags":[],"created_utc":1751678427,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n1eg5fw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"tipherr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1e26mc","score":3,"author_fullname":"t2_vd3ib31i","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The kind of repo that doesn't step a version for a major change is also the kind of repo that a rebase will blow this method up.\\n\\nIt's a 'fix', but only until it doesn't work. -which is ironically the exact same landmine the op originally stepped on.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1eg5fw","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The kind of repo that doesn&amp;#39;t step a version for a major change is also the kind of repo that a rebase will blow this method up.&lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s a &amp;#39;fix&amp;#39;, but only until it doesn&amp;#39;t work. -which is ironically the exact same landmine the op originally stepped on.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrjy15","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/n1eg5fw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751678051,"author_flair_text":null,"treatment_tags":[],"created_utc":1751678051,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n1e26mc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"TheTerrasque","can_mod_post":false,"created_utc":1751672307,"send_replies":true,"parent_id":"t1_n1brptn","score":-1,"author_fullname":"t2_9uv8v","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; vLLM re-released version v0.3.3 with &gt;5 extra commits, one of which removed support for LoRA (punica?) kernels on V100 GPUs. \\n\\n\\nYou can target specific git commit hashes\\n\\n\\n&gt;  I have never seen this thing happen in my ~10 years of programming. \\n\\n\\nI have.. which is why I know you can target git commit hashes, and do if I want it to be super stable","edited":1751672853,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1e26mc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;vLLM re-released version v0.3.3 with &amp;gt;5 extra commits, one of which removed support for LoRA (punica?) kernels on V100 GPUs. &lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;You can target specific git commit hashes&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;  I have never seen this thing happen in my ~10 years of programming. &lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I have.. which is why I know you can target git commit hashes, and do if I want it to be super stable&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrjy15","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/n1e26mc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751672307,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1brptn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"DELETED","no_follow":false,"author":"[deleted]","can_mod_post":false,"send_replies":true,"parent_id":"t3_1lrjy15","score":42,"approved_by":null,"report_reasons":null,"all_awardings":[],"subreddit_id":"t5_81eyvm","body":"[deleted]","edited":1751645394,"downs":0,"author_flair_css_class":null,"collapsed":true,"is_submitter":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;[deleted]&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"associated_award":null,"stickied":false,"subreddit_type":"public","can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/n1brptn/","num_reports":null,"locked":false,"name":"t1_n1brptn","created":1751645058,"subreddit":"LocalLLaMA","author_flair_text":null,"treatment_tags":[],"created_utc":1751645058,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"","collapsed_because_crowd_control":null,"mod_reports":[],"mod_note":null,"distinguished":null}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1cidq8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Chromix_","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1cd7x8","score":2,"author_fullname":"t2_k7w2h","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes, the shorter the better. Restricting to 32k also means more compatibility with different models.\\n\\nIt's interesting though that Gemini performed slightly worse in the OpenAI MRCR test - which just \\"just\\" a Needle in Haystack retrieval variant, whereas fiction.liveBench requires making connections across the context to find the desired answers. Maybe that's just within the noise margins of those benchmarks though.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1cidq8","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, the shorter the better. Restricting to 32k also means more compatibility with different models.&lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s interesting though that Gemini performed slightly worse in the OpenAI MRCR test - which just &amp;quot;just&amp;quot; a Needle in Haystack retrieval variant, whereas fiction.liveBench requires making connections across the context to find the desired answers. Maybe that&amp;#39;s just within the noise margins of those benchmarks though.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrjy15","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/n1cidq8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751653174,"author_flair_text":null,"treatment_tags":[],"created_utc":1751653174,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1cd7x8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Agreeable-Market-692","can_mod_post":false,"created_utc":1751651561,"send_replies":true,"parent_id":"t1_n1be5jv","score":11,"author_fullname":"t2_ajuhoi00","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm firmly in the camp of 'find ways to keep your queries less than 32k tokens using tool calls on chunked data' because the truth is not even Gemini handles contexts longer than that well. \\n\\n[https://www.reddit.com/r/Bard/comments/1k25zfy/gemini\\\\_25\\\\_results\\\\_on\\\\_openaimrcr\\\\_long\\\\_context/](https://www.reddit.com/r/Bard/comments/1k25zfy/gemini_25_results_on_openaimrcr_long_context/)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1cd7x8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m firmly in the camp of &amp;#39;find ways to keep your queries less than 32k tokens using tool calls on chunked data&amp;#39; because the truth is not even Gemini handles contexts longer than that well. &lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.reddit.com/r/Bard/comments/1k25zfy/gemini_25_results_on_openaimrcr_long_context/\\"&gt;https://www.reddit.com/r/Bard/comments/1k25zfy/gemini_25_results_on_openaimrcr_long_context/&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrjy15","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/n1cd7x8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751651561,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1bja4o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Chromix_","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1bfvtj","score":4,"author_fullname":"t2_k7w2h","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes, there are only a few models that are relatively VRAM-efficient at longer context sizes. I haven't found one so far that provides an answer quality (or instruction-following) at 128k like it does at 4k. According to fiction.liveBench the only options for long context seem to be the API-only o3 and Gemini 2.5 pro, as well as the open Minimax-M1 which however requires quite a bit of VRAM and some optimized offloading to system RAM.\\n\\nI haven't tried gradientai/Llama-3-8B-Instruct-262 from your list yet. If the only complaint about it is that it's only speaking English then that'd be worth a try for me.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1bja4o","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, there are only a few models that are relatively VRAM-efficient at longer context sizes. I haven&amp;#39;t found one so far that provides an answer quality (or instruction-following) at 128k like it does at 4k. According to fiction.liveBench the only options for long context seem to be the API-only o3 and Gemini 2.5 pro, as well as the open Minimax-M1 which however requires quite a bit of VRAM and some optimized offloading to system RAM.&lt;/p&gt;\\n\\n&lt;p&gt;I haven&amp;#39;t tried gradientai/Llama-3-8B-Instruct-262 from your list yet. If the only complaint about it is that it&amp;#39;s only speaking English then that&amp;#39;d be worth a try for me.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrjy15","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/n1bja4o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751642532,"author_flair_text":null,"treatment_tags":[],"created_utc":1751642532,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n1bfvtj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"LinkSea8324","can_mod_post":false,"created_utc":1751641511,"send_replies":true,"parent_id":"t1_n1be5jv","score":23,"author_fullname":"t2_152zyn72n4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Most of the good working models on this benchmark are not open weighted or requiring a lot of VRAM.\\n\\nWe re-ran tests today with : \\n\\n\\n* Llama-3.1-Nemotron-8B-UltraLong-1M-Instruct : Not following instructions correctly\\n* gradientai/Llama-3-8B-Instruct-262k following instructions but struggles to speak anything else than english\\n* 01-ai/Yi-9B-200K byebye template chat\\n* phi-3 128k not enough vram for 128k context\\n* Menlo/Jan-nano-128k really meh result, not following instructions correctly\\n* aws-prototyping/MegaBeam-Mistral-7B-512k same issues as above\\n\\n\\nAlways with vLLM","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1bfvtj","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Most of the good working models on this benchmark are not open weighted or requiring a lot of VRAM.&lt;/p&gt;\\n\\n&lt;p&gt;We re-ran tests today with : &lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Llama-3.1-Nemotron-8B-UltraLong-1M-Instruct : Not following instructions correctly&lt;/li&gt;\\n&lt;li&gt;gradientai/Llama-3-8B-Instruct-262k following instructions but struggles to speak anything else than english&lt;/li&gt;\\n&lt;li&gt;01-ai/Yi-9B-200K byebye template chat&lt;/li&gt;\\n&lt;li&gt;phi-3 128k not enough vram for 128k context&lt;/li&gt;\\n&lt;li&gt;Menlo/Jan-nano-128k really meh result, not following instructions correctly&lt;/li&gt;\\n&lt;li&gt;aws-prototyping/MegaBeam-Mistral-7B-512k same issues as above&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Always with vLLM&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrjy15","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/n1bfvtj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751641511,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":23}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1ds0js","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Commercial-Celery769","can_mod_post":false,"created_utc":1751668468,"send_replies":true,"parent_id":"t1_n1be5jv","score":2,"author_fullname":"t2_zws5yqyow","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Your right about bleeding edge on everything AI, I mean take wan 2.1 training for example, there is very limited info on how to actually train good LORA'S because people gatekeep for whatever reason and because its still pretty new (yes several months old in AI time is like years but whatever.) Ive learned all I know from trail and error with 500k+ token chats with gemini 2.5 pro, none of which is code, and some random guy on civit AI. Ive noticed this from just constantly experimenting with wan 2.1 training ever since it launched, the people who have the best info on training are those random creators on civit and not even large creators. Also automagic optimiser is incredibly good in my experience, no more manually pausing runs to force a new LR when things stagnate. ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ds0js","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Your right about bleeding edge on everything AI, I mean take wan 2.1 training for example, there is very limited info on how to actually train good LORA&amp;#39;S because people gatekeep for whatever reason and because its still pretty new (yes several months old in AI time is like years but whatever.) Ive learned all I know from trail and error with 500k+ token chats with gemini 2.5 pro, none of which is code, and some random guy on civit AI. Ive noticed this from just constantly experimenting with wan 2.1 training ever since it launched, the people who have the best info on training are those random creators on civit and not even large creators. Also automagic optimiser is incredibly good in my experience, no more manually pausing runs to force a new LR when things stagnate. &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrjy15","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/n1ds0js/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751668468,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1be5jv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Chromix_","can_mod_post":false,"created_utc":1751640989,"send_replies":true,"parent_id":"t3_1lrjy15","score":56,"author_fullname":"t2_k7w2h","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt;I spend more time at the office hammering away at libraries than actually working on the project that's supposed to use these libraries.\\n\\nYou're living on the bleeding edge - of a field that's moving forward at the speed of light, with some people contributing code whose main profession isn't software engineering. What you experience is what life is like in the place where you chose to be. Thanks for your contributions that improve things.\\n\\n&gt;Qwen 7B/14B 1M, THE ONLY (open weight) model that seriously handles long context\\n\\nFrom my not-that-extensive tests it doesn't seem to me that it even handles 160k context that well. But it's not been tested with [fiction.liveBench](https://fiction.live/stories/Fiction-liveBench-May-06-2025/oQdzQvKHw8JyXbN87) yet. Minimax-M1 seems to handle long context rather well - for an open model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1be5jv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;I spend more time at the office hammering away at libraries than actually working on the project that&amp;#39;s supposed to use these libraries.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;You&amp;#39;re living on the bleeding edge - of a field that&amp;#39;s moving forward at the speed of light, with some people contributing code whose main profession isn&amp;#39;t software engineering. What you experience is what life is like in the place where you chose to be. Thanks for your contributions that improve things.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Qwen 7B/14B 1M, THE ONLY (open weight) model that seriously handles long context&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;From my not-that-extensive tests it doesn&amp;#39;t seem to me that it even handles 160k context that well. But it&amp;#39;s not been tested with &lt;a href=\\"https://fiction.live/stories/Fiction-liveBench-May-06-2025/oQdzQvKHw8JyXbN87\\"&gt;fiction.liveBench&lt;/a&gt; yet. Minimax-M1 seems to handle long context rather well - for an open model.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/n1be5jv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751640989,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrjy15","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":56}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1bttvc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"u_3WaD","can_mod_post":false,"created_utc":1751645701,"send_replies":true,"parent_id":"t3_1lrjy15","score":14,"author_fullname":"t2_v602iphq","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yes. This whole AI topic moves too fast to be focused on quality. Before you finish implementing one thing, a new one is already released by someone who wants to be a few % better than the others.\\n\\nI am not sure if it's even meant to be \\"*production-ready\\"*. I personally see it as one big race for the best beta features.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1bttvc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes. This whole AI topic moves too fast to be focused on quality. Before you finish implementing one thing, a new one is already released by someone who wants to be a few % better than the others.&lt;/p&gt;\\n\\n&lt;p&gt;I am not sure if it&amp;#39;s even meant to be &amp;quot;&lt;em&gt;production-ready&amp;quot;&lt;/em&gt;. I personally see it as one big race for the best beta features.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/n1bttvc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751645701,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrjy15","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":14}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1bvd1k","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Marksta","can_mod_post":false,"created_utc":1751646165,"send_replies":true,"parent_id":"t3_1lrjy15","score":10,"author_fullname":"t2_559a1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Touching anything relating to LLMs software is a full day endeavor at the least, weekend project more often. I've never seen anything like this either, the new age is bringing out new concepts. Lying readme's, like straight up the user created readme raffling off features and OS support that you find out in open issues and replies from the devs themselves they 100% do not support... yet.\\n\\nThen you got the ones that their github readme is literally nothing but their PR releases and self-accolades. Then you check it out and yeah, they did do those things and features, on that specific build. No, no that doesn't build today. But back then it did, and it totally did that thing. And nope, no releases and maybe not even build tags. Go find the commit that worked roughly by date of PR article, I guess.\\n\\nThe cutting edge has never been more sharp.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1bvd1k","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Touching anything relating to LLMs software is a full day endeavor at the least, weekend project more often. I&amp;#39;ve never seen anything like this either, the new age is bringing out new concepts. Lying readme&amp;#39;s, like straight up the user created readme raffling off features and OS support that you find out in open issues and replies from the devs themselves they 100% do not support... yet.&lt;/p&gt;\\n\\n&lt;p&gt;Then you got the ones that their github readme is literally nothing but their PR releases and self-accolades. Then you check it out and yeah, they did do those things and features, on that specific build. No, no that doesn&amp;#39;t build today. But back then it did, and it totally did that thing. And nope, no releases and maybe not even build tags. Go find the commit that worked roughly by date of PR article, I guess.&lt;/p&gt;\\n\\n&lt;p&gt;The cutting edge has never been more sharp.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/n1bvd1k/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751646165,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrjy15","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1bhw3t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"BidWestern1056","can_mod_post":false,"created_utc":1751642114,"send_replies":true,"parent_id":"t3_1lrjy15","score":19,"author_fullname":"t2_uzxql7po","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"| Holy shit, I spend more time at the office hammering away at libraries than actually working on the project that's supposed to use these libraries.\\n\\nthis is the \\"90% of my job is just x\\" of software engineering","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1bhw3t","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;| Holy shit, I spend more time at the office hammering away at libraries than actually working on the project that&amp;#39;s supposed to use these libraries.&lt;/p&gt;\\n\\n&lt;p&gt;this is the &amp;quot;90% of my job is just x&amp;quot; of software engineering&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/n1bhw3t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751642114,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrjy15","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":19}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1c8e6b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Homeschooled316","can_mod_post":false,"created_utc":1751650108,"send_replies":true,"parent_id":"t3_1lrjy15","score":6,"author_fullname":"t2_5jzui","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you can believe it, it was even worse before LLMs. We had versions of libraries like fastai with dependencies on nightly versions of torch that no longer exist, so simply restarting a cloud instance could break your stuff.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1c8e6b","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you can believe it, it was even worse before LLMs. We had versions of libraries like fastai with dependencies on nightly versions of torch that no longer exist, so simply restarting a cloud instance could break your stuff.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/n1c8e6b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751650108,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrjy15","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1bh685","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Ok_Cow1976","can_mod_post":false,"created_utc":1751641900,"send_replies":true,"parent_id":"t3_1lrjy15","score":7,"author_fullname":"t2_3pwbsmdr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Ggerganov and you all are our heroes!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1bh685","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ggerganov and you all are our heroes!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/n1bh685/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751641900,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrjy15","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"total_awards_received":0,"approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"ups":3,"removal_reason":null,"link_id":"t3_1lrjy15","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1cfvyx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"drulee","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1buc54","score":2,"author_fullname":"t2_5j652","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"E.g. I’ve found the Gunicorn python webserver experiencing memory leaks. We’ve set it to --max-requests 40 --max-requests-jitter 20 therefore and we’re not the only ones:\\n- https://github.com/benoitc/gunicorn/discussions/3042\\n\\n- https://www.reddit.com/r/django/comments/1az7z6a/django_gunicorn_do_you_guys_use_maxrequests_and/\\n\\nElse response time increases from 500ms to 650ms in load tests after 10 minutes with a few dozen worker threads.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1cfvyx","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;E.g. I’ve found the Gunicorn python webserver experiencing memory leaks. We’ve set it to --max-requests 40 --max-requests-jitter 20 therefore and we’re not the only ones:\\n- &lt;a href=\\"https://github.com/benoitc/gunicorn/discussions/3042\\"&gt;https://github.com/benoitc/gunicorn/discussions/3042&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;a href=\\"https://www.reddit.com/r/django/comments/1az7z6a/django_gunicorn_do_you_guys_use_maxrequests_and/\\"&gt;https://www.reddit.com/r/django/comments/1az7z6a/django_gunicorn_do_you_guys_use_maxrequests_and/&lt;/a&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Else response time increases from 500ms to 650ms in load tests after 10 minutes with a few dozen worker threads.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrjy15","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/n1cfvyx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751652380,"author_flair_text":null,"treatment_tags":[],"created_utc":1751652380,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1buc54","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"DELETED","no_follow":true,"author":"[deleted]","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1bny96","score":3,"approved_by":null,"report_reasons":null,"all_awardings":[],"subreddit_id":"t5_81eyvm","body":"[deleted]","edited":1751646067,"author_flair_css_class":null,"collapsed":true,"downs":0,"is_submitter":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;[deleted]&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"associated_award":null,"stickied":false,"subreddit_type":"public","can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/n1buc54/","num_reports":null,"locked":false,"name":"t1_n1buc54","created":1751645854,"subreddit":"LocalLLaMA","author_flair_text":null,"treatment_tags":[],"created_utc":1751645854,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"","collapsed_because_crowd_control":null,"mod_reports":[],"mod_note":null,"distinguished":null}}],"before":null}},"user_reports":[],"saved":false,"id":"n1bny96","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"vacationcelebration","can_mod_post":false,"created_utc":1751643926,"send_replies":true,"parent_id":"t3_1lrjy15","score":5,"author_fullname":"t2_106kea","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Definitely agree (I'm also patiently waiting for vllm PRs to be merged) but that's life at the bleeding edge. Also I think that python is the language of choice for many AI projects/servers is a huge downside with all the dependency issues and/or just plain bad implementation. Memory leaks, weird cuda errors, outdated requirements so old I can only run it in a docker image, the list is endless.\\n\\nBut hey, it's a constantly changing landscape, always new things to try out and discover. My job certainly won't get boring any time soon. Just more stressful lol.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1bny96","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Definitely agree (I&amp;#39;m also patiently waiting for vllm PRs to be merged) but that&amp;#39;s life at the bleeding edge. Also I think that python is the language of choice for many AI projects/servers is a huge downside with all the dependency issues and/or just plain bad implementation. Memory leaks, weird cuda errors, outdated requirements so old I can only run it in a docker image, the list is endless.&lt;/p&gt;\\n\\n&lt;p&gt;But hey, it&amp;#39;s a constantly changing landscape, always new things to try out and discover. My job certainly won&amp;#39;t get boring any time soon. Just more stressful lol.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/n1bny96/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751643926,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrjy15","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1c2s8v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"__SlimeQ__","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1bdf1c","score":7,"author_fullname":"t2_olbav","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"i mean, you're the dork talking standards in a python repo\\n\\nignoring standards is pythonic","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1c2s8v","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;i mean, you&amp;#39;re the dork talking standards in a python repo&lt;/p&gt;\\n\\n&lt;p&gt;ignoring standards is pythonic&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrjy15","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/n1c2s8v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751648427,"author_flair_text":null,"treatment_tags":[],"created_utc":1751648427,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1bqsyd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"lompocus","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1bdf1c","score":2,"author_fullname":"t2_f4boq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"honestly i wouldn't bother to check, if setting exif with libexif then i would hope the libexif manual itself would provide hints or else give me a warning when i tried to do something incorrect. be real, there are innumerable details that are only documented in source code databases or living xml-only documents these days.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1bqsyd","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;honestly i wouldn&amp;#39;t bother to check, if setting exif with libexif then i would hope the libexif manual itself would provide hints or else give me a warning when i tried to do something incorrect. be real, there are innumerable details that are only documented in source code databases or living xml-only documents these days.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrjy15","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/n1bqsyd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751644783,"author_flair_text":null,"treatment_tags":[],"created_utc":1751644783,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1bxrsf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"starkruzr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1bdf1c","score":0,"author_fullname":"t2_34g6p","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm just sitting at home by myself scrolling Reddit before lunch and when I tell you the fucking CACKLE I emitted reading this, lmao","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1bxrsf","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m just sitting at home by myself scrolling Reddit before lunch and when I tell you the fucking CACKLE I emitted reading this, lmao&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrjy15","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/n1bxrsf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751646908,"author_flair_text":null,"treatment_tags":[],"created_utc":1751646908,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n1bdf1c","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"LinkSea8324","can_mod_post":false,"created_utc":1751640766,"send_replies":true,"parent_id":"t1_n1b88n3","score":8,"author_fullname":"t2_152zyn72n4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Last month a zoomer sent me by mail the emojis \\"☝️🤓\\" after I told him the JEITA CP-3451 at page 36 didn't allow Orientation exif tag to be at 0","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1bdf1c","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Last month a zoomer sent me by mail the emojis &amp;quot;☝️🤓&amp;quot; after I told him the JEITA CP-3451 at page 36 didn&amp;#39;t allow Orientation exif tag to be at 0&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrjy15","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/n1bdf1c/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751640766,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}}],"before":null}},"user_reports":[],"saved":false,"id":"n1b88n3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"lompocus","can_mod_post":false,"created_utc":1751639177,"send_replies":true,"parent_id":"t3_1lrjy15","score":8,"author_fullname":"t2_f4boq","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"nobody knows the trouble i've seen... in trying to explain to ai developers that python pep#12345-i-forgot exists specifying package definitions. also stahp putting ur entire program into setup.py. somehow it has gotten worse, as thine sprained keyboard-fingers hath observe'd.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1b88n3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;nobody knows the trouble i&amp;#39;ve seen... in trying to explain to ai developers that python pep#12345-i-forgot exists specifying package definitions. also stahp putting ur entire program into setup.py. somehow it has gotten worse, as thine sprained keyboard-fingers hath observe&amp;#39;d.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/n1b88n3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751639177,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrjy15","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1byqd8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"zacksiri","can_mod_post":false,"created_utc":1751647205,"send_replies":true,"parent_id":"t3_1lrjy15","score":3,"author_fullname":"t2_e8crr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I can relate to this. At some point I did feel like I was going insane. However it made me realize how early we we are in all this and how much further we have to go. \\n\\nI managed to get qwen 3 working stable on my local setup and mostly everything works well.\\n\\nI also test my setup against api based models to make sure things work consistently. For the most part I do feel vLLM 0.9.1 works well enough and SGlang 0.4.8 is stable enough for my setup. \\n\\nI think one of your issue is you are using 5090 which is new hardware and things take time to stabilize on newer hardware. I saw one GitHub issue someone was complaining their b200 is performing worse than h100.\\n\\nThese are all signs that drivers have not stabilized and it’s going to take time before everything clicks.\\n\\nHang in there, if you just need to get stuff done just sign up for an api model and put in $5 credit to do sanity check that your stuff works every now and then.\\n\\nI test my agent flow against every major model so I know where I need to improve in my system and I know which models are simply broken.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1byqd8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I can relate to this. At some point I did feel like I was going insane. However it made me realize how early we we are in all this and how much further we have to go. &lt;/p&gt;\\n\\n&lt;p&gt;I managed to get qwen 3 working stable on my local setup and mostly everything works well.&lt;/p&gt;\\n\\n&lt;p&gt;I also test my setup against api based models to make sure things work consistently. For the most part I do feel vLLM 0.9.1 works well enough and SGlang 0.4.8 is stable enough for my setup. &lt;/p&gt;\\n\\n&lt;p&gt;I think one of your issue is you are using 5090 which is new hardware and things take time to stabilize on newer hardware. I saw one GitHub issue someone was complaining their b200 is performing worse than h100.&lt;/p&gt;\\n\\n&lt;p&gt;These are all signs that drivers have not stabilized and it’s going to take time before everything clicks.&lt;/p&gt;\\n\\n&lt;p&gt;Hang in there, if you just need to get stuff done just sign up for an api model and put in $5 credit to do sanity check that your stuff works every now and then.&lt;/p&gt;\\n\\n&lt;p&gt;I test my agent flow against every major model so I know where I need to improve in my system and I know which models are simply broken.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/n1byqd8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751647205,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrjy15","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1ca5n2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Agreeable-Market-692","can_mod_post":false,"created_utc":1751650638,"send_replies":true,"parent_id":"t3_1lrjy15","score":3,"author_fullname":"t2_ajuhoi00","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You can have SOTA or you can have production. \\n\\nI'm sticking with my 4090 for a while longer. If I had to build a server tomorrow that was going to production I would shove 4090s in it or whatever ADA or even Ampere silicon I could get my hands on before I'd go with Blackwell.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ca5n2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can have SOTA or you can have production. &lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m sticking with my 4090 for a while longer. If I had to build a server tomorrow that was going to production I would shove 4090s in it or whatever ADA or even Ampere silicon I could get my hands on before I&amp;#39;d go with Blackwell.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/n1ca5n2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751650638,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrjy15","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1d8uet","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppealSame4367","can_mod_post":false,"created_utc":1751661783,"send_replies":true,"parent_id":"t3_1lrjy15","score":3,"author_fullname":"t2_sxud8ccv4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You work with very new tech. It's always like that with every new wave of tech. You can either use mature frameworks OR you can use the newest tech.\\n\\nFrom experience they are mutually exclusive.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1d8uet","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You work with very new tech. It&amp;#39;s always like that with every new wave of tech. You can either use mature frameworks OR you can use the newest tech.&lt;/p&gt;\\n\\n&lt;p&gt;From experience they are mutually exclusive.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/n1d8uet/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751661783,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrjy15","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1c808f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"plankalkul-z1","can_mod_post":false,"created_utc":1751649991,"send_replies":true,"parent_id":"t3_1lrjy15","score":6,"author_fullname":"t2_w73n3yrsx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt; Am I going crazy or do you guys also notice this is a COMPLETE SHITSHOW????\\n\\n\\nI write about this stuff all the time. Let me quote one of my many posts on this subject:\\n\\n\\n&gt; ... what's going on with Llama 4 is a perfect illustration of the status quo in LLM world: everyone is rushing to accommodate the latest and greatest arch or optimization, but no-one seems to be concerned with the overall quality. It's somewhat understandable, but it's still an undestandable mess. \\\\&lt;...&gt;\\n\\n\\n&gt; So... what I see looks to me as if brilliant (I mean it!) scientists, with little or no commercial software development experience, are cranking up top-class software that is buggy and convoluted as hell. Well, I am a \\"glass half full\\" guy, so I'm very glad and grateful (again, I mean it) that I have it, but my goodness...\\n\\n\\nEvery update of python-based inference engines (vLLM, SGLang, etc.) breaks *something*. After some, it's just unfixable, so I have to re-install, gradually re-adding components (FA, FlashInfer etc.) until I figure out what broke it: walls of exceptions' stack traces are of no help.\\n\\n\\nSometimes, my frustration boils over, and I just completely dump an engine. This happened with tabbyAPI, for instance: it refused to start after upgrade, with very cryptic message; nothing would help, so I looked into the code. Well, the reason (for the cryptic/unrelared message) was the \`catch\` block: the author would search for a substring in the exception message text (!) and would completely disregard the possibility of the text not being found... The exception would be left essentially unhandled.\\n\\n\\nThere's not enough pushback from the community, unfortunately... So we have what we have.\\n\\n\\nHence, thank you for your post.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1c808f","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Am I going crazy or do you guys also notice this is a COMPLETE SHITSHOW????&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I write about this stuff all the time. Let me quote one of my many posts on this subject:&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;... what&amp;#39;s going on with Llama 4 is a perfect illustration of the status quo in LLM world: everyone is rushing to accommodate the latest and greatest arch or optimization, but no-one seems to be concerned with the overall quality. It&amp;#39;s somewhat understandable, but it&amp;#39;s still an undestandable mess. &amp;lt;...&amp;gt;&lt;/p&gt;\\n\\n&lt;p&gt;So... what I see looks to me as if brilliant (I mean it!) scientists, with little or no commercial software development experience, are cranking up top-class software that is buggy and convoluted as hell. Well, I am a &amp;quot;glass half full&amp;quot; guy, so I&amp;#39;m very glad and grateful (again, I mean it) that I have it, but my goodness...&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Every update of python-based inference engines (vLLM, SGLang, etc.) breaks &lt;em&gt;something&lt;/em&gt;. After some, it&amp;#39;s just unfixable, so I have to re-install, gradually re-adding components (FA, FlashInfer etc.) until I figure out what broke it: walls of exceptions&amp;#39; stack traces are of no help.&lt;/p&gt;\\n\\n&lt;p&gt;Sometimes, my frustration boils over, and I just completely dump an engine. This happened with tabbyAPI, for instance: it refused to start after upgrade, with very cryptic message; nothing would help, so I looked into the code. Well, the reason (for the cryptic/unrelared message) was the &lt;code&gt;catch&lt;/code&gt; block: the author would search for a substring in the exception message text (!) and would completely disregard the possibility of the text not being found... The exception would be left essentially unhandled.&lt;/p&gt;\\n\\n&lt;p&gt;There&amp;#39;s not enough pushback from the community, unfortunately... So we have what we have.&lt;/p&gt;\\n\\n&lt;p&gt;Hence, thank you for your post.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/n1c808f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751649991,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrjy15","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1boode","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"adel_b","can_mod_post":false,"created_utc":1751644144,"send_replies":true,"parent_id":"t3_1lrjy15","score":2,"author_fullname":"t2_b0t7h","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"yes doing https://github.com/netdur/llama_cpp_dart\\n\\nI maintain two APIs because llama.cpp ones is insane \\n\\nalso has to provide binaries, because it seems building llama.cpp is not easy \\n\\ndecided to not keep but doing periodic updates","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1boode","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;yes doing &lt;a href=\\"https://github.com/netdur/llama_cpp_dart\\"&gt;https://github.com/netdur/llama_cpp_dart&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;I maintain two APIs because llama.cpp ones is insane &lt;/p&gt;\\n\\n&lt;p&gt;also has to provide binaries, because it seems building llama.cpp is not easy &lt;/p&gt;\\n\\n&lt;p&gt;decided to not keep but doing periodic updates&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/n1boode/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751644144,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrjy15","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1c5t5j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"croninsiglos","can_mod_post":false,"created_utc":1751649332,"send_replies":true,"parent_id":"t3_1lrjy15","score":2,"author_fullname":"t2_v7qje","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This is not related to LLMs as much as NVIDIA/CUDA. It’s been over a decade of this with their software and drivers lagging behind the cards they are selling. This then causes delays for developers who build software on top of these.\\n\\nI’m grateful for the technological advances, I just wish they had drivers ready on day 1.\\n\\nFor LLM applications, I prototype on what works and optimize for speed later.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1c5t5j","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is not related to LLMs as much as NVIDIA/CUDA. It’s been over a decade of this with their software and drivers lagging behind the cards they are selling. This then causes delays for developers who build software on top of these.&lt;/p&gt;\\n\\n&lt;p&gt;I’m grateful for the technological advances, I just wish they had drivers ready on day 1.&lt;/p&gt;\\n\\n&lt;p&gt;For LLM applications, I prototype on what works and optimize for speed later.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/n1c5t5j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751649332,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrjy15","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1c97wb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ChristopherRoberto","can_mod_post":false,"created_utc":1751650357,"send_replies":true,"parent_id":"t3_1lrjy15","score":2,"author_fullname":"t2_9k391xhs","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"There have been far worse dependency hells, but the python and node ecosystems are a shitshow in general.  AI inherited the mess.  We're back in the \\"updated some stuff\\" age of software development, it's not really due to tracking the bleeding edge.  Even if you hang back a year or two it's the same mess.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1c97wb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There have been far worse dependency hells, but the python and node ecosystems are a shitshow in general.  AI inherited the mess.  We&amp;#39;re back in the &amp;quot;updated some stuff&amp;quot; age of software development, it&amp;#39;s not really due to tracking the bleeding edge.  Even if you hang back a year or two it&amp;#39;s the same mess.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/n1c97wb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751650357,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrjy15","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1dgxxm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"arousedsquirel","can_mod_post":false,"created_utc":1751664476,"send_replies":true,"parent_id":"t3_1lrjy15","score":2,"author_fullname":"t2_509a63kb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It's à relief to read this, does make one feel less the only one in this shxthole trying to make things running and focus on what really has to be done.\\nCheerz","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1dgxxm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s à relief to read this, does make one feel less the only one in this shxthole trying to make things running and focus on what really has to be done.\\nCheerz&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/n1dgxxm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751664476,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrjy15","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1cpeas","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"robogame_dev","can_mod_post":false,"created_utc":1751655424,"send_replies":true,"parent_id":"t3_1lrjy15","score":1,"author_fullname":"t2_kgsoqf03k","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The life expectancy for code is rapidly dropping. But so is the gestation time.\\n\\nCode is becoming more of a fungible, regrow it where it’s needed, kind of a thing.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1cpeas","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The life expectancy for code is rapidly dropping. But so is the gestation time.&lt;/p&gt;\\n\\n&lt;p&gt;Code is becoming more of a fungible, regrow it where it’s needed, kind of a thing.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/n1cpeas/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751655424,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrjy15","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1ddvfj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"IrisColt","can_mod_post":false,"created_utc":1751663426,"send_replies":true,"parent_id":"t3_1lrjy15","score":1,"author_fullname":"t2_c2f558x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt;Holy shit, I spend more time at the office hammering away at libraries than actually working on the project that's supposed to use these libraries.\\n\\n\\nThanks, seriously.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ddvfj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Holy shit, I spend more time at the office hammering away at libraries than actually working on the project that&amp;#39;s supposed to use these libraries.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Thanks, seriously.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/n1ddvfj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751663426,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrjy15","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1dgtp6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1751664436,"send_replies":true,"parent_id":"t3_1lrjy15","score":1,"author_fullname":"t2_h5utwre7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I've been able to solve most issues with occasional LLM help and regularly ignore developer recommended environments. All but a handful of projects have compiled for me. Over time you figure out what works and what doesn't in terms of deps.\\n\\nThere's so many models and different configurations, I can absolutely see how they can't test every use. Once you put in the effort to get it how you like.. perhaps don't update until you have to. Your dual chunk thing sounds very specific so its par for the course. Worked once, not very popular, gets buggy until someone needs it again and does the dew.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1dgtp6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve been able to solve most issues with occasional LLM help and regularly ignore developer recommended environments. All but a handful of projects have compiled for me. Over time you figure out what works and what doesn&amp;#39;t in terms of deps.&lt;/p&gt;\\n\\n&lt;p&gt;There&amp;#39;s so many models and different configurations, I can absolutely see how they can&amp;#39;t test every use. Once you put in the effort to get it how you like.. perhaps don&amp;#39;t update until you have to. Your dual chunk thing sounds very specific so its par for the course. Worked once, not very popular, gets buggy until someone needs it again and does the dew.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/n1dgtp6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751664436,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrjy15","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1eo6sz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ttkciar","can_mod_post":false,"created_utc":1751681554,"send_replies":true,"parent_id":"t3_1lrjy15","score":1,"author_fullname":"t2_cpegz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yes, I have definitely noticed.  It's one of the reasons I've stuck to llama.cpp; it's more self-contained and thus has more control over the code it depends upon.\\n\\nvLLM is a nightmare by comparison.  However, it is emerging as the dominant inference run-time for enterprise applications, so I keep expecting some corporate entity to subsume the project and try to impose some sanity.\\n\\nRed Hat seems like a leading contender.  It has a track record of doing that with other open source projects (Gluster, Ceph, GCC to a degree, etc), and they have chosen to base RHEAI on vLLM which gives them a vested interest.\\n\\nEven if that happens, though, I plan on sticking with llama.cpp.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1eo6sz","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, I have definitely noticed.  It&amp;#39;s one of the reasons I&amp;#39;ve stuck to llama.cpp; it&amp;#39;s more self-contained and thus has more control over the code it depends upon.&lt;/p&gt;\\n\\n&lt;p&gt;vLLM is a nightmare by comparison.  However, it is emerging as the dominant inference run-time for enterprise applications, so I keep expecting some corporate entity to subsume the project and try to impose some sanity.&lt;/p&gt;\\n\\n&lt;p&gt;Red Hat seems like a leading contender.  It has a track record of doing that with other open source projects (Gluster, Ceph, GCC to a degree, etc), and they have chosen to base RHEAI on vLLM which gives them a vested interest.&lt;/p&gt;\\n\\n&lt;p&gt;Even if that happens, though, I plan on sticking with llama.cpp.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/n1eo6sz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751681554,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lrjy15","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1eoz24","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Different-Toe-955","can_mod_post":false,"created_utc":1751681888,"send_replies":true,"parent_id":"t3_1lrjy15","score":1,"author_fullname":"t2_16thtphc23","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yup, and it applies to all models. The high level models are always changing, how they process is changing, and the low level drivers/processing methods are also changing. AI is the first time I've ever seen hardware actually matter. The type of floating point processors your GPU has matters. Right now AMD is basically completely cut off from doing any CUDA processing, because a lot of software requires CUDA.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1eoz24","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yup, and it applies to all models. The high level models are always changing, how they process is changing, and the low level drivers/processing methods are also changing. AI is the first time I&amp;#39;ve ever seen hardware actually matter. The type of floating point processors your GPU has matters. Right now AMD is basically completely cut off from doing any CUDA processing, because a lot of software requires CUDA.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/n1eoz24/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751681888,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrjy15","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1et6b2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Lesser-than","can_mod_post":false,"created_utc":1751683764,"send_replies":true,"parent_id":"t3_1lrjy15","score":1,"author_fullname":"t2_98d256k","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This is what happens when, large things need to change change overnight and the way alot of llm projects are strung together from bleeding edge projects (often through python ). At some point you need to freeze features and stop updating libs soon as you find a sweet spot of \\"everythings working pretty good\\" . Its not just LLM libs, I kind of blame docker and python together for these practices, like if you can only get it to work in a very specific environment there is a bigger issue going on.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1et6b2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is what happens when, large things need to change change overnight and the way alot of llm projects are strung together from bleeding edge projects (often through python ). At some point you need to freeze features and stop updating libs soon as you find a sweet spot of &amp;quot;everythings working pretty good&amp;quot; . Its not just LLM libs, I kind of blame docker and python together for these practices, like if you can only get it to work in a very specific environment there is a bigger issue going on.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/n1et6b2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751683764,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrjy15","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1h0k01","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"sync_co","can_mod_post":false,"created_utc":1751724652,"send_replies":true,"parent_id":"t3_1lrjy15","score":1,"author_fullname":"t2_efastwj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'm stuck just on the dependency nightmares... I make such massive progress on my personal projects until I hit these nightmares of which version to install which for me is not possible to figure out. I had to hire others to fix it because I just ran out of patience.","edited":1751725586,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1h0k01","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m stuck just on the dependency nightmares... I make such massive progress on my personal projects until I hit these nightmares of which version to install which for me is not possible to figure out. I had to hire others to fix it because I just ran out of patience.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/n1h0k01/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751724652,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrjy15","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1hjxw2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"karaposu","can_mod_post":false,"created_utc":1751730895,"send_replies":true,"parent_id":"t3_1lrjy15","score":1,"author_fullname":"t2_qzy7otr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I am data scientist and I own a package called llmservice (pypi) which is used by couple of companies (ds teams mostly but still in prod) \\n\\nLast night I ended up refactoring again and I had to make breaking changes.  Do I feel bad? Not really becuase I am maintaining this without any compansation. \\n\\nI dont like bloated libs langchain but it is part of the process. Things will get better","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1hjxw2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I am data scientist and I own a package called llmservice (pypi) which is used by couple of companies (ds teams mostly but still in prod) &lt;/p&gt;\\n\\n&lt;p&gt;Last night I ended up refactoring again and I had to make breaking changes.  Do I feel bad? Not really becuase I am maintaining this without any compansation. &lt;/p&gt;\\n\\n&lt;p&gt;I dont like bloated libs langchain but it is part of the process. Things will get better&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/n1hjxw2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751730895,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrjy15","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1ipsdg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"yeah-ok","can_mod_post":false,"created_utc":1751744270,"send_replies":true,"parent_id":"t3_1lrjy15","score":1,"author_fullname":"t2_3xlrs","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I totally agree, it's as if the subject itself precludes clear thinking-about and evaluation-of what works best and most commonsensically. \\n\\nThis also means that a lot of effort in terms of tool-running is only starting to come together gradually now with MCP protocol in place and even then it's start-stop-cough-cough style.\\n\\nFinally it seems to stop reasonable defaults from ever finding their ways onto the scene of local LLM'ing - it is like a vast ocean of quantity over quality both in terms of settings and models (and variations thereof, how many \\"optimized\\" models that actually perform overtly worse than the originals have been released by now?!)\\n\\nFinally finally, I love the potential inherent in local LLMs and I'm hopeful all this will get ironed out (and maybe I should start taking some responsibility and partake out-right!)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ipsdg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I totally agree, it&amp;#39;s as if the subject itself precludes clear thinking-about and evaluation-of what works best and most commonsensically. &lt;/p&gt;\\n\\n&lt;p&gt;This also means that a lot of effort in terms of tool-running is only starting to come together gradually now with MCP protocol in place and even then it&amp;#39;s start-stop-cough-cough style.&lt;/p&gt;\\n\\n&lt;p&gt;Finally it seems to stop reasonable defaults from ever finding their ways onto the scene of local LLM&amp;#39;ing - it is like a vast ocean of quantity over quality both in terms of settings and models (and variations thereof, how many &amp;quot;optimized&amp;quot; models that actually perform overtly worse than the originals have been released by now?!)&lt;/p&gt;\\n\\n&lt;p&gt;Finally finally, I love the potential inherent in local LLMs and I&amp;#39;m hopeful all this will get ironed out (and maybe I should start taking some responsibility and partake out-right!)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/n1ipsdg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751744270,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrjy15","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1caldb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"__SlimeQ__","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1c7abh","score":2,"author_fullname":"t2_olbav","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"that's cool, seems like they botched the release though huh? maybe not a reliable library\\n\\nin any case this stuff is normal when you're at the bleeding edge. i had to hack in qwen3 support on oobabooga. had to update to a specific nightly transformers, and deal with all the random issues that popped up because of that. i'm finding o3 is actually really good at figuring this stuff out, since the answers lie in the last month of commit messages from each dependency.\\n\\ni have other bones to pick with vllm, it doesn't run on windows for some reason and in general I don't actually want any of this cuda stuff happening in python in my main process so i don't like the programmatic bindings.\\n\\n(good luck, geniunely)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1caldb","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;that&amp;#39;s cool, seems like they botched the release though huh? maybe not a reliable library&lt;/p&gt;\\n\\n&lt;p&gt;in any case this stuff is normal when you&amp;#39;re at the bleeding edge. i had to hack in qwen3 support on oobabooga. had to update to a specific nightly transformers, and deal with all the random issues that popped up because of that. i&amp;#39;m finding o3 is actually really good at figuring this stuff out, since the answers lie in the last month of commit messages from each dependency.&lt;/p&gt;\\n\\n&lt;p&gt;i have other bones to pick with vllm, it doesn&amp;#39;t run on windows for some reason and in general I don&amp;#39;t actually want any of this cuda stuff happening in python in my main process so i don&amp;#39;t like the programmatic bindings.&lt;/p&gt;\\n\\n&lt;p&gt;(good luck, geniunely)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrjy15","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/n1caldb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751650770,"author_flair_text":null,"treatment_tags":[],"created_utc":1751650770,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1c7abh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"LinkSea8324","can_mod_post":false,"created_utc":1751649776,"send_replies":true,"parent_id":"t1_n1c2b5o","score":5,"author_fullname":"t2_152zyn72n4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"vLLM is the only lib that implemented Dual Chunk attention for Qwen 2.5 1M, which is the only decent model with long context you can run easily","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1c7abh","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;vLLM is the only lib that implemented Dual Chunk attention for Qwen 2.5 1M, which is the only decent model with long context you can run easily&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrjy15","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/n1c7abh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751649776,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n1c2b5o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"__SlimeQ__","can_mod_post":false,"created_utc":1751648288,"send_replies":true,"parent_id":"t3_1lrjy15","score":1,"author_fullname":"t2_olbav","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"there is absolutely no reason to use vllm. what you're experiencing is not normal\\n\\nuse oobabooga\\n\\nuse it over a rest api with streaming\\n\\nseparate your llm environment from your project so this type of dumb shit doesn't happen again.\\n\\nin this configuration you will also be able to drop in an alternative if your main one gets borked. maybe ollama\\n\\nvllm is a useless and wrong headed library","edited":1751648623,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1c2b5o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;there is absolutely no reason to use vllm. what you&amp;#39;re experiencing is not normal&lt;/p&gt;\\n\\n&lt;p&gt;use oobabooga&lt;/p&gt;\\n\\n&lt;p&gt;use it over a rest api with streaming&lt;/p&gt;\\n\\n&lt;p&gt;separate your llm environment from your project so this type of dumb shit doesn&amp;#39;t happen again.&lt;/p&gt;\\n\\n&lt;p&gt;in this configuration you will also be able to drop in an alternative if your main one gets borked. maybe ollama&lt;/p&gt;\\n\\n&lt;p&gt;vllm is a useless and wrong headed library&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/n1c2b5o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751648288,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrjy15","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(t,{data:l});export{r as default};
