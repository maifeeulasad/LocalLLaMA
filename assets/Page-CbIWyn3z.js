import{j as e}from"./index-BgwOAK4-.js";import{R as l}from"./RedditPostRenderer-BOBjDTFu.js";import"./index-BL22wVg5.js";const a=[{kind:"Listing",data:{after:null,dist:1,modhash:"",geo_filter:"",children:[{kind:"t3",data:{approved_at_utc:null,subreddit:"LocalLLaMA",selftext:`I'm a fan of LLaMA 3 70B and its Deepseek variants, but i find that local inference makes conversations way too laggy.  
  
What is the best model for fast inference, as of July 2025? I'm happy to use up to 48 gig of VRAM, but I'm mainly interested in a model that gives snappy replies. What model, and what size and quant would you recommend?  
  
Thanks!  `,user_reports:[],saved:!1,mod_reason_title:null,gilded:0,clicked:!1,title:"Best Local Model for Snappy Conversations?",link_flair_richtext:[{e:"text",t:"Question | Help"}],subreddit_name_prefixed:"r/LocalLLaMA",hidden:!1,pwls:6,link_flair_css_class:"",downs:0,thumbnail_height:null,top_awarded_type:null,hide_score:!1,name:"t3_1lxlgjk",quarantine:!1,link_flair_text_color:"dark",upvote_ratio:1,author_flair_background_color:null,subreddit_type:"public",ups:5,total_awards_received:0,media_embed:{},thumbnail_width:null,author_flair_template_id:null,is_original_content:!1,author_fullname:"t2_4z9wumnt",secure_media:null,is_reddit_media_domain:!1,is_meta:!1,category:null,secure_media_embed:{},link_flair_text:"Question | Help",can_mod_post:!1,score:5,approved_by:null,is_created_from_ads_ui:!1,author_premium:!1,thumbnail:"self",edited:!1,author_flair_css_class:null,author_flair_richtext:[],gildings:{},content_categories:null,is_self:!0,mod_note:null,created:1752275904,link_flair_type:"richtext",wls:6,removed_by_category:null,banned_by:null,author_flair_type:"text",domain:"self.LocalLLaMA",allow_live_comments:!1,selftext_html:`&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I&amp;#39;m a fan of LLaMA 3 70B and its Deepseek variants, but i find that local inference makes conversations way too laggy.  &lt;/p&gt;

&lt;p&gt;What is the best model for fast inference, as of July 2025? I&amp;#39;m happy to use up to 48 gig of VRAM, but I&amp;#39;m mainly interested in a model that gives snappy replies. What model, and what size and quant would you recommend?  &lt;/p&gt;

&lt;p&gt;Thanks!  &lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;`,likes:null,suggested_sort:null,banned_at_utc:null,view_count:null,archived:!1,no_follow:!1,is_crosspostable:!1,pinned:!1,over_18:!1,all_awardings:[],awarders:[],media_only:!1,link_flair_template_id:"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",can_gild:!1,spoiler:!1,locked:!1,author_flair_text:null,treatment_tags:[],visited:!1,removed_by:null,num_reports:null,distinguished:null,subreddit_id:"t5_81eyvm",author_is_blocked:!1,mod_reason_by:null,removal_reason:null,link_flair_background_color:"#5a74cc",id:"1lxlgjk",is_robot_indexable:!0,num_duplicates:0,report_reasons:null,author:"Harvard_Med_USMLE267",discussion_type:null,num_comments:0,send_replies:!0,media:null,contest_mode:!1,author_patreon_flair:!1,author_flair_text_color:null,permalink:"/r/LocalLLaMA/comments/1lxlgjk/best_local_model_for_snappy_conversations/",stickied:!1,url:"https://www.reddit.com/r/LocalLLaMA/comments/1lxlgjk/best_local_model_for_snappy_conversations/",subreddit_subscribers:498115,created_utc:1752275904,num_crossposts:0,mod_reports:[],is_video:!1}}],before:null}},{kind:"Listing",data:{after:null,dist:null,modhash:"",geo_filter:"",children:[],before:null}}],o=()=>e.jsx(l,{data:a});export{o as default};
