import{j as e}from"./index-C1Ow6vlP.js";import{R as l}from"./RedditPostRenderer-DsOAP7P6.js";import"./index-DduDModv.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I pay for Claude to assist with coding / tool calling which I use for my job all day. I feel a strong urge to waste tons of money on a nice GPU, but realistically the models aren't as strong or even as cheap as the cloud models.\\n\\nI'm trying to self-reflect hard and in this moment of clarity, I see this as a distract of an expensive new toy I won't use much.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Please convince me not to get a GPU I don't need. Can any local LLM compare with cloud models?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lnsax9","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.71,"author_flair_background_color":null,"subreddit_type":"public","ups":19,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1bl579qtd6","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":19,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751238331,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I pay for Claude to assist with coding / tool calling which I use for my job all day. I feel a strong urge to waste tons of money on a nice GPU, but realistically the models aren&amp;#39;t as strong or even as cheap as the cloud models.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m trying to self-reflect hard and in this moment of clarity, I see this as a distract of an expensive new toy I won&amp;#39;t use much.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lnsax9","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"TumbleweedDeep825","discussion_type":null,"num_comments":92,"send_replies":false,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/","subreddit_subscribers":492838,"created_utc":1751238331,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0j5d25","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Elderberry_9132","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0i7iig","score":1,"author_fullname":"t2_byh4ysuoh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I run it on 4 GPU strap, not a big problem, you can even run the full model on CPU with 2-3 tokens generation per second, but takes 1TB of ram :)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0j5d25","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I run it on 4 GPU strap, not a big problem, you can even run the full model on CPU with 2-3 tokens generation per second, but takes 1TB of ram :)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0j5d25/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751262016,"author_flair_text":null,"treatment_tags":[],"created_utc":1751262016,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ixyvg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Karyo_Ten","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0imk68","score":10,"author_fullname":"t2_tbdqg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You mean upgrade from 2x 5090 to 8x5090 ?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ixyvg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You mean upgrade from 2x 5090 to 8x5090 ?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0ixyvg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751258262,"author_flair_text":null,"treatment_tags":[],"created_utc":1751258262,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jewgf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Serprotease","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0jechj","score":1,"author_fullname":"t2_odh3w8c","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It’s a MoE.  \\nThe token output speed is about twice that of a 70b q8 in my setup.  \\nFrom ~7 tps to ~15 tps.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n0jewgf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It’s a MoE.&lt;br/&gt;\\nThe token output speed is about twice that of a 70b q8 in my setup.&lt;br/&gt;\\nFrom ~7 tps to ~15 tps.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jewgf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751267385,"author_flair_text":null,"treatment_tags":[],"created_utc":1751267385,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0jechj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"night0x63","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0jc5pa","score":1,"author_fullname":"t2_3h2irqtz","approved_by":null,"mod_note":null,"all_awardings":[],"body":"How is 235b faster than 70b?\\n\\n\\nI would expect about 4x slower.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0jechj","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How is 235b faster than 70b?&lt;/p&gt;\\n\\n&lt;p&gt;I would expect about 4x slower.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jechj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751267058,"author_flair_text":null,"treatment_tags":[],"created_utc":1751267058,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jeho3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"night0x63","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0jc5pa","score":1,"author_fullname":"t2_3h2irqtz","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Re similar to llama3.1 to llama3.3 gap: that sounds like a big jump!","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0jeho3","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Re similar to llama3.1 to llama3.3 gap: that sounds like a big jump!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jeho3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751267143,"author_flair_text":null,"treatment_tags":[],"created_utc":1751267143,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0jc5pa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Serprotease","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ipfbf","score":1,"author_fullname":"t2_odh3w8c","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It feels somewhat similar to the gap llama3.1 and llama3.3. \\nIt’s better, for sure. But maybe not as large as the gap between llama2 and llama3 for example.  \\n\\nThe big thing is that if you can run it locally, it’s most likely faster than any 70b on the same hardware.","edited":false,"author_flair_css_class":null,"name":"t1_n0jc5pa","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It feels somewhat similar to the gap llama3.1 and llama3.3. \\nIt’s better, for sure. But maybe not as large as the gap between llama2 and llama3 for example.  &lt;/p&gt;\\n\\n&lt;p&gt;The big thing is that if you can run it locally, it’s most likely faster than any 70b on the same hardware.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jc5pa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751265799,"author_flair_text":null,"collapsed":false,"created_utc":1751265799,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ipfbf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"night0x63","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0imk68","score":3,"author_fullname":"t2_3h2irqtz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"How does qwen 235b compare to llama3.3:70b?\\n\\n\\nAre there other 70b?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ipfbf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How does qwen 235b compare to llama3.3:70b?&lt;/p&gt;\\n\\n&lt;p&gt;Are there other 70b?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0ipfbf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751254374,"author_flair_text":null,"treatment_tags":[],"created_utc":1751254374,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0je9rb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mxforest","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0imk68","score":1,"author_fullname":"t2_kenmq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Wow!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0je9rb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Wow!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0je9rb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751267014,"author_flair_text":null,"treatment_tags":[],"created_utc":1751267014,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0imk68","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"YouDontSeemRight","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0i7iig","score":-5,"author_fullname":"t2_1b7gjxtue9","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"If you can run a 70B, Qwen 235B isn't far off.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0imk68","is_submitter":false,"collapsed":true,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you can run a 70B, Qwen 235B isn&amp;#39;t far off.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0imk68/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751253159,"author_flair_text":null,"treatment_tags":[],"created_utc":1751253159,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-5}}],"before":null}},"user_reports":[],"saved":false,"id":"n0i7iig","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Edzomatic","can_mod_post":false,"created_utc":1751247257,"send_replies":true,"parent_id":"t1_n0hk6bd","score":15,"author_fullname":"t2_9ist3ny0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Come close means running deepseek which is most likely not gonna happen. But 70b at a decent quant can serve you well depending on the use case","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0i7iig","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Come close means running deepseek which is most likely not gonna happen. But 70b at a decent quant can serve you well depending on the use case&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0i7iig/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751247257,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":15}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0j1tw3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BlueSwordM","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0irxsl","score":3,"author_fullname":"t2_qhqon","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You use the 3090 to greatly boost prompt processing and context.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0j1tw3","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You use the 3090 to greatly boost prompt processing and context.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0j1tw3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751260164,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1751260164,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0izxk6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0irxsl","score":5,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Prompt processing.GPU massively speeds it up, even if inference us 100% on cpu.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0izxk6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Prompt processing.GPU massively speeds it up, even if inference us 100% on cpu.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0izxk6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751259212,"author_flair_text":null,"treatment_tags":[],"created_utc":1751259212,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0iztan","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ixip8","score":5,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"No gpu = non existent PP speed","edited":false,"author_flair_css_class":null,"name":"t1_n0iztan","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No gpu = non existent PP speed&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0iztan/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751259154,"author_flair_text":null,"collapsed":false,"created_utc":1751259154,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ixip8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ambitious_Subject108","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0irxsl","score":2,"author_fullname":"t2_iaffzj2q","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It'll hurt more than it helps. CPU to memory latency is an order of magnitude faster than CPU to GPU.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ixip8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;ll hurt more than it helps. CPU to memory latency is an order of magnitude faster than CPU to GPU.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0ixip8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751258044,"author_flair_text":null,"treatment_tags":[],"created_utc":1751258044,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0irxsl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"gigaflops_","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0iivh7","score":5,"author_fullname":"t2_1t2n2s9f","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Question: what good would the 3090 do in this build? My understanding is that since the vast majority of the model (~85% of it) is stored on system RAM, it'll be computer almost entirely on the CPU. Unless of course you configure it to run entirely on GPU and transfer hundreds of gigabytes across the pcie lane every token, which in my experience is slower than CPU inference.\\n\\nI'm trying to understand this stuff better, so lmk where my thinking is wrong.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0irxsl","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Question: what good would the 3090 do in this build? My understanding is that since the vast majority of the model (~85% of it) is stored on system RAM, it&amp;#39;ll be computer almost entirely on the CPU. Unless of course you configure it to run entirely on GPU and transfer hundreds of gigabytes across the pcie lane every token, which in my experience is slower than CPU inference.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m trying to understand this stuff better, so lmk where my thinking is wrong.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0irxsl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751255472,"author_flair_text":null,"treatment_tags":[],"created_utc":1751255472,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jft2p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0jfhea","score":1,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"~$300 for 256 and ~$500 for 512","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jft2p","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;~$300 for 256 and ~$500 for 512&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jft2p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751267921,"author_flair_text":null,"treatment_tags":[],"created_utc":1751267921,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0jfhea","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"forhorglingrads","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0iivh7","score":1,"author_fullname":"t2_4pqxe","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"why is the 2nd half of the ram $100 cheaper than the first half","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0jfhea","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;why is the 2nd half of the ram $100 cheaper than the first half&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jfhea/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751267728,"author_flair_text":null,"treatment_tags":[],"created_utc":1751267728,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0iivh7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"created_utc":1751251652,"send_replies":true,"parent_id":"t1_n0hk6bd","score":-1,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"At full speed? Yeah, you can’t come close to Claude at home.\\n\\nSlowly? You can buy an old workstation on ebay for $300, spend $300 to drop in 256gb of RAM, put a 3090 in, and run Deepseek R1 0528 quantized to Q2 pretty easily. You’ll just get only 5 tokens/sec. \\n\\nIf Q2 is too small for you, spend $200 more and you’ll get 512gb RAM, and that lets you run Deepseek at Q5 which is pretty close to full quality.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0iivh7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;At full speed? Yeah, you can’t come close to Claude at home.&lt;/p&gt;\\n\\n&lt;p&gt;Slowly? You can buy an old workstation on ebay for $300, spend $300 to drop in 256gb of RAM, put a 3090 in, and run Deepseek R1 0528 quantized to Q2 pretty easily. You’ll just get only 5 tokens/sec. &lt;/p&gt;\\n\\n&lt;p&gt;If Q2 is too small for you, spend $200 more and you’ll get 512gb RAM, and that lets you run Deepseek at Q5 which is pretty close to full quality.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0iivh7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751251652,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hk6bd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Current-Ticket4214","can_mod_post":false,"created_utc":1751238618,"send_replies":true,"parent_id":"t3_1lnsax9","score":76,"author_fullname":"t2_6yvd3nyy","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You won’t match Claude with a local LLM running on high end consumer hardware. You need a commercial grade server with a ton of compute to even come close.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hk6bd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You won’t match Claude with a local LLM running on high end consumer hardware. You need a commercial grade server with a ton of compute to even come close.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hk6bd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751238618,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":76}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0iyasl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Karyo_Ten","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0iigj4","score":3,"author_fullname":"t2_tbdqg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt;\\"Provably secure\\" depends on your threat model but it's also a well solved problem.\\n\\nI would be out of a job if that was the case (cryptography).\\n\\nBut yeah, if you're asking, it's good enough for you.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0iyasl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;&amp;quot;Provably secure&amp;quot; depends on your threat model but it&amp;#39;s also a well solved problem.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I would be out of a job if that was the case (cryptography).&lt;/p&gt;\\n\\n&lt;p&gt;But yeah, if you&amp;#39;re asking, it&amp;#39;s good enough for you.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0iyasl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751258423,"author_flair_text":null,"treatment_tags":[],"created_utc":1751258423,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0iigj4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Steve_Streza","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0iai45","score":5,"author_fullname":"t2_35vw6osy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Probably not too long. A weekend if you know what you're doing, a few days or a week or so if you aren't familiar with the... quirks of AWS or GCM or Azure or whatever.\\n\\nOnce you get the tools set up, it's really not much different from setting up a virtual machine. You'd just SSH into it and set up your software and model. Shut it down when you're not using it (important! maybe set up a \\"shut down automatically after inactivity\\" script), and it'll probably only take a couple minutes (maybe a little longer for a model of that size) to boot up again.\\n\\n\\"Provably secure\\" depends on your threat model but it's also a well solved problem.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0iigj4","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Probably not too long. A weekend if you know what you&amp;#39;re doing, a few days or a week or so if you aren&amp;#39;t familiar with the... quirks of AWS or GCM or Azure or whatever.&lt;/p&gt;\\n\\n&lt;p&gt;Once you get the tools set up, it&amp;#39;s really not much different from setting up a virtual machine. You&amp;#39;d just SSH into it and set up your software and model. Shut it down when you&amp;#39;re not using it (important! maybe set up a &amp;quot;shut down automatically after inactivity&amp;quot; script), and it&amp;#39;ll probably only take a couple minutes (maybe a little longer for a model of that size) to boot up again.&lt;/p&gt;\\n\\n&lt;p&gt;&amp;quot;Provably secure&amp;quot; depends on your threat model but it&amp;#39;s also a well solved problem.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0iigj4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751251487,"author_flair_text":null,"treatment_tags":[],"created_utc":1751251487,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jayyn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Effect3325","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0iai45","score":2,"author_fullname":"t2_kkv4u2t2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I use serverless Runpod to run my own model. Upload your model to HuggingFace and then you can run your model from Runpod serverless taking the model from the HuggingFace. It costs you per second of model inference (I use flex worker)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0jayyn","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I use serverless Runpod to run my own model. Upload your model to HuggingFace and then you can run your model from Runpod serverless taking the model from the HuggingFace. It costs you per second of model inference (I use flex worker)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jayyn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751265114,"author_flair_text":null,"treatment_tags":[],"created_utc":1751265114,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0iai45","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ElectronSpiderwort","can_mod_post":false,"created_utc":1751248392,"send_replies":true,"parent_id":"t1_n0hocvs","score":5,"author_fullname":"t2_mxbu5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm curious what is the time investment for this; I know GPUs can be had for like $1.50/gpu/hour, but I don't know how long it takes to spin up a container or whatever that has the right software and is provably secure etc. etc..  How long does it take, starting from nothing, to get to hosting a quite large model (say Qwen 235b for instance) on a rented GPU cluster?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0iai45","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m curious what is the time investment for this; I know GPUs can be had for like $1.50/gpu/hour, but I don&amp;#39;t know how long it takes to spin up a container or whatever that has the right software and is provably secure etc. etc..  How long does it take, starting from nothing, to get to hosting a quite large model (say Qwen 235b for instance) on a rented GPU cluster?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0iai45/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751248392,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0imbmv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"TheRealGentlefox","can_mod_post":false,"created_utc":1751253061,"send_replies":true,"parent_id":"t1_n0hocvs","score":6,"author_fullname":"t2_f471r","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"No reason to rent a GPU, they can set up OpenRouter in literally 5 minutes to try out every model known to man. No configuring needed, just run them all in chat completion mode.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0imbmv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No reason to rent a GPU, they can set up OpenRouter in literally 5 minutes to try out every model known to man. No configuring needed, just run them all in chat completion mode.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0imbmv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751253061,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hocvs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Daemontatox","can_mod_post":false,"created_utc":1751240112,"send_replies":true,"parent_id":"t3_1lnsax9","score":34,"author_fullname":"t2_1rm9syq1nb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"How about renting a gpu online and trying out the models you think will do great in use case before committing to buying a gpu.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hocvs","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How about renting a gpu online and trying out the models you think will do great in use case before committing to buying a gpu.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hocvs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751240112,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":34}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0i96k6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Secure_Reflection409","can_mod_post":false,"created_utc":1751247890,"send_replies":true,"parent_id":"t3_1lnsax9","score":13,"author_fullname":"t2_by77ogdhr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You won't match the cloud for speed, cost or competence.\\n\\n\\nThe cloud will never match local for privacy.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0i96k6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You won&amp;#39;t match the cloud for speed, cost or competence.&lt;/p&gt;\\n\\n&lt;p&gt;The cloud will never match local for privacy.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0i96k6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751247890,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":13}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0iu05c","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"hapliniste","can_mod_post":false,"created_utc":1751256404,"send_replies":true,"parent_id":"t1_n0hk8tr","score":-4,"author_fullname":"t2_fc7rd","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Who upvote this? 🤨","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0iu05c","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Who upvote this? 🤨&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0iu05c/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751256404,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jfcs8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"photodesignch","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0i0yrd","score":1,"author_fullname":"t2_dnfi3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"LLM do not directly modify your file system.  Claude does.  When it presents a code, one agent goes into your file I/O to do file open and file save. When editing current file, there is one agent does file diff so it automatically marks diff code and you can apply over current file. Etc etc… it’s a multi-agent task from the cloud services under the hood.\\n\\nNormally LLM just takes input and report back output to you. It can’t interact with your system with separate agents with tools.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0jfcs8","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;LLM do not directly modify your file system.  Claude does.  When it presents a code, one agent goes into your file I/O to do file open and file save. When editing current file, there is one agent does file diff so it automatically marks diff code and you can apply over current file. Etc etc… it’s a multi-agent task from the cloud services under the hood.&lt;/p&gt;\\n\\n&lt;p&gt;Normally LLM just takes input and report back output to you. It can’t interact with your system with separate agents with tools.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jfcs8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751267652,"author_flair_text":null,"treatment_tags":[],"created_utc":1751267652,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0i0yrd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"pineh2","can_mod_post":false,"created_utc":1751244798,"send_replies":true,"parent_id":"t1_n0hk8tr","score":-7,"author_fullname":"t2_fifter7i","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This is awesome. \\n\\nCan you tell us about Claude being  a a multi agents system behind it?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0i0yrd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is awesome. &lt;/p&gt;\\n\\n&lt;p&gt;Can you tell us about Claude being  a a multi agents system behind it?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0i0yrd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751244798,"author_flair_text":null,"treatment_tags":[],"collapsed":true,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-7}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hk8tr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"photodesignch","can_mod_post":false,"created_utc":1751238642,"send_replies":true,"parent_id":"t3_1lnsax9","score":32,"author_fullname":"t2_dnfi3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Don’t. You won’t get the same performance locally. Invest gpu only if you need to run normal LLM.  Claude is not just LLM itself. It’s multi agents system behind it which isn’t gonna to be replaced locally easily.  There is a point why cloud services are popular.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hk8tr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Don’t. You won’t get the same performance locally. Invest gpu only if you need to run normal LLM.  Claude is not just LLM itself. It’s multi agents system behind it which isn’t gonna to be replaced locally easily.  There is a point why cloud services are popular.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hk8tr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751238642,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":32}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hrkvg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Terminator857","can_mod_post":false,"created_utc":1751241278,"send_replies":true,"parent_id":"t3_1lnsax9","score":8,"author_fullname":"t2_m40tjcn","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Maybe next year it won't cost too much to run deepseek at home.  As soon as the hardware costs drop to under $5K for reasonable speed I'm buying.  \\n\\nMaybe software improvements like diffusion for LLMs will make this more probable.    [https://www.reddit.com/r/LocalLLM/comments/1ljbajp/diffusion\\\\_language\\\\_models\\\\_will\\\\_cut\\\\_the\\\\_cost\\\\_of/](https://www.reddit.com/r/LocalLLM/comments/1ljbajp/diffusion_language_models_will_cut_the_cost_of/)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hrkvg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Maybe next year it won&amp;#39;t cost too much to run deepseek at home.  As soon as the hardware costs drop to under $5K for reasonable speed I&amp;#39;m buying.  &lt;/p&gt;\\n\\n&lt;p&gt;Maybe software improvements like diffusion for LLMs will make this more probable.    &lt;a href=\\"https://www.reddit.com/r/LocalLLM/comments/1ljbajp/diffusion_language_models_will_cut_the_cost_of/\\"&gt;https://www.reddit.com/r/LocalLLM/comments/1ljbajp/diffusion_language_models_will_cut_the_cost_of/&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hrkvg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751241278,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0j0s90","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Karyo_Ten","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0j0d9v","score":2,"author_fullname":"t2_tbdqg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"https://www.youtube.com/watch?v=hSoCmAoIMOU","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0j0s90","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://www.youtube.com/watch?v=hSoCmAoIMOU\\"&gt;https://www.youtube.com/watch?v=hSoCmAoIMOU&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0j0s90/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751259635,"author_flair_text":null,"treatment_tags":[],"created_utc":1751259635,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0j0d9v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"moncallikta","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0iygcb","score":2,"author_fullname":"t2_15ju4l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Can it run Doom?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0j0d9v","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Can it run Doom?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0j0d9v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751259427,"author_flair_text":null,"treatment_tags":[],"created_utc":1751259427,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0iygcb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Karyo_Ten","can_mod_post":false,"created_utc":1751258496,"send_replies":true,"parent_id":"t1_n0htp47","score":2,"author_fullname":"t2_tbdqg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"If we follow your reasoning, OP should buy a dam instead.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0iygcb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If we follow your reasoning, OP should buy a dam instead.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0iygcb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751258496,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0htp47","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FunnyAsparagus1253","can_mod_post":false,"created_utc":1751242066,"send_replies":true,"parent_id":"t3_1lnsax9","score":12,"author_fullname":"t2_i6c8tay3w","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Dude just buy a 3090 for fun. Then when the apocalypse hits and nobody has internet any more you’ll still have someone smart to chat to. Run quantized mistral small or whatever - sure it’s not as smart or good at coding as claude but so what? :)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0htp47","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Dude just buy a 3090 for fun. Then when the apocalypse hits and nobody has internet any more you’ll still have someone smart to chat to. Run quantized mistral small or whatever - sure it’s not as smart or good at coding as claude but so what? :)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0htp47/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751242066,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"body":"Which M processor do you have and what is the generation speed?","subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0j2cbe","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"edeltoaster","can_mod_post":false,"created_utc":1751260426,"send_replies":true,"parent_id":"t1_n0hl8fe","score":1,"author_fullname":"t2_c7rlb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"author_cakeday":true,"edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0j2cbe","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Which M processor do you have and what is the generation speed?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0j2cbe/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751260426,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hl8fe","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"mantafloppy","can_mod_post":false,"created_utc":1751238998,"send_replies":true,"parent_id":"t3_1lnsax9","score":12,"author_fullname":"t2_co2hf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I bought a Mac with 64gb ram/VRAM to play with LLM.\\n\\n\\nI can run 70b q4k_m guff model.\\n\\n\\n\\nMy main use is coding.\\n\\n\\nI pay for Claude for when i need serious help coding.\\n\\n\\nThe only \\"local\\" model that comes close to closed source model are the 600b+ model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hl8fe","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I bought a Mac with 64gb ram/VRAM to play with LLM.&lt;/p&gt;\\n\\n&lt;p&gt;I can run 70b q4k_m guff model.&lt;/p&gt;\\n\\n&lt;p&gt;My main use is coding.&lt;/p&gt;\\n\\n&lt;p&gt;I pay for Claude for when i need serious help coding.&lt;/p&gt;\\n\\n&lt;p&gt;The only &amp;quot;local&amp;quot; model that comes close to closed source model are the 600b+ model.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hl8fe/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751238998,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hs1li","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"GustaveVonZarovich","can_mod_post":false,"created_utc":1751241450,"send_replies":true,"parent_id":"t3_1lnsax9","score":4,"author_fullname":"t2_ftfadtol","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"size issue: cloud llms are huge, near 1t, local llms are generally around 32b ~ 70b, 14b makes more mistakes compared to 32b, and 32b makes more mistakes compared to 70b, you get the picture,\\n\\nspeed issue: local llms are like snails compared to cloud llms, if it's ok to wait 4 hrs for a response, go local, but remember that a cloud llm can generate that response in 4 minutes,\\n\\nfor a local llm qwen3 is good, but demands a good setup with 256gb ram/vram and that's expensive, \\n\\nand it's always too early to make a final decision, everyday everything is changing rapidly, tomorrow we might be lucky enough to run a great llm with only 32gb ram with no gpu","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hs1li","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;size issue: cloud llms are huge, near 1t, local llms are generally around 32b ~ 70b, 14b makes more mistakes compared to 32b, and 32b makes more mistakes compared to 70b, you get the picture,&lt;/p&gt;\\n\\n&lt;p&gt;speed issue: local llms are like snails compared to cloud llms, if it&amp;#39;s ok to wait 4 hrs for a response, go local, but remember that a cloud llm can generate that response in 4 minutes,&lt;/p&gt;\\n\\n&lt;p&gt;for a local llm qwen3 is good, but demands a good setup with 256gb ram/vram and that&amp;#39;s expensive, &lt;/p&gt;\\n\\n&lt;p&gt;and it&amp;#39;s always too early to make a final decision, everyday everything is changing rapidly, tomorrow we might be lucky enough to run a great llm with only 32gb ram with no gpu&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hs1li/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751241450,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hzg2v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"CMDR-Bugsbunny","can_mod_post":false,"created_utc":1751244230,"send_replies":true,"parent_id":"t1_n0hu8ri","score":13,"author_fullname":"t2_lj6an","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah, it really depends on the use case. Anyone that says that model x is better than model y without context is clueless.\\n\\nQwen 32b (and others) with Context7 can be run locally and would have very solid coding capabilities. If I was using Claude for coding this would come close. \\n\\nIf you wanted writing capabilities, then there are many choices Mixtral, Gemma 3, Llama and combined with RAG can create a serious domain specific workspace. You can easily use OpenRouter suggested by No-Refrigerator-1672 and use a vector database and n8n for RAG.\\n\\nDon't want to figure out how to use n8n, vector database, etc. Then get AnythingLLM and drop a bunch of PDFs and viola - a smarter local LLM with specific domain knowledge.\\n\\nI dropped 50,000+ pages of PDF on a business topic with Gemma 3 27b QAT and it performs near ChatGPT/Claude/Gemini on this domain. I used AnythingLLM with LM Studio and used over 1GB of PDFs. It took about 10-15 minutes and I was prompting against a vast library.\\n\\nSo it really depends.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hzg2v","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, it really depends on the use case. Anyone that says that model x is better than model y without context is clueless.&lt;/p&gt;\\n\\n&lt;p&gt;Qwen 32b (and others) with Context7 can be run locally and would have very solid coding capabilities. If I was using Claude for coding this would come close. &lt;/p&gt;\\n\\n&lt;p&gt;If you wanted writing capabilities, then there are many choices Mixtral, Gemma 3, Llama and combined with RAG can create a serious domain specific workspace. You can easily use OpenRouter suggested by No-Refrigerator-1672 and use a vector database and n8n for RAG.&lt;/p&gt;\\n\\n&lt;p&gt;Don&amp;#39;t want to figure out how to use n8n, vector database, etc. Then get AnythingLLM and drop a bunch of PDFs and viola - a smarter local LLM with specific domain knowledge.&lt;/p&gt;\\n\\n&lt;p&gt;I dropped 50,000+ pages of PDF on a business topic with Gemma 3 27b QAT and it performs near ChatGPT/Claude/Gemini on this domain. I used AnythingLLM with LM Studio and used over 1GB of PDFs. It took about 10-15 minutes and I was prompting against a vast library.&lt;/p&gt;\\n\\n&lt;p&gt;So it really depends.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hzg2v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751244230,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":13}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hu8ri","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No-Refrigerator-1672","can_mod_post":false,"created_utc":1751242269,"send_replies":true,"parent_id":"t3_1lnsax9","score":14,"author_fullname":"t2_baavelp5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Whoah, so much comments and none gave you the correct answer. Most of the locally-runnable models are awailable on OpenRouter; go ahead, spend 10 bucks on credits, try them out and determine it for yourself. Other commenters keep telling you how non-local models are smarter; and while it is for the flagship ones, it's actually possible that your particular tasks don't require this flagship intelligence, so you really should to your own tests instead of relying on crowd instinct. And should you find out that locals actually can do your tasks, then you can just divide the price of the GPU by your monthly credit spending and use that number to get objective judgement what's actually better for you.","edited":1751242471,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hu8ri","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Whoah, so much comments and none gave you the correct answer. Most of the locally-runnable models are awailable on OpenRouter; go ahead, spend 10 bucks on credits, try them out and determine it for yourself. Other commenters keep telling you how non-local models are smarter; and while it is for the flagship ones, it&amp;#39;s actually possible that your particular tasks don&amp;#39;t require this flagship intelligence, so you really should to your own tests instead of relying on crowd instinct. And should you find out that locals actually can do your tasks, then you can just divide the price of the GPU by your monthly credit spending and use that number to get objective judgement what&amp;#39;s actually better for you.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hu8ri/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751242269,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":14}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hytpp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"grim-432","can_mod_post":false,"created_utc":1751243998,"send_replies":true,"parent_id":"t3_1lnsax9","score":7,"author_fullname":"t2_l3u9f39ym","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Spent $10,000 on my local llm rig.\\n\\nStill use cloud every day.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hytpp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Spent $10,000 on my local llm rig.&lt;/p&gt;\\n\\n&lt;p&gt;Still use cloud every day.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hytpp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751243998,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hkd9i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No_Reveal_7826","can_mod_post":false,"created_utc":1751238685,"send_replies":true,"parent_id":"t3_1lnsax9","score":3,"author_fullname":"t2_1i5q2gxarw","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Some find local good enough, but nothing can compare to online models. At least when it comes to what can load in 24 GB GPU memory which is what I have.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hkd9i","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Some find local good enough, but nothing can compare to online models. At least when it comes to what can load in 24 GB GPU memory which is what I have.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hkd9i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751238685,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ixauk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Space__Whiskey","can_mod_post":false,"created_utc":1751257941,"send_replies":true,"parent_id":"t3_1lnsax9","score":4,"author_fullname":"t2_1cuwlonegr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"run comfyUI on it, and go on a comfyUI bender. That will pay it off.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ixauk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;run comfyUI on it, and go on a comfyUI bender. That will pay it off.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0ixauk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751257941,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0j19l9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"entsnack","can_mod_post":false,"created_utc":1751259876,"send_replies":true,"parent_id":"t3_1lnsax9","score":4,"author_fullname":"t2_1a48h7vf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Don't buy one for LLMs. Buy one for ComfyUI. No good APIs for the image/video generation stuff yet.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0j19l9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Don&amp;#39;t buy one for LLMs. Buy one for ComfyUI. No good APIs for the image/video generation stuff yet.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0j19l9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751259876,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0iey5z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MerlinTrashMan","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0i64gu","score":1,"author_fullname":"t2_7ghjkhuo","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It should be a lot of fun to play with the data.  I found the first step for me was to study the correlated items and build better signals from them and then use uncorrelated values to either scale the new signals or invalidate them.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0iey5z","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It should be a lot of fun to play with the data.  I found the first step for me was to study the correlated items and build better signals from them and then use uncorrelated values to either scale the new signals or invalidate them.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0iey5z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751250097,"author_flair_text":null,"treatment_tags":[],"created_utc":1751250097,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0i64gu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MengerianMango","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0i0r4o","score":1,"author_fullname":"t2_mcvyi","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Meta modeling. We already have awesome models. But they're generally simple and derived from first principles. To greatly oversimplify, we basically have built a pretty successful fund out of simply adding 500ish simple models. I want to use NNs to distill a better signal from the model soup than we're currently doing by just summing.\\n\\nIn theory, I should be using xgboost for this. But we'll see. Anything uncorrelated is valuable.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0i64gu","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Meta modeling. We already have awesome models. But they&amp;#39;re generally simple and derived from first principles. To greatly oversimplify, we basically have built a pretty successful fund out of simply adding 500ish simple models. I want to use NNs to distill a better signal from the model soup than we&amp;#39;re currently doing by just summing.&lt;/p&gt;\\n\\n&lt;p&gt;In theory, I should be using xgboost for this. But we&amp;#39;ll see. Anything uncorrelated is valuable.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0i64gu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751246728,"author_flair_text":null,"treatment_tags":[],"created_utc":1751246728,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0i0r4o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MerlinTrashMan","can_mod_post":false,"created_utc":1751244721,"send_replies":true,"parent_id":"t1_n0hll17","score":1,"author_fullname":"t2_7ghjkhuo","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Id love to chat on your GPU use for quant finance.  I have been writing kernels for option portfolio optimization that run on a 3090 but I am running out of vram.  I am curious what you plan to do.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0i0r4o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Id love to chat on your GPU use for quant finance.  I have been writing kernels for option portfolio optimization that run on a 3090 but I am running out of vram.  I am curious what you plan to do.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0i0r4o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751244721,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jdfhv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MengerianMango","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0jchum","score":1,"author_fullname":"t2_mcvyi","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Bro i like fancy toys too but OP is asking for objectivity. Literal fact is that you're not getting your money's worth out of your hardware. Unless you live in your mom's basement, assuming you have a job and arent running the tits off your gpus 34/7, you could rent better for the same amount of actual usage/time for way less than you paid. I could too. I mean i do get it. Fancy toys are nice and nothing gets me harder than caressing my 6000, but yk, ion need it and it's not putting money in my bank account. It's not paying my rent. Except it kinda might. I got the thing bc pnl gets me a profit bonus. Excessive success in the project I'm working on could easily net me a 10x return on my money.\\n\\nObjectively, any sane person will tell you that you shouldn't spend 5k+ on an item that doesn't generate some kinda return for you. Unless you're making 400k, in which case why even ask?\\n\\nBut yeah the amount of people judging models by their...... fiction writing abilities. It might not be illegal but its certainly not a use of time or money that I respect very much","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0jdfhv","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Bro i like fancy toys too but OP is asking for objectivity. Literal fact is that you&amp;#39;re not getting your money&amp;#39;s worth out of your hardware. Unless you live in your mom&amp;#39;s basement, assuming you have a job and arent running the tits off your gpus 34/7, you could rent better for the same amount of actual usage/time for way less than you paid. I could too. I mean i do get it. Fancy toys are nice and nothing gets me harder than caressing my 6000, but yk, ion need it and it&amp;#39;s not putting money in my bank account. It&amp;#39;s not paying my rent. Except it kinda might. I got the thing bc pnl gets me a profit bonus. Excessive success in the project I&amp;#39;m working on could easily net me a 10x return on my money.&lt;/p&gt;\\n\\n&lt;p&gt;Objectively, any sane person will tell you that you shouldn&amp;#39;t spend 5k+ on an item that doesn&amp;#39;t generate some kinda return for you. Unless you&amp;#39;re making 400k, in which case why even ask?&lt;/p&gt;\\n\\n&lt;p&gt;But yeah the amount of people judging models by their...... fiction writing abilities. It might not be illegal but its certainly not a use of time or money that I respect very much&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jdfhv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751266524,"author_flair_text":null,"treatment_tags":[],"created_utc":1751266524,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0jchum","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FunnyAsparagus1253","can_mod_post":false,"created_utc":1751265992,"send_replies":true,"parent_id":"t1_n0hll17","score":1,"author_fullname":"t2_i6c8tay3w","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Saying that the only real use case for having a home setup is *illegal porn* is a really weird response given that we’re in r/LocalLLaMa . Is that *seriously* what you think? Maybe you’re in the wrong place 🤨","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jchum","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Saying that the only real use case for having a home setup is &lt;em&gt;illegal porn&lt;/em&gt; is a really weird response given that we’re in &lt;a href=\\"/r/LocalLLaMa\\"&gt;r/LocalLLaMa&lt;/a&gt; . Is that &lt;em&gt;seriously&lt;/em&gt; what you think? Maybe you’re in the wrong place 🤨&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jchum/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751265992,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ibq0a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MengerianMango","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0i773r","score":2,"author_fullname":"t2_mcvyi","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"PP=prompt processing=prefill.\\n\\nThanks for all the info. I'm super jelly ngl. I'm a straight man but id give my booty for a dual epyc system. Unfortunately this is a side project so I dont have company funding for it. The 6000 could directly yield ROI for me in terms of performance bonus but I dont think id feel the same positive impact in my wallet from a dual epyc, as much as id love have one.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n0ibq0a","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;PP=prompt processing=prefill.&lt;/p&gt;\\n\\n&lt;p&gt;Thanks for all the info. I&amp;#39;m super jelly ngl. I&amp;#39;m a straight man but id give my booty for a dual epyc system. Unfortunately this is a side project so I dont have company funding for it. The 6000 could directly yield ROI for me in terms of performance bonus but I dont think id feel the same positive impact in my wallet from a dual epyc, as much as id love have one.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0ibq0a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751248852,"author_flair_text":null,"treatment_tags":[],"created_utc":1751248852,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0i773r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"createthiscom","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0i5ngq","score":1,"author_fullname":"t2_ozxxf","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I use Open Hands AI. I’m not sure what you mean by PP. I get 22 tok/s starting out. prefill is between 18 and 45 tok/s depending on the input. By 50k context I’m usually down to 18 tok/s. I’m quite happy with that performance for V3. It’s a little slow for R1 with the extra thinking, but I use R1 sometimes anyway for more complex tasks and sometimes it’s worthwhile. I used to prefer ktransformers because their ktransformer backend had zero delay for long context. The prefill cache was perfect because there was only one session. However, I get a lot of crashes with ktransformers and I have a lot of available VRAM with the 6000 pro, so I use llama.cpp now because it’s so reliable. Check the video I linked for examples.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0i773r","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I use Open Hands AI. I’m not sure what you mean by PP. I get 22 tok/s starting out. prefill is between 18 and 45 tok/s depending on the input. By 50k context I’m usually down to 18 tok/s. I’m quite happy with that performance for V3. It’s a little slow for R1 with the extra thinking, but I use R1 sometimes anyway for more complex tasks and sometimes it’s worthwhile. I used to prefer ktransformers because their ktransformer backend had zero delay for long context. The prefill cache was perfect because there was only one session. However, I get a lot of crashes with ktransformers and I have a lot of available VRAM with the 6000 pro, so I use llama.cpp now because it’s so reliable. Check the video I linked for examples.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0i773r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751247137,"author_flair_text":null,"treatment_tags":[],"created_utc":1751247137,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0i5ngq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MengerianMango","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0i06d4","score":1,"author_fullname":"t2_mcvyi","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What tps do you get tho, especially for PP? Personally, I'm pretty sensitive to delay for \\"vibe\\"/agentic coding. It would need to be something close to as fast as the APIs to feel useful. Do you use it for agentic coding (eg cline/aider) or just chat?","edited":false,"author_flair_css_class":null,"name":"t1_n0i5ngq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What tps do you get tho, especially for PP? Personally, I&amp;#39;m pretty sensitive to delay for &amp;quot;vibe&amp;quot;/agentic coding. It would need to be something close to as fast as the APIs to feel useful. Do you use it for agentic coding (eg cline/aider) or just chat?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0i5ngq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751246553,"author_flair_text":null,"collapsed":false,"created_utc":1751246553,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0iot40","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"createthiscom","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ih8sa","score":1,"author_fullname":"t2_ozxxf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yeah I haven’t noticed R1’s code to be better than V3’s. It tends to not follow instructions quite as faithfully too. I keep using it now and then trying to find a niche I like it in though.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n0iot40","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah I haven’t noticed R1’s code to be better than V3’s. It tends to not follow instructions quite as faithfully too. I keep using it now and then trying to find a niche I like it in though.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0iot40/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751254108,"author_flair_text":null,"treatment_tags":[],"created_utc":1751254108,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ih8sa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MengerianMango","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0igv0u","score":3,"author_fullname":"t2_mcvyi","approved_by":null,"mod_note":null,"all_awardings":[],"body":"r1 (and thinking models in general) tend to be worse at just doing stuff. Like v3 will outperform at editing code given the same quality instructions. People often use r1 as a master model to command v3 for ex. r1 will do the high level architectural design and hand off to v3 to do the grunt work.\\n\\nI think for ppl who already know how to code, v3 is more useful. I generally can do the architectural stuff myself as well or better than r1. I just want a \\"dumb\\" grunt to do the work 10x faster than I could","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0ih8sa","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;r1 (and thinking models in general) tend to be worse at just doing stuff. Like v3 will outperform at editing code given the same quality instructions. People often use r1 as a master model to command v3 for ex. r1 will do the high level architectural design and hand off to v3 to do the grunt work.&lt;/p&gt;\\n\\n&lt;p&gt;I think for ppl who already know how to code, v3 is more useful. I generally can do the architectural stuff myself as well or better than r1. I just want a &amp;quot;dumb&amp;quot; grunt to do the work 10x faster than I could&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0ih8sa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751251004,"author_flair_text":null,"treatment_tags":[],"created_utc":1751251004,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0igv0u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Few-Yam9901","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0i06d4","score":1,"author_fullname":"t2_1rhlf3bcfk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Isn’t r1 0528 better? I run it with mixed Nvidia gpus 2bit, 3-400 pp ts and 30-40 gen ts. Its Good","edited":false,"author_flair_css_class":null,"name":"t1_n0igv0u","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Isn’t r1 0528 better? I run it with mixed Nvidia gpus 2bit, 3-400 pp ts and 30-40 gen ts. Its Good&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0igv0u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751250850,"author_flair_text":null,"collapsed":false,"created_utc":1751250850,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0i06d4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"createthiscom","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0hzefi","score":3,"author_fullname":"t2_ozxxf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I don’t know if threadripper’s memory bandwidth is sufficient, but yes. I have a dual CPU EPYC system that was roughly 14k without the GPU. However, I can literally run V3 all day for only about $30/mo in electricity.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0i06d4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don’t know if threadripper’s memory bandwidth is sufficient, but yes. I have a dual CPU EPYC system that was roughly 14k without the GPU. However, I can literally run V3 all day for only about $30/mo in electricity.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0i06d4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751244505,"author_flair_text":null,"treatment_tags":[],"created_utc":1751244505,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hzefi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MengerianMango","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0hz6y4","score":4,"author_fullname":"t2_mcvyi","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"So roughly another 10k for a ddr5 threadripper or epyc build. You're still talking 9k gpu + ~10k host.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0hzefi","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So roughly another 10k for a ddr5 threadripper or epyc build. You&amp;#39;re still talking 9k gpu + ~10k host.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hzefi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751244213,"author_flair_text":null,"treatment_tags":[],"created_utc":1751244213,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jd4lw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"sc166","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0hz6y4","score":1,"author_fullname":"t2_twa0i","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Do you mind sharing instructions? I have 7985x with 768gb (8 channel 6000mt) and single 6000 pro (waiting for 2nd one). I don’t think my system has as high memory bandwidth but would like to test how close I can in terms of tok/s. Thanks","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0jd4lw","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Do you mind sharing instructions? I have 7985x with 768gb (8 channel 6000mt) and single 6000 pro (waiting for 2nd one). I don’t think my system has as high memory bandwidth but would like to test how close I can in terms of tok/s. Thanks&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jd4lw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751266352,"author_flair_text":null,"treatment_tags":[],"created_utc":1751266352,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hz6y4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"createthiscom","can_mod_post":false,"created_utc":1751244135,"send_replies":true,"parent_id":"t1_n0hll17","score":1,"author_fullname":"t2_ozxxf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You can run V3 with considerably less than a single blackwell 6000 pro, but with it I get about 22 tok/s. The host needs about 400gb ram and 720gb/s of bandwidth to achieve those speeds. Video evidence: https://youtu.be/vfi9LRJxgHs?si=FJ7NLiaDos1wOhIW","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hz6y4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can run V3 with considerably less than a single blackwell 6000 pro, but with it I get about 22 tok/s. The host needs about 400gb ram and 720gb/s of bandwidth to achieve those speeds. Video evidence: &lt;a href=\\"https://youtu.be/vfi9LRJxgHs?si=FJ7NLiaDos1wOhIW\\"&gt;https://youtu.be/vfi9LRJxgHs?si=FJ7NLiaDos1wOhIW&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hz6y4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751244135,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hll17","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MengerianMango","can_mod_post":false,"created_utc":1751239122,"send_replies":true,"parent_id":"t3_1lnsax9","score":7,"author_fullname":"t2_mcvyi","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I bought a 6000 Blackwell and I still can't even run Qwen3 235b in 4 bit. I can run in 2 bit with 64k context, even 128k context is too much.\\n\\nAnd really you want to run Deepseek v3 for real work. For that you need roughly 3 6000 Blackwells, so roughly $27k. And you'd still be disappointed vs Claude.\\n\\nI knew this going in tho. I work in quant finance and intend to use the GPU for non-llm work. I wouldnt have bought it just for llms.\\n\\nGPUs dont really make sense unless you're generating illegal porn and really dont want any trace of your crimes on other people's servers or maybe if you're discussing incredibly valuable IP. I talk with qwen3 about the internal secrets from my firm, play with new ideas, etc. Obv I dont trust OpenAI to talk with gpt about such things. I could leak million dollar secrets if they lie about not training on my chats. Other than a few very limited circumstances, just use openrouter.","edited":1751239307,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hll17","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I bought a 6000 Blackwell and I still can&amp;#39;t even run Qwen3 235b in 4 bit. I can run in 2 bit with 64k context, even 128k context is too much.&lt;/p&gt;\\n\\n&lt;p&gt;And really you want to run Deepseek v3 for real work. For that you need roughly 3 6000 Blackwells, so roughly $27k. And you&amp;#39;d still be disappointed vs Claude.&lt;/p&gt;\\n\\n&lt;p&gt;I knew this going in tho. I work in quant finance and intend to use the GPU for non-llm work. I wouldnt have bought it just for llms.&lt;/p&gt;\\n\\n&lt;p&gt;GPUs dont really make sense unless you&amp;#39;re generating illegal porn and really dont want any trace of your crimes on other people&amp;#39;s servers or maybe if you&amp;#39;re discussing incredibly valuable IP. I talk with qwen3 about the internal secrets from my firm, play with new ideas, etc. Obv I dont trust OpenAI to talk with gpt about such things. I could leak million dollar secrets if they lie about not training on my chats. Other than a few very limited circumstances, just use openrouter.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hll17/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751239122,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0j6u3p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Noiselexer","can_mod_post":false,"created_utc":1751262816,"send_replies":true,"parent_id":"t3_1lnsax9","score":2,"author_fullname":"t2_n4xpz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I try models, never use them and use cloud for all my coding stuff.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0j6u3p","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I try models, never use them and use cloud for all my coding stuff.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0j6u3p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751262816,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hq1jx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Federal_Order4324","can_mod_post":false,"created_utc":1751240722,"send_replies":true,"parent_id":"t1_n0hkbc2","score":0,"author_fullname":"t2_rlhobztpn","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"No one realistically does.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hq1jx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No one realistically does.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hq1jx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751240722,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hvnpk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Koksny","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0htyqi","score":16,"author_fullname":"t2_olk3n","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"\\\\* For coding only  \\n\\\\*\\\\* Requires 400GB of DDR  \\n\\\\*\\\\*\\\\* At Q4  \\n\\\\*\\\\*\\\\*\\\\* OoM@&gt;8k context.","edited":1751242997,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0hvnpk","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;* For coding only&lt;br/&gt;\\n** Requires 400GB of DDR&lt;br/&gt;\\n*** At Q4&lt;br/&gt;\\n**** OoM@&amp;gt;8k context.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hvnpk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751242799,"author_flair_text":null,"treatment_tags":[],"created_utc":1751242799,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":16}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0i1ab3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullstackSensei","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0hxflk","score":2,"author_fullname":"t2_17n3nqtj56","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The CPUs are cheap. It's the DDR5 RAM and motherboard where you'll spend a lot.\\n\\nAnd if you need more than 8k context, you'll need a second 24GB GPU","edited":false,"author_flair_css_class":null,"name":"t1_n0i1ab3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The CPUs are cheap. It&amp;#39;s the DDR5 RAM and motherboard where you&amp;#39;ll spend a lot.&lt;/p&gt;\\n\\n&lt;p&gt;And if you need more than 8k context, you&amp;#39;ll need a second 24GB GPU&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0i1ab3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751244918,"author_flair_text":null,"collapsed":false,"created_utc":1751244918,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hxflk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Agreeable_Patience47","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0hw4vd","score":-1,"author_fullname":"t2_8levrzbi","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It’s pretty cheap in my country and it's not required, it's just faster","edited":1751246718,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hxflk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It’s pretty cheap in my country and it&amp;#39;s not required, it&amp;#39;s just faster&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hxflk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751243475,"author_flair_text":null,"treatment_tags":[],"created_utc":1751243475,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hw4vd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"netvyper","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0htyqi","score":2,"author_fullname":"t2_v6nk8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"But doesn't that require an AMX compatible CPU, which is more than the 24GB GPU at this point?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0hw4vd","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;But doesn&amp;#39;t that require an AMX compatible CPU, which is more than the 24GB GPU at this point?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hw4vd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751242982,"author_flair_text":null,"treatment_tags":[],"created_utc":1751242982,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0htyqi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Agreeable_Patience47","can_mod_post":false,"created_utc":1751242166,"send_replies":true,"parent_id":"t1_n0hkbc2","score":-4,"author_fullname":"t2_8levrzbi","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Your information is outdated. R1 actually runs on a single 24G GPU. https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/DeepseekR1_V3_tutorial.md","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0htyqi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Your information is outdated. R1 actually runs on a single 24G GPU. &lt;a href=\\"https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/DeepseekR1_V3_tutorial.md\\"&gt;https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/DeepseekR1_V3_tutorial.md&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0htyqi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751242166,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-4}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hkbc2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"z_3454_pfk","can_mod_post":false,"created_utc":1751238667,"send_replies":true,"parent_id":"t3_1lnsax9","score":3,"author_fullname":"t2_askwa","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"no local model can really match claude for swe. r1 0528 is decent but requires too much VRAM (unless you have that kind of money)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hkbc2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;no local model can really match claude for swe. r1 0528 is decent but requires too much VRAM (unless you have that kind of money)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hkbc2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751238667,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hpq0w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"loyalekoinu88","can_mod_post":false,"created_utc":1751240605,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_1x5p0ubz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Tiny Qwen3 models can do tool calling. Coding doesn’t have the best options right now.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hpq0w","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Tiny Qwen3 models can do tool calling. Coding doesn’t have the best options right now.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hpq0w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751240605,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hthcl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"KDCreerStudios","can_mod_post":false,"created_utc":1751241986,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_1qfbu2cvzc","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"With tool callings and fine-tuned specialization it can beat out Claude in certain tasks. But a general purpose one, no, not really.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hthcl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;With tool callings and fine-tuned specialization it can beat out Claude in certain tasks. But a general purpose one, no, not really.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hthcl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751241986,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hxrlv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Syzeon","can_mod_post":false,"created_utc":1751243603,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_82nlaimd","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"rent a runpod or something, try out a few local you have in mind, evaluate yourself if local model is what you're after. Make your decision after you tried it out","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hxrlv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;rent a runpod or something, try out a few local you have in mind, evaluate yourself if local model is what you&amp;#39;re after. Make your decision after you tried it out&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hxrlv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751243603,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hy8zc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"evilbarron2","can_mod_post":false,"created_utc":1751243784,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_gr2fr79s1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"No, they can’t, but they can still do a lot. They just have far less “presence” and adaptability. I’ve been using frontier for dev and local for production - more work but I can sleep at night not worrying about a huge bill","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hy8zc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No, they can’t, but they can still do a lot. They just have far less “presence” and adaptability. I’ve been using frontier for dev and local for production - more work but I can sleep at night not worrying about a huge bill&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hy8zc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751243784,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0i30ep","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"makinggrace","can_mod_post":false,"created_utc":1751245563,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_z9vf1vr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Try before you buy. \\n\\nLocal LLM isn't (yet) IMHO a great tool for execution unless the use case is extremely specific and cannot be performed in cloud for reasons. It is too slow and too expensive to do tasks locally that can be done in cloud.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0i30ep","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Try before you buy. &lt;/p&gt;\\n\\n&lt;p&gt;Local LLM isn&amp;#39;t (yet) IMHO a great tool for execution unless the use case is extremely specific and cannot be performed in cloud for reasons. It is too slow and too expensive to do tasks locally that can be done in cloud.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0i30ep/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751245563,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0i4pyh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"robberviet","can_mod_post":false,"created_utc":1751246206,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_jxc5a","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"No, just use services. You don't get that performance selfhost.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0i4pyh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No, just use services. You don&amp;#39;t get that performance selfhost.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0i4pyh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751246206,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0i7zlk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Top-Winter938","can_mod_post":false,"created_utc":1751247436,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_c1i53eiz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Try out some of the smaller models you could run locally via OpenRouter first. If they’re good enough for what you need, figure out your break-even point: is it cheaper to just keep using the API, or does it make sense to buy a GPU? Will you still be using the model after you hit that break-even? That usually gives you your answer. \\n\\nAs you’re using it for coding and used to claude, you’ll probably be frustrated by these smaller models. They are good for smaller, specific tasks","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0i7zlk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Try out some of the smaller models you could run locally via OpenRouter first. If they’re good enough for what you need, figure out your break-even point: is it cheaper to just keep using the API, or does it make sense to buy a GPU? Will you still be using the model after you hit that break-even? That usually gives you your answer. &lt;/p&gt;\\n\\n&lt;p&gt;As you’re using it for coding and used to claude, you’ll probably be frustrated by these smaller models. They are good for smaller, specific tasks&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0i7zlk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751247436,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0i90lt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kironlau","can_mod_post":false,"created_utc":1751247828,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_tb0dz2ds","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Go to openrouter, get a free trial. In field, will get you a answer.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0i90lt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Go to openrouter, get a free trial. In field, will get you a answer.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0i90lt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751247828,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0idned","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Little-Parfait-423","can_mod_post":false,"created_utc":1751249589,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_1i51wbv406","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I have a rtx 3060 and a rtx 3090. The VRAM constraint of 12GB vs 24GB between them allows me to run larger models (barely) but at significant loss in context and speed. I use it to debug code, quick scripts, look at logs, review output from tests, and prepare prompts for larger models over API. In all honestly you get decent results with 8b models but no tool use. If you prompt well and add context you get tool use (ie edit for you) at 14b models you can sometimes get tool use for small files. 24-30b models are smarter but even with 24gb VRAM there is no context window to load/edit files anyway. My advice to anyone is get a used rtx3060 12gb vram ~200-300 USD (or rtx 4060 16gb if you can find one) and run 8b/14b models to prep your work for cloud based models. Don’t mind the gap between local LLM with beefy/expensive cards it’s not worth being just as frustrated and broke on top of it :2cents:","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0idned","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have a rtx 3060 and a rtx 3090. The VRAM constraint of 12GB vs 24GB between them allows me to run larger models (barely) but at significant loss in context and speed. I use it to debug code, quick scripts, look at logs, review output from tests, and prepare prompts for larger models over API. In all honestly you get decent results with 8b models but no tool use. If you prompt well and add context you get tool use (ie edit for you) at 14b models you can sometimes get tool use for small files. 24-30b models are smarter but even with 24gb VRAM there is no context window to load/edit files anyway. My advice to anyone is get a used rtx3060 12gb vram ~200-300 USD (or rtx 4060 16gb if you can find one) and run 8b/14b models to prep your work for cloud based models. Don’t mind the gap between local LLM with beefy/expensive cards it’s not worth being just as frustrated and broke on top of it :2cents:&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0idned/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751249589,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ii0i2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Violaze27","can_mod_post":false,"created_utc":1751251311,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_fe4u4syr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For games absolutely","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ii0i2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For games absolutely&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0ii0i2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751251311,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ikgln","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ardalok","can_mod_post":false,"created_utc":1751252293,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_vgnewja","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The only locally deployable model that can compete with Claude, GPT, and similar large language models is DeepSeek, and it cannot even be hosted on a single GPU.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ikgln","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The only locally deployable model that can compete with Claude, GPT, and similar large language models is DeepSeek, and it cannot even be hosted on a single GPU.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0ikgln/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751252293,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0j2lrt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cguy1234","can_mod_post":false,"created_utc":1751260561,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_mhjuy","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Claude seems way ahead of any local LLM that I’ve tried in terms of quality of outputs and also just dragging a big PDF into it and getting quite good analysis. Even with a good GPU, I haven’t seen a local model that has the same quality.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0j2lrt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Claude seems way ahead of any local LLM that I’ve tried in terms of quality of outputs and also just dragging a big PDF into it and getting quite good analysis. Even with a good GPU, I haven’t seen a local model that has the same quality.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0j2lrt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751260561,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0j4xvq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"redditisunproductive","can_mod_post":false,"created_utc":1751261790,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_19353jsswd","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Finetuned local models can outperform SOTA models on very narrow tasks.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0j4xvq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Finetuned local models can outperform SOTA models on very narrow tasks.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0j4xvq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751261790,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0j6foc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Elderberry_9132","can_mod_post":false,"created_utc":1751262600,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_byh4ysuoh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Well, here is an answer, you don’t understand how this AI magic works, mainly LLM models perform particular task of either helping to reason, or generate text, their purpose is to guess the next token a user expects, it is just guessing it with enough correct answers for you to believe it is actually intelligent. \\n\\nSo, keeping this in mind, “AI” is a huge pile of different algorithms, services, stages and pipelines. \\n\\nYou won’t be able to run it locally unless you have the time to developer it and deploy, and manage it","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0j6foc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Well, here is an answer, you don’t understand how this AI magic works, mainly LLM models perform particular task of either helping to reason, or generate text, their purpose is to guess the next token a user expects, it is just guessing it with enough correct answers for you to believe it is actually intelligent. &lt;/p&gt;\\n\\n&lt;p&gt;So, keeping this in mind, “AI” is a huge pile of different algorithms, services, stages and pipelines. &lt;/p&gt;\\n\\n&lt;p&gt;You won’t be able to run it locally unless you have the time to developer it and deploy, and manage it&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0j6foc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751262600,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jamiu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Spiritual-Spend8187","can_mod_post":false,"created_utc":1751264917,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_4aevhf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Closest you will get to cloud level on local is deepseek but that is still very expensive on consumer hardware possible but insane.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jamiu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Closest you will get to cloud level on local is deepseek but that is still very expensive on consumer hardware possible but insane.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jamiu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751264917,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jdg5y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Fast-Satisfaction482","can_mod_post":false,"created_utc":1751266535,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_9ceux4xp","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I have access to a pretty decent workstation with dual 4090 at work and while it's all fun and games as long as your company pays for it, it's by far not as good as OpenAI or Anthropic's offerings.\\n\\n\\nOf course unless your use case is against the terms of service for the big cloud services, then it's the only option. (Or cloud is against regulation at your job ) ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jdg5y","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have access to a pretty decent workstation with dual 4090 at work and while it&amp;#39;s all fun and games as long as your company pays for it, it&amp;#39;s by far not as good as OpenAI or Anthropic&amp;#39;s offerings.&lt;/p&gt;\\n\\n&lt;p&gt;Of course unless your use case is against the terms of service for the big cloud services, then it&amp;#39;s the only option. (Or cloud is against regulation at your job ) &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jdg5y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751266535,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jegxi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Liringlass","can_mod_post":false,"created_utc":1751267131,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_dfewdhav","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Not only anything local can only come close, not reach Claude for coding.\\n\\nBut anything local won't upgrade magically. What you buy today will still have the same capacity in 2 years, and while same size LLMs improve over time you won't be able to upgrade your hardware \\"for free\\".\\n\\nLike someone else said Deepseek (the big model) might be the closest one but you either need a big ram/cpu server (just a few K$) but it's gonna be very very slow or you  need multiple 10/20k$ GPUs (and even here i'm not sure how fast it will be compared to Claude on subscription services).\\n\\nPeople mention 70b models which could be fine for some help but don't expect Cursor / GHCP / Cline / RooCode agent mode level of help, it will be more basic.\\n\\nOne more thing is that subscription services seem very cheap to me compared to the cost of doing it yourself, might be due to competition? Last time I checked cursor was USD 20 a month and that's really cheap for all the work you can get done on big models.\\n\\nLastly, anything local or cloud based need work to setup and maintain. That's also something to take into account if you look at it in a ROI perspective.\\n\\nThe only cases where I would advise local LLMs are: \\n\\n\\\\- When confidentiality is a must and deserves the added cost / lower level of service. And even then you might be better off renting the equipment in the cloud, and running your own private API, rather than buying 20 or 40k USD worth of equipment.\\n\\n\\\\- When you want to DIY and learn, focusing on that journey rather than on the direct result, and added cost is not a problem","edited":1751267314,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jegxi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not only anything local can only come close, not reach Claude for coding.&lt;/p&gt;\\n\\n&lt;p&gt;But anything local won&amp;#39;t upgrade magically. What you buy today will still have the same capacity in 2 years, and while same size LLMs improve over time you won&amp;#39;t be able to upgrade your hardware &amp;quot;for free&amp;quot;.&lt;/p&gt;\\n\\n&lt;p&gt;Like someone else said Deepseek (the big model) might be the closest one but you either need a big ram/cpu server (just a few K$) but it&amp;#39;s gonna be very very slow or you  need multiple 10/20k$ GPUs (and even here i&amp;#39;m not sure how fast it will be compared to Claude on subscription services).&lt;/p&gt;\\n\\n&lt;p&gt;People mention 70b models which could be fine for some help but don&amp;#39;t expect Cursor / GHCP / Cline / RooCode agent mode level of help, it will be more basic.&lt;/p&gt;\\n\\n&lt;p&gt;One more thing is that subscription services seem very cheap to me compared to the cost of doing it yourself, might be due to competition? Last time I checked cursor was USD 20 a month and that&amp;#39;s really cheap for all the work you can get done on big models.&lt;/p&gt;\\n\\n&lt;p&gt;Lastly, anything local or cloud based need work to setup and maintain. That&amp;#39;s also something to take into account if you look at it in a ROI perspective.&lt;/p&gt;\\n\\n&lt;p&gt;The only cases where I would advise local LLMs are: &lt;/p&gt;\\n\\n&lt;p&gt;- When confidentiality is a must and deserves the added cost / lower level of service. And even then you might be better off renting the equipment in the cloud, and running your own private API, rather than buying 20 or 40k USD worth of equipment.&lt;/p&gt;\\n\\n&lt;p&gt;- When you want to DIY and learn, focusing on that journey rather than on the direct result, and added cost is not a problem&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jegxi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751267131,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jfi1n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"madaradess007","can_mod_post":false,"created_utc":1751267738,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_79slapln","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"no, local cant compare to cloud, but you can build workflows locally that will scale if you make a switch to external APIs","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jfi1n","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;no, local cant compare to cloud, but you can build workflows locally that will scale if you make a switch to external APIs&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jfi1n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751267738,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hkpsv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"polandtown","can_mod_post":false,"created_utc":1751238812,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_g1ws1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Ok. Want to really learn how to use LLMs at the enterprise level? That's done though the cloud: let that be on an on-premise vlan or a 'traditional' cloud instance. You get to learn the required networking side ontop of everything else, making your portfolio stronger.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hkpsv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ok. Want to really learn how to use LLMs at the enterprise level? That&amp;#39;s done though the cloud: let that be on an on-premise vlan or a &amp;#39;traditional&amp;#39; cloud instance. You get to learn the required networking side ontop of everything else, making your portfolio stronger.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hkpsv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751238812,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hwsnb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fizzy1242","can_mod_post":false,"created_utc":1751243233,"send_replies":true,"parent_id":"t1_n0hw3au","score":3,"author_fullname":"t2_16zcsx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"waste for you, invaluable for others","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hwsnb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;waste for you, invaluable for others&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hwsnb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751243233,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hw3au","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"INtuitiveTJop","can_mod_post":false,"created_utc":1751242966,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_u16k63kl","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I paid 2k only to end up not using it. Don’t waste the money","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hw3au","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I paid 2k only to end up not using it. Don’t waste the money&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hw3au/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751242966,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hvitu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"digiwiggles","can_mod_post":false,"created_utc":1751242748,"send_replies":true,"parent_id":"t1_n0hr3lg","score":3,"author_fullname":"t2_5284u","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I've been asking around, and can't get anyone to answer.   So please forgive me, I have to try with you as well.  What have you had good results with R1-0528?  I can't get anything decent code or any work based task out of it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hvitu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve been asking around, and can&amp;#39;t get anyone to answer.   So please forgive me, I have to try with you as well.  What have you had good results with R1-0528?  I can&amp;#39;t get anything decent code or any work based task out of it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hvitu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751242748,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hr3lg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Daniel_H212","can_mod_post":false,"created_utc":1751241103,"send_replies":true,"parent_id":"t3_1lnsax9","score":0,"author_fullname":"t2_1vi6fut","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yes, R1-0528 is on a similar level to Claude.\\n\\nNo, you won't be running it locally with any reasonable speed unless you drop enough money to buy a car.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hr3lg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, R1-0528 is on a similar level to Claude.&lt;/p&gt;\\n\\n&lt;p&gt;No, you won&amp;#39;t be running it locally with any reasonable speed unless you drop enough money to buy a car.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hr3lg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751241103,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}}]`),r=()=>e.jsx(l,{data:a});export{r as default};
