import{j as e}from"./index-BOnf-UhU.js";import{R as l}from"./RedditPostRenderer-Ce39qICS.js";import"./index-CDase6VD.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"With Nvlink or something... Sorry if this question has already sounded before","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Is it possible to get a common memory pool of 48 on two 3090?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m0tkly","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":1,"author_flair_background_color":null,"subreddit_type":"public","ups":1,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_uitx4","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":1,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752612994,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;With Nvlink or something... Sorry if this question has already sounded before&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m0tkly","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Andre4s11","discussion_type":null,"num_comments":23,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m0tkly/is_it_possible_to_get_a_common_memory_pool_of_48/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m0tkly/is_it_possible_to_get_a_common_memory_pool_of_48/","subreddit_subscribers":499773,"created_utc":1752612994,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3e1qgk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Conscious_Cut_6144","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3dm1tw","score":1,"author_fullname":"t2_9hl4ymvj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Not with inference tools that I have seen.  \\nProbably yes on closed source stuff.\\n\\nThe best I've seen is loading the model on one gpu and the ??other parts?? on a second gpu.\\n\\nI don't know much about this, but I think the other parts are like a text encoders and a vae","edited":false,"author_flair_css_class":null,"name":"t1_n3e1qgk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not with inference tools that I have seen.&lt;br/&gt;\\nProbably yes on closed source stuff.&lt;/p&gt;\\n\\n&lt;p&gt;The best I&amp;#39;ve seen is loading the model on one gpu and the ??other parts?? on a second gpu.&lt;/p&gt;\\n\\n&lt;p&gt;I don&amp;#39;t know much about this, but I think the other parts are like a text encoders and a vae&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m0tkly","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0tkly/is_it_possible_to_get_a_common_memory_pool_of_48/n3e1qgk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752638999,"author_flair_text":null,"collapsed":false,"created_utc":1752638999,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3dm1tw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"throwaway131072","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3c7s6s","score":1,"author_fullname":"t2_bf05e","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Do image gen models work better with nvl?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dm1tw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Do image gen models work better with nvl?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0tkly","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0tkly/is_it_possible_to_get_a_common_memory_pool_of_48/n3dm1tw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752632884,"author_flair_text":null,"treatment_tags":[],"created_utc":1752632884,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3c7s6s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Conscious_Cut_6144","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3c1n71","score":3,"author_fullname":"t2_9hl4ymvj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I mean it work poorly for image gen models.  \\nLLM's work great with multiple cards though.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3c7s6s","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I mean it work poorly for image gen models.&lt;br/&gt;\\nLLM&amp;#39;s work great with multiple cards though.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0tkly","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0tkly/is_it_possible_to_get_a_common_memory_pool_of_48/n3c7s6s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752615811,"author_flair_text":null,"treatment_tags":[],"created_utc":1752615811,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3d3jqv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"guchdog","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3d02ot","score":1,"author_fullname":"t2_23trc","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'm not certain but I don't this works the same with like video and image diffusion models. Maybe using GGUF for your image/video generation fixes that problem if you need to split. Food for thought.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n3d3jqv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m not certain but I don&amp;#39;t this works the same with like video and image diffusion models. Maybe using GGUF for your image/video generation fixes that problem if you need to split. Food for thought.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m0tkly","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0tkly/is_it_possible_to_get_a_common_memory_pool_of_48/n3d3jqv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752626379,"author_flair_text":null,"treatment_tags":[],"created_utc":1752626379,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":0,"name":"t1__","id":"_","parent_id":"t1_n3dblvk","depth":10,"children":[]}}],"before":null}},"user_reports":[],"saved":false,"id":"n3dblvk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Andre4s11","can_mod_post":false,"created_utc":1752629199,"send_replies":true,"parent_id":"t1_n3da910","score":2,"author_fullname":"t2_uitx4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"relatively low speed of exchange between cards and sharding and offload  in dram memory ,  like a 128-256 gb ddr4","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dblvk","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;relatively low speed of exchange between cards and sharding and offload  in dram memory ,  like a 128-256 gb ddr4&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0tkly/is_it_possible_to_get_a_common_memory_pool_of_48/n3dblvk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752629199,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0tkly","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":9,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3da910","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LA_rent_Aficionado","can_mod_post":false,"created_utc":1752628720,"send_replies":true,"parent_id":"t1_n3d9mzt","score":1,"author_fullname":"t2_t8zbiflk","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Expensive, yes. Obviously/realistic, perhaps if you shared a budget. I was just sharing that you can indeed run larger models with multiple 3090s but with tradeoffs vs equal VRAM alternatives.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3da910","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Expensive, yes. Obviously/realistic, perhaps if you shared a budget. I was just sharing that you can indeed run larger models with multiple 3090s but with tradeoffs vs equal VRAM alternatives.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0tkly/is_it_possible_to_get_a_common_memory_pool_of_48/n3da910/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752628720,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0tkly","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":8,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3d9mzt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Andre4s11","can_mod_post":false,"created_utc":1752628503,"send_replies":true,"parent_id":"t1_n3d3n32","score":1,"author_fullname":"t2_uitx4","approved_by":null,"mod_note":null,"all_awardings":[],"body":"obviously these are too expensive options and they are not realistic","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n3d9mzt","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;obviously these are too expensive options and they are not realistic&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m0tkly","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0tkly/is_it_possible_to_get_a_common_memory_pool_of_48/n3d9mzt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752628503,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3d3n32","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LA_rent_Aficionado","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3d02ot","score":1,"author_fullname":"t2_t8zbiflk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yes and no. You will have a 96GB pool of VRAM to shard models across but there are tradeoffs.  You could for instance use llama.cpp and squeeze the most out of this available VRAM but it will significantly reduce speed vs a more tensor parallel engine like VLLM that will be MUCH faster at the expense of more overhead eating up your available VRAM.  3090s are the GOAT of cost vs performance, but in most cases you're better off buying a single 48GB, 96GB card (budget permitting) vs 3090s for better performance, power utilization, ease of use, thermals, and compatability with newer advancements on the compute side.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n3d3n32","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes and no. You will have a 96GB pool of VRAM to shard models across but there are tradeoffs.  You could for instance use llama.cpp and squeeze the most out of this available VRAM but it will significantly reduce speed vs a more tensor parallel engine like VLLM that will be MUCH faster at the expense of more overhead eating up your available VRAM.  3090s are the GOAT of cost vs performance, but in most cases you&amp;#39;re better off buying a single 48GB, 96GB card (budget permitting) vs 3090s for better performance, power utilization, ease of use, thermals, and compatability with newer advancements on the compute side.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m0tkly","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0tkly/is_it_possible_to_get_a_common_memory_pool_of_48/n3d3n32/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752626412,"author_flair_text":null,"treatment_tags":[],"created_utc":1752626412,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3d02ot","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Andre4s11","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3cradd","score":1,"author_fullname":"t2_uitx4","approved_by":null,"mod_note":null,"all_awardings":[],"body":"think there should be a precise and unambiguous answer here, that even for 4 3090 cards I won't need anything extra to fully combine all the memory into one stack","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3d02ot","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;think there should be a precise and unambiguous answer here, that even for 4 3090 cards I won&amp;#39;t need anything extra to fully combine all the memory into one stack&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m0tkly","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0tkly/is_it_possible_to_get_a_common_memory_pool_of_48/n3d02ot/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752625161,"author_flair_text":null,"treatment_tags":[],"created_utc":1752625161,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3cradd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Zigtronik","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3ckpc2","score":1,"author_fullname":"t2_t5cnu","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I am not trying to make a statement at large about the efficiency of multi-GPU in general with what I said, just about what happens if you have nvlink or not and doing inference. \\n\\nMy experience with multi-GPU is that there certainly is overhead if that is what you are talking about too. ","edited":false,"author_flair_css_class":null,"name":"t1_n3cradd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I am not trying to make a statement at large about the efficiency of multi-GPU in general with what I said, just about what happens if you have nvlink or not and doing inference. &lt;/p&gt;\\n\\n&lt;p&gt;My experience with multi-GPU is that there certainly is overhead if that is what you are talking about too. &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m0tkly","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0tkly/is_it_possible_to_get_a_common_memory_pool_of_48/n3cradd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752622184,"author_flair_text":null,"collapsed":false,"created_utc":1752622184,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ckpc2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"guchdog","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3c5ll7","score":1,"author_fullname":"t2_23trc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Wait I thought one thing was even you had 2 cards only one of 1 of 2 GPUs were utilized, but the VRAM for both were 100%.  Meaning it adds latency for 1GPU talking to two cards.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ckpc2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Wait I thought one thing was even you had 2 cards only one of 1 of 2 GPUs were utilized, but the VRAM for both were 100%.  Meaning it adds latency for 1GPU talking to two cards.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0tkly","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0tkly/is_it_possible_to_get_a_common_memory_pool_of_48/n3ckpc2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752620022,"author_flair_text":null,"treatment_tags":[],"created_utc":1752620022,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3c7u3h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Zigtronik","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3c5y92","score":1,"author_fullname":"t2_t5cnu","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"yeah not having a NVLink when doing inference has been tested and IIRC was a margin of error difference for most workloads. For your own understanding, it is just because there not much data being passed between cards. what CAN help though is PCIE lanes or speeds, which for inference would help with model and context loading.","edited":false,"author_flair_css_class":null,"name":"t1_n3c7u3h","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;yeah not having a NVLink when doing inference has been tested and IIRC was a margin of error difference for most workloads. For your own understanding, it is just because there not much data being passed between cards. what CAN help though is PCIE lanes or speeds, which for inference would help with model and context loading.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m0tkly","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0tkly/is_it_possible_to_get_a_common_memory_pool_of_48/n3c7u3h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752615827,"author_flair_text":null,"collapsed":false,"created_utc":1752615827,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3c5y92","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Andre4s11","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3c5ll7","score":1,"author_fullname":"t2_uitx4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It turns out that Deepiseek and GPT lied to me","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3c5y92","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It turns out that Deepiseek and GPT lied to me&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0tkly","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0tkly/is_it_possible_to_get_a_common_memory_pool_of_48/n3c5y92/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752615267,"author_flair_text":null,"treatment_tags":[],"created_utc":1752615267,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3c5ll7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Zigtronik","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3c1n71","score":1,"author_fullname":"t2_t5cnu","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"None at all","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3c5ll7","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;None at all&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0tkly","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0tkly/is_it_possible_to_get_a_common_memory_pool_of_48/n3c5ll7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752615163,"author_flair_text":null,"treatment_tags":[],"created_utc":1752615163,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3c1n71","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Andre4s11","can_mod_post":false,"created_utc":1752613989,"send_replies":true,"parent_id":"t1_n3c1ai3","score":1,"author_fullname":"t2_uitx4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What limitations will I experience?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3c1n71","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What limitations will I experience?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0tkly","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0tkly/is_it_possible_to_get_a_common_memory_pool_of_48/n3c1n71/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752613989,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3c1ai3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Conscious_Cut_6144","can_mod_post":false,"created_utc":1752613886,"send_replies":true,"parent_id":"t3_1m0tkly","score":4,"author_fullname":"t2_9hl4ymvj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yes and No.\\n\\nIt's still 2 cards with 24GB each.  \\nBut you can run 48GB models as if it's a single card.  \\nNVlink is not needed for inference.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3c1ai3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes and No.&lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s still 2 cards with 24GB each.&lt;br/&gt;\\nBut you can run 48GB models as if it&amp;#39;s a single card.&lt;br/&gt;\\nNVlink is not needed for inference.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0tkly/is_it_possible_to_get_a_common_memory_pool_of_48/n3c1ai3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752613886,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0tkly","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3d3wba","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LA_rent_Aficionado","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3c1360","score":1,"author_fullname":"t2_t8zbiflk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"this although 2x card will certainly be faster for interfacing all things being equal.  For running models locally OP wont need NVlink, this would only really benefit training.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3d3wba","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;this although 2x card will certainly be faster for interfacing all things being equal.  For running models locally OP wont need NVlink, this would only really benefit training.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0tkly","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0tkly/is_it_possible_to_get_a_common_memory_pool_of_48/n3d3wba/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752626501,"author_flair_text":null,"treatment_tags":[],"created_utc":1752626501,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3c1360","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jacek2023","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3c02om","score":2,"author_fullname":"t2_vqgbql9w","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"with llama.cpp you can run models bigger than 24GB on two 3090, no nvlink is needed","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3c1360","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;with llama.cpp you can run models bigger than 24GB on two 3090, no nvlink is needed&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0tkly","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0tkly/is_it_possible_to_get_a_common_memory_pool_of_48/n3c1360/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752613828,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752613828,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3c02om","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Andre4s11","can_mod_post":false,"created_utc":1752613538,"send_replies":true,"parent_id":"t1_n3bzvaz","score":1,"author_fullname":"t2_uitx4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I want run model locally in one pool memory 48,g","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3c02om","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I want run model locally in one pool memory 48,g&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0tkly","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0tkly/is_it_possible_to_get_a_common_memory_pool_of_48/n3c02om/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752613538,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3bzvaz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jacek2023","can_mod_post":false,"created_utc":1752613479,"send_replies":true,"parent_id":"t3_1m0tkly","score":1,"author_fullname":"t2_vqgbql9w","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"what exactly do you want to achieve? (I use two 3090)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3bzvaz","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;what exactly do you want to achieve? (I use two 3090)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0tkly/is_it_possible_to_get_a_common_memory_pool_of_48/n3bzvaz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752613479,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m0tkly","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3etsvr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"__JockY__","can_mod_post":false,"created_utc":1752653846,"send_replies":true,"parent_id":"t3_1m0tkly","score":1,"author_fullname":"t2_qf8h7ka8","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You don’t need anything special because all the major LLM inference software (llama.cpp, vLLM, Sglang, TensorRT, etc) already support spreading the model across multiple GPUs without any fuss.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3etsvr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You don’t need anything special because all the major LLM inference software (llama.cpp, vLLM, Sglang, TensorRT, etc) already support spreading the model across multiple GPUs without any fuss.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0tkly/is_it_possible_to_get_a_common_memory_pool_of_48/n3etsvr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752653846,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0tkly","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(l,{data:a});export{n as default};
