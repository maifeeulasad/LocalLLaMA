import{j as e}from"./index-CWmJdUH_.js";import{R as l}from"./RedditPostRenderer-D2iunoQ9.js";import"./index-BCg9RP6g.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Can you guys please tell me what am i doing wrong here.  \\nMy model keeps calling tool for every response, even if it's not necessary even for simple \\"hey\\".\\n\\n    import ollama\\n    from tools import (\\n        read_file, write_file,\\n    )\\n    \\n    class Cron:\\n        def __init__(self, model_name: str = \\"llama3.1:latest\\", mood : str = \\"sarcastic: fast, speaks in memes.\\"):\\n            self.model_name = model_name\\n            self.messages = []\\n            self.tools = [read_file,write_file]\\n            self.mood = mood\\n            self.system_prompt = f\\"Don't call tools unless it's necessary.\\"\\n            self.messages.append(\\n                { \\"role\\": \\"system\\", \\"content\\": self.system_prompt }\\n            )\\n    \\n        def handle_tool_calls(self, model_response: ollama.ChatResponse):\\n            while model_response.message.tool_calls:\\n                self.messages.append(\\n                    { \\"role\\": \\"assistant\\", \\"content\\": model_response.message.content }\\n                )\\n    \\n                print(f\\"\\\\nTool Calls: {model_response}\\")\\n    \\n                for tool in model_response.message.tool_calls:\\n                    tool_name = tool.function.name\\n                    tool_arg = tool.function.arguments\\n    \\n                    tool_response = run_tool(tool_name, tool_arg)\\n    \\n                    self.messages.append({\\n                        \\"role\\": \\"tool\\",\\n                        \\"content\\": tool_response\\n                    })\\n    \\n                model_response = None\\n    \\n                model_response = ollama.chat(\\n                    model = self.model_name,\\n                    messages = self.messages,\\n                    tools = self.tools,\\n                )\\n    \\n                print(f\\"Model response : {self.messages}\\")\\n            \\n            return model_response\\n    \\n    \\n        def chat(self, user_prompt: str):\\n            self.messages.append(\\n                { \\"role\\": \\"user\\", \\"content\\": user_prompt }\\n            )\\n            response = ollama.chat(\\n                model = self.model_name,\\n                messages = self.messages,\\n                tools = self.tools,\\n            )\\n    \\n            if response.message.tool_calls:\\n                response = self.handle_tool_calls(response)\\n    \\n            content = response.message.content\\n            self.messages.append(\\n                { \\"role\\": \\"assistant\\", \\"content\\": content }\\n            )\\n    \\n            return response.message.content\\n        \\n    \\n    def main():\\n        cron = Cron()\\n    \\n        while True:\\n            print(\\"=\\" * 50)\\n            user_prompt = input(\\"\\\\nYou: \\").strip()\\n            \\n            if user_prompt.lower() == \\"exit\\":\\n                exit()\\n    \\n            response = cron.chat(user_prompt=user_prompt)\\n            print(f\\"\\\\nCron: {response}\\")\\n    \\n    if __name__ == \\"__main__\\":\\n        main()\\n    \\n    \\n    \\n\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"I want to create a local AI Agent that can call tools. but my model call tools even for \\"hey\\"","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m3spek","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.67,"author_flair_background_color":null,"subreddit_type":"public","ups":1,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_9w29orwe","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":1,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1752920645,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752919956,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Can you guys please tell me what am i doing wrong here.&lt;br/&gt;\\nMy model keeps calling tool for every response, even if it&amp;#39;s not necessary even for simple &amp;quot;hey&amp;quot;.&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;import ollama\\nfrom tools import (\\n    read_file, write_file,\\n)\\n\\nclass Cron:\\n    def __init__(self, model_name: str = &amp;quot;llama3.1:latest&amp;quot;, mood : str = &amp;quot;sarcastic: fast, speaks in memes.&amp;quot;):\\n        self.model_name = model_name\\n        self.messages = []\\n        self.tools = [read_file,write_file]\\n        self.mood = mood\\n        self.system_prompt = f&amp;quot;Don&amp;#39;t call tools unless it&amp;#39;s necessary.&amp;quot;\\n        self.messages.append(\\n            { &amp;quot;role&amp;quot;: &amp;quot;system&amp;quot;, &amp;quot;content&amp;quot;: self.system_prompt }\\n        )\\n\\n    def handle_tool_calls(self, model_response: ollama.ChatResponse):\\n        while model_response.message.tool_calls:\\n            self.messages.append(\\n                { &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: model_response.message.content }\\n            )\\n\\n            print(f&amp;quot;\\\\nTool Calls: {model_response}&amp;quot;)\\n\\n            for tool in model_response.message.tool_calls:\\n                tool_name = tool.function.name\\n                tool_arg = tool.function.arguments\\n\\n                tool_response = run_tool(tool_name, tool_arg)\\n\\n                self.messages.append({\\n                    &amp;quot;role&amp;quot;: &amp;quot;tool&amp;quot;,\\n                    &amp;quot;content&amp;quot;: tool_response\\n                })\\n\\n            model_response = None\\n\\n            model_response = ollama.chat(\\n                model = self.model_name,\\n                messages = self.messages,\\n                tools = self.tools,\\n            )\\n\\n            print(f&amp;quot;Model response : {self.messages}&amp;quot;)\\n\\n        return model_response\\n\\n\\n    def chat(self, user_prompt: str):\\n        self.messages.append(\\n            { &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: user_prompt }\\n        )\\n        response = ollama.chat(\\n            model = self.model_name,\\n            messages = self.messages,\\n            tools = self.tools,\\n        )\\n\\n        if response.message.tool_calls:\\n            response = self.handle_tool_calls(response)\\n\\n        content = response.message.content\\n        self.messages.append(\\n            { &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: content }\\n        )\\n\\n        return response.message.content\\n\\n\\ndef main():\\n    cron = Cron()\\n\\n    while True:\\n        print(&amp;quot;=&amp;quot; * 50)\\n        user_prompt = input(&amp;quot;\\\\nYou: &amp;quot;).strip()\\n\\n        if user_prompt.lower() == &amp;quot;exit&amp;quot;:\\n            exit()\\n\\n        response = cron.chat(user_prompt=user_prompt)\\n        print(f&amp;quot;\\\\nCron: {response}&amp;quot;)\\n\\nif __name__ == &amp;quot;__main__&amp;quot;:\\n    main()\\n&lt;/code&gt;&lt;/pre&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m3spek","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Prajwell","discussion_type":null,"num_comments":9,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m3spek/i_want_to_create_a_local_ai_agent_that_can_call/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m3spek/i_want_to_create_a_local_ai_agent_that_can_call/","subreddit_subscribers":501753,"created_utc":1752919956,"num_crossposts":1,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n40jntt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"TechnoByte_","can_mod_post":false,"created_utc":1752940927,"send_replies":true,"parent_id":"t3_1m3spek","score":2,"author_fullname":"t2_4w91lkml","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This is a problem with llama models, try another LLM like qwen3 or mistral-small3.2","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n40jntt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is a problem with llama models, try another LLM like qwen3 or mistral-small3.2&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3spek/i_want_to_create_a_local_ai_agent_that_can_call/n40jntt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752940927,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3spek","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n406zki","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Prajwell","can_mod_post":false,"send_replies":true,"parent_id":"t1_n406nrp","score":1,"author_fullname":"t2_9w29orwe","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Will try them out. Thanks","edited":false,"author_flair_css_class":null,"name":"t1_n406zki","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Will try them out. Thanks&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m3spek","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3spek/i_want_to_create_a_local_ai_agent_that_can_call/n406zki/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752937009,"author_flair_text":null,"collapsed":false,"created_utc":1752937009,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n406nrp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HauntingTechnician30","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4033ni","score":2,"author_fullname":"t2_j21p2h1x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I never used any of those but Qwen-3-8b and Ministral-8b-Instruct are two similar sized models that should perform better.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n406nrp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I never used any of those but Qwen-3-8b and Ministral-8b-Instruct are two similar sized models that should perform better.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3spek","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3spek/i_want_to_create_a_local_ai_agent_that_can_call/n406nrp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752936907,"author_flair_text":null,"treatment_tags":[],"created_utc":1752936907,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4033ni","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Prajwell","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3zricl","score":1,"author_fullname":"t2_9w29orwe","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What other models should i use for agents??\\nI using mac m2 with 8gb ramðŸ¥¹","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4033ni","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What other models should i use for agents??\\nI using mac m2 with 8gb ramðŸ¥¹&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3spek","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3spek/i_want_to_create_a_local_ai_agent_that_can_call/n4033ni/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752935783,"author_flair_text":null,"treatment_tags":[],"created_utc":1752935783,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3zricl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"svachalek","can_mod_post":false,"created_utc":1752931915,"send_replies":true,"parent_id":"t1_n3z95yf","score":3,"author_fullname":"t2_6cey3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Bullseye. Also note that llama3.1 is quite old at this point, itâ€™s likely youâ€™ll do better with something newer.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3zricl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Bullseye. Also note that llama3.1 is quite old at this point, itâ€™s likely youâ€™ll do better with something newer.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3spek","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3spek/i_want_to_create_a_local_ai_agent_that_can_call/n3zricl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752931915,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n402qt4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Prajwell","can_mod_post":false,"created_utc":1752935669,"send_replies":true,"parent_id":"t1_n3z95yf","score":1,"author_fullname":"t2_9w29orwe","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks bud.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n402qt4","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks bud.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3spek","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3spek/i_want_to_create_a_local_ai_agent_that_can_call/n402qt4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752935669,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3z95yf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HauntingTechnician30","can_mod_post":false,"created_utc":1752924636,"send_replies":true,"parent_id":"t3_1m3spek","score":3,"author_fullname":"t2_j21p2h1x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"https://github.com/ollama/ollama/issues/6127","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3z95yf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://github.com/ollama/ollama/issues/6127\\"&gt;https://github.com/ollama/ollama/issues/6127&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3spek/i_want_to_create_a_local_ai_agent_that_can_call/n3z95yf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752924636,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3spek","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}}]`),o=()=>e.jsx(l,{data:a});export{o as default};
