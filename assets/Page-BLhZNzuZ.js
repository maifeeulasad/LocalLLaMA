import{j as e}from"./index-DLSqWzaI.js";import{R as t}from"./RedditPostRenderer-CysRo2D_.js";import"./index-COXiL3Lo.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Nowadays there are tons of benchmarks \\n\\nFor general intelligence?! (MMLU, GPQA,…etc)\\n\\nFor other stuff like Coding \\n\\n\\nAnd tons of ELO based arenas \\nFor different task and so on\\n\\n\\nBut I test on questions relevant to my field and with answer criteria and some other times subjectively by looking at it\\n\\n\\n\\nI work in cybersecurity so I ask for queries to investigate certain cyber attacks (a bit of coding but not real coding and a bit of SQL but really SQL)\\n\\n\\nI couldn’t help but notice that despite not a great performance on coding benchmarks \\nBut mistral model returns the answer that is to the point \\nSometimes it misses a bit of inaccuracy (like niche attacking techniques and tricks)\\nBut it doesn’t have gibberish or garbage \\n\\n\\nAll other models including top closed and open models just plainly don’t do it \\n\\nChatGPT generates long answers that are going to be a performance intensive search and with many things that could be simpler and get to the point\\n\\n\\nGemini even generates more lengthy answers\\n\\nDeepseek somehow in between \\n\\nClaude and Llama also generate some weird stuff (interestingly llama answers well but in a generic query language not the vendor specific query language that I asked for, but it captures most of the tricky techniques!)\\n\\n\\n\\n\\nThis is weirding me out and I want to know if there is a better way to pick a good model to such a subjective criteria (tbh I feel like I just am biased for some reason but I am developing an automation agent and the most useful output for such a program to use is mistral output \\n\\nSo it has what I can name better ecosystem integration \\n\\n(I have a python script that will query the model static query “give me the query to push to system x to investigate attack technique y” and sometimes with context “for user z on machine alpha …etc”)\\n\\nThe most useful output comes from mistral (to the point and efficient, and it captures the general technique and for my use the niche or tricky parts should be verified by a human analysts on the output of the model or automation agent! (Which could then by passed again to another LLM based agent for tricky parts deeper investigation!)\\n\\n(Llama comes in second it knows the tricky parts but its output isn’t automate able (so maybe it gives more knowledge for the human analysts, but can’t be as easily automated))\\n\\n\\nParadoxically enough, closed models and open models that perform better on benchmarks (such as deepseek!) doesn’t give better results\\n\\nIt gives much more noisy garbage \\n\\n\\nI want to hear from others who faced similar challenges and how did you solve it \\n\\nThanks ","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Subjectivity of LLM performance vs benchmarks (Garbage In Garbage Out!)","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lvz9ic","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.5,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_jbjmmax41","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752108113,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Nowadays there are tons of benchmarks &lt;/p&gt;\\n\\n&lt;p&gt;For general intelligence?! (MMLU, GPQA,…etc)&lt;/p&gt;\\n\\n&lt;p&gt;For other stuff like Coding &lt;/p&gt;\\n\\n&lt;p&gt;And tons of ELO based arenas \\nFor different task and so on&lt;/p&gt;\\n\\n&lt;p&gt;But I test on questions relevant to my field and with answer criteria and some other times subjectively by looking at it&lt;/p&gt;\\n\\n&lt;p&gt;I work in cybersecurity so I ask for queries to investigate certain cyber attacks (a bit of coding but not real coding and a bit of SQL but really SQL)&lt;/p&gt;\\n\\n&lt;p&gt;I couldn’t help but notice that despite not a great performance on coding benchmarks \\nBut mistral model returns the answer that is to the point \\nSometimes it misses a bit of inaccuracy (like niche attacking techniques and tricks)\\nBut it doesn’t have gibberish or garbage &lt;/p&gt;\\n\\n&lt;p&gt;All other models including top closed and open models just plainly don’t do it &lt;/p&gt;\\n\\n&lt;p&gt;ChatGPT generates long answers that are going to be a performance intensive search and with many things that could be simpler and get to the point&lt;/p&gt;\\n\\n&lt;p&gt;Gemini even generates more lengthy answers&lt;/p&gt;\\n\\n&lt;p&gt;Deepseek somehow in between &lt;/p&gt;\\n\\n&lt;p&gt;Claude and Llama also generate some weird stuff (interestingly llama answers well but in a generic query language not the vendor specific query language that I asked for, but it captures most of the tricky techniques!)&lt;/p&gt;\\n\\n&lt;p&gt;This is weirding me out and I want to know if there is a better way to pick a good model to such a subjective criteria (tbh I feel like I just am biased for some reason but I am developing an automation agent and the most useful output for such a program to use is mistral output &lt;/p&gt;\\n\\n&lt;p&gt;So it has what I can name better ecosystem integration &lt;/p&gt;\\n\\n&lt;p&gt;(I have a python script that will query the model static query “give me the query to push to system x to investigate attack technique y” and sometimes with context “for user z on machine alpha …etc”)&lt;/p&gt;\\n\\n&lt;p&gt;The most useful output comes from mistral (to the point and efficient, and it captures the general technique and for my use the niche or tricky parts should be verified by a human analysts on the output of the model or automation agent! (Which could then by passed again to another LLM based agent for tricky parts deeper investigation!)&lt;/p&gt;\\n\\n&lt;p&gt;(Llama comes in second it knows the tricky parts but its output isn’t automate able (so maybe it gives more knowledge for the human analysts, but can’t be as easily automated))&lt;/p&gt;\\n\\n&lt;p&gt;Paradoxically enough, closed models and open models that perform better on benchmarks (such as deepseek!) doesn’t give better results&lt;/p&gt;\\n\\n&lt;p&gt;It gives much more noisy garbage &lt;/p&gt;\\n\\n&lt;p&gt;I want to hear from others who faced similar challenges and how did you solve it &lt;/p&gt;\\n\\n&lt;p&gt;Thanks &lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lvz9ic","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Potential_Block4598","discussion_type":null,"num_comments":6,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lvz9ic/subjectivity_of_llm_performance_vs_benchmarks/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lvz9ic/subjectivity_of_llm_performance_vs_benchmarks/","subreddit_subscribers":497354,"created_utc":1752108113,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2am37v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"NNN_Throwaway2","can_mod_post":false,"created_utc":1752114735,"send_replies":true,"parent_id":"t3_1lvz9ic","score":2,"author_fullname":"t2_8rrihts9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Benchmarks only assess a tiny sliver of a model's capabilities, and its been demonstrated multiple times now that LLM knowledge doesn't generalize. So, benchmark performance can't reliably predict output quality on specific tasks. \\n\\nBut, benchmarks are what generates hype and investor dollars, so that's what we're stuck with.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2am37v","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Benchmarks only assess a tiny sliver of a model&amp;#39;s capabilities, and its been demonstrated multiple times now that LLM knowledge doesn&amp;#39;t generalize. So, benchmark performance can&amp;#39;t reliably predict output quality on specific tasks. &lt;/p&gt;\\n\\n&lt;p&gt;But, benchmarks are what generates hype and investor dollars, so that&amp;#39;s what we&amp;#39;re stuck with.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvz9ic/subjectivity_of_llm_performance_vs_benchmarks/n2am37v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752114735,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvz9ic","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2bw67j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Potential_Block4598","can_mod_post":false,"created_utc":1752136731,"send_replies":true,"parent_id":"t1_n2apsqc","score":1,"author_fullname":"t2_jbjmmax41","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Exactly less Slop that is why I prefer it now to other flagship models","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2bw67j","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Exactly less Slop that is why I prefer it now to other flagship models&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvz9ic","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvz9ic/subjectivity_of_llm_performance_vs_benchmarks/n2bw67j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752136731,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2apsqc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Pedalnomica","can_mod_post":false,"created_utc":1752116100,"send_replies":true,"parent_id":"t3_1lvz9ic","score":3,"author_fullname":"t2_b0d7j6x9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It's been awhile since I was heavily using a mistral model, but OG large was ahead of its time imo. Great answers with way less slop.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2apsqc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s been awhile since I was heavily using a mistral model, but OG large was ahead of its time imo. Great answers with way less slop.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvz9ic/subjectivity_of_llm_performance_vs_benchmarks/n2apsqc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752116100,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvz9ic","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ba5cr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DinoAmino","can_mod_post":false,"created_utc":1752124763,"send_replies":true,"parent_id":"t3_1lvz9ic","score":2,"author_fullname":"t2_j1v7f","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Which Mistral are you talking about? The new Small 3.2? Is the primary issue the format of the output? If you like Mistral then you might want to  try Devstral and look into using structured outputs. Or pair it up with smolagents or whatever agent framework you like. Either way you should be able to get the answers you want in the structure you require.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ba5cr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Which Mistral are you talking about? The new Small 3.2? Is the primary issue the format of the output? If you like Mistral then you might want to  try Devstral and look into using structured outputs. Or pair it up with smolagents or whatever agent framework you like. Either way you should be able to get the answers you want in the structure you require.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvz9ic/subjectivity_of_llm_performance_vs_benchmarks/n2ba5cr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752124763,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvz9ic","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),o=()=>e.jsx(t,{data:a});export{o as default};
