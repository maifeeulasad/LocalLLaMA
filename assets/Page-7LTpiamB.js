import{j as e}from"./index-BUtHYhT3.js";import{R as l}from"./RedditPostRenderer-BaN1Fn7z.js";import"./index-Cli9kp5v.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"### Anyone here run llama4 with 1 million to 10 million context?\\n\\nJust curious if anyone has. If yes please list your software platform (i.e. vLLM, Ollama, llama.cpp, etc), your GPU count and make models.\\n\\nWhat are vram/ram requirements for 1m context? 10m context?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Anyone here run llama4 scout/Maverick with 1 million to 10 million context?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lqmbh3","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.78,"author_flair_background_color":null,"subreddit_type":"public","ups":15,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_3h2irqtz","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":15,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1751540265,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751539102,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;h3&gt;Anyone here run llama4 with 1 million to 10 million context?&lt;/h3&gt;\\n\\n&lt;p&gt;Just curious if anyone has. If yes please list your software platform (i.e. vLLM, Ollama, llama.cpp, etc), your GPU count and make models.&lt;/p&gt;\\n\\n&lt;p&gt;What are vram/ram requirements for 1m context? 10m context?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lqmbh3","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"night0x63","discussion_type":null,"num_comments":26,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lqmbh3/anyone_here_run_llama4_scoutmaverick_with_1/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lqmbh3/anyone_here_run_llama4_scoutmaverick_with_1/","subreddit_subscribers":494198,"created_utc":1751539102,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n14o7cr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"night0x63","can_mod_post":false,"created_utc":1751551741,"send_replies":true,"parent_id":"t1_n13whiy","score":3,"author_fullname":"t2_3h2irqtz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I did a ChatGPT assignment to have it analyze n got reposted to do: description, commits in last year, open issues, etc etc. Analyze to see if healthy. With ChatGPT 4.0 only did one and ignored all others. Then did with ChatGPT o4-mini-high and it worked perfect. ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n14o7cr","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I did a ChatGPT assignment to have it analyze n got reposted to do: description, commits in last year, open issues, etc etc. Analyze to see if healthy. With ChatGPT 4.0 only did one and ignored all others. Then did with ChatGPT o4-mini-high and it worked perfect. &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqmbh3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqmbh3/anyone_here_run_llama4_scoutmaverick_with_1/n14o7cr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751551741,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n14p196","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"__JockY__","can_mod_post":false,"send_replies":true,"parent_id":"t1_n14j44c","score":3,"author_fullname":"t2_qf8h7ka8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I haven’t seen a local model yet that doesn’t start getting stupid past ~ 30k tokens.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n14p196","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I haven’t seen a local model yet that doesn’t start getting stupid past ~ 30k tokens.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqmbh3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqmbh3/anyone_here_run_llama4_scoutmaverick_with_1/n14p196/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751551993,"author_flair_text":null,"treatment_tags":[],"created_utc":1751551993,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n14j44c","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"night0x63","can_mod_post":false,"created_utc":1751550163,"send_replies":true,"parent_id":"t1_n13whiy","score":1,"author_fullname":"t2_3h2irqtz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Hm. Interesting. So the specs say 1m context... But in practice it is... Probably like 128k or something. But... Honestly I'll take that if it actually works with that ... But it sucks to be mis labeled. \\n\\n\\nP.s.\\n\\n\\nI had similar story. With llama3.2 I tried long context and it failed miserable with just like five or ten files. \\n\\n\\nMy conclusion: with llama3.2 it says Max context is 128k but actual is way way less. Probably 4 to 30k.\\n\\n\\nThen I tried llama3.3 and it worked perfect. \\n\\n\\nAlso llama3.3 worked way way better than llama3.2... followed instructions way way better with large context... But again looking like 30k.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n14j44c","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hm. Interesting. So the specs say 1m context... But in practice it is... Probably like 128k or something. But... Honestly I&amp;#39;ll take that if it actually works with that ... But it sucks to be mis labeled. &lt;/p&gt;\\n\\n&lt;p&gt;P.s.&lt;/p&gt;\\n\\n&lt;p&gt;I had similar story. With llama3.2 I tried long context and it failed miserable with just like five or ten files. &lt;/p&gt;\\n\\n&lt;p&gt;My conclusion: with llama3.2 it says Max context is 128k but actual is way way less. Probably 4 to 30k.&lt;/p&gt;\\n\\n&lt;p&gt;Then I tried llama3.3 and it worked perfect. &lt;/p&gt;\\n\\n&lt;p&gt;Also llama3.3 worked way way better than llama3.2... followed instructions way way better with large context... But again looking like 30k.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqmbh3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqmbh3/anyone_here_run_llama4_scoutmaverick_with_1/n14j44c/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751550163,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n13whiy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Lissanro","can_mod_post":false,"created_utc":1751541768,"send_replies":true,"parent_id":"t3_1lqmbh3","score":14,"author_fullname":"t2_fpfao9g","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I could but there is no point because effective context size is much smaller, unfortunately.\\n\\nI think Llama 4 could have been an excellent model if its large context performed well. In one of my tests that I thought should be trivial, I put few long articles from Wikipedia to fill 0.5M context and asked to list article titles and to provide summary for each, but it only summarized the last article, ignoring the rest, on multiple tries to regenerate with different seeds, both with Scout and Maverick. For the same reason Maverick cannot do well with large code bases, quality would be bad compared to selectively giving files to R1 or Qwen3 235B, both of them would produce far better results.","edited":1751543928,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n13whiy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I could but there is no point because effective context size is much smaller, unfortunately.&lt;/p&gt;\\n\\n&lt;p&gt;I think Llama 4 could have been an excellent model if its large context performed well. In one of my tests that I thought should be trivial, I put few long articles from Wikipedia to fill 0.5M context and asked to list article titles and to provide summary for each, but it only summarized the last article, ignoring the rest, on multiple tries to regenerate with different seeds, both with Scout and Maverick. For the same reason Maverick cannot do well with large code bases, quality would be bad compared to selectively giving files to R1 or Qwen3 235B, both of them would produce far better results.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqmbh3/anyone_here_run_llama4_scoutmaverick_with_1/n13whiy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751541768,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqmbh3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":14}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"609bf7d4-01f3-11f0-9760-5611c8333bee","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"609bf7d4-01f3-11f0-9760-5611c8333bee","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n14o74m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"You_Wen_AzzHu","can_mod_post":false,"send_replies":true,"parent_id":"t1_n14npy6","score":1,"author_fullname":"t2_p4oxcufl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"More like a hit or miss according to our needle in the haystack test.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n14o74m","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"exllama"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;More like a hit or miss according to our needle in the haystack test.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqmbh3","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqmbh3/anyone_here_run_llama4_scoutmaverick_with_1/n14o74m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751551740,"author_flair_text":"exllama","treatment_tags":[],"created_utc":1751551740,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n14npy6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"night0x63","can_mod_post":false,"created_utc":1751551594,"send_replies":true,"parent_id":"t1_n143sox","score":1,"author_fullname":"t2_3h2irqtz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Even 100k?!\\n\\n\\nSo... The max context is like total lie.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n14npy6","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Even 100k?!&lt;/p&gt;\\n\\n&lt;p&gt;So... The max context is like total lie.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqmbh3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqmbh3/anyone_here_run_llama4_scoutmaverick_with_1/n14npy6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751551594,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"609bf7d4-01f3-11f0-9760-5611c8333bee","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n17enbo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"You_Wen_AzzHu","can_mod_post":false,"send_replies":true,"parent_id":"t1_n17dxvw","score":1,"author_fullname":"t2_p4oxcufl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"No.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n17enbo","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"exllama"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqmbh3","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqmbh3/anyone_here_run_llama4_scoutmaverick_with_1/n17enbo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751580473,"author_flair_text":"exllama","treatment_tags":[],"created_utc":1751580473,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n17dxvw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SlaveZelda","can_mod_post":false,"created_utc":1751580237,"send_replies":true,"parent_id":"t1_n143sox","score":1,"author_fullname":"t2_7cbr10bw","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"do you run a quant version ?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n17dxvw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;do you run a quant version ?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqmbh3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqmbh3/anyone_here_run_llama4_scoutmaverick_with_1/n17dxvw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751580237,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n143sox","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"You_Wen_AzzHu","can_mod_post":false,"created_utc":1751544799,"send_replies":true,"parent_id":"t3_1lqmbh3","score":4,"author_fullname":"t2_p4oxcufl","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"We run this in Dev with 100k. It doesn't perform well with long context.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n143sox","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"exllama"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;We run this in Dev with 100k. It doesn&amp;#39;t perform well with long context.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqmbh3/anyone_here_run_llama4_scoutmaverick_with_1/n143sox/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751544799,"author_flair_text":"exllama","treatment_tags":[],"link_id":"t3_1lqmbh3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n17ztpa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"night0x63","can_mod_post":false,"send_replies":true,"parent_id":"t1_n155v92","score":1,"author_fullname":"t2_3h2irqtz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You are correct. Large context still requires large vram. My statement was oversimplifying. I should have said with smaller context.","edited":false,"author_flair_css_class":null,"name":"t1_n17ztpa","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You are correct. Large context still requires large vram. My statement was oversimplifying. I should have said with smaller context.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lqmbh3","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqmbh3/anyone_here_run_llama4_scoutmaverick_with_1/n17ztpa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751587737,"author_flair_text":null,"collapsed":false,"created_utc":1751587737,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n155v92","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"astralDangers","can_mod_post":false,"send_replies":false,"parent_id":"t1_n14jlo6","score":1,"author_fullname":"t2_96t64s3h","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This is incorrect, it's not about the model layers it's the context window and having to calculate up to 2m tokens. That uses a massive amount of memory. You can't fit that much RAM in a consumer PC, it has to be server on CPU or a split. This is a memory intensive process and even though it's doable on a sever with a couple of TB it will extremely slow (walk away to take lunch slow) to get the first token generated do to bottlenecks in ram speed vs vram speed.. \\n\\nEven with extreme quantization you're still talking about quadratic scaling on token context..","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n155v92","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is incorrect, it&amp;#39;s not about the model layers it&amp;#39;s the context window and having to calculate up to 2m tokens. That uses a massive amount of memory. You can&amp;#39;t fit that much RAM in a consumer PC, it has to be server on CPU or a split. This is a memory intensive process and even though it&amp;#39;s doable on a sever with a couple of TB it will extremely slow (walk away to take lunch slow) to get the first token generated do to bottlenecks in ram speed vs vram speed.. &lt;/p&gt;\\n\\n&lt;p&gt;Even with extreme quantization you&amp;#39;re still talking about quadratic scaling on token context..&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqmbh3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqmbh3/anyone_here_run_llama4_scoutmaverick_with_1/n155v92/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751556831,"author_flair_text":null,"treatment_tags":[],"created_utc":1751556831,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n14jlo6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"night0x63","can_mod_post":false,"send_replies":true,"parent_id":"t1_n14fvhe","score":1,"author_fullname":"t2_3h2irqtz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah... That's what I'm talking about... Single GPU works amazing! But you need to do chatgpt4/claude4 coding... And nope. IMO it was designed for single GPU (only 17b active parameters)... But that constraint is too limiting.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n14jlo6","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah... That&amp;#39;s what I&amp;#39;m talking about... Single GPU works amazing! But you need to do chatgpt4/claude4 coding... And nope. IMO it was designed for single GPU (only 17b active parameters)... But that constraint is too limiting.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqmbh3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqmbh3/anyone_here_run_llama4_scoutmaverick_with_1/n14jlo6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751550319,"author_flair_text":null,"treatment_tags":[],"created_utc":1751550319,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n159w58","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"tomz17","can_mod_post":false,"send_replies":true,"parent_id":"t1_n156kbr","score":0,"author_fullname":"t2_1mhx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"That's a separate issue...  I'm replying directly to a person who claims the llama4 models themselves are not runnable without 500-2TB of VRAM, which is false.  They will run on any computer with lots of sufficient moderately fast system RAM, and run reasonably well (due to the MOE architecture).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n159w58","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s a separate issue...  I&amp;#39;m replying directly to a person who claims the llama4 models themselves are not runnable without 500-2TB of VRAM, which is false.  They will run on any computer with lots of sufficient moderately fast system RAM, and run reasonably well (due to the MOE architecture).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqmbh3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqmbh3/anyone_here_run_llama4_scoutmaverick_with_1/n159w58/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751557955,"author_flair_text":null,"treatment_tags":[],"created_utc":1751557955,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n156kbr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"astralDangers","can_mod_post":false,"send_replies":false,"parent_id":"t1_n14fvhe","score":0,"author_fullname":"t2_96t64s3h","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I think you missed the key problem.. there's no way you're getting anywhere near 1m token context certainly not 2m.. splitting layers isn't the issue..","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n156kbr","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think you missed the key problem.. there&amp;#39;s no way you&amp;#39;re getting anywhere near 1m token context certainly not 2m.. splitting layers isn&amp;#39;t the issue..&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqmbh3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqmbh3/anyone_here_run_llama4_scoutmaverick_with_1/n156kbr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751557027,"author_flair_text":null,"treatment_tags":[],"created_utc":1751557027,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n14fvhe","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"tomz17","can_mod_post":false,"created_utc":1751549106,"send_replies":true,"parent_id":"t1_n13s1gv","score":6,"author_fullname":"t2_1mhx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; 500GB-2TB only a cluster of commerical GPUs\\n\\nThe really nice thing about llama4 is that they are MOE's... so I can get like 50t/s on maverick on a single 3090 and a 12-channel DDR5 system.\\n\\nWorthless for commercial levels of inference, but fine for a hobbyist.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n14fvhe","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;500GB-2TB only a cluster of commerical GPUs&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;The really nice thing about llama4 is that they are MOE&amp;#39;s... so I can get like 50t/s on maverick on a single 3090 and a 12-channel DDR5 system.&lt;/p&gt;\\n\\n&lt;p&gt;Worthless for commercial levels of inference, but fine for a hobbyist.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqmbh3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqmbh3/anyone_here_run_llama4_scoutmaverick_with_1/n14fvhe/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751549106,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n13s1gv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"astralDangers","can_mod_post":false,"created_utc":1751539695,"send_replies":false,"parent_id":"t3_1lqmbh3","score":6,"author_fullname":"t2_96t64s3h","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yeah right.. like anyone has that much VRAM.. 500GB-2TB only a cluster of commerical GPUs and a hell of a lot of work is going to run that.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n13s1gv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah right.. like anyone has that much VRAM.. 500GB-2TB only a cluster of commerical GPUs and a hell of a lot of work is going to run that.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqmbh3/anyone_here_run_llama4_scoutmaverick_with_1/n13s1gv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751539695,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqmbh3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n13zlkv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1751543116,"send_replies":true,"parent_id":"t3_1lqmbh3","score":4,"author_fullname":"t2_h5utwre7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'm sure with the low active parameters and shared experts, between 96gb of vram and all my gobs of sysram I could get the context way way out there.\\n\\nToo bad the models themselves are terrible and have heard their ctx rating is exaggerated in practice.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n13zlkv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m sure with the low active parameters and shared experts, between 96gb of vram and all my gobs of sysram I could get the context way way out there.&lt;/p&gt;\\n\\n&lt;p&gt;Too bad the models themselves are terrible and have heard their ctx rating is exaggerated in practice.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqmbh3/anyone_here_run_llama4_scoutmaverick_with_1/n13zlkv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751543116,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqmbh3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"c07aa42e-51fe-11f0-afcc-462aad931709","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"c07aa42e-51fe-11f0-afcc-462aad931709","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n17ay20","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"iamgladiator","can_mod_post":false,"send_replies":true,"parent_id":"t1_n15r8t4","score":1,"author_fullname":"t2_capf2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thanks for your contribution. Also curiouswhat your using it for or how your finding the model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n17ay20","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for your contribution. Also curiouswhat your using it for or how your finding the model.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqmbh3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqmbh3/anyone_here_run_llama4_scoutmaverick_with_1/n17ay20/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751579277,"author_flair_text":null,"treatment_tags":[],"created_utc":1751579277,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n15r8t4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"send_replies":true,"parent_id":"t1_n14ncxi","score":3,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Good tests, will post back.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n15r8t4","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"a":":X:","u":"https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X","e":"emoji"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Good tests, will post back.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqmbh3","unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqmbh3/anyone_here_run_llama4_scoutmaverick_with_1/n15r8t4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751562858,"author_flair_text":":X:","treatment_tags":[],"created_utc":1751562858,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"transparent","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n14ncxi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"night0x63","can_mod_post":false,"created_utc":1751551483,"send_replies":true,"parent_id":"t1_n13yu3z","score":2,"author_fullname":"t2_3h2irqtz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Test 1: Try putting n Wikipedia articles in context with title. Then have it summarize all n articles and make sure it gets all articles and has good summary of each. Idea from other commenter. Tests long context IMO.\\n\\n\\nTest 2: feed it all 0.8 million tokens for three js example code. Have it at a feature. From Google demonstrating their long context. https://m.youtube.com/watch?v=SSnsmqIj1MI this requires shell script to print filename and content... Then at the end the prompt.\\n\\n\\nTest 3: For me I would try: \\n\\n\\nFed it all code for big code base then have it describe in depth some parts you know. Or have it wrote a new function or something.\\n\\n\\nThere's a long code bench... But I don't know how to run that.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n14ncxi","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Test 1: Try putting n Wikipedia articles in context with title. Then have it summarize all n articles and make sure it gets all articles and has good summary of each. Idea from other commenter. Tests long context IMO.&lt;/p&gt;\\n\\n&lt;p&gt;Test 2: feed it all 0.8 million tokens for three js example code. Have it at a feature. From Google demonstrating their long context. &lt;a href=\\"https://m.youtube.com/watch?v=SSnsmqIj1MI\\"&gt;https://m.youtube.com/watch?v=SSnsmqIj1MI&lt;/a&gt; this requires shell script to print filename and content... Then at the end the prompt.&lt;/p&gt;\\n\\n&lt;p&gt;Test 3: For me I would try: &lt;/p&gt;\\n\\n&lt;p&gt;Fed it all code for big code base then have it describe in depth some parts you know. Or have it wrote a new function or something.&lt;/p&gt;\\n\\n&lt;p&gt;There&amp;#39;s a long code bench... But I don&amp;#39;t know how to run that.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqmbh3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqmbh3/anyone_here_run_llama4_scoutmaverick_with_1/n14ncxi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751551483,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n13yu3z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"created_utc":1751542796,"send_replies":true,"parent_id":"t3_1lqmbh3","score":1,"author_fullname":"t2_1a48h7vf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I use Llama 4 on a Runpod cluster but haven't actually filled up its 1M context (far from it).\\n\\nWhat do you want to know? If you give me something I can easily dump into its context I can figure out how much VRAM it needs.\\n\\nAlso lol Ollama/llama.cpp, you better be using vLLM on a Linux server with this model on some enteprise workload, it's not for amateur use.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n13yu3z","is_submitter":false,"downs":0,"author_flair_richtext":[{"a":":X:","u":"https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X","e":"emoji"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I use Llama 4 on a Runpod cluster but haven&amp;#39;t actually filled up its 1M context (far from it).&lt;/p&gt;\\n\\n&lt;p&gt;What do you want to know? If you give me something I can easily dump into its context I can figure out how much VRAM it needs.&lt;/p&gt;\\n\\n&lt;p&gt;Also lol Ollama/llama.cpp, you better be using vLLM on a Linux server with this model on some enteprise workload, it&amp;#39;s not for amateur use.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqmbh3/anyone_here_run_llama4_scoutmaverick_with_1/n13yu3z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751542796,"author_flair_text":":X:","treatment_tags":[],"link_id":"t3_1lqmbh3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"transparent","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n13yz00","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FinalsMVPZachZarba","can_mod_post":false,"send_replies":true,"parent_id":"t1_n13udcd","score":4,"author_fullname":"t2_p36owpxz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I calculate outputs with a pencil and paper. I'll let you know tokens per second in a few thousand years.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n13yz00","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I calculate outputs with a pencil and paper. I&amp;#39;ll let you know tokens per second in a few thousand years.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqmbh3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqmbh3/anyone_here_run_llama4_scoutmaverick_with_1/n13yz00/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751542854,"author_flair_text":null,"treatment_tags":[],"created_utc":1751542854,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n13udcd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"vegatx40","can_mod_post":false,"created_utc":1751540803,"send_replies":true,"parent_id":"t1_n13tdia","score":-2,"author_fullname":"t2_18dhiarv40","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Wow I'm only getting 95 tokens per second on my phone","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n13udcd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Wow I&amp;#39;m only getting 95 tokens per second on my phone&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqmbh3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqmbh3/anyone_here_run_llama4_scoutmaverick_with_1/n13udcd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751540803,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-2}}],"before":null}},"user_reports":[],"saved":false,"id":"n13tdia","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jbutlerdev","can_mod_post":false,"created_utc":1751540334,"send_replies":true,"parent_id":"t3_1lqmbh3","score":-1,"author_fullname":"t2_azse6ibv","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Sure I run it on my MBP with ollama at 100T/s","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n13tdia","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sure I run it on my MBP with ollama at 100T/s&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqmbh3/anyone_here_run_llama4_scoutmaverick_with_1/n13tdia/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751540334,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqmbh3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1420wh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Calm_List3479","can_mod_post":false,"created_utc":1751544096,"send_replies":true,"parent_id":"t3_1lqmbh3","score":0,"author_fullname":"t2_nrekyf7l","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You need 3-4 8xH200 to run either. [https://blog.vllm.ai/2025/04/05/llama4.html](https://blog.vllm.ai/2025/04/05/llama4.html)\\n\\n  \\nOn a single 8xH200 running Scout FP8 was able to get \\\\~120,000 input tk/s and 3.6M context. Output was around 120 tk/s. This is where Blackwell and FP4 is going to shine.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1420wh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You need 3-4 8xH200 to run either. &lt;a href=\\"https://blog.vllm.ai/2025/04/05/llama4.html\\"&gt;https://blog.vllm.ai/2025/04/05/llama4.html&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;On a single 8xH200 running Scout FP8 was able to get ~120,000 input tk/s and 3.6M context. Output was around 120 tk/s. This is where Blackwell and FP4 is going to shine.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqmbh3/anyone_here_run_llama4_scoutmaverick_with_1/n1420wh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751544096,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqmbh3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}}]`),o=()=>e.jsx(l,{data:t});export{o as default};
