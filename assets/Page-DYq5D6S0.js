import{j as e}from"./index-Cd3v0jxz.js";import{R as l}from"./RedditPostRenderer-cI96YLhy.js";import"./index-DGBoKyQm.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hello !\\n\\nI'm trying to run devstral using llama server, and it's working fine, i'm using this command to serve the model, as you see I'm using the alias to be able to select it more easily in openhand.\\n\\nThen in openhand advanced settings, I tried every prefix in front of my model name like openai, lm\\\\_studio, custom and even without even any prefix, litellm cannot access it\\n\\nFor the endpoint, I tried [http://127.0.0.1:8080/v1](http://127.0.0.1:8080/v1) and [http://127.0.0.1:8080](http://127.0.0.1:8080)\\n\\nWhen I try with the openai prefix, it tries to connect to the openai api.\\n\\nDid someone here managed to make openhands works with llama server ? \\n\\nThank you in advance and I wish you a good day, take care\\n\\n    ./llama-server.exe --model \\"thisismyfolder\\\\models\\\\unsloth\\\\Devstral-Small-2507-GGUF\\\\Devstral-Small-2507-UD-Q5_K_XL.gguf\\" --threads -1 --ctx-size 131072 --cache-type-k q8_0 --n-gpu-layers 99 --seed 3407 --prio 2 --temp 0.15 --repeat-penalty 1.0 --min-p 0.01 --top-k 64 --top-p 0.95 --host 127.0.0.1 --port 8080 --mlock --no-mmap --alias \\"devstral\\"","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"How did you manage to use llama server with openhands ?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m0ssma","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":1,"author_flair_background_color":null,"subreddit_type":"public","ups":3,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_3t20nkoj","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":3,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752611204,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hello !&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m trying to run devstral using llama server, and it&amp;#39;s working fine, i&amp;#39;m using this command to serve the model, as you see I&amp;#39;m using the alias to be able to select it more easily in openhand.&lt;/p&gt;\\n\\n&lt;p&gt;Then in openhand advanced settings, I tried every prefix in front of my model name like openai, lm_studio, custom and even without even any prefix, litellm cannot access it&lt;/p&gt;\\n\\n&lt;p&gt;For the endpoint, I tried &lt;a href=\\"http://127.0.0.1:8080/v1\\"&gt;http://127.0.0.1:8080/v1&lt;/a&gt; and &lt;a href=\\"http://127.0.0.1:8080\\"&gt;http://127.0.0.1:8080&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;When I try with the openai prefix, it tries to connect to the openai api.&lt;/p&gt;\\n\\n&lt;p&gt;Did someone here managed to make openhands works with llama server ? &lt;/p&gt;\\n\\n&lt;p&gt;Thank you in advance and I wish you a good day, take care&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;./llama-server.exe --model &amp;quot;thisismyfolder\\\\models\\\\unsloth\\\\Devstral-Small-2507-GGUF\\\\Devstral-Small-2507-UD-Q5_K_XL.gguf&amp;quot; --threads -1 --ctx-size 131072 --cache-type-k q8_0 --n-gpu-layers 99 --seed 3407 --prio 2 --temp 0.15 --repeat-penalty 1.0 --min-p 0.01 --top-k 64 --top-p 0.95 --host 127.0.0.1 --port 8080 --mlock --no-mmap --alias &amp;quot;devstral&amp;quot;\\n&lt;/code&gt;&lt;/pre&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m0ssma","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Wemos_D1","discussion_type":null,"num_comments":3,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m0ssma/how_did_you_manage_to_use_llama_server_with/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m0ssma/how_did_you_manage_to_use_llama_server_with/","subreddit_subscribers":499773,"created_utc":1752611204,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3bxlgl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Wemos_D1","can_mod_post":false,"created_utc":1752612840,"send_replies":true,"parent_id":"t1_n3bwj9r","score":1,"author_fullname":"t2_3t20nkoj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes I can, but when I use the openai prefix in front of the model, it will use the openai api url instead of mine","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3bxlgl","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes I can, but when I use the openai prefix in front of the model, it will use the openai api url instead of mine&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0ssma","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0ssma/how_did_you_manage_to_use_llama_server_with/n3bxlgl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752612840,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3bwj9r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Capable-Ad-7494","can_mod_post":false,"created_utc":1752612547,"send_replies":true,"parent_id":"t3_1m0ssma","score":1,"author_fullname":"t2_9so78ol2","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"are you changing your api base url?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3bwj9r","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;are you changing your api base url?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0ssma/how_did_you_manage_to_use_llama_server_with/n3bwj9r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752612547,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0ssma","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3cvi3g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Not_A_Cookie","can_mod_post":false,"created_utc":1752623582,"send_replies":true,"parent_id":"t3_1m0ssma","score":1,"author_fullname":"t2_6tmmq","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Hmm are you are using docker as suggested by all hands/openhands documentation? If so I think the url should be http://host.docker.internal:8080/v1 or similar. If using wsl you might need to enable mirrored networking mode.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3cvi3g","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hmm are you are using docker as suggested by all hands/openhands documentation? If so I think the url should be &lt;a href=\\"http://host.docker.internal:8080/v1\\"&gt;http://host.docker.internal:8080/v1&lt;/a&gt; or similar. If using wsl you might need to enable mirrored networking mode.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0ssma/how_did_you_manage_to_use_llama_server_with/n3cvi3g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752623582,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0ssma","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),s=()=>e.jsx(l,{data:t});export{s as default};
