import{j as e}from"./index-BgwOAK4-.js";import{R as t}from"./RedditPostRenderer-BOBjDTFu.js";import"./index-BL22wVg5.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I'm trying to understand why does something like say llama 3.1 8b need further instruction by something like alpaca? If you just load the base model and ask something of it it just responds with gibberish. If you train it with say even just 1000 samples of alpaca data it starts responding coherently. But why does that happen when the original is already trained on next token generation? The q/a instruction training is also next token generation why does a little nudge in the weights from alpaca or other small data sets suddenly get it to respond with coherent responses. When I've looked around in sites etc it just says the further instruction gets the model to align to respond but doesn't say why. How come a few samples (say just 1000 alpaca samples) of 'fine tuning' next token generation suddenly go from gibberish to coherent responses when that is also just doing next token generation as well. I get its training directed towards producing responses to questions so it would shift the weights towards that but the original next token training would have had similar q/a data sets in it already so why doesn't it already do it?\\n\\n\\n\\nJust for context i'm using [https://huggingface.co/meta-llama/Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) with lora to train on the alpaca data.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Why do base models give gibberish and need further 'fine tuning'","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lwk84b","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.89,"author_flair_background_color":null,"subreddit_type":"public","ups":36,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1h4o7f23eh","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":36,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1752172244,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1752172046,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m trying to understand why does something like say llama 3.1 8b need further instruction by something like alpaca? If you just load the base model and ask something of it it just responds with gibberish. If you train it with say even just 1000 samples of alpaca data it starts responding coherently. But why does that happen when the original is already trained on next token generation? The q/a instruction training is also next token generation why does a little nudge in the weights from alpaca or other small data sets suddenly get it to respond with coherent responses. When I&amp;#39;ve looked around in sites etc it just says the further instruction gets the model to align to respond but doesn&amp;#39;t say why. How come a few samples (say just 1000 alpaca samples) of &amp;#39;fine tuning&amp;#39; next token generation suddenly go from gibberish to coherent responses when that is also just doing next token generation as well. I get its training directed towards producing responses to questions so it would shift the weights towards that but the original next token training would have had similar q/a data sets in it already so why doesn&amp;#39;t it already do it?&lt;/p&gt;\\n\\n&lt;p&gt;Just for context i&amp;#39;m using &lt;a href=\\"https://huggingface.co/meta-llama/Llama-3.1-8B\\"&gt;https://huggingface.co/meta-llama/Llama-3.1-8B&lt;/a&gt; with lora to train on the alpaca data.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/qzajd4RwBTlSnF53474mqxh_kM2yowrJDAipXW6ciKo.png?auto=webp&amp;s=bb56ed1972a4758624102c003d823e903d87bcb2","width":1200,"height":648},"resolutions":[{"url":"https://external-preview.redd.it/qzajd4RwBTlSnF53474mqxh_kM2yowrJDAipXW6ciKo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0a573e4b70357cc304f738189e72c00b44622fc1","width":108,"height":58},{"url":"https://external-preview.redd.it/qzajd4RwBTlSnF53474mqxh_kM2yowrJDAipXW6ciKo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e66f71d268dc1e20de7ad544103444fce5f85488","width":216,"height":116},{"url":"https://external-preview.redd.it/qzajd4RwBTlSnF53474mqxh_kM2yowrJDAipXW6ciKo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b4002d63751a26c36e1b700aa599d15f0e652d30","width":320,"height":172},{"url":"https://external-preview.redd.it/qzajd4RwBTlSnF53474mqxh_kM2yowrJDAipXW6ciKo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7bfdad6461fb7b0282d5a1a935024a36731858fd","width":640,"height":345},{"url":"https://external-preview.redd.it/qzajd4RwBTlSnF53474mqxh_kM2yowrJDAipXW6ciKo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=724d0bec544adf6ea5bb67e4a291ff7a10fe78eb","width":960,"height":518},{"url":"https://external-preview.redd.it/qzajd4RwBTlSnF53474mqxh_kM2yowrJDAipXW6ciKo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c8c9bcd52d928c8a6142e9d4bb6f78ddfe70d726","width":1080,"height":583}],"variants":{},"id":"qzajd4RwBTlSnF53474mqxh_kM2yowrJDAipXW6ciKo"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lwk84b","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"QFGTrialByFire","discussion_type":null,"num_comments":13,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lwk84b/why_do_base_models_give_gibberish_and_need/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lwk84b/why_do_base_models_give_gibberish_and_need/","subreddit_subscribers":497505,"created_utc":1752172046,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2f3e5q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Kooshi_Govno","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2f1v1e","score":15,"author_fullname":"t2_7kg5p","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It's been a couple years since I used it, but [oobabooga's text-generation-webui](https://github.com/oobabooga/text-generation-webui) has a nice Notebook mode where you can do this. It's basically just a text editor window where you can have the LLM start autocompleting, then you can stop it, edit, and have it continue.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2f3e5q","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s been a couple years since I used it, but &lt;a href=\\"https://github.com/oobabooga/text-generation-webui\\"&gt;oobabooga&amp;#39;s text-generation-webui&lt;/a&gt; has a nice Notebook mode where you can do this. It&amp;#39;s basically just a text editor window where you can have the LLM start autocompleting, then you can stop it, edit, and have it continue.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwk84b","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwk84b/why_do_base_models_give_gibberish_and_need/n2f3e5q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752175623,"author_flair_text":null,"treatment_tags":[],"created_utc":1752175623,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":15}}],"before":null}},"user_reports":[],"saved":false,"id":"n2f1v1e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"YouDontSeemRight","can_mod_post":false,"created_utc":1752175180,"send_replies":true,"parent_id":"t1_n2ev399","score":2,"author_fullname":"t2_1b7gjxtue9","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Interesting concept, it's almost like a conversation simulation. If you could steer the conversation it could be helpful","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2f1v1e","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Interesting concept, it&amp;#39;s almost like a conversation simulation. If you could steer the conversation it could be helpful&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwk84b","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwk84b/why_do_base_models_give_gibberish_and_need/n2f1v1e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752175180,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ev399","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Kooshi_Govno","can_mod_post":false,"created_utc":1752173252,"send_replies":true,"parent_id":"t3_1lwk84b","score":59,"author_fullname":"t2_7kg5p","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Base models are nothing more than autocomplete. You can actually use this to make them respond somewhat coherently if you give them a bunch of context so that it _looks like_ they should respond to you. They will also just continue after that though, also responding _as_ you.\\n\\nThey're very fun to play with in their own way.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ev399","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Base models are nothing more than autocomplete. You can actually use this to make them respond somewhat coherently if you give them a bunch of context so that it &lt;em&gt;looks like&lt;/em&gt; they should respond to you. They will also just continue after that though, also responding &lt;em&gt;as&lt;/em&gt; you.&lt;/p&gt;\\n\\n&lt;p&gt;They&amp;#39;re very fun to play with in their own way.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwk84b/why_do_base_models_give_gibberish_and_need/n2ev399/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752173252,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwk84b","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":59}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2evtju","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"QFGTrialByFire","can_mod_post":false,"created_utc":1752173459,"send_replies":true,"parent_id":"t1_n2esrkh","score":5,"author_fullname":"t2_1h4o7f23eh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks for that, your link and summary is really useful I was searching around and didn't come across that with my searching. Yes, I've noticed you need to run it on a little bit of something like alpaca for understanding instruction following (if that's what you need) then switch to the pattern of input/output you want it to start focusing on and that works. I guess I was expecting the base model to just respond to normal questions and was surprised it doesn't respond normally and then surprised again how little further training was needed to get it to suddenly sound coherent.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2evtju","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for that, your link and summary is really useful I was searching around and didn&amp;#39;t come across that with my searching. Yes, I&amp;#39;ve noticed you need to run it on a little bit of something like alpaca for understanding instruction following (if that&amp;#39;s what you need) then switch to the pattern of input/output you want it to start focusing on and that works. I guess I was expecting the base model to just respond to normal questions and was surprised it doesn&amp;#39;t respond normally and then surprised again how little further training was needed to get it to suddenly sound coherent.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwk84b","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwk84b/why_do_base_models_give_gibberish_and_need/n2evtju/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752173459,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ij314","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"phree_radical","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2i3mkk","score":1,"author_fullname":"t2_44nkp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Not those of today.  Smaller models need way more training to reach in-context learning capabilities","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2ij314","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not those of today.  Smaller models need way more training to reach in-context learning capabilities&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwk84b","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwk84b/why_do_base_models_give_gibberish_and_need/n2ij314/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752221998,"author_flair_text":null,"treatment_tags":[],"created_utc":1752221998,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2i3mkk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"callmedevilthebad","can_mod_post":false,"created_utc":1752213531,"send_replies":true,"parent_id":"t1_n2esrkh","score":1,"author_fullname":"t2_intoh3lv","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Is it true for SLMs as well ?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2i3mkk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Is it true for SLMs as well ?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwk84b","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwk84b/why_do_base_models_give_gibberish_and_need/n2i3mkk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752213531,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2esrkh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"phree_radical","can_mod_post":false,"created_utc":1752172583,"send_replies":true,"parent_id":"t3_1lwk84b","score":19,"author_fullname":"t2_44nkp","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Just think of how many different types of documents it's trained on.  To accurately predict what type of document it is, what part of the document it is, what universe it's in, what type of person is writing, what's being communicated, and so on, you most likely just aren't giving it *enough* (longer) context\\n\\nAnd fine-tuning to imitate only a chat isn't really a \\"need,\\" unless you need instruction-following.  If you're interested in the power of base models, you should experiment with few-shot prompting like the examples here https://www.reddit.com/r/LocalLLaMA/comments/1c7r2jw/wow_llama38bs_incontext_learning_is_unbelievable/","edited":1752172835,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2esrkh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just think of how many different types of documents it&amp;#39;s trained on.  To accurately predict what type of document it is, what part of the document it is, what universe it&amp;#39;s in, what type of person is writing, what&amp;#39;s being communicated, and so on, you most likely just aren&amp;#39;t giving it &lt;em&gt;enough&lt;/em&gt; (longer) context&lt;/p&gt;\\n\\n&lt;p&gt;And fine-tuning to imitate only a chat isn&amp;#39;t really a &amp;quot;need,&amp;quot; unless you need instruction-following.  If you&amp;#39;re interested in the power of base models, you should experiment with few-shot prompting like the examples here &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1c7r2jw/wow_llama38bs_incontext_learning_is_unbelievable/\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1c7r2jw/wow_llama38bs_incontext_learning_is_unbelievable/&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwk84b/why_do_base_models_give_gibberish_and_need/n2esrkh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752172583,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwk84b","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":19}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2f1kz1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"YouDontSeemRight","can_mod_post":false,"created_utc":1752175101,"send_replies":true,"parent_id":"t3_1lwk84b","score":9,"author_fullname":"t2_1b7gjxtue9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Base models are internet regurgitation machines. Go to an obscure website and copy paste half a paragraph into the model. See if it reproduces the other half.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2f1kz1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Base models are internet regurgitation machines. Go to an obscure website and copy paste half a paragraph into the model. See if it reproduces the other half.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwk84b/why_do_base_models_give_gibberish_and_need/n2f1kz1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752175101,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwk84b","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"body":"In the US, when I meet a stranger and say \\"How are you doing?\\", I have a high probability of an automatic \\"I'm fine, thank you.\\" If I did the same thing in the UK, the \\"How are you doing?\\" would likely confuse the stranger because it's a very personal question to be asking a stranger.\\n\\nThe difference is that in the US we've been trained to understand that the \\"How are you doing\\" is a simple alternative pattern to \\"Hello\\" with a specific response, \\"I'm fine, thanks.\\" The words are sort of \\"gibberish\\", unless you've been trained on the pattern of the back and forth flow of a specific style of conversation.\\n\\nA base LLM has the understanding of language structure, but not the patterns of back and forth, question and response patterns that we call spoken language. A spoken language isn't simply proper sentences, but specific patterns trained in popular culture that we all recognize and pattern match back to.\\n\\nAnother example of this is that some US Americans learn Japanese from watching Anime. To someone in Japan when they hear them speak, it feels \\"off\\". Like, too formal, not really grounded in the modern day to day way of speaking in Japan. You may understand the words and how the words can connect in Japanese, but you also need the patterns for the proper back and forth of the constructed sentences that the modern speaker uses.","subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2gtdvj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"synn89","can_mod_post":false,"created_utc":1752195015,"send_replies":true,"parent_id":"t3_1lwk84b","score":3,"author_fullname":"t2_3jm4t","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"author_cakeday":true,"edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2gtdvj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;In the US, when I meet a stranger and say &amp;quot;How are you doing?&amp;quot;, I have a high probability of an automatic &amp;quot;I&amp;#39;m fine, thank you.&amp;quot; If I did the same thing in the UK, the &amp;quot;How are you doing?&amp;quot; would likely confuse the stranger because it&amp;#39;s a very personal question to be asking a stranger.&lt;/p&gt;\\n\\n&lt;p&gt;The difference is that in the US we&amp;#39;ve been trained to understand that the &amp;quot;How are you doing&amp;quot; is a simple alternative pattern to &amp;quot;Hello&amp;quot; with a specific response, &amp;quot;I&amp;#39;m fine, thanks.&amp;quot; The words are sort of &amp;quot;gibberish&amp;quot;, unless you&amp;#39;ve been trained on the pattern of the back and forth flow of a specific style of conversation.&lt;/p&gt;\\n\\n&lt;p&gt;A base LLM has the understanding of language structure, but not the patterns of back and forth, question and response patterns that we call spoken language. A spoken language isn&amp;#39;t simply proper sentences, but specific patterns trained in popular culture that we all recognize and pattern match back to.&lt;/p&gt;\\n\\n&lt;p&gt;Another example of this is that some US Americans learn Japanese from watching Anime. To someone in Japan when they hear them speak, it feels &amp;quot;off&amp;quot;. Like, too formal, not really grounded in the modern day to day way of speaking in Japan. You may understand the words and how the words can connect in Japanese, but you also need the patterns for the proper back and forth of the constructed sentences that the modern speaker uses.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwk84b/why_do_base_models_give_gibberish_and_need/n2gtdvj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752195015,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwk84b","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2gqb9i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Wheynelau","can_mod_post":false,"created_utc":1752193933,"send_replies":true,"parent_id":"t3_1lwk84b","score":2,"author_fullname":"t2_vezlk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Base models are frequently trained with raw text, rather than instruction like \\"question\\", \\"answer\\" or chat format. You can imagine the first stage is sort of like general grammar and vocabulary training, which usually means the output should actually be coherent, but may not be what you are looking for.\\n\\nPretraining usually does not have any QA sets. When you said gibberish, do you mean coherent gibberish or really gibberish?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2gqb9i","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Base models are frequently trained with raw text, rather than instruction like &amp;quot;question&amp;quot;, &amp;quot;answer&amp;quot; or chat format. You can imagine the first stage is sort of like general grammar and vocabulary training, which usually means the output should actually be coherent, but may not be what you are looking for.&lt;/p&gt;\\n\\n&lt;p&gt;Pretraining usually does not have any QA sets. When you said gibberish, do you mean coherent gibberish or really gibberish?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwk84b/why_do_base_models_give_gibberish_and_need/n2gqb9i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752193933,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwk84b","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ifxtr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Herr_Drosselmeyer","can_mod_post":false,"created_utc":1752220204,"send_replies":true,"parent_id":"t3_1lwk84b","score":2,"author_fullname":"t2_1zr9gwsn","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt;If you just load the base model and ask something of it it just responds with gibberish.\\n\\nGiven an actual 'system prompt', i.e. a paragraph or two of explanation of the situation like \\"This is a conversation between an AI assistand and a user. You are the AI assistant, giving helpful answers (etc.)\\", even base models should not produce nonsense. \\n\\nInstruct tuned models are basically just trained a little further on the question --&gt; answer format, along with some special tokens, to make them more reliable in their role as a conversation partner vs just being text completion machines.\\n\\nI also think it depends on how the base model was trained. Many models these days include a large amount of synthetic data, such as from ChatGPT, so if the base already has a lot of this in it, instruct training becomes less important.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ifxtr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;If you just load the base model and ask something of it it just responds with gibberish.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Given an actual &amp;#39;system prompt&amp;#39;, i.e. a paragraph or two of explanation of the situation like &amp;quot;This is a conversation between an AI assistand and a user. You are the AI assistant, giving helpful answers (etc.)&amp;quot;, even base models should not produce nonsense. &lt;/p&gt;\\n\\n&lt;p&gt;Instruct tuned models are basically just trained a little further on the question --&amp;gt; answer format, along with some special tokens, to make them more reliable in their role as a conversation partner vs just being text completion machines.&lt;/p&gt;\\n\\n&lt;p&gt;I also think it depends on how the base model was trained. Many models these days include a large amount of synthetic data, such as from ChatGPT, so if the base already has a lot of this in it, instruct training becomes less important.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwk84b/why_do_base_models_give_gibberish_and_need/n2ifxtr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752220204,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwk84b","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2fylhk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GatePorters","can_mod_post":false,"created_utc":1752184746,"send_replies":true,"parent_id":"t3_1lwk84b","score":3,"author_fullname":"t2_vryl95sg1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Why do babies give gibberish and need further fine tuning? \\n\\n\\nThe first phase of training is making a bunch of random concepts in the higher dimensional latent space. \\n\\nThe second phase is to structure those into a more directed and cohesive network. \\n\\nAny further phases are to fine tune based on use.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2fylhk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why do babies give gibberish and need further fine tuning? &lt;/p&gt;\\n\\n&lt;p&gt;The first phase of training is making a bunch of random concepts in the higher dimensional latent space. &lt;/p&gt;\\n\\n&lt;p&gt;The second phase is to structure those into a more directed and cohesive network. &lt;/p&gt;\\n\\n&lt;p&gt;Any further phases are to fine tune based on use.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwk84b/why_do_base_models_give_gibberish_and_need/n2fylhk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752184746,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwk84b","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ijgil","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Mart-McUH","can_mod_post":false,"created_utc":1752222214,"send_replies":true,"parent_id":"t3_1lwk84b","score":1,"author_fullname":"t2_q3eqbw2b","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I don't use base models much but you need different way of thinking when using them, eg to understand it is just continuing text. So if you ask question, prefill it with start of answer and then it should follow naturally I guess. Like:\\n\\n\\\\---\\n\\nWhat is the smallest city in Slovakia?\\n\\nThe smallest city in Slovakia is \\n\\n\\\\---\\n\\nAnd then it should provide some answer (probably wrong :-)) because that is most likely continuation of given text. Being pedantic if you give just random question the most likely continuation (at least for human) would be something along the lines \\"I don't know\\" (not very useful).\\n\\nAnother problem is they are often not really trained to stop. So it will then likely continue with whatever ramblings about the provided answer and later can lead to anything not even related. Depends on model I suppose, been ages since I tried true base model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ijgil","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t use base models much but you need different way of thinking when using them, eg to understand it is just continuing text. So if you ask question, prefill it with start of answer and then it should follow naturally I guess. Like:&lt;/p&gt;\\n\\n&lt;p&gt;---&lt;/p&gt;\\n\\n&lt;p&gt;What is the smallest city in Slovakia?&lt;/p&gt;\\n\\n&lt;p&gt;The smallest city in Slovakia is &lt;/p&gt;\\n\\n&lt;p&gt;---&lt;/p&gt;\\n\\n&lt;p&gt;And then it should provide some answer (probably wrong :-)) because that is most likely continuation of given text. Being pedantic if you give just random question the most likely continuation (at least for human) would be something along the lines &amp;quot;I don&amp;#39;t know&amp;quot; (not very useful).&lt;/p&gt;\\n\\n&lt;p&gt;Another problem is they are often not really trained to stop. So it will then likely continue with whatever ramblings about the provided answer and later can lead to anything not even related. Depends on model I suppose, been ages since I tried true base model.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwk84b/why_do_base_models_give_gibberish_and_need/n2ijgil/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752222214,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwk84b","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),s=()=>e.jsx(t,{data:a});export{s as default};
