import{j as e}from"./index-Bu7qcPAU.js";import{R as l}from"./RedditPostRenderer-CbHA7O5q.js";import"./index-BKgbfxhf.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I’m working on a small AI app called **Preceptor** — think of it like a privacy-first accountability partner that helps you stay focused **without spying on your screen**\\n\\nHere’s the idea:\\n\\n* It runs **entirely offline**, using local LLMs via [Ollama](https://ollama.com/)\\n* Tracks **which app or browser tab** you’re on (via local system APIs + a lightweight browser extension)\\n* Compares that with your focus goals (e.g., “write more, avoid Reddit”)\\n* And gives you **gentle nudges** when you drift\\n\\nEven with small-ish models (e.g. LLaMA 3 8B or Mistral via Ollama), I’m hitting response time issues. It might only be 1–3 seconds to generate a short message, but in a flow-focused app, that pause breaks the vibe. It's not just about speed but it's also about *feeling instant*. With mistral 7b , which produces a good nudge message but takes like 30 seconds for the api to call\\n\\nHow should i go with this?\\n\\nIf you want to join the waitlist for the app , comment and i will reply with the link. I want to make this less of a promotion post as i am seeking serious suggestions","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Building a Focus App with Local LLMs — But Latency Is a Real Challenge , seeking suggestions","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lzwps3","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.45,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_18z668t0lo","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1752522956,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I’m working on a small AI app called &lt;strong&gt;Preceptor&lt;/strong&gt; — think of it like a privacy-first accountability partner that helps you stay focused &lt;strong&gt;without spying on your screen&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Here’s the idea:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;It runs &lt;strong&gt;entirely offline&lt;/strong&gt;, using local LLMs via &lt;a href=\\"https://ollama.com/\\"&gt;Ollama&lt;/a&gt;&lt;/li&gt;\\n&lt;li&gt;Tracks &lt;strong&gt;which app or browser tab&lt;/strong&gt; you’re on (via local system APIs + a lightweight browser extension)&lt;/li&gt;\\n&lt;li&gt;Compares that with your focus goals (e.g., “write more, avoid Reddit”)&lt;/li&gt;\\n&lt;li&gt;And gives you &lt;strong&gt;gentle nudges&lt;/strong&gt; when you drift&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Even with small-ish models (e.g. LLaMA 3 8B or Mistral via Ollama), I’m hitting response time issues. It might only be 1–3 seconds to generate a short message, but in a flow-focused app, that pause breaks the vibe. It&amp;#39;s not just about speed but it&amp;#39;s also about &lt;em&gt;feeling instant&lt;/em&gt;. With mistral 7b , which produces a good nudge message but takes like 30 seconds for the api to call&lt;/p&gt;\\n\\n&lt;p&gt;How should i go with this?&lt;/p&gt;\\n\\n&lt;p&gt;If you want to join the waitlist for the app , comment and i will reply with the link. I want to make this less of a promotion post as i am seeking serious suggestions&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?auto=webp&amp;s=a080c4707584d3aa14134960cda9ba2d339b93a3","width":1200,"height":630},"resolutions":[{"url":"https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3dc759de0e8fa36d241c5728d41ee3cf022cab96","width":108,"height":56},{"url":"https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6ccf136f5d3091254a0067a3bc5d6c7df9d62d89","width":216,"height":113},{"url":"https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2530aa4ecbcf7899ec0d023e217fe24af15fe0a6","width":320,"height":168},{"url":"https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8e51add1cab39c7614eb13e6195f23c5b4eeb417","width":640,"height":336},{"url":"https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=750a6d42fd91c5a6e9a9c069e74247c877644e97","width":960,"height":504},{"url":"https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9eab390b865b031211658564ad5fe5241c9661c5","width":1080,"height":567}],"variants":{},"id":"krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lzwps3","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Frosty-Cap-4282","discussion_type":null,"num_comments":10,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lzwps3/building_a_focus_app_with_local_llms_but_latency/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lzwps3/building_a_focus_app_with_local_llms_but_latency/","subreddit_subscribers":499295,"created_utc":1752522956,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n35kkgb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Fit-Produce420","can_mod_post":false,"send_replies":true,"parent_id":"t1_n35hfpe","score":2,"author_fullname":"t2_tewf9bdwg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Maybe develop a small amount of self control?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n35kkgb","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Maybe develop a small amount of self control?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzwps3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzwps3/building_a_focus_app_with_local_llms_but_latency/n35kkgb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752530051,"author_flair_text":null,"treatment_tags":[],"created_utc":1752530051,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n35hfpe","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Frosty-Cap-4282","can_mod_post":false,"created_utc":1752529118,"send_replies":true,"parent_id":"t1_n34wok5","score":-2,"author_fullname":"t2_18z668t0lo","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"i mean it does work. I can just make the nudging system go on effect every 1-2 min. Just exploring for options","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n35hfpe","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;i mean it does work. I can just make the nudging system go on effect every 1-2 min. Just exploring for options&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzwps3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzwps3/building_a_focus_app_with_local_llms_but_latency/n35hfpe/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752529118,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-2}}],"before":null}},"user_reports":[],"saved":false,"id":"n34wok5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Fit-Produce420","can_mod_post":false,"created_utc":1752523235,"send_replies":true,"parent_id":"t3_1lzwps3","score":8,"author_fullname":"t2_tewf9bdwg","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Wait list for an application that doesn't work?\\n\\n\\nTruly, AI is the future!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n34wok5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Wait list for an application that doesn&amp;#39;t work?&lt;/p&gt;\\n\\n&lt;p&gt;Truly, AI is the future!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzwps3/building_a_focus_app_with_local_llms_but_latency/n34wok5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752523235,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzwps3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n350fz3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"remghoost7","can_mod_post":false,"created_utc":1752524293,"send_replies":true,"parent_id":"t1_n34wnuc","score":2,"author_fullname":"t2_sejql","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Agreed. Hardware is definitely the most important factor here.\\n\\nEven CPU alone (on a \\"modern\\" desktop CPU) on an 8B model should be getting quicker generation times.  \\nI wonder what quantization level OP is using. Q4 should be more than fine for this use-case.   \\nMight even be able to go lower with [dynamic quants.](https://huggingface.co/unsloth/Llama-3.1-8B-Instruct-GGUF/tree/main)\\n\\nIt comes down to the initial prompt tokens as well (prompt processing can take a few seconds).  \\nToo large of a system prompt will cause \\"lag\\" when getting to the first output token.\\n\\nAnd building an app like this should go straight to the source (llamacpp) not using a wrapper (ollama).  \\nNo need for extra overhead when you're developing the front-end.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n350fz3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Agreed. Hardware is definitely the most important factor here.&lt;/p&gt;\\n\\n&lt;p&gt;Even CPU alone (on a &amp;quot;modern&amp;quot; desktop CPU) on an 8B model should be getting quicker generation times.&lt;br/&gt;\\nI wonder what quantization level OP is using. Q4 should be more than fine for this use-case.&lt;br/&gt;\\nMight even be able to go lower with &lt;a href=\\"https://huggingface.co/unsloth/Llama-3.1-8B-Instruct-GGUF/tree/main\\"&gt;dynamic quants.&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;It comes down to the initial prompt tokens as well (prompt processing can take a few seconds).&lt;br/&gt;\\nToo large of a system prompt will cause &amp;quot;lag&amp;quot; when getting to the first output token.&lt;/p&gt;\\n\\n&lt;p&gt;And building an app like this should go straight to the source (llamacpp) not using a wrapper (ollama).&lt;br/&gt;\\nNo need for extra overhead when you&amp;#39;re developing the front-end.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzwps3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzwps3/building_a_focus_app_with_local_llms_but_latency/n350fz3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752524293,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n35jusr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Frosty-Cap-4282","can_mod_post":false,"created_utc":1752529835,"send_replies":true,"parent_id":"t1_n34wnuc","score":0,"author_fullname":"t2_18z668t0lo","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"yeah i was just looking for a balanced implementation. That would kind of work ok-ish on a mid end hardware too","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n35jusr","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;yeah i was just looking for a balanced implementation. That would kind of work ok-ish on a mid end hardware too&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzwps3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzwps3/building_a_focus_app_with_local_llms_but_latency/n35jusr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752529835,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n34wnuc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rainbowColoredBalls","can_mod_post":false,"created_utc":1752523229,"send_replies":true,"parent_id":"t3_1lzwps3","score":3,"author_fullname":"t2_23ovx259","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Wouldn't this be a function of the hardware you're running it on?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n34wnuc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Wouldn&amp;#39;t this be a function of the hardware you&amp;#39;re running it on?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzwps3/building_a_focus_app_with_local_llms_but_latency/n34wnuc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752523229,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzwps3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n35l1bh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Frosty-Cap-4282","can_mod_post":false,"created_utc":1752530193,"send_replies":true,"parent_id":"t1_n353ziq","score":1,"author_fullname":"t2_18z668t0lo","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"thanks i will look at it","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n35l1bh","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;thanks i will look at it&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzwps3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzwps3/building_a_focus_app_with_local_llms_but_latency/n35l1bh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752530193,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n353ziq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"One_Grade435","can_mod_post":false,"created_utc":1752525273,"send_replies":true,"parent_id":"t3_1lzwps3","score":1,"author_fullname":"t2_oi0k6cm9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think the task is manageable with a smaller model. Maybe try it with: https://huggingface.co/unsloth/Qwen3-4B-GGUF","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n353ziq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think the task is manageable with a smaller model. Maybe try it with: &lt;a href=\\"https://huggingface.co/unsloth/Qwen3-4B-GGUF\\"&gt;https://huggingface.co/unsloth/Qwen3-4B-GGUF&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzwps3/building_a_focus_app_with_local_llms_but_latency/n353ziq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752525273,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzwps3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n366k27","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Frosty-Cap-4282","can_mod_post":false,"created_utc":1752537089,"send_replies":true,"parent_id":"t1_n35tx0s","score":1,"author_fullname":"t2_18z668t0lo","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You will keep a precept of like i will code for an hour. There is a extension and other local apis to notify the app which tab i am on , and based on tab and precept data the llm will decide and remind you if you are drifted from your precept. There can be many precepts and many tabs , its just not possible to write if else for everything , need an intelligent system to decide if the current tab matches with your goals","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n366k27","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You will keep a precept of like i will code for an hour. There is a extension and other local apis to notify the app which tab i am on , and based on tab and precept data the llm will decide and remind you if you are drifted from your precept. There can be many precepts and many tabs , its just not possible to write if else for everything , need an intelligent system to decide if the current tab matches with your goals&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzwps3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzwps3/building_a_focus_app_with_local_llms_but_latency/n366k27/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752537089,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n35tx0s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"bornfree4ever","can_mod_post":false,"created_utc":1752532995,"send_replies":true,"parent_id":"t3_1lzwps3","score":2,"author_fullname":"t2_12einrypio","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"why do you need an llm, if all you are doing is 'tracking which app or browser tab you are on'\\n\\nyou wil legt instant results if you take the llm part out and use normal if-then checks","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n35tx0s","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;why do you need an llm, if all you are doing is &amp;#39;tracking which app or browser tab you are on&amp;#39;&lt;/p&gt;\\n\\n&lt;p&gt;you wil legt instant results if you take the llm part out and use normal if-then checks&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzwps3/building_a_focus_app_with_local_llms_but_latency/n35tx0s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752532995,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzwps3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),n=()=>e.jsx(l,{data:t});export{n as default};
