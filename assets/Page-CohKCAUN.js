import{j as e}from"./index-F0NXdzZX.js";import{R as t}from"./RedditPostRenderer-CoEZB1dt.js";import"./index-DrtravXm.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I am looking to build a system that can run DeepSeekR1 and Kimi K2.\\nItems I am not sure of, they are shown side by side.\\n\\nAMD Epyc 9175F/9375F/9655P - $2,617/$3,550/$5,781\\nSP5 Cooler - $130\\nH13SSL-NT Motherboard - $730\\nCorsair 1500W PSU - $350\\n64GB/96GBx12 6400 ECC DDR5 - $4,585 / $7,000\\nNvidia 5090 - $3,000\\nCase - $200\\n\\nIt was mentioned a 9015 may work, but I am not sure if would be enough.\\n\\nI am hoping for ~20 tokens/second.  The math seems to support that range but the cpu is an unknown what the lowest I can get away with without affecting throughput.\\n\\nI was originally planning to do Q8, but the ram costs are just too much, especially when you factor in the speed hit.  I could get away with 64GB modules, but I'd be limited to less than the full context window.  \\n\\nWith the middle CPU and 96GB ram, it is looking around $15K.  I do have a 3090 lying around, that would shave $3K off the price, from what I understand the difference in through put will be very minor, but it is significantly faster for prompt processing.  I can always add it later when nvidia gets back to me with the reserve program.  \\n\\nI do plan on using together.ai to test my use case against DeepSeekR1 and Kimi K2 to see which works best for what I need and if there is enough benefit over Qwen3 32B/235B to justify it.  \\n\\n~20 tokens/second I feel is a good speed that I can justify running local, much lower and it is just too slow to be practical.\\n\\nI really wanted to go the route of a RTX 6000 Pro, but unless I am running a 32B/70B model, it just doesn't provide enough performance to justify it with the larger models and I can't justify 7-10 of them.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Thoughts on this DeepSeekR1/Kimi K2 build","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m386sc","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.56,"author_flair_background_color":null,"subreddit_type":"public","ups":1,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_ijzb7","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":1,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752859002,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I am looking to build a system that can run DeepSeekR1 and Kimi K2.\\nItems I am not sure of, they are shown side by side.&lt;/p&gt;\\n\\n&lt;p&gt;AMD Epyc 9175F/9375F/9655P - $2,617/$3,550/$5,781\\nSP5 Cooler - $130\\nH13SSL-NT Motherboard - $730\\nCorsair 1500W PSU - $350\\n64GB/96GBx12 6400 ECC DDR5 - $4,585 / $7,000\\nNvidia 5090 - $3,000\\nCase - $200&lt;/p&gt;\\n\\n&lt;p&gt;It was mentioned a 9015 may work, but I am not sure if would be enough.&lt;/p&gt;\\n\\n&lt;p&gt;I am hoping for ~20 tokens/second.  The math seems to support that range but the cpu is an unknown what the lowest I can get away with without affecting throughput.&lt;/p&gt;\\n\\n&lt;p&gt;I was originally planning to do Q8, but the ram costs are just too much, especially when you factor in the speed hit.  I could get away with 64GB modules, but I&amp;#39;d be limited to less than the full context window.  &lt;/p&gt;\\n\\n&lt;p&gt;With the middle CPU and 96GB ram, it is looking around $15K.  I do have a 3090 lying around, that would shave $3K off the price, from what I understand the difference in through put will be very minor, but it is significantly faster for prompt processing.  I can always add it later when nvidia gets back to me with the reserve program.  &lt;/p&gt;\\n\\n&lt;p&gt;I do plan on using together.ai to test my use case against DeepSeekR1 and Kimi K2 to see which works best for what I need and if there is enough benefit over Qwen3 32B/235B to justify it.  &lt;/p&gt;\\n\\n&lt;p&gt;~20 tokens/second I feel is a good speed that I can justify running local, much lower and it is just too slow to be practical.&lt;/p&gt;\\n\\n&lt;p&gt;I really wanted to go the route of a RTX 6000 Pro, but unless I am running a 32B/70B model, it just doesn&amp;#39;t provide enough performance to justify it with the larger models and I can&amp;#39;t justify 7-10 of them.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1m386sc","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"MidnightProgrammer","discussion_type":null,"num_comments":29,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m386sc/thoughts_on_this_deepseekr1kimi_k2_build/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m386sc/thoughts_on_this_deepseekr1kimi_k2_build/","subreddit_subscribers":501527,"created_utc":1752859002,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3xbcio","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MidnightProgrammer","can_mod_post":false,"created_utc":1752890106,"send_replies":true,"parent_id":"t1_n3x24om","score":1,"author_fullname":"t2_ijzb7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Amazing yet depressing comment.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3xbcio","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Amazing yet depressing comment.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m386sc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m386sc/thoughts_on_this_deepseekr1kimi_k2_build/n3xbcio/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752890106,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3x24om","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"KernQ","can_mod_post":false,"created_utc":1752886632,"send_replies":true,"parent_id":"t3_1m386sc","score":2,"author_fullname":"t2_1pozn81kn1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"9355p, 12x64gb 5600... I get 9t/s on Ubergarm's quantized K2. I've not done a deep dive on tweaking it, so there may be some juice to squeeze, but this is also similar to my R1 Q8 speed.\\n\\nCPU is at 50% utilisation. I think this is a reflection of 50% RAM usage and being bandwidth constrained (ie not CPU bound in terms of compute, but bound by waiting for RAM).\\n\\nMy 5090 was at ~31GB usage running Gnome and some Firefox windows as well. It didn't look like it was doing much work (~150W total). I wouldn't expect GPU offloading to have a profound effect on generation speed.\\n\\nI doubt that you will hit 20t/s.\\n\\nHowever, these quants feel good and perplexity scores suggest decent performance as well. So \\"almost as good as Q8\\" may be achievable at speed on 4x RTX Pro 6000. (You should also consider whether 12x32GB or 48GB would be viable for these quantized versions in RAM)\\n\\n‚ö†Ô∏è Be aware that making the RAM work hard like this is hot work. Not sure if it's the ECC or general clock speed, but the DIMMs are at risk of critical temperatures if your fan curves are not properly configured. You will need active cooling on the RAM (fans blowing downwards works). I think I fried one of my DIMMs by running R1 Q8 with my fans too low.\\n\\nI would also look at the non-NT mobo (with a separate 10gbe NIC). The NIC chip gets hot and it's buried under your pcie cards, so airflow is difficult and the 5090 doesn't help the temps.\\n\\nIn a workstation case, vertical mount the 5090 otherwise it will take 3 slots; or a case that allows you to put a 3-slot card in the last x16 slot. This is kinda obvious, but if you rackmount you're losing those slots (not that pcie lanes are in short supply, but still). My 5090 doesn't fit in the first slot, as the NIC heatsink gets in the way. I have it in slot 3, which blocks 4 &amp; 5 and the USB3 header. I needed a low profile USB3 header extension cable in order to drive the front ports.\\n\\nThe mobo has no audio, wifi or bluetooth. The broadcom NICs don't seem to be supported by Windows. It also has no USB 2.0 internal header (which you need if you get a WiFi+bluetooth card, or some fan/rgb hubs, or octocomputer, etc etc). Pro tip: look at the Lian Li PSU that has its own internal USB headers hub. Using a USB A-to-A cable you can drive the internal headers from one of the external ports (so plug cable in back of PC, then feed it back into the case and plug into PSU - this cable is provided). This QOL stuff is worth knowing upfront if this is your first \\"server\\" board and you want it to be a \\"desktop workstation\\". Oh yeah - also no header at all for front port USB-C (and no USB-C on the back either) ü´†\\n\\n100% start with air cooling. It's a server board and the VRMs/NIC/RAM demand airflow. I went water-cooling and it's a lot of expense for something that STILL needs reasonable airflow. Trying to make this a silent build is a fool's errand. \\n\\nThis was my first \\"server hardware in a workststion\\" build and it has been and still is a lot of work. If you look at the H13SSL-NT OS compatibility list, something like 1 version of RedHat is officially supported. Stuff like suspend-to-RAM doesn't work well, because it's not supposed to suspend as a server board, etc etc.\\n\\nBottom line: you won't get 20t/s inference and it's a pain in the ass machine. However it's also overkill powerful and fun as a workstation. Probably too powerful - my entire homelab would easily fit into and run faster on this machine. I have no regrets going down this path, but CPU/RAM inferencing hasn't lived up to the hype.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3x24om","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;9355p, 12x64gb 5600... I get 9t/s on Ubergarm&amp;#39;s quantized K2. I&amp;#39;ve not done a deep dive on tweaking it, so there may be some juice to squeeze, but this is also similar to my R1 Q8 speed.&lt;/p&gt;\\n\\n&lt;p&gt;CPU is at 50% utilisation. I think this is a reflection of 50% RAM usage and being bandwidth constrained (ie not CPU bound in terms of compute, but bound by waiting for RAM).&lt;/p&gt;\\n\\n&lt;p&gt;My 5090 was at ~31GB usage running Gnome and some Firefox windows as well. It didn&amp;#39;t look like it was doing much work (~150W total). I wouldn&amp;#39;t expect GPU offloading to have a profound effect on generation speed.&lt;/p&gt;\\n\\n&lt;p&gt;I doubt that you will hit 20t/s.&lt;/p&gt;\\n\\n&lt;p&gt;However, these quants feel good and perplexity scores suggest decent performance as well. So &amp;quot;almost as good as Q8&amp;quot; may be achievable at speed on 4x RTX Pro 6000. (You should also consider whether 12x32GB or 48GB would be viable for these quantized versions in RAM)&lt;/p&gt;\\n\\n&lt;p&gt;‚ö†Ô∏è Be aware that making the RAM work hard like this is hot work. Not sure if it&amp;#39;s the ECC or general clock speed, but the DIMMs are at risk of critical temperatures if your fan curves are not properly configured. You will need active cooling on the RAM (fans blowing downwards works). I think I fried one of my DIMMs by running R1 Q8 with my fans too low.&lt;/p&gt;\\n\\n&lt;p&gt;I would also look at the non-NT mobo (with a separate 10gbe NIC). The NIC chip gets hot and it&amp;#39;s buried under your pcie cards, so airflow is difficult and the 5090 doesn&amp;#39;t help the temps.&lt;/p&gt;\\n\\n&lt;p&gt;In a workstation case, vertical mount the 5090 otherwise it will take 3 slots; or a case that allows you to put a 3-slot card in the last x16 slot. This is kinda obvious, but if you rackmount you&amp;#39;re losing those slots (not that pcie lanes are in short supply, but still). My 5090 doesn&amp;#39;t fit in the first slot, as the NIC heatsink gets in the way. I have it in slot 3, which blocks 4 &amp;amp; 5 and the USB3 header. I needed a low profile USB3 header extension cable in order to drive the front ports.&lt;/p&gt;\\n\\n&lt;p&gt;The mobo has no audio, wifi or bluetooth. The broadcom NICs don&amp;#39;t seem to be supported by Windows. It also has no USB 2.0 internal header (which you need if you get a WiFi+bluetooth card, or some fan/rgb hubs, or octocomputer, etc etc). Pro tip: look at the Lian Li PSU that has its own internal USB headers hub. Using a USB A-to-A cable you can drive the internal headers from one of the external ports (so plug cable in back of PC, then feed it back into the case and plug into PSU - this cable is provided). This QOL stuff is worth knowing upfront if this is your first &amp;quot;server&amp;quot; board and you want it to be a &amp;quot;desktop workstation&amp;quot;. Oh yeah - also no header at all for front port USB-C (and no USB-C on the back either) ü´†&lt;/p&gt;\\n\\n&lt;p&gt;100% start with air cooling. It&amp;#39;s a server board and the VRMs/NIC/RAM demand airflow. I went water-cooling and it&amp;#39;s a lot of expense for something that STILL needs reasonable airflow. Trying to make this a silent build is a fool&amp;#39;s errand. &lt;/p&gt;\\n\\n&lt;p&gt;This was my first &amp;quot;server hardware in a workststion&amp;quot; build and it has been and still is a lot of work. If you look at the H13SSL-NT OS compatibility list, something like 1 version of RedHat is officially supported. Stuff like suspend-to-RAM doesn&amp;#39;t work well, because it&amp;#39;s not supposed to suspend as a server board, etc etc.&lt;/p&gt;\\n\\n&lt;p&gt;Bottom line: you won&amp;#39;t get 20t/s inference and it&amp;#39;s a pain in the ass machine. However it&amp;#39;s also overkill powerful and fun as a workstation. Probably too powerful - my entire homelab would easily fit into and run faster on this machine. I have no regrets going down this path, but CPU/RAM inferencing hasn&amp;#39;t lived up to the hype.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m386sc/thoughts_on_this_deepseekr1kimi_k2_build/n3x24om/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752886632,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m386sc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3wnl21","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Informal-Spinach-345","can_mod_post":false,"created_utc":1752881393,"send_replies":true,"parent_id":"t1_n3vzrqe","score":3,"author_fullname":"t2_e8woqvcq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This is exactly why I'm investing in another Blackwell card. Hosting a collection of really high quality Q8 32B/70B models and switching personas is a far better experience IMO, as nice as the big models are.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3wnl21","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is exactly why I&amp;#39;m investing in another Blackwell card. Hosting a collection of really high quality Q8 32B/70B models and switching personas is a far better experience IMO, as nice as the big models are.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m386sc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m386sc/thoughts_on_this_deepseekr1kimi_k2_build/n3wnl21/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752881393,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n3vzrqe","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GTT444","can_mod_post":false,"created_utc":1752873467,"send_replies":true,"parent_id":"t3_1m386sc","score":3,"author_fullname":"t2_gmg5i5ir2","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I see the pull that these large models have, and can understand your desire to jump on the train, but would advise you against doing it. \\n\\nYour setup sounds fine, but 12k (excluding the 5090) for a system whose only compute power for AI comes from RAM and cpu is a bad deal imo.\\n\\nFirst, I would be very surprised if you can hit 20 tok/s on these large models and even then after some time, 20 tok/s will feel slow and you would like 30 or ideally 40 tok/s. Also R1-0528 is very talkative and uses way more thinking tokens than the variant from January. \\n\\nSo whatever you plan on doing with these large models locally, if you cannot run them with a fast inference speed, it will remain an expensive gimmick. \\n\\nMy advice would be to not try to build a system that can host these large models at slow speed, but rather invest in an extendable system that can host smaller models. Wait for a year and we will have a Kimi2 level model with 70B or 200B parameters. These are then easier to host on 4√ó3090 or 8√ó3090.\\n\\nI built myself a 2x 4090 PC 9 months ago and only regret not getting a motherboard that can support more gpus. I get up 40 tok/s on 70B models with vllm which is very useable.\\n\\nSo my advice: Get a threadripper cpu that can support up to 256gb ram, then buy 3 more 3090s and you will have 350gb ram / vram. This will allow you to host at least a small deepseek quant and will help you wait out until a 200B or smaller model is released with similiar performance to the current Kimi K2.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3vzrqe","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I see the pull that these large models have, and can understand your desire to jump on the train, but would advise you against doing it. &lt;/p&gt;\\n\\n&lt;p&gt;Your setup sounds fine, but 12k (excluding the 5090) for a system whose only compute power for AI comes from RAM and cpu is a bad deal imo.&lt;/p&gt;\\n\\n&lt;p&gt;First, I would be very surprised if you can hit 20 tok/s on these large models and even then after some time, 20 tok/s will feel slow and you would like 30 or ideally 40 tok/s. Also R1-0528 is very talkative and uses way more thinking tokens than the variant from January. &lt;/p&gt;\\n\\n&lt;p&gt;So whatever you plan on doing with these large models locally, if you cannot run them with a fast inference speed, it will remain an expensive gimmick. &lt;/p&gt;\\n\\n&lt;p&gt;My advice would be to not try to build a system that can host these large models at slow speed, but rather invest in an extendable system that can host smaller models. Wait for a year and we will have a Kimi2 level model with 70B or 200B parameters. These are then easier to host on 4√ó3090 or 8√ó3090.&lt;/p&gt;\\n\\n&lt;p&gt;I built myself a 2x 4090 PC 9 months ago and only regret not getting a motherboard that can support more gpus. I get up 40 tok/s on 70B models with vllm which is very useable.&lt;/p&gt;\\n\\n&lt;p&gt;So my advice: Get a threadripper cpu that can support up to 256gb ram, then buy 3 more 3090s and you will have 350gb ram / vram. This will allow you to host at least a small deepseek quant and will help you wait out until a 200B or smaller model is released with similiar performance to the current Kimi K2.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m386sc/thoughts_on_this_deepseekr1kimi_k2_build/n3vzrqe/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752873467,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m386sc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3w12ty","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MidnightProgrammer","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3w0rrt","score":1,"author_fullname":"t2_ijzb7","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah that‚Äôs the problem.  No one to mentor off as it is so niche.  Everything I have read suggests I can get 20t/s with a single cpu if I get one that is 576gb/s but I am still highly skeptical.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3w12ty","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah that‚Äôs the problem.  No one to mentor off as it is so niche.  Everything I have read suggests I can get 20t/s with a single cpu if I get one that is 576gb/s but I am still highly skeptical.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m386sc","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m386sc/thoughts_on_this_deepseekr1kimi_k2_build/n3w12ty/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752873878,"author_flair_text":null,"treatment_tags":[],"created_utc":1752873878,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3w0rrt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Baldur-Norddahl","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3vvc3n","score":1,"author_fullname":"t2_bvqb8ng0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"He is reporting 8.5 tps DeepSeek V3 at q4 on the 9175F system. Add 50% because he doesn't have the RAM fully populated. That gets you 12 tps. Then add another 50% for the dual CPU to get 19 tps (we are assuming they found the solution to the scaling issue). Then add a little because V3 has slightly more active parameters. And yes this is all before slowdown due to context buildup.\\n\\nBut then add a good GPU for the shared expert and prompt processing. Should get you past 20 tps even with context. Maybe even 30 tps could be possible. But only at q4.\\n\\nIt is a big risk. Very few have a system like that and can give you actual numbers.","edited":false,"author_flair_css_class":null,"name":"t1_n3w0rrt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;He is reporting 8.5 tps DeepSeek V3 at q4 on the 9175F system. Add 50% because he doesn&amp;#39;t have the RAM fully populated. That gets you 12 tps. Then add another 50% for the dual CPU to get 19 tps (we are assuming they found the solution to the scaling issue). Then add a little because V3 has slightly more active parameters. And yes this is all before slowdown due to context buildup.&lt;/p&gt;\\n\\n&lt;p&gt;But then add a good GPU for the shared expert and prompt processing. Should get you past 20 tps even with context. Maybe even 30 tps could be possible. But only at q4.&lt;/p&gt;\\n\\n&lt;p&gt;It is a big risk. Very few have a system like that and can give you actual numbers.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m386sc","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m386sc/thoughts_on_this_deepseekr1kimi_k2_build/n3w0rrt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752873779,"author_flair_text":null,"collapsed":false,"created_utc":1752873779,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3vvc3n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MidnightProgrammer","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3vow54","score":1,"author_fullname":"t2_ijzb7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thanks.  Was skimming it on my phone. Those token speeds are so low. Everything I‚Äôve seen puts the configuration im looking at around 20 tokens a second which is usable.  But I‚Äôm not 100% convinced I‚Äôd see that especially as the context fills up.   Much lower and it is just too slow.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3vvc3n","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks.  Was skimming it on my phone. Those token speeds are so low. Everything I‚Äôve seen puts the configuration im looking at around 20 tokens a second which is usable.  But I‚Äôm not 100% convinced I‚Äôd see that especially as the context fills up.   Much lower and it is just too slow.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m386sc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m386sc/thoughts_on_this_deepseekr1kimi_k2_build/n3vvc3n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752872108,"author_flair_text":null,"treatment_tags":[],"created_utc":1752872108,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3vow54","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Baldur-Norddahl","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3utjpp","score":2,"author_fullname":"t2_bvqb8ng0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You are going to find this thread very interesting: https://github.com/ggml-org/llama.cpp/discussions/11733\\n\\nIt is somewhat old (February) so might already be fixed.\\n\\nThey tested multiple dual AMD EPYC setups including a dual 9175F. Unfortunately only with 16 DDR5 instead of the full 24, so less than maximum possible bandwidth.\\n\\nThe result is that dual CPU scales well but not with the DeepSeek architecture. Which is unfortunate because K2 is also based on that.\\n\\nIt is however unclear if this is just a problem with llama.cpp. They mentioned that ktransformers might do better.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3vow54","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You are going to find this thread very interesting: &lt;a href=\\"https://github.com/ggml-org/llama.cpp/discussions/11733\\"&gt;https://github.com/ggml-org/llama.cpp/discussions/11733&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;It is somewhat old (February) so might already be fixed.&lt;/p&gt;\\n\\n&lt;p&gt;They tested multiple dual AMD EPYC setups including a dual 9175F. Unfortunately only with 16 DDR5 instead of the full 24, so less than maximum possible bandwidth.&lt;/p&gt;\\n\\n&lt;p&gt;The result is that dual CPU scales well but not with the DeepSeek architecture. Which is unfortunate because K2 is also based on that.&lt;/p&gt;\\n\\n&lt;p&gt;It is however unclear if this is just a problem with llama.cpp. They mentioned that ktransformers might do better.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m386sc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m386sc/thoughts_on_this_deepseekr1kimi_k2_build/n3vow54/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752870214,"author_flair_text":null,"treatment_tags":[],"created_utc":1752870214,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"7dba5c08-72f1-11ee-9b6f-ca195bc297d4","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3v3jri","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"PraxisOG","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3utjpp","score":1,"author_fullname":"t2_3f9vjjno","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"AFAIK you'd be limited by the bandwidth of the cpu to cpu interconnect. It might be massively faster on newer platforms, I've only really looked into 2nd and 3rd gen epyc seriously.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3v3jri","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"Llama 70B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;AFAIK you&amp;#39;d be limited by the bandwidth of the cpu to cpu interconnect. It might be massively faster on newer platforms, I&amp;#39;ve only really looked into 2nd and 3rd gen epyc seriously.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m386sc","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m386sc/thoughts_on_this_deepseekr1kimi_k2_build/n3v3jri/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752863903,"author_flair_text":"Llama 70B","treatment_tags":[],"created_utc":1752863903,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3utjpp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MidnightProgrammer","can_mod_post":false,"created_utc":1752860995,"send_replies":true,"parent_id":"t1_n3ut57u","score":2,"author_fullname":"t2_ijzb7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I read that dual CPU is only marginally faster and sometimes even worse.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3utjpp","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I read that dual CPU is only marginally faster and sometimes even worse.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m386sc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m386sc/thoughts_on_this_deepseekr1kimi_k2_build/n3utjpp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752860995,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ut57u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Baldur-Norddahl","can_mod_post":false,"created_utc":1752860882,"send_replies":true,"parent_id":"t3_1m386sc","score":1,"author_fullname":"t2_bvqb8ng0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What about dual socket motherboard and 2x 9175F ? With 24x 64 GB RAM. More expensive, but could potentially double tps. Ktransformers has a trick with doubling the weights in memory, so each CPU has direct access to the weights without going through the interconnect. Having two CPUs should also be superior to a single with double the cores due to power limits and higher clock frequency.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ut57u","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What about dual socket motherboard and 2x 9175F ? With 24x 64 GB RAM. More expensive, but could potentially double tps. Ktransformers has a trick with doubling the weights in memory, so each CPU has direct access to the weights without going through the interconnect. Having two CPUs should also be superior to a single with double the cores due to power limits and higher clock frequency.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m386sc/thoughts_on_this_deepseekr1kimi_k2_build/n3ut57u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752860882,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m386sc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3w1a7j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MidnightProgrammer","can_mod_post":false,"created_utc":1752873944,"send_replies":true,"parent_id":"t1_n3w01id","score":2,"author_fullname":"t2_ijzb7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I think the new ones are a ways though but very tempting.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3w1a7j","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think the new ones are a ways though but very tempting.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m386sc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m386sc/thoughts_on_this_deepseekr1kimi_k2_build/n3w1a7j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752873944,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3w01id","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"bick_nyers","can_mod_post":false,"created_utc":1752873550,"send_replies":true,"parent_id":"t3_1m386sc","score":1,"author_fullname":"t2_6nwld4d3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Choosing the CPU is tricky, accurate flops numbers are hard to come by, and it's not immediately obvious how well raw flops on a CPU will translate to tokens per second.\\n\\n\\nIt would be cool if anyone knows how to find a specific CPU model to rent in a cloud environment, the hyperscalers just sell slices of the big 64+ core CPUs generally.\\n\\n\\nMe personally I would opt for enough RAM to hold the model unquantized, even if you run a quantization for memory bandwidth considerations (which depending on the CPU, you might be flops-bound), it's still really convenient to be able to load the weights fully when doing things like making quants. Could also play with the idea of training a Lora on CPU.\\n\\n\\nBtw, Zen 6 EPYC is rumored to have more than double the memory bandwidth, and the next Intel Xeon is expected to have AMX with, if I remember correctly, native FP8 support (whether that will translate to 4x flops vs 32-bit float math is yet to be seen).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3w01id","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Choosing the CPU is tricky, accurate flops numbers are hard to come by, and it&amp;#39;s not immediately obvious how well raw flops on a CPU will translate to tokens per second.&lt;/p&gt;\\n\\n&lt;p&gt;It would be cool if anyone knows how to find a specific CPU model to rent in a cloud environment, the hyperscalers just sell slices of the big 64+ core CPUs generally.&lt;/p&gt;\\n\\n&lt;p&gt;Me personally I would opt for enough RAM to hold the model unquantized, even if you run a quantization for memory bandwidth considerations (which depending on the CPU, you might be flops-bound), it&amp;#39;s still really convenient to be able to load the weights fully when doing things like making quants. Could also play with the idea of training a Lora on CPU.&lt;/p&gt;\\n\\n&lt;p&gt;Btw, Zen 6 EPYC is rumored to have more than double the memory bandwidth, and the next Intel Xeon is expected to have AMX with, if I remember correctly, native FP8 support (whether that will translate to 4x flops vs 32-bit float math is yet to be seen).&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m386sc/thoughts_on_this_deepseekr1kimi_k2_build/n3w01id/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752873550,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m386sc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3wkfad","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"tenebreoscure","can_mod_post":false,"created_utc":1752880324,"send_replies":true,"parent_id":"t3_1m386sc","score":1,"author_fullname":"t2_9ct7f","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Check these two discussions [https://forum.level1techs.com/t/deepseek-deep-dive-r1-at-home/225826](https://forum.level1techs.com/t/deepseek-deep-dive-r1-at-home/225826) and [https://github.com/ikawrakow/ik\\\\_llama.cpp/discussions/477](https://github.com/ikawrakow/ik_llama.cpp/discussions/477), they are about your use case. This thread is also interesting, showing very good BW for Turin Epycs [https://www.reddit.com/r/LocalLLaMA/comments/1h3doy8/stream\\\\_triad\\\\_memory\\\\_bandwidth\\\\_benchmark\\\\_values/](https://www.reddit.com/r/LocalLLaMA/comments/1h3doy8/stream_triad_memory_bandwidth_benchmark_values/) relatively inexpensive. But as it has been pointed out, the real boon would make the two processors configuration deliver the double bandwidth.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3wkfad","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Check these two discussions &lt;a href=\\"https://forum.level1techs.com/t/deepseek-deep-dive-r1-at-home/225826\\"&gt;https://forum.level1techs.com/t/deepseek-deep-dive-r1-at-home/225826&lt;/a&gt; and &lt;a href=\\"https://github.com/ikawrakow/ik_llama.cpp/discussions/477\\"&gt;https://github.com/ikawrakow/ik_llama.cpp/discussions/477&lt;/a&gt;, they are about your use case. This thread is also interesting, showing very good BW for Turin Epycs &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1h3doy8/stream_triad_memory_bandwidth_benchmark_values/\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1h3doy8/stream_triad_memory_bandwidth_benchmark_values/&lt;/a&gt; relatively inexpensive. But as it has been pointed out, the real boon would make the two processors configuration deliver the double bandwidth.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m386sc/thoughts_on_this_deepseekr1kimi_k2_build/n3wkfad/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752880324,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m386sc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3zuv6c","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MidnightProgrammer","can_mod_post":false,"created_utc":1752933089,"send_replies":true,"parent_id":"t1_n3ykt7b","score":1,"author_fullname":"t2_ijzb7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Too slow and limited to 512g","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3zuv6c","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Too slow and limited to 512g&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m386sc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m386sc/thoughts_on_this_deepseekr1kimi_k2_build/n3zuv6c/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752933089,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ykt7b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"megadonkeyx","can_mod_post":false,"created_utc":1752911462,"send_replies":true,"parent_id":"t3_1m386sc","score":1,"author_fullname":"t2_unvzb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For that money just buy a 512gb mac studio","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ykt7b","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For that money just buy a 512gb mac studio&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m386sc/thoughts_on_this_deepseekr1kimi_k2_build/n3ykt7b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752911462,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m386sc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3z1n4c","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GPTrack_ai","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3vub2y","score":1,"author_fullname":"t2_1tpuoj72sa","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"ollama run Kimi --verbose, Error: llama runner process has terminated: GGML\\\\_ASSERT(hparams.n\\\\_expert &lt;= LLAMA\\\\_MAX\\\\_EXPERTS) failed\\n\\nI need to switch to vLLM...","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n3z1n4c","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;ollama run Kimi --verbose, Error: llama runner process has terminated: GGML_ASSERT(hparams.n_expert &amp;lt;= LLAMA_MAX_EXPERTS) failed&lt;/p&gt;\\n\\n&lt;p&gt;I need to switch to vLLM...&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m386sc","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m386sc/thoughts_on_this_deepseekr1kimi_k2_build/n3z1n4c/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752920958,"author_flair_text":null,"treatment_tags":[],"created_utc":1752920958,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3vub2y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MidnightProgrammer","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3vn2tp","score":1,"author_fullname":"t2_ijzb7","approved_by":null,"mod_note":null,"all_awardings":[],"body":"DGX is going to dog slow like the AMD Max 395+. It‚Äôs close to 500gb with a small context window but way higher if you use the 128k context window. The version I am looking to run is 768g plus 24g vram to run it. It‚Äôs a mix of q8 and q4.  \\n\\nYes $350k is way out of my budget.  This is for my basement to crunch data for me. \\n\\nI have a 128g unified ram amd 395+ and it is useless for anything. I‚Äôm not buying 6 6000 Pros.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3vub2y","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;DGX is going to dog slow like the AMD Max 395+. It‚Äôs close to 500gb with a small context window but way higher if you use the 128k context window. The version I am looking to run is 768g plus 24g vram to run it. It‚Äôs a mix of q8 and q4.  &lt;/p&gt;\\n\\n&lt;p&gt;Yes $350k is way out of my budget.  This is for my basement to crunch data for me. &lt;/p&gt;\\n\\n&lt;p&gt;I have a 128g unified ram amd 395+ and it is useless for anything. I‚Äôm not buying 6 6000 Pros.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m386sc","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m386sc/thoughts_on_this_deepseekr1kimi_k2_build/n3vub2y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752871802,"author_flair_text":null,"treatment_tags":[],"created_utc":1752871802,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3vn2tp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GPTrack_ai","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3vicpv","score":1,"author_fullname":"t2_1tpuoj72sa","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It very close to 500GB. That is doable with RTX Pro 6000s. But there is better stuff obviously. I am currently checking tokens/s on GH200 624GB. DGX station would also work.  Best would 8x B200, but I guess 350k is too much for you, right?","edited":false,"author_flair_css_class":null,"name":"t1_n3vn2tp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It very close to 500GB. That is doable with RTX Pro 6000s. But there is better stuff obviously. I am currently checking tokens/s on GH200 624GB. DGX station would also work.  Best would 8x B200, but I guess 350k is too much for you, right?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m386sc","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m386sc/thoughts_on_this_deepseekr1kimi_k2_build/n3vn2tp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752869681,"author_flair_text":null,"collapsed":false,"created_utc":1752869681,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3vlhtk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GenLabsAI","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3vicpv","score":0,"author_fullname":"t2_14cl94t8ha","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It's MoE, you can use as little as 24GB Vram, but very slow.  \\n2x rtx6000 might be the best","edited":false,"author_flair_css_class":null,"name":"t1_n3vlhtk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s MoE, you can use as little as 24GB Vram, but very slow.&lt;br/&gt;\\n2x rtx6000 might be the best&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m386sc","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m386sc/thoughts_on_this_deepseekr1kimi_k2_build/n3vlhtk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752869215,"author_flair_text":null,"collapsed":false,"created_utc":1752869215,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n3vicpv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MidnightProgrammer","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3vhz4c","score":2,"author_fullname":"t2_ijzb7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Q4 it is closer to 1T with cache and full context window.  Not remotely close to fitting on a 6000.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3vicpv","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Q4 it is closer to 1T with cache and full context window.  Not remotely close to fitting on a 6000.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m386sc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m386sc/thoughts_on_this_deepseekr1kimi_k2_build/n3vicpv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752868285,"author_flair_text":null,"treatment_tags":[],"created_utc":1752868285,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3vhz4c","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GPTrack_ai","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3v0nrx","score":0,"author_fullname":"t2_1tpuoj72sa","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"No you don't. LLMs are nowadays inferenced in FP4 (best tradoff between accuracy and speed/size, Blackwell supports FP4 natively). So even kimi needs only approx. 500GB.","edited":1752868806,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3vhz4c","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No you don&amp;#39;t. LLMs are nowadays inferenced in FP4 (best tradoff between accuracy and speed/size, Blackwell supports FP4 natively). So even kimi needs only approx. 500GB.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m386sc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m386sc/thoughts_on_this_deepseekr1kimi_k2_build/n3vhz4c/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752868173,"author_flair_text":null,"treatment_tags":[],"created_utc":1752868173,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n3v0nrx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MidnightProgrammer","can_mod_post":false,"created_utc":1752863054,"send_replies":true,"parent_id":"t1_n3v02m9","score":3,"author_fullname":"t2_ijzb7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Not enough vram would need 8-16 of them.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3v0nrx","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not enough vram would need 8-16 of them.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m386sc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m386sc/thoughts_on_this_deepseekr1kimi_k2_build/n3v0nrx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752863054,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n3v02m9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GPTrack_ai","can_mod_post":false,"created_utc":1752862882,"send_replies":true,"parent_id":"t3_1m386sc","score":0,"author_fullname":"t2_1tpuoj72sa","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"RTX Pro 6000...","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3v02m9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;RTX Pro 6000...&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m386sc/thoughts_on_this_deepseekr1kimi_k2_build/n3v02m9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752862882,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m386sc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n40dcxc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MidnightProgrammer","can_mod_post":false,"send_replies":true,"parent_id":"t1_n40cz0q","score":1,"author_fullname":"t2_ijzb7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Not the case with high bandwidth memory and high end cpus","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n40dcxc","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not the case with high bandwidth memory and high end cpus&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m386sc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m386sc/thoughts_on_this_deepseekr1kimi_k2_build/n40dcxc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752938979,"author_flair_text":null,"treatment_tags":[],"created_utc":1752938979,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n40cz0q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"presidentbidden","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3ve08j","score":0,"author_fullname":"t2_rxqqxmit","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"wow, sounds like you are setting up yourselves to burn lot of money with very little return. you are going to run the entire model in CPU as you implied. the t/s will be SLOW. nfw you will get 20t/s. if i have to guess perhaps 5 t/s. this is what I get when I run models entirely in CPU. perhaps you could look into chaining few mac studios, but even then the t/s will be in the same ball park from what I hear.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n40cz0q","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;wow, sounds like you are setting up yourselves to burn lot of money with very little return. you are going to run the entire model in CPU as you implied. the t/s will be SLOW. nfw you will get 20t/s. if i have to guess perhaps 5 t/s. this is what I get when I run models entirely in CPU. perhaps you could look into chaining few mac studios, but even then the t/s will be in the same ball park from what I hear.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m386sc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m386sc/thoughts_on_this_deepseekr1kimi_k2_build/n40cz0q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752938860,"author_flair_text":null,"treatment_tags":[],"created_utc":1752938860,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ve08j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MidnightProgrammer","can_mod_post":false,"created_utc":1752866991,"send_replies":true,"parent_id":"t1_n3vcn86","score":3,"author_fullname":"t2_ijzb7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"R1 and Kimi are the models.  671B and 1T.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ve08j","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;R1 and Kimi are the models.  671B and 1T.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m386sc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m386sc/thoughts_on_this_deepseekr1kimi_k2_build/n3ve08j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752866991,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n3vcn86","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"presidentbidden","can_mod_post":false,"created_utc":1752866586,"send_replies":true,"parent_id":"t3_1m386sc","score":-2,"author_fullname":"t2_rxqqxmit","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"its not clear what you want to do. what models are you looking to run for R1/K2 ? are you going full CPU ? 3090 is a decent card, you can get good speeds on 32b Q4 models (20 t/s).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3vcn86","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;its not clear what you want to do. what models are you looking to run for R1/K2 ? are you going full CPU ? 3090 is a decent card, you can get good speeds on 32b Q4 models (20 t/s).&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m386sc/thoughts_on_this_deepseekr1kimi_k2_build/n3vcn86/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752866586,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m386sc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-2}}],"before":null}}]`),s=()=>e.jsx(t,{data:l});export{s as default};
