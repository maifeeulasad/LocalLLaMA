import{j as e}from"./index-BrqAgJWx.js";import{R as l}from"./RedditPostRenderer-chN7TDMj.js";import"./index-DlMtF8rT.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I am looking for the best local LLMs that I can use with cursor for my professional work. So, I am willing to invest a few grands on the GPU.  \\nWhich are the best models for GPUs with 12gb, 16gb and 24gb vram?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Best model tuned specifically for Programming?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lmpd8j","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.64,"author_flair_background_color":null,"subreddit_type":"public","ups":6,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1fsmo8fhzg","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":6,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1751133781,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751124089,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I am looking for the best local LLMs that I can use with cursor for my professional work. So, I am willing to invest a few grands on the GPU.&lt;br/&gt;\\nWhich are the best models for GPUs with 12gb, 16gb and 24gb vram?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lmpd8j","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Fragrant-Review-5055","discussion_type":null,"num_comments":29,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lmpd8j/best_model_tuned_specifically_for_programming/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lmpd8j/best_model_tuned_specifically_for_programming/","subreddit_subscribers":492626,"created_utc":1751124089,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0c57lz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"loyalekoinu88","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0aiu0m","score":1,"author_fullname":"t2_1x5p0ubz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Sneakily? After it told everyone and stood at the door waiting for someone to be like “leaving already?” but it never came.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0c57lz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sneakily? After it told everyone and stood at the door waiting for someone to be like “leaving already?” but it never came.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmpd8j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmpd8j/best_model_tuned_specifically_for_programming/n0c57lz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751160246,"author_flair_text":null,"treatment_tags":[],"created_utc":1751160246,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0aiu0m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"mxforest","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0acorl","score":7,"author_fullname":"t2_kenmq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Llama 4 has sneakily left the chat.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0aiu0m","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Llama 4 has sneakily left the chat.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmpd8j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmpd8j/best_model_tuned_specifically_for_programming/n0aiu0m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751139666,"author_flair_text":null,"treatment_tags":[],"created_utc":1751139666,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}}],"before":null}},"user_reports":[],"saved":false,"id":"n0acorl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"smahs9","can_mod_post":false,"created_utc":1751137660,"send_replies":true,"parent_id":"t1_n0a4k0m","score":10,"author_fullname":"t2_neyagc1uz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Always\\n&gt; The best model is the next model","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0acorl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Always&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;The best model is the next model&lt;/p&gt;\\n&lt;/blockquote&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmpd8j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmpd8j/best_model_tuned_specifically_for_programming/n0acorl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751137660,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}}],"before":null}},"user_reports":[],"saved":false,"id":"n0a4k0m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"loyalekoinu88","can_mod_post":false,"created_utc":1751135087,"send_replies":true,"parent_id":"t3_1lmpd8j","score":8,"author_fullname":"t2_1x5p0ubz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The best model is the next model: [Qwen 3 coder is on the way : r/singularity](https://www.reddit.com/r/singularity/comments/1lmox2e/qwen_3_coder_is_on_the_way/)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0a4k0m","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The best model is the next model: &lt;a href=\\"https://www.reddit.com/r/singularity/comments/1lmox2e/qwen_3_coder_is_on_the_way/\\"&gt;Qwen 3 coder is on the way : r/singularity&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmpd8j/best_model_tuned_specifically_for_programming/n0a4k0m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751135087,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmpd8j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n09f5dp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GustaveVonZarovich","can_mod_post":false,"created_utc":1751127203,"send_replies":true,"parent_id":"t3_1lmpd8j","score":5,"author_fullname":"t2_ftfadtol","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"[https://huggingface.co/spaces/Qwen/Qwen3-Demo](https://huggingface.co/spaces/Qwen/Qwen3-Demo)  \\ntry this for small projects","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n09f5dp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://huggingface.co/spaces/Qwen/Qwen3-Demo\\"&gt;https://huggingface.co/spaces/Qwen/Qwen3-Demo&lt;/a&gt;&lt;br/&gt;\\ntry this for small projects&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmpd8j/best_model_tuned_specifically_for_programming/n09f5dp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751127203,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmpd8j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0b7289","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AXYZE8","can_mod_post":false,"created_utc":1751147652,"send_replies":true,"parent_id":"t1_n0b66rj","score":2,"author_fullname":"t2_10dacl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Just checked the prices of used A770 16GB. $200. What a steal.\\n\\n\\nEdit: Also I see that they compile their ipex-llm backend into ollama https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_portable_zip_quickstart.md and llama.cpp https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/llamacpp_portable_zip_gpu_quickstart.md\\n\\n\\nSo its as easy to deploy as it gets now, zero manual stuff to have maximum performance.","edited":1751148672,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0b7289","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just checked the prices of used A770 16GB. $200. What a steal.&lt;/p&gt;\\n\\n&lt;p&gt;Edit: Also I see that they compile their ipex-llm backend into ollama &lt;a href=\\"https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_portable_zip_quickstart.md\\"&gt;https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/ollama_portable_zip_quickstart.md&lt;/a&gt; and llama.cpp &lt;a href=\\"https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/llamacpp_portable_zip_gpu_quickstart.md\\"&gt;https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/llamacpp_portable_zip_gpu_quickstart.md&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;So its as easy to deploy as it gets now, zero manual stuff to have maximum performance.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmpd8j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmpd8j/best_model_tuned_specifically_for_programming/n0b7289/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751147652,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0dqej4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AXYZE8","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0cdtfg","score":1,"author_fullname":"t2_10dacl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Cursor doesnt have that option, because its cloud based.\\n\\n\\nOnly thing you can do is expose your local LLM through tunnel like Ngrok, but the prompt and code(embeddings) still go to Cursor servers and then Cursor server talks to your LLM. Calling this local is a biiiig stretch. Its not private, you still cannot use it without internet connection.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0dqej4","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Cursor doesnt have that option, because its cloud based.&lt;/p&gt;\\n\\n&lt;p&gt;Only thing you can do is expose your local LLM through tunnel like Ngrok, but the prompt and code(embeddings) still go to Cursor servers and then Cursor server talks to your LLM. Calling this local is a biiiig stretch. Its not private, you still cannot use it without internet connection.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmpd8j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmpd8j/best_model_tuned_specifically_for_programming/n0dqej4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751188684,"author_flair_text":null,"treatment_tags":[],"created_utc":1751188684,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0cdtfg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"sleepy_roger","can_mod_post":false,"created_utc":1751163693,"send_replies":true,"parent_id":"t1_n0b66rj","score":1,"author_fullname":"t2_usojvms","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm using local llms just fine with cursor.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0cdtfg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m using local llms just fine with cursor.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmpd8j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmpd8j/best_model_tuned_specifically_for_programming/n0cdtfg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751163693,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ezzm7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AXYZE8","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ey8ud","score":1,"author_fullname":"t2_10dacl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes, you can point it and it works like that:\\n\\n\\nCursor (app) -&gt; Cursor server receives prompt and your code(embeddings) -&gt; OpenAI endpoint\\n\\n\\nIts not a local LLM support at all. With something like Cline or Continue and local LLM code doesnt exit your machine and you can work totally offline.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0ezzm7","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, you can point it and it works like that:&lt;/p&gt;\\n\\n&lt;p&gt;Cursor (app) -&amp;gt; Cursor server receives prompt and your code(embeddings) -&amp;gt; OpenAI endpoint&lt;/p&gt;\\n\\n&lt;p&gt;Its not a local LLM support at all. With something like Cline or Continue and local LLM code doesnt exit your machine and you can work totally offline.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmpd8j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmpd8j/best_model_tuned_specifically_for_programming/n0ezzm7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751208927,"author_flair_text":null,"treatment_tags":[],"created_utc":1751208927,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ey8ud","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"YouAreTheCornhole","can_mod_post":false,"created_utc":1751208373,"send_replies":true,"parent_id":"t1_n0b66rj","score":1,"author_fullname":"t2_v16kqwyo","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You can point Cursor to any endpoint that's OpenAI compatible.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ey8ud","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can point Cursor to any endpoint that&amp;#39;s OpenAI compatible.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmpd8j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmpd8j/best_model_tuned_specifically_for_programming/n0ey8ud/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751208373,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0b66rj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AXYZE8","can_mod_post":false,"created_utc":1751147350,"send_replies":true,"parent_id":"t3_1lmpd8j","score":7,"author_fullname":"t2_10dacl","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Cursor doesn't support local LLM. \\n\\n\\n\\n\\nYou need to use GitHub Copilot (it has Ollama support) or some extension like Continue, Kilocode or Cline. Cline is the safest best I would say for local models, but all of them are free so just check which works the best for you.\\n\\n\\n\\n\\nModel — use **Devstral 2505**, because that one works great as agent and codes nicely. \\nIQ3_XXS for 12GB VRAM, IQ4_XS for 16GB VRAM, Q5_K_XL for 24GB. You may go lower with quants if you have other apps running on the machine (Steam, Discord etc all take VRAM) or require longer context window, but these that I suggested will be an ideal starting point or even the sweetspot you're searching for.\\n\\n\\n\\n\\nYou really want most VRAM for coding. Intel Arc A770 16GB is very cheap nowadays ($290 new in my country) and its on 256bit bus so its fast (560GB/s). Hidden gem, Intel fixed most issues, Vulkan backend works fine, so you it works out of the box and if you want to experiment more docs are polished and theres tons of guides online. \\n\\n\\n\\n\\nQwen3 Coder family will have different model sizes and it will likely gives you better quality at every VRAM capacity, so dont miss out on this, it should be released next month.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0b66rj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Cursor doesn&amp;#39;t support local LLM. &lt;/p&gt;\\n\\n&lt;p&gt;You need to use GitHub Copilot (it has Ollama support) or some extension like Continue, Kilocode or Cline. Cline is the safest best I would say for local models, but all of them are free so just check which works the best for you.&lt;/p&gt;\\n\\n&lt;p&gt;Model — use &lt;strong&gt;Devstral 2505&lt;/strong&gt;, because that one works great as agent and codes nicely. \\nIQ3_XXS for 12GB VRAM, IQ4_XS for 16GB VRAM, Q5_K_XL for 24GB. You may go lower with quants if you have other apps running on the machine (Steam, Discord etc all take VRAM) or require longer context window, but these that I suggested will be an ideal starting point or even the sweetspot you&amp;#39;re searching for.&lt;/p&gt;\\n\\n&lt;p&gt;You really want most VRAM for coding. Intel Arc A770 16GB is very cheap nowadays ($290 new in my country) and its on 256bit bus so its fast (560GB/s). Hidden gem, Intel fixed most issues, Vulkan backend works fine, so you it works out of the box and if you want to experiment more docs are polished and theres tons of guides online. &lt;/p&gt;\\n\\n&lt;p&gt;Qwen3 Coder family will have different model sizes and it will likely gives you better quality at every VRAM capacity, so dont miss out on this, it should be released next month.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmpd8j/best_model_tuned_specifically_for_programming/n0b66rj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751147350,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmpd8j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n09koxh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"sxales","can_mod_post":false,"created_utc":1751128986,"send_replies":true,"parent_id":"t3_1lmpd8j","score":3,"author_fullname":"t2_5h1ye","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I've been satisfied with GLM 4-0414","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n09koxh","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve been satisfied with GLM 4-0414&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmpd8j/best_model_tuned_specifically_for_programming/n09koxh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751128986,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lmpd8j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0b81dq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AXYZE8","can_mod_post":false,"created_utc":1751147988,"send_replies":true,"parent_id":"t1_n09bsvb","score":1,"author_fullname":"t2_10dacl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thats new. QwQ was experimental \\"Qwen with Questions\\" and it was merged with mainline Qwen in Qwen3 family.\\n\\n\\nWhen/where did Qwen announce that QwQ will have successor? I didnt saw it","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0b81dq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thats new. QwQ was experimental &amp;quot;Qwen with Questions&amp;quot; and it was merged with mainline Qwen in Qwen3 family.&lt;/p&gt;\\n\\n&lt;p&gt;When/where did Qwen announce that QwQ will have successor? I didnt saw it&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmpd8j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmpd8j/best_model_tuned_specifically_for_programming/n0b81dq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751147988,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n09bsvb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ObscuraMirage","can_mod_post":false,"created_utc":1751126138,"send_replies":true,"parent_id":"t3_1lmpd8j","score":1,"author_fullname":"t2_1lhlr1gno5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The more ram is always better but last time I researched (couple months ago when qwen3 got released) codestral, qwq and Qwen2.5 32B. You need at least 32gb for these at Q4 (32gb MacM4 here)\\n\\nThese are my 0.02 at least. I think they were waiting from a successor for QWQ that Qwen announced is in the works but I haven’t seen it if it’s out or not yet","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n09bsvb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The more ram is always better but last time I researched (couple months ago when qwen3 got released) codestral, qwq and Qwen2.5 32B. You need at least 32gb for these at Q4 (32gb MacM4 here)&lt;/p&gt;\\n\\n&lt;p&gt;These are my 0.02 at least. I think they were waiting from a successor for QWQ that Qwen announced is in the works but I haven’t seen it if it’s out or not yet&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmpd8j/best_model_tuned_specifically_for_programming/n09bsvb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751126138,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmpd8j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0cbhxc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Mountain_Chicken7644","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0a133u","score":1,"author_fullname":"t2_qvl85rth","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Sglang is another one of those lm runtimes","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0cbhxc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sglang is another one of those lm runtimes&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmpd8j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmpd8j/best_model_tuned_specifically_for_programming/n0cbhxc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751162751,"author_flair_text":null,"treatment_tags":[],"created_utc":1751162751,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0a133u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Fragrant-Review-5055","can_mod_post":false,"send_replies":true,"parent_id":"t1_n09paay","score":1,"author_fullname":"t2_1fsmo8fhzg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"tested what on what?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0a133u","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;tested what on what?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmpd8j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmpd8j/best_model_tuned_specifically_for_programming/n0a133u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751133993,"author_flair_text":null,"treatment_tags":[],"created_utc":1751133993,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n09paay","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ben1984th","can_mod_post":false,"created_utc":1751130413,"send_replies":true,"parent_id":"t1_n09mtcm","score":1,"author_fullname":"t2_13o91s","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I tested it with sglang… it’s crap with RooCode.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n09paay","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I tested it with sglang… it’s crap with RooCode.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmpd8j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmpd8j/best_model_tuned_specifically_for_programming/n09paay/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751130413,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0apyi4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No-Consequence-1779","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0a4ohb","score":1,"author_fullname":"t2_1ping1tiaw","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Tru dis yo! You can watch the cuda usage switch between gpus. ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0apyi4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Tru dis yo! You can watch the cuda usage switch between gpus. &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmpd8j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmpd8j/best_model_tuned_specifically_for_programming/n0apyi4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751141982,"author_flair_text":null,"treatment_tags":[],"created_utc":1751141982,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0a4ohb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Baldur-Norddahl","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0a11wq","score":3,"author_fullname":"t2_bvqb8ng0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You should get the same performance of a single 3090 but with twice the VRAM. It won't process in parallel, but simply put one half of the layers on each card.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0a4ohb","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You should get the same performance of a single 3090 but with twice the VRAM. It won&amp;#39;t process in parallel, but simply put one half of the layers on each card.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmpd8j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmpd8j/best_model_tuned_specifically_for_programming/n0a4ohb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751135126,"author_flair_text":null,"treatment_tags":[],"created_utc":1751135126,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0a11wq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Fragrant-Review-5055","can_mod_post":false,"created_utc":1751133982,"send_replies":true,"parent_id":"t1_n09mtcm","score":1,"author_fullname":"t2_1fsmo8fhzg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"dual 3090? Is it even possible for llms? I heard the performance set back for distributed GPUs is not worth it","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0a11wq","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;dual 3090? Is it even possible for llms? I heard the performance set back for distributed GPUs is not worth it&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmpd8j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmpd8j/best_model_tuned_specifically_for_programming/n0a11wq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751133982,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n09mtcm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Baldur-Norddahl","can_mod_post":false,"created_utc":1751129656,"send_replies":true,"parent_id":"t3_1lmpd8j","score":1,"author_fullname":"t2_bvqb8ng0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"We are currently waiting for GGUF for the brand new Hunyuan-A13B (released yesterday). It has the potential to beat everything else running on reasonable hardware. But it will likely require 40-50G of VRAM with 256k context at 4 bit quantization.\\n\\nI think even 24 GB is very limiting with respect to practical coding LLMs. Yes you can do the 32b models, but those are just barely good. I would invest in unified memory. Preferably a M4 Mac, but if that is not on the table, then I would consider the new AMD AI 395 with 128 GB memory.\\n\\nOn the other hand, a Nvidia 5090 24 GB VRAM (or even second hand 3090) is going to beat both the Mac and the AI 395 by leagues in terms of speed. It is just that you are a bit limited to what models you can run. A dual 3090 might be the dream for the Hunyuan-A13B.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n09mtcm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;We are currently waiting for GGUF for the brand new Hunyuan-A13B (released yesterday). It has the potential to beat everything else running on reasonable hardware. But it will likely require 40-50G of VRAM with 256k context at 4 bit quantization.&lt;/p&gt;\\n\\n&lt;p&gt;I think even 24 GB is very limiting with respect to practical coding LLMs. Yes you can do the 32b models, but those are just barely good. I would invest in unified memory. Preferably a M4 Mac, but if that is not on the table, then I would consider the new AMD AI 395 with 128 GB memory.&lt;/p&gt;\\n\\n&lt;p&gt;On the other hand, a Nvidia 5090 24 GB VRAM (or even second hand 3090) is going to beat both the Mac and the AI 395 by leagues in terms of speed. It is just that you are a bit limited to what models you can run. A dual 3090 might be the dream for the Hunyuan-A13B.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmpd8j/best_model_tuned_specifically_for_programming/n09mtcm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751129656,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmpd8j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0cpuo1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No-Consequence-1779","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0bugbd","score":1,"author_fullname":"t2_1ping1tiaw","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes, not that it’s needed. Running 1 model split across 2 gpus just puts them to work at 50% so still 350 watts max. I’m adding a 5090 so I’ll see what happens then. ","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0cpuo1","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, not that it’s needed. Running 1 model split across 2 gpus just puts them to work at 50% so still 350 watts max. I’m adding a 5090 so I’ll see what happens then. &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmpd8j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmpd8j/best_model_tuned_specifically_for_programming/n0cpuo1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751168824,"author_flair_text":null,"treatment_tags":[],"created_utc":1751168824,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0bugbd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"skipfish","can_mod_post":false,"created_utc":1751156112,"send_replies":true,"parent_id":"t1_n0anih1","score":2,"author_fullname":"t2_5jbk4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"1200W PSU?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0bugbd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;1200W PSU?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmpd8j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmpd8j/best_model_tuned_specifically_for_programming/n0bugbd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751156112,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0cor47","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No-Consequence-1779","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0cn78k","score":1,"author_fullname":"t2_1ping1tiaw","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"For inference, multiple gpus do not increase the speed so if it’s about vram, it might be better to do an ampere gpu with 48gb ram. \\n\\nIf you’re doing 4x distribution then the power will be 1/4 watts for each gpu - it wouldn’t necessarily add more power demands. \\n\\nRunning a 96gb vram model will be much slower in the single digits. \\n\\nRtx 6000 might be better. \\n\\nI have tried larger models up to 70b and for coding at least, I see no benefit. \\n\\n4x would fill my 4 pci slots and those are pretty much the valuable commodity. \\n\\nWhat are you thinking for 4x3099s? \\n\\nFor coding a 4K budget, I’d probably and did pay a scalper 3k for a 5090.  It has 21,000 cuda cores (2x of the 3090 and 4090) and beam speed is much faster. \\n\\n32gb vram will fit a coder LLM 32 b parameter with a large context size. So even 1 5090 would be better. \\n\\nI left out an important thing. The after market cards are usually 2.5 slot width which will kill the spacing.  Usually FE cards are 2 slot wide. And server cards. With a blower. It could get toasty also but easily fixed with a leaf blower. ","edited":1751169970,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0cor47","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For inference, multiple gpus do not increase the speed so if it’s about vram, it might be better to do an ampere gpu with 48gb ram. &lt;/p&gt;\\n\\n&lt;p&gt;If you’re doing 4x distribution then the power will be 1/4 watts for each gpu - it wouldn’t necessarily add more power demands. &lt;/p&gt;\\n\\n&lt;p&gt;Running a 96gb vram model will be much slower in the single digits. &lt;/p&gt;\\n\\n&lt;p&gt;Rtx 6000 might be better. &lt;/p&gt;\\n\\n&lt;p&gt;I have tried larger models up to 70b and for coding at least, I see no benefit. &lt;/p&gt;\\n\\n&lt;p&gt;4x would fill my 4 pci slots and those are pretty much the valuable commodity. &lt;/p&gt;\\n\\n&lt;p&gt;What are you thinking for 4x3099s? &lt;/p&gt;\\n\\n&lt;p&gt;For coding a 4K budget, I’d probably and did pay a scalper 3k for a 5090.  It has 21,000 cuda cores (2x of the 3090 and 4090) and beam speed is much faster. &lt;/p&gt;\\n\\n&lt;p&gt;32gb vram will fit a coder LLM 32 b parameter with a large context size. So even 1 5090 would be better. &lt;/p&gt;\\n\\n&lt;p&gt;I left out an important thing. The after market cards are usually 2.5 slot width which will kill the spacing.  Usually FE cards are 2 slot wide. And server cards. With a blower. It could get toasty also but easily fixed with a leaf blower. &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmpd8j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmpd8j/best_model_tuned_specifically_for_programming/n0cor47/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751168334,"author_flair_text":null,"treatment_tags":[],"created_utc":1751168334,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0cn78k","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Fragrant-Review-5055","can_mod_post":false,"created_utc":1751167645,"send_replies":true,"parent_id":"t1_n0anih1","score":1,"author_fullname":"t2_1fsmo8fhzg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What's you thought on running 4x3090 side by side?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0cn78k","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What&amp;#39;s you thought on running 4x3090 side by side?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmpd8j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmpd8j/best_model_tuned_specifically_for_programming/n0cn78k/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751167645,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0anih1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No-Consequence-1779","can_mod_post":false,"created_utc":1751141195,"send_replies":true,"parent_id":"t3_1lmpd8j","score":1,"author_fullname":"t2_1ping1tiaw","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I’d recommend a used 3090, 4090, or 5090. 1.2.3k$. Time is money so more vram = larger model and larger context. \\n\\nThere are specific coder models. I prefer qwen. There is deepseek, cline, and others. Check out huggingface.  \\n\\nMy specific setup is threadripper 16/32 64 pcilanes, 2x3090 1x5090 , 128gb ram. 1200 watt PSU. \\n\\nI run qwen2.5-coder-32b-instruct Q8_O bartowski 128k with a 62,000 context. Lm studio as I use visual studio enterprise for work. \\n\\nI sometimes run a 14b model for simpler stuff as the speed is 24-26 versus 14 tokens per second. \\n\\nOnce I replace the 2nd 3090 with a 5090 (3k each) it’s much faster. \\n\\nThe limits are ultimately pcie slots and your power supply. Then your 15 amp house circuit when running many gpus. Miner problems. \\n\\nSo it’s better to go higher ram and performance as budget allows. Less than 24gb vram is a waste of space for most. \\n\\nLarger cards for enterprise like a600ada Rtx pro 6000… note the generation ampere, Lovelace, Blackwell and cude core counts matters. VRAM speed directly affects token speed by model parameters. For inference. Not training or fine tuning. \\n\\nThis is why a 3090 and 4090 are very similar cuda count 10k , sane vram speed so the 3090 is a better deal for inference. ","edited":1751141824,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0anih1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I’d recommend a used 3090, 4090, or 5090. 1.2.3k$. Time is money so more vram = larger model and larger context. &lt;/p&gt;\\n\\n&lt;p&gt;There are specific coder models. I prefer qwen. There is deepseek, cline, and others. Check out huggingface.  &lt;/p&gt;\\n\\n&lt;p&gt;My specific setup is threadripper 16/32 64 pcilanes, 2x3090 1x5090 , 128gb ram. 1200 watt PSU. &lt;/p&gt;\\n\\n&lt;p&gt;I run qwen2.5-coder-32b-instruct Q8_O bartowski 128k with a 62,000 context. Lm studio as I use visual studio enterprise for work. &lt;/p&gt;\\n\\n&lt;p&gt;I sometimes run a 14b model for simpler stuff as the speed is 24-26 versus 14 tokens per second. &lt;/p&gt;\\n\\n&lt;p&gt;Once I replace the 2nd 3090 with a 5090 (3k each) it’s much faster. &lt;/p&gt;\\n\\n&lt;p&gt;The limits are ultimately pcie slots and your power supply. Then your 15 amp house circuit when running many gpus. Miner problems. &lt;/p&gt;\\n\\n&lt;p&gt;So it’s better to go higher ram and performance as budget allows. Less than 24gb vram is a waste of space for most. &lt;/p&gt;\\n\\n&lt;p&gt;Larger cards for enterprise like a600ada Rtx pro 6000… note the generation ampere, Lovelace, Blackwell and cude core counts matters. VRAM speed directly affects token speed by model parameters. For inference. Not training or fine tuning. &lt;/p&gt;\\n\\n&lt;p&gt;This is why a 3090 and 4090 are very similar cuda count 10k , sane vram speed so the 3090 is a better deal for inference. &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmpd8j/best_model_tuned_specifically_for_programming/n0anih1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751141195,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmpd8j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0bpw4p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ylsid","can_mod_post":false,"created_utc":1751154421,"send_replies":true,"parent_id":"t3_1lmpd8j","score":1,"author_fullname":"t2_6lmlc","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I haven't found many useful for too much. None of them are optimized for refactoring code","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0bpw4p","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I haven&amp;#39;t found many useful for too much. None of them are optimized for refactoring code&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmpd8j/best_model_tuned_specifically_for_programming/n0bpw4p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751154421,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmpd8j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0dg367","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MatricesRL","can_mod_post":false,"created_utc":1751182371,"send_replies":true,"parent_id":"t3_1lmpd8j","score":1,"author_fullname":"t2_1rrk5mb0fr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"DeepSeek R1 and Qwen 2.5 Coder","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0dg367","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;DeepSeek R1 and Qwen 2.5 Coder&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmpd8j/best_model_tuned_specifically_for_programming/n0dg367/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751182371,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmpd8j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0dtopt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1751190709,"send_replies":true,"parent_id":"t3_1lmpd8j","score":1,"author_fullname":"t2_uz37qfx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"My goto model for dumb stuff like simple refactoring, adding/removing debug prints, generating testcode I use Qwen 3 30b A3B. For more serious stuff Qwen3-32B non-coding. I used to use mostly Qwen2.5-coder-14b (will work just fine on 12B), but these non-coding Qwen3 often produces better results in my niche. \\n\\nI have 20 GiB VRAM.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0dtopt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;My goto model for dumb stuff like simple refactoring, adding/removing debug prints, generating testcode I use Qwen 3 30b A3B. For more serious stuff Qwen3-32B non-coding. I used to use mostly Qwen2.5-coder-14b (will work just fine on 12B), but these non-coding Qwen3 often produces better results in my niche. &lt;/p&gt;\\n\\n&lt;p&gt;I have 20 GiB VRAM.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmpd8j/best_model_tuned_specifically_for_programming/n0dtopt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751190709,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmpd8j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(l,{data:t});export{n as default};
