import{j as e}from"./index-Cd3v0jxz.js";import{R as t}from"./RedditPostRenderer-cI96YLhy.js";import"./index-DGBoKyQm.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hello everyone, i was scrolling on LM studio and always saw model like \\"model_name_q4_k_m.gguf\\" everything before the _k is clear to me but i didnt get the last part about _k_m, i saw somewhere that the _k stand for some \\"dynamic quantization\\" but what does the _M or _S and _L mean? Small, medium, large? But still didnt tell me what is small, medium or large?\\n\\nthank by advance ","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"What does the _K _S _M _L mean behind the quantization of a model?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m6tbhm","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.89,"author_flair_background_color":null,"subreddit_type":"public","ups":27,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_jgegifux8","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":27,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753226118,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hello everyone, i was scrolling on LM studio and always saw model like &amp;quot;model_name_q4_k_m.gguf&amp;quot; everything before the _k is clear to me but i didnt get the last part about _k_m, i saw somewhere that the _k stand for some &amp;quot;dynamic quantization&amp;quot; but what does the _M or _S and _L mean? Small, medium, large? But still didnt tell me what is small, medium or large?&lt;/p&gt;\\n\\n&lt;p&gt;thank by advance &lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m6tbhm","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Hurtcraft01","discussion_type":null,"num_comments":15,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m6tbhm/what_does_the_k_s_m_l_mean_behind_the/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m6tbhm/what_does_the_k_s_m_l_mean_behind_the/","subreddit_subscribers":503519,"created_utc":1753226118,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4oy4qk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BumbleSlob","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4oblyi","score":3,"author_fullname":"t2_1j7fhlcqkp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Iâ€™d love a write up. Thanks for the corrections. Itâ€™s been a while since I looked at this directly and seems I need to refresh myself a bit.Â ","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4oy4qk","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Iâ€™d love a write up. Thanks for the corrections. Itâ€™s been a while since I looked at this directly and seems I need to refresh myself a bit.Â &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6tbhm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6tbhm/what_does_the_k_s_m_l_mean_behind_the/n4oy4qk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753269324,"author_flair_text":null,"treatment_tags":[],"created_utc":1753269324,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n4oblyi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"gofiend","can_mod_post":false,"created_utc":1753257187,"send_replies":true,"parent_id":"t1_n4mjyc0","score":18,"author_fullname":"t2_2roqrw5l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This isn't quite right. The general overview of K quants is roughly right, but not the details on \\\\_S \\\\_M \\\\_L \\\\_XL.\\n\\nQX\\\\_K\\\\_X quants are mixed precision to do a better job of getting to the target memory usage. The idea is to allocate more precision if it will significantly improve model accuracy without taking up too much bandwidth.\\n\\nThe current state (e.g. how Unsloth does it) looks like this: (focusing on Q4\\\\_K\\\\_...)\\n\\n* Layer norms are very sensitive, and there are very few of them per block so leave at full F32 precision\\n* The token embedding, and v vector weights reward greater precision so keep them at Q5/Q6\\n* The ffn and k, q weights are the most numerous, and the least sensitive, so use the target quant for them (i.e. Q4)\\n\\nI put this table together for my own edification a little while back ... it really helps me understand just how sophisticated the llama.cpp folks (and u/ikawrakow) have gotten.\\n\\nhttps://preview.redd.it/l6f3mjshalef1.png?width=713&amp;format=png&amp;auto=webp&amp;s=2dbd584a2c6dbb26bc9a9286373f5cfa0b9341ff\\n\\n*Source: Huggingface's summary view of Unsloth's Qwen 3 4B quants (*[*Q4\\\\_K\\\\_S*](https://huggingface.co/unsloth/Qwen3-4B-GGUF?show_file_info=Qwen3-4B-Q4_K_S.gguf)*,* [*Q4\\\\_K\\\\_M*](https://huggingface.co/unsloth/Qwen3-4B-GGUF?show_file_info=Qwen3-4B-Q3_K_M.gguf)*,* [*Q4\\\\_K\\\\_XL*](https://huggingface.co/unsloth/Qwen3-4B-GGUF?show_file_info=Qwen3-4B-UD-Q4_K_XL.gguf)*) ... scroll down to blk0 etc.)*\\n\\nAll that is just about the last letters in the quant name. The first part of the name covers the fact that there are currently three broad families of GGUF quants:\\n\\n* Q4\\\\_0, Q4\\\\_1: These are the OG method for quantizing, and closest to what /u/[BumbleSlob](https://www.reddit.com/user/BumbleSlob/) described\\n* Q4\\\\_K\\\\_S Q4\\\\_K\\\\_M etc.: The K quants, which we all use, use two levels of quantization blocks (\\"superblocks\\").  In order to improve efficiency, the scaling and offset factors are themselves stored in quantized blocks.\\n* IQ4\\\\_XS, IQ3\\\\_M etc.: The IQ quants, use a totally different (and cool) method: the weights are recorded as lookups into a table of mostly orthogonal 8D vectors\\n\\nI've been meaning to do a more narrative / visual write up of this ... **please let me know if you are interested**.\\n\\nIf you have time, Julia Turc's got a [phenomenal youtube](https://www.youtube.com/watch?v=vW30o4U9BFE) that goes into this (and other related topics) in superb detail.","edited":1753261605,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4oblyi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This isn&amp;#39;t quite right. The general overview of K quants is roughly right, but not the details on _S _M _L _XL.&lt;/p&gt;\\n\\n&lt;p&gt;QX_K_X quants are mixed precision to do a better job of getting to the target memory usage. The idea is to allocate more precision if it will significantly improve model accuracy without taking up too much bandwidth.&lt;/p&gt;\\n\\n&lt;p&gt;The current state (e.g. how Unsloth does it) looks like this: (focusing on Q4_K_...)&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Layer norms are very sensitive, and there are very few of them per block so leave at full F32 precision&lt;/li&gt;\\n&lt;li&gt;The token embedding, and v vector weights reward greater precision so keep them at Q5/Q6&lt;/li&gt;\\n&lt;li&gt;The ffn and k, q weights are the most numerous, and the least sensitive, so use the target quant for them (i.e. Q4)&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;I put this table together for my own edification a little while back ... it really helps me understand just how sophisticated the llama.cpp folks (and &lt;a href=\\"/u/ikawrakow\\"&gt;u/ikawrakow&lt;/a&gt;) have gotten.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/l6f3mjshalef1.png?width=713&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2dbd584a2c6dbb26bc9a9286373f5cfa0b9341ff\\"&gt;https://preview.redd.it/l6f3mjshalef1.png?width=713&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2dbd584a2c6dbb26bc9a9286373f5cfa0b9341ff&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;em&gt;Source: Huggingface&amp;#39;s summary view of Unsloth&amp;#39;s Qwen 3 4B quants (&lt;/em&gt;&lt;a href=\\"https://huggingface.co/unsloth/Qwen3-4B-GGUF?show_file_info=Qwen3-4B-Q4_K_S.gguf\\"&gt;&lt;em&gt;Q4_K_S&lt;/em&gt;&lt;/a&gt;&lt;em&gt;,&lt;/em&gt; &lt;a href=\\"https://huggingface.co/unsloth/Qwen3-4B-GGUF?show_file_info=Qwen3-4B-Q3_K_M.gguf\\"&gt;&lt;em&gt;Q4_K_M&lt;/em&gt;&lt;/a&gt;&lt;em&gt;,&lt;/em&gt; &lt;a href=\\"https://huggingface.co/unsloth/Qwen3-4B-GGUF?show_file_info=Qwen3-4B-UD-Q4_K_XL.gguf\\"&gt;&lt;em&gt;Q4_K_XL&lt;/em&gt;&lt;/a&gt;&lt;em&gt;) ... scroll down to blk0 etc.)&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;p&gt;All that is just about the last letters in the quant name. The first part of the name covers the fact that there are currently three broad families of GGUF quants:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Q4_0, Q4_1: These are the OG method for quantizing, and closest to what /u/&lt;a href=\\"https://www.reddit.com/user/BumbleSlob/\\"&gt;BumbleSlob&lt;/a&gt; described&lt;/li&gt;\\n&lt;li&gt;Q4_K_S Q4_K_M etc.: The K quants, which we all use, use two levels of quantization blocks (&amp;quot;superblocks&amp;quot;).  In order to improve efficiency, the scaling and offset factors are themselves stored in quantized blocks.&lt;/li&gt;\\n&lt;li&gt;IQ4_XS, IQ3_M etc.: The IQ quants, use a totally different (and cool) method: the weights are recorded as lookups into a table of mostly orthogonal 8D vectors&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;I&amp;#39;ve been meaning to do a more narrative / visual write up of this ... &lt;strong&gt;please let me know if you are interested&lt;/strong&gt;.&lt;/p&gt;\\n\\n&lt;p&gt;If you have time, Julia Turc&amp;#39;s got a &lt;a href=\\"https://www.youtube.com/watch?v=vW30o4U9BFE\\"&gt;phenomenal youtube&lt;/a&gt; that goes into this (and other related topics) in superb detail.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6tbhm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6tbhm/what_does_the_k_s_m_l_mean_behind_the/n4oblyi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753257187,"media_metadata":{"l6f3mjshalef1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":66,"x":108,"u":"https://preview.redd.it/l6f3mjshalef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7cbcf6b163916a41436be60cd87427123a38785e"},{"y":132,"x":216,"u":"https://preview.redd.it/l6f3mjshalef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b0ac1d45cb4974a7baa958b8ac78f3ff3478f04e"},{"y":195,"x":320,"u":"https://preview.redd.it/l6f3mjshalef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=09244f10a9c4d28c20e718bf2b5707f5e413976c"},{"y":391,"x":640,"u":"https://preview.redd.it/l6f3mjshalef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=101d91696a05b2ed327c229d64bc8fc5f68c5d49"}],"s":{"y":436,"x":713,"u":"https://preview.redd.it/l6f3mjshalef1.png?width=713&amp;format=png&amp;auto=webp&amp;s=2dbd584a2c6dbb26bc9a9286373f5cfa0b9341ff"},"id":"l6f3mjshalef1"}},"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":18}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ofdd8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"gofiend","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4oam8p","score":1,"author_fullname":"t2_2roqrw5l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"To add a bit more color based on my longer answer below - there are other quantization approaches that store FP8 weights, but GGUF does not store weights as FP8 (or even FP16 many times).\\n\\nEven a [Q8\\\\_0](https://huggingface.co/unsloth/Qwen3-4B-GGUF?show_file_info=Qwen3-4B-Q8_0.gguf) quant is a mix of F32 and blocks of Q8 weights (i.e. with scaling factors, offsets etc.). In general having the offsets and scaling factors make Q8 wieghts *more accurate* than just using FP8 (at the cost of being slightly larger).\\n\\nEven on your GPU, GGUFs are (typically?) not inferenced by explictly converting each weight to an FP8, BF16 number and then multiplying. Instead the kernels do fancy matrix operations to try and directly compute each layer's output using the weights in their quantized block form.\\n\\nOther inferencing engines (and their bespoke quantization approaches) might actually live in FP8, but that's mostly not how llama.cpp / GGUFs work.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4ofdd8","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;To add a bit more color based on my longer answer below - there are other quantization approaches that store FP8 weights, but GGUF does not store weights as FP8 (or even FP16 many times).&lt;/p&gt;\\n\\n&lt;p&gt;Even a &lt;a href=\\"https://huggingface.co/unsloth/Qwen3-4B-GGUF?show_file_info=Qwen3-4B-Q8_0.gguf\\"&gt;Q8_0&lt;/a&gt; quant is a mix of F32 and blocks of Q8 weights (i.e. with scaling factors, offsets etc.). In general having the offsets and scaling factors make Q8 wieghts &lt;em&gt;more accurate&lt;/em&gt; than just using FP8 (at the cost of being slightly larger).&lt;/p&gt;\\n\\n&lt;p&gt;Even on your GPU, GGUFs are (typically?) not inferenced by explictly converting each weight to an FP8, BF16 number and then multiplying. Instead the kernels do fancy matrix operations to try and directly compute each layer&amp;#39;s output using the weights in their quantized block form.&lt;/p&gt;\\n\\n&lt;p&gt;Other inferencing engines (and their bespoke quantization approaches) might actually live in FP8, but that&amp;#39;s mostly not how llama.cpp / GGUFs work.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6tbhm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6tbhm/what_does_the_k_s_m_l_mean_behind_the/n4ofdd8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753259364,"author_flair_text":null,"treatment_tags":[],"created_utc":1753259364,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4oam8p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Hurtcraft01","can_mod_post":false,"created_utc":1753256624,"send_replies":true,"parent_id":"t1_n4mjyc0","score":2,"author_fullname":"t2_jgegifux8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Very clear thank you, but i got an other question about this part: \\n\\n&gt;Each block has both the shift and the scaling factor. If I recall correctly, which I might not be, _S refers to using a FP8, _M refers to FP16, and _L refers to using FP32\\n\\nSo if i understood correctly for example if my original weights are stored in fp32, then i quantized them into q4, and if the model name end with _s that will mean the target format after the recovering will be fp8? Same reasoning for the _m and _L, but in this case why dont we just quantize the model into fp8 directly? So there is no compute with the scale and shift etc?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4oam8p","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Very clear thank you, but i got an other question about this part: &lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Each block has both the shift and the scaling factor. If I recall correctly, which I might not be, _S refers to using a FP8, _M refers to FP16, and _L refers to using FP32&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;So if i understood correctly for example if my original weights are stored in fp32, then i quantized them into q4, and if the model name end with _s that will mean the target format after the recovering will be fp8? Same reasoning for the _m and _L, but in this case why dont we just quantize the model into fp8 directly? So there is no compute with the scale and shift etc?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6tbhm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6tbhm/what_does_the_k_s_m_l_mean_behind_the/n4oam8p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753256624,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4q77he","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Agreeable-Prompt-666","can_mod_post":false,"created_utc":1753284069,"send_replies":true,"parent_id":"t1_n4mjyc0","score":1,"author_fullname":"t2_1l3z4stvkq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Did you end up building the engine, interfacing with gguf models?\\n\\n I'm doing this now, a tiny toy engine connection with gguf, but having hell of a time.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4q77he","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Did you end up building the engine, interfacing with gguf models?&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m doing this now, a tiny toy engine connection with gguf, but having hell of a time.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6tbhm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6tbhm/what_does_the_k_s_m_l_mean_behind_the/n4q77he/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753284069,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4mjyc0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"BumbleSlob","can_mod_post":false,"created_utc":1753230156,"send_replies":true,"parent_id":"t3_1m6tbhm","score":42,"author_fullname":"t2_1j7fhlcqkp","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"_K refers to using the K quantization mechanism which Iâ€™ll detail more particularly below. Â \\n\\nThe basic gist is _S, _M, _L are referring to the size of what is called the block â€œscaling factorâ€. A block is a collection of weights sitting in a tensor (i.e. a matrix) â€” but importantly, these weights are quantized and thus just representative of an integer (ex: Q4 has 4 bits per weight, so it can represent all values between 0-15).\\n\\nTo actually use the weights, they have to be unquantized (which is effectively uncompressing them). This is done by applying the formula\\n\\nFloat weight = Quantized Weight Int * Scaling Factor + Shift\\n\\nEach block has both the shift and the scaling factor. If I recall correctly, which I might not be, _S refers to using a FP8, _M refers to FP16, and _L refers to using FP32. So you are increasing the accuracy of the recovered weights, which may or may not make a difference depending on the particular model and quantization. Since, IIRC, a block is 256 weights, you donâ€™t really end up saving that much space when you do the math of how many bits you save overall.\\n\\nSo anyway, now that youâ€™ve uncompressed the weight, you can actually start using it as intended in the tensor (ie matrix).Â \\n\\nSource: me, I got deep into reading llama.cppâ€™s source code while writing my own inference engine and needing to understand how to decode GGUF files\\n\\nLast thing: for folks who always wondered why you donâ€™t actually get nice round numbers for â€œbits per weightâ€, this is the â€œwhyâ€.","edited":1753237234,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4mjyc0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;_K refers to using the K quantization mechanism which Iâ€™ll detail more particularly below. Â &lt;/p&gt;\\n\\n&lt;p&gt;The basic gist is _S, _M, _L are referring to the size of what is called the block â€œscaling factorâ€. A block is a collection of weights sitting in a tensor (i.e. a matrix) â€” but importantly, these weights are quantized and thus just representative of an integer (ex: Q4 has 4 bits per weight, so it can represent all values between 0-15).&lt;/p&gt;\\n\\n&lt;p&gt;To actually use the weights, they have to be unquantized (which is effectively uncompressing them). This is done by applying the formula&lt;/p&gt;\\n\\n&lt;p&gt;Float weight = Quantized Weight Int * Scaling Factor + Shift&lt;/p&gt;\\n\\n&lt;p&gt;Each block has both the shift and the scaling factor. If I recall correctly, which I might not be, _S refers to using a FP8, _M refers to FP16, and _L refers to using FP32. So you are increasing the accuracy of the recovered weights, which may or may not make a difference depending on the particular model and quantization. Since, IIRC, a block is 256 weights, you donâ€™t really end up saving that much space when you do the math of how many bits you save overall.&lt;/p&gt;\\n\\n&lt;p&gt;So anyway, now that youâ€™ve uncompressed the weight, you can actually start using it as intended in the tensor (ie matrix).Â &lt;/p&gt;\\n\\n&lt;p&gt;Source: me, I got deep into reading llama.cppâ€™s source code while writing my own inference engine and needing to understand how to decode GGUF files&lt;/p&gt;\\n\\n&lt;p&gt;Last thing: for folks who always wondered why you donâ€™t actually get nice round numbers for â€œbits per weightâ€, this is the â€œwhyâ€.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6tbhm/what_does_the_k_s_m_l_mean_behind_the/n4mjyc0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753230156,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6tbhm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":42}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ma56b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"NNN_Throwaway2","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4m9edn","score":3,"author_fullname":"t2_8rrihts9","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Models have multiple layers and components, the quantization doesn't need to be homogeneous.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4ma56b","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Models have multiple layers and components, the quantization doesn&amp;#39;t need to be homogeneous.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6tbhm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6tbhm/what_does_the_k_s_m_l_mean_behind_the/n4ma56b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753226914,"author_flair_text":null,"treatment_tags":[],"created_utc":1753226914,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"2ff18162-05ce-11ee-aa52-6a828e39b56c","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ogq6d","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RelicDerelict","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4m9edn","score":2,"author_fullname":"t2_c9l5cm1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Quantization strips weight of the real number R and assign integer. Dequantization which happening during inference won't assign the same R number to the weight as it began with. Most of the time that is not a problem. Sometimes tough it affects the precision too much and the result is not desired. Therefore some weights are more important than others and those weights are assigned to be quantized with higher quants (Q5, Q6 etc.). Thus S, M and L indicate how bigger the quant is in certain weights.\\n\\nImagine when MP3 algorithm has been invented. In early days it compressed all frequencies equally with constant bit rate. Later, variable bit rate has been invented to preserve some frequencies which suffered more than others from compression. Thus in certain song passages (more important or sensitive to compression ones) you have smaller compression (higher bit rate) than in others.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4ogq6d","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"Orca"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Quantization strips weight of the real number R and assign integer. Dequantization which happening during inference won&amp;#39;t assign the same R number to the weight as it began with. Most of the time that is not a problem. Sometimes tough it affects the precision too much and the result is not desired. Therefore some weights are more important than others and those weights are assigned to be quantized with higher quants (Q5, Q6 etc.). Thus S, M and L indicate how bigger the quant is in certain weights.&lt;/p&gt;\\n\\n&lt;p&gt;Imagine when MP3 algorithm has been invented. In early days it compressed all frequencies equally with constant bit rate. Later, variable bit rate has been invented to preserve some frequencies which suffered more than others from compression. Thus in certain song passages (more important or sensitive to compression ones) you have smaller compression (higher bit rate) than in others.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6tbhm","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6tbhm/what_does_the_k_s_m_l_mean_behind_the/n4ogq6d/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753260152,"author_flair_text":"Orca","treatment_tags":[],"created_utc":1753260152,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4m9edn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Hurtcraft01","can_mod_post":false,"created_utc":1753226676,"send_replies":true,"parent_id":"t1_n4m8pno","score":1,"author_fullname":"t2_jgegifux8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"But the precision isnt determined by the scale and the quantization ? Like if its q4 the weight will be stored on 4 bit independently of the letter s m or l right?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4m9edn","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;But the precision isnt determined by the scale and the quantization ? Like if its q4 the weight will be stored on 4 bit independently of the letter s m or l right?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6tbhm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6tbhm/what_does_the_k_s_m_l_mean_behind_the/n4m9edn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753226676,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4m8pno","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"NNN_Throwaway2","can_mod_post":false,"created_utc":1753226456,"send_replies":true,"parent_id":"t3_1m6tbhm","score":6,"author_fullname":"t2_8rrihts9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"small, medium, large referring to the precision and thus quality. The technical implications will vary depending on quant author and the exact methods they used.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4m8pno","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;small, medium, large referring to the precision and thus quality. The technical implications will vary depending on quant author and the exact methods they used.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6tbhm/what_does_the_k_s_m_l_mean_behind_the/n4m8pno/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753226456,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6tbhm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4obww6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"LambdaHominem","can_mod_post":false,"created_utc":1753257361,"send_replies":true,"parent_id":"t3_1m6tbhm","score":4,"author_fullname":"t2_3f0adwhq","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"https://github.com/iuliaturc/gguf-docs/blob/main/naming.md","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4obww6","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://github.com/iuliaturc/gguf-docs/blob/main/naming.md\\"&gt;https://github.com/iuliaturc/gguf-docs/blob/main/naming.md&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6tbhm/what_does_the_k_s_m_l_mean_behind_the/n4obww6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753257361,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m6tbhm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4p6evf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Hurtcraft01","can_mod_post":false,"created_utc":1753272680,"send_replies":true,"parent_id":"t1_n4opl1c","score":3,"author_fullname":"t2_jgegifux8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Because its interesting as hell :D","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4p6evf","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Because its interesting as hell :D&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6tbhm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6tbhm/what_does_the_k_s_m_l_mean_behind_the/n4p6evf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753272680,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n4opl1c","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Icy_Ideal_6994","can_mod_post":false,"created_utc":1753265149,"send_replies":true,"parent_id":"t3_1m6tbhm","score":2,"author_fullname":"t2_ri7l9czm2","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"how you guys even able to explain this in such details? what background or foundation one must acquire to comprehend all these.. iâ€™m damn jealous of the brain you gus having now","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4opl1c","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;how you guys even able to explain this in such details? what background or foundation one must acquire to comprehend all these.. iâ€™m damn jealous of the brain you gus having now&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6tbhm/what_does_the_k_s_m_l_mean_behind_the/n4opl1c/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753265149,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6tbhm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4muifa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CabinetNational3461","can_mod_post":false,"created_utc":1753233779,"send_replies":true,"parent_id":"t3_1m6tbhm","score":1,"author_fullname":"t2_sq5ja6nh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"https://youtu.be/vW30o4U9BFE?si=Es7Zknb_-CLKd8P7\\n\\nAnswer to your question","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4muifa","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://youtu.be/vW30o4U9BFE?si=Es7Zknb_-CLKd8P7\\"&gt;https://youtu.be/vW30o4U9BFE?si=Es7Zknb_-CLKd8P7&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Answer to your question&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6tbhm/what_does_the_k_s_m_l_mean_behind_the/n4muifa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753233779,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6tbhm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4m91s7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Toooooool","can_mod_post":false,"created_utc":1753226563,"send_replies":true,"parent_id":"t3_1m6tbhm","score":0,"author_fullname":"t2_8llornh4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The medium and small versions are pruned for lesser used weights in order to save a little extra memory without quantizing the model further.\\n\\nThink of it like this; You got the word ice cream, but you've also got words like sorbet, gelato, sundae, affogato.. these words are less used variations for the same thing so to save that little extra memory these lesser-used words are removed in favor of just relying on the more commonly used \\"ice cream\\"\\n\\n(super oversimplified but ye)\\n\\n  \\nedit: no wait i'm getting it mixed up with the parameter count i think ðŸ˜­","edited":1753227171,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4m91s7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The medium and small versions are pruned for lesser used weights in order to save a little extra memory without quantizing the model further.&lt;/p&gt;\\n\\n&lt;p&gt;Think of it like this; You got the word ice cream, but you&amp;#39;ve also got words like sorbet, gelato, sundae, affogato.. these words are less used variations for the same thing so to save that little extra memory these lesser-used words are removed in favor of just relying on the more commonly used &amp;quot;ice cream&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;(super oversimplified but ye)&lt;/p&gt;\\n\\n&lt;p&gt;edit: no wait i&amp;#39;m getting it mixed up with the parameter count i think ðŸ˜­&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6tbhm/what_does_the_k_s_m_l_mean_behind_the/n4m91s7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753226563,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6tbhm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}}]`),r=()=>e.jsx(t,{data:a});export{r as default};
