import{j as t}from"./index-Bqs-ekb2.js";import{R as e}from"./RedditPostRenderer-DUVdf0-i.js";import"./index-D52ORTDm.js";const n=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"In the past two days, we explored what positional embeddings are and even coded it.\\n\\nToday, we’re diving into a more advanced and powerful concept used in many state-of-the-art models: Rotary Positional Embeddings (RoPE).\\n\\n# Recap: Why Transformers Need Positional Embeddings\\n\\nTransformers process tokens in parallel, which makes them efficient, but it also means they don’t inherently know the order of the tokens.\\n\\nTo a transformer, these sentences look identical:\\n\\n* \\"The cat sat on the mat.\\"\\n* \\"The mat sat on the cat.\\"\\n\\nThat’s a problem. Order matters, especially in language.\\n\\nTo fix this, we add *positional embeddings* to inform the model about token positions.\\n\\n# Traditional Positional Embeddings\\n\\nTwo popular approaches:\\n\\n* **Learned positional embeddings** – Each position (1, 2, 3...) gets a trainable vector.\\n* **Sinusoidal embeddings** – Use sin/cos functions to generate fixed vectors per position.\\n\\nBut they have limitations:\\n\\n* Fixed or learned per-position (no flexibility)\\n* Poor generalization to longer sequences\\n* Don't integrate naturally with attention scores\\n\\n# What Is RoPE and Why Is It Better?\\n\\nRoPE was introduced in RoFormer (Su et al., 2021) and is now used in models like LLaMA and DeepSeek.\\n\\nInstead of adding a position vector, RoPE rotates token embeddings in space based on their position,  directly inside the attention mechanism (on query and key vectors).\\n\\nThis encodes relative position information in a more elegant and flexible way.\\n\\nFor each position, the token embedding is rotated by an angle proportional to that position.\\n\\nA simplified pseudocode:\\n\\n    for i in range(0, dim, 2):\\n        x1, x2 = x[i], x[i+1]\\n        angle = theta * position\\n        x[i]   = x1 * cos(angle) - x2 * sin(angle)\\n        x[i+1] = x1 * sin(angle) + x2 * cos(angle)\\n    \\n\\nThis allows attention to naturally reflect *how far apart* two tokens are, something traditional embeddings can’t do.\\n\\n# RoPE vs Traditional Positional Embeddings\\n\\n|Feature|Traditional Embeddings|Rotary Positional Embeddings (RoPE)|\\n|:-|:-|:-|\\n|Position Injected|Added to input embeddings|Applied inside attention mechanism|\\n|Absolute or Relative?|Absolute|Relative|\\n|Generalizes to Long Sequences?|Poor|Strong|\\n|Learnable Parameters?|Sometimes (if learned)|No|\\n|Adopted in SOTA models?|Less common now|Yes (LLaMA, DeepSeek)|\\n\\n# Why RoPE Is So Useful\\n\\n* **Encodes relative positions** directly in attention scores\\n* **No extra parameters** – it's deterministic\\n* **Handles long sequences** more gracefully\\n* **Simple implementation** using trigonometric rotation\\n\\n# Use in Real Models\\n\\n* **LLaMA (Meta):** Uses RoPE for better generalization and long-context performance.\\n* **DeepSeek:** Uses a decoupled RoPE mechanism where rotary embeddings are applied to separate query/key heads, enabling efficient long-context attention without bloating memory.\\n\\n# Final Thoughts\\n\\nRotary Positional Embeddings are an elegant solution to a core transformer weakness. If you’re building models for long documents, code, or stories, RoPE should be on your radar.\\n\\n# Coming Up Tomorrow\\n\\nWe'll implement RoPE in code and walk through how it’s used in the open-source  \\n[DeepSeek-Children-Stories-15M model](https://github.com/ideaweaver-ai/DeepSeek-Children-Stories-15M-model)\\n\\nFollow along,  we’re just getting started.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Day 8/50: Building a Small Language Model from Scratch – Rotary Positional Embeddings (RoPE)","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lq3tuu","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.93,"author_flair_background_color":null,"subreddit_type":"public","ups":34,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_8ht7a116","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":34,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1751481803,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;In the past two days, we explored what positional embeddings are and even coded it.&lt;/p&gt;\\n\\n&lt;p&gt;Today, we’re diving into a more advanced and powerful concept used in many state-of-the-art models: Rotary Positional Embeddings (RoPE).&lt;/p&gt;\\n\\n&lt;h1&gt;Recap: Why Transformers Need Positional Embeddings&lt;/h1&gt;\\n\\n&lt;p&gt;Transformers process tokens in parallel, which makes them efficient, but it also means they don’t inherently know the order of the tokens.&lt;/p&gt;\\n\\n&lt;p&gt;To a transformer, these sentences look identical:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&amp;quot;The cat sat on the mat.&amp;quot;&lt;/li&gt;\\n&lt;li&gt;&amp;quot;The mat sat on the cat.&amp;quot;&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;That’s a problem. Order matters, especially in language.&lt;/p&gt;\\n\\n&lt;p&gt;To fix this, we add &lt;em&gt;positional embeddings&lt;/em&gt; to inform the model about token positions.&lt;/p&gt;\\n\\n&lt;h1&gt;Traditional Positional Embeddings&lt;/h1&gt;\\n\\n&lt;p&gt;Two popular approaches:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;strong&gt;Learned positional embeddings&lt;/strong&gt; – Each position (1, 2, 3...) gets a trainable vector.&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Sinusoidal embeddings&lt;/strong&gt; – Use sin/cos functions to generate fixed vectors per position.&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;But they have limitations:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Fixed or learned per-position (no flexibility)&lt;/li&gt;\\n&lt;li&gt;Poor generalization to longer sequences&lt;/li&gt;\\n&lt;li&gt;Don&amp;#39;t integrate naturally with attention scores&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;h1&gt;What Is RoPE and Why Is It Better?&lt;/h1&gt;\\n\\n&lt;p&gt;RoPE was introduced in RoFormer (Su et al., 2021) and is now used in models like LLaMA and DeepSeek.&lt;/p&gt;\\n\\n&lt;p&gt;Instead of adding a position vector, RoPE rotates token embeddings in space based on their position,  directly inside the attention mechanism (on query and key vectors).&lt;/p&gt;\\n\\n&lt;p&gt;This encodes relative position information in a more elegant and flexible way.&lt;/p&gt;\\n\\n&lt;p&gt;For each position, the token embedding is rotated by an angle proportional to that position.&lt;/p&gt;\\n\\n&lt;p&gt;A simplified pseudocode:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;for i in range(0, dim, 2):\\n    x1, x2 = x[i], x[i+1]\\n    angle = theta * position\\n    x[i]   = x1 * cos(angle) - x2 * sin(angle)\\n    x[i+1] = x1 * sin(angle) + x2 * cos(angle)\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;This allows attention to naturally reflect &lt;em&gt;how far apart&lt;/em&gt; two tokens are, something traditional embeddings can’t do.&lt;/p&gt;\\n\\n&lt;h1&gt;RoPE vs Traditional Positional Embeddings&lt;/h1&gt;\\n\\n&lt;table&gt;&lt;thead&gt;\\n&lt;tr&gt;\\n&lt;th align=\\"left\\"&gt;Feature&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;Traditional Embeddings&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;Rotary Positional Embeddings (RoPE)&lt;/th&gt;\\n&lt;/tr&gt;\\n&lt;/thead&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;Position Injected&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;Added to input embeddings&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;Applied inside attention mechanism&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;Absolute or Relative?&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;Absolute&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;Relative&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;Generalizes to Long Sequences?&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;Poor&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;Strong&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;Learnable Parameters?&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;Sometimes (if learned)&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;No&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;Adopted in SOTA models?&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;Less common now&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;Yes (LLaMA, DeepSeek)&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;/tbody&gt;&lt;/table&gt;\\n\\n&lt;h1&gt;Why RoPE Is So Useful&lt;/h1&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;strong&gt;Encodes relative positions&lt;/strong&gt; directly in attention scores&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;No extra parameters&lt;/strong&gt; – it&amp;#39;s deterministic&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Handles long sequences&lt;/strong&gt; more gracefully&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Simple implementation&lt;/strong&gt; using trigonometric rotation&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;h1&gt;Use in Real Models&lt;/h1&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;strong&gt;LLaMA (Meta):&lt;/strong&gt; Uses RoPE for better generalization and long-context performance.&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;DeepSeek:&lt;/strong&gt; Uses a decoupled RoPE mechanism where rotary embeddings are applied to separate query/key heads, enabling efficient long-context attention without bloating memory.&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;h1&gt;Final Thoughts&lt;/h1&gt;\\n\\n&lt;p&gt;Rotary Positional Embeddings are an elegant solution to a core transformer weakness. If you’re building models for long documents, code, or stories, RoPE should be on your radar.&lt;/p&gt;\\n\\n&lt;h1&gt;Coming Up Tomorrow&lt;/h1&gt;\\n\\n&lt;p&gt;We&amp;#39;ll implement RoPE in code and walk through how it’s used in the open-source&lt;br/&gt;\\n&lt;a href=\\"https://github.com/ideaweaver-ai/DeepSeek-Children-Stories-15M-model\\"&gt;DeepSeek-Children-Stories-15M model&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Follow along,  we’re just getting started.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/yk95gNYDHwq6XIleth3O7MrgXkPOE1Hr9ZzOPDx-3XE.png?auto=webp&amp;s=aaf4557bdc852e74628b9a80be3094608111b971","width":1200,"height":600},"resolutions":[{"url":"https://external-preview.redd.it/yk95gNYDHwq6XIleth3O7MrgXkPOE1Hr9ZzOPDx-3XE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bdcabe31093a0a8eb032266a1279c0d7415a3fd7","width":108,"height":54},{"url":"https://external-preview.redd.it/yk95gNYDHwq6XIleth3O7MrgXkPOE1Hr9ZzOPDx-3XE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c7ccd3cc1cd68ed805c75d8bd83411f26087f2fc","width":216,"height":108},{"url":"https://external-preview.redd.it/yk95gNYDHwq6XIleth3O7MrgXkPOE1Hr9ZzOPDx-3XE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0f60d176e3abd7b1151e118c49209d7eec7a901b","width":320,"height":160},{"url":"https://external-preview.redd.it/yk95gNYDHwq6XIleth3O7MrgXkPOE1Hr9ZzOPDx-3XE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f2480d1a61ec09f5b4ed612393052d527f3c24bf","width":640,"height":320},{"url":"https://external-preview.redd.it/yk95gNYDHwq6XIleth3O7MrgXkPOE1Hr9ZzOPDx-3XE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=10f726da67cf6450203cd209e8dbc33a73334d48","width":960,"height":480},{"url":"https://external-preview.redd.it/yk95gNYDHwq6XIleth3O7MrgXkPOE1Hr9ZzOPDx-3XE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e04b5b7ddd416d60272d1ccfff6f07687e64d488","width":1080,"height":540}],"variants":{},"id":"yk95gNYDHwq6XIleth3O7MrgXkPOE1Hr9ZzOPDx-3XE"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lq3tuu","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Prashant-Lakhera","discussion_type":null,"num_comments":0,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lq3tuu/day_850_building_a_small_language_model_from/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lq3tuu/day_850_building_a_small_language_model_from/","subreddit_subscribers":494001,"created_utc":1751481803,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[],"before":null}}]`),i=()=>t.jsx(e,{data:n});export{i as default};
