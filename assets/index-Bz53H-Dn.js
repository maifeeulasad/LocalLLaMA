const e=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I pay for Claude to assist with coding / tool calling which I use for my job all day. I feel a strong urge to waste tons of money on a nice GPU, but realistically the models aren't as strong or even as cheap as the cloud models.\\n\\nI'm trying to self-reflect hard and in this moment of clarity, I see this as a distract of an expensive new toy I won't use much.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Please convince me not to get a GPU I don't need. Can any local LLM compare with cloud models?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lnsax9","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.75,"author_flair_background_color":null,"subreddit_type":"public","ups":37,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1bl579qtd6","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":37,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751238331,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I pay for Claude to assist with coding / tool calling which I use for my job all day. I feel a strong urge to waste tons of money on a nice GPU, but realistically the models aren&amp;#39;t as strong or even as cheap as the cloud models.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m trying to self-reflect hard and in this moment of clarity, I see this as a distract of an expensive new toy I won&amp;#39;t use much.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lnsax9","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"TumbleweedDeep825","discussion_type":null,"num_comments":119,"send_replies":false,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/","subreddit_subscribers":492928,"created_utc":1751238331,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0j5d25","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No_Elderberry_9132","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0i7iig","score":5,"author_fullname":"t2_byh4ysuoh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I run it on 4 GPU strap, not a big problem, you can even run the full model on CPU with 2-3 tokens generation per second, but takes 1TB of ram :)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0j5d25","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I run it on 4 GPU strap, not a big problem, you can even run the full model on CPU with 2-3 tokens generation per second, but takes 1TB of ram :)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0j5d25/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751262016,"author_flair_text":null,"treatment_tags":[],"created_utc":1751262016,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0k9a9c","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Karyo_Ten","can_mod_post":false,"created_utc":1751284408,"send_replies":true,"parent_id":"t1_n0k3nzb","score":2,"author_fullname":"t2_tbdqg","approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt;at acceptable* speeds (5t/s or so).\\n\\nI explicitly said that 10 tok/s is too slow.\\n\\n&gt;All the hostility and downvotes are completely uncalled for and a disgrace imo.\\n\\nMy first comment was:\\n\\n&gt; You mean upgrade from 2x 5090 to 8x5090 ?\\n\\nThen that guy went on a passive-aggressive crusade saying people here have no clue that when you solve 70b 235b is just around the corner.\\n\\nHe played the contempt and patronizing game, he gets called out on that.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n0k9a9c","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;at acceptable* speeds (5t/s or so).&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I explicitly said that 10 tok/s is too slow.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;All the hostility and downvotes are completely uncalled for and a disgrace imo.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;My first comment was:&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;You mean upgrade from 2x 5090 to 8x5090 ?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Then that guy went on a passive-aggressive crusade saying people here have no clue that when you solve 70b 235b is just around the corner.&lt;/p&gt;\\n\\n&lt;p&gt;He played the contempt and patronizing game, he gets called out on that.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0k9a9c/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751284408,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0k3nzb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Threatening-Silence-","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0jygee","score":4,"author_fullname":"t2_15wqsifdjf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You can run Qwen3-235B-A22B on 256gb system ram and a single 3090 at acceptable* speeds (5t/s or so).\\n\\nYou can put that together for under a grand. \\n\\nAll the hostility and downvotes are completely uncalled for and a disgrace imo.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n0k3nzb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can run Qwen3-235B-A22B on 256gb system ram and a single 3090 at acceptable* speeds (5t/s or so).&lt;/p&gt;\\n\\n&lt;p&gt;You can put that together for under a grand. &lt;/p&gt;\\n\\n&lt;p&gt;All the hostility and downvotes are completely uncalled for and a disgrace imo.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0k3nzb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751281852,"author_flair_text":null,"treatment_tags":[],"created_utc":1751281852,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n0jygee","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Karyo_Ten","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0jxt70","score":-1,"author_fullname":"t2_tbdqg","approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt;No, apparently local llama has no clue about running models locally or what a dense vs MOE model is. If you have a 70B your likely running it partially on GPU.\\n\\nStats that you pulled out of your ass?\\n\\nIt's more likely that people here running 70b are actually running them on 2x3090, rather than deal with less than 10 tok/s due to CPU.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0jygee","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;No, apparently local llama has no clue about running models locally or what a dense vs MOE model is. If you have a 70B your likely running it partially on GPU.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Stats that you pulled out of your ass?&lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s more likely that people here running 70b are actually running them on 2x3090, rather than deal with less than 10 tok/s due to CPU.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jygee/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751279182,"author_flair_text":null,"treatment_tags":[],"created_utc":1751279182,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0jxt70","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"YouDontSeemRight","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ixyvg","score":3,"author_fullname":"t2_1b7gjxtue9","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"No, apparently local llama has no clue about running models locally or what a dense vs MOE model is. If you have a 70B your likely running it partially on GPU. With a MOE you dump the static weights on GPU and the experts in CPU RAM so in the end 235B just requires cheaper CPU RAM to run.","edited":false,"author_flair_css_class":null,"name":"t1_n0jxt70","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No, apparently local llama has no clue about running models locally or what a dense vs MOE model is. If you have a 70B your likely running it partially on GPU. With a MOE you dump the static weights on GPU and the experts in CPU RAM so in the end 235B just requires cheaper CPU RAM to run.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jxt70/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751278824,"author_flair_text":null,"collapsed":false,"created_utc":1751278824,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ixyvg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Karyo_Ten","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0imk68","score":23,"author_fullname":"t2_tbdqg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You mean upgrade from 2x 5090 to 8x5090 ?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ixyvg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You mean upgrade from 2x 5090 to 8x5090 ?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0ixyvg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751258262,"author_flair_text":null,"treatment_tags":[],"created_utc":1751258262,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":23}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jyj4e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"night0x63","can_mod_post":false,"created_utc":1751279224,"send_replies":true,"parent_id":"t1_n0jewgf","score":1,"author_fullname":"t2_3h2irqtz","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks for the information! Super super helpful. I greatly appreciate!\\n\\n\\nI haven't run any MOE models. And only started with llama3.1, 3.2, 3.3.\\n\\n\\nI haven't tried llama4 because everyone and their mother have negative reviews for llama4.\\n\\n\\nYour post gives me the encouragement to try: mixtral 8x22b, llama4.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n0jyj4e","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for the information! Super super helpful. I greatly appreciate!&lt;/p&gt;\\n\\n&lt;p&gt;I haven&amp;#39;t run any MOE models. And only started with llama3.1, 3.2, 3.3.&lt;/p&gt;\\n\\n&lt;p&gt;I haven&amp;#39;t tried llama4 because everyone and their mother have negative reviews for llama4.&lt;/p&gt;\\n\\n&lt;p&gt;Your post gives me the encouragement to try: mixtral 8x22b, llama4.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jyj4e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751279224,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0jewgf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Serprotease","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0jechj","score":8,"author_fullname":"t2_odh3w8c","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It’s a MoE.  \\nThe token output speed is about twice that of a 70b q8 in my setup.  \\nFrom ~7 tps to ~15 tps.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n0jewgf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It’s a MoE.&lt;br/&gt;\\nThe token output speed is about twice that of a 70b q8 in my setup.&lt;br/&gt;\\nFrom ~7 tps to ~15 tps.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jewgf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751267385,"author_flair_text":null,"treatment_tags":[],"created_utc":1751267385,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jufj7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Threatening-Silence-","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0jechj","score":3,"author_fullname":"t2_15wqsifdjf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The full name of it is Qwen3 235b-A22b.\\n\\n22b active parameters, versus all 70b for the dense model.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n0jufj7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The full name of it is Qwen3 235b-A22b.&lt;/p&gt;\\n\\n&lt;p&gt;22b active parameters, versus all 70b for the dense model.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jufj7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751276894,"author_flair_text":null,"treatment_tags":[],"created_utc":1751276894,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0jechj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"night0x63","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0jc5pa","score":1,"author_fullname":"t2_3h2irqtz","approved_by":null,"mod_note":null,"all_awardings":[],"body":"How is 235b faster than 70b?\\n\\n\\nI would expect about 4x slower.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0jechj","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How is 235b faster than 70b?&lt;/p&gt;\\n\\n&lt;p&gt;I would expect about 4x slower.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jechj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751267058,"author_flair_text":null,"treatment_tags":[],"created_utc":1751267058,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jeho3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"night0x63","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0jc5pa","score":1,"author_fullname":"t2_3h2irqtz","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Re similar to llama3.1 to llama3.3 gap: that sounds like a big jump!","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0jeho3","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Re similar to llama3.1 to llama3.3 gap: that sounds like a big jump!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jeho3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751267143,"author_flair_text":null,"treatment_tags":[],"created_utc":1751267143,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0jc5pa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Serprotease","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ipfbf","score":3,"author_fullname":"t2_odh3w8c","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It feels somewhat similar to the gap llama3.1 and llama3.3. \\nIt’s better, for sure. But maybe not as large as the gap between llama2 and llama3 for example.  \\n\\nThe big thing is that if you can run it locally, it’s most likely faster than any 70b on the same hardware.","edited":false,"author_flair_css_class":null,"name":"t1_n0jc5pa","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It feels somewhat similar to the gap llama3.1 and llama3.3. \\nIt’s better, for sure. But maybe not as large as the gap between llama2 and llama3 for example.  &lt;/p&gt;\\n\\n&lt;p&gt;The big thing is that if you can run it locally, it’s most likely faster than any 70b on the same hardware.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jc5pa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751265799,"author_flair_text":null,"collapsed":false,"created_utc":1751265799,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ipfbf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"night0x63","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0imk68","score":6,"author_fullname":"t2_3h2irqtz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"How does qwen 235b compare to llama3.3:70b?\\n\\n\\nAre there other 70b?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ipfbf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How does qwen 235b compare to llama3.3:70b?&lt;/p&gt;\\n\\n&lt;p&gt;Are there other 70b?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0ipfbf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751254374,"author_flair_text":null,"treatment_tags":[],"created_utc":1751254374,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0k36rm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Threatening-Silence-","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0imk68","score":2,"author_fullname":"t2_15wqsifdjf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Not sure why you're getting downvoted, I think from a lot of ignorant people who don't know Qwen3-235B-**A22B**  is a MoE model with only 22B active parameters, very amenable to partial offloading to RAM. You can run the whole thing with 256GB system ram and one 3090.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0k36rm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not sure why you&amp;#39;re getting downvoted, I think from a lot of ignorant people who don&amp;#39;t know Qwen3-235B-&lt;strong&gt;A22B&lt;/strong&gt;  is a MoE model with only 22B active parameters, very amenable to partial offloading to RAM. You can run the whole thing with 256GB system ram and one 3090.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0k36rm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751281617,"author_flair_text":null,"treatment_tags":[],"created_utc":1751281617,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0je9rb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mxforest","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0imk68","score":1,"author_fullname":"t2_kenmq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Wow!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0je9rb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Wow!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0je9rb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751267014,"author_flair_text":null,"treatment_tags":[],"created_utc":1751267014,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0imk68","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"YouDontSeemRight","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0i7iig","score":-7,"author_fullname":"t2_1b7gjxtue9","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"If you can run a 70B, Qwen 235B isn't far off.\\n\\nEdit: wow look at these downvotes. Guess not many on local llama have ran 70b or 235B locally. 235B just required more system RAM. Static weights go to GPU.","edited":1751278660,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0imk68","is_submitter":false,"collapsed":true,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you can run a 70B, Qwen 235B isn&amp;#39;t far off.&lt;/p&gt;\\n\\n&lt;p&gt;Edit: wow look at these downvotes. Guess not many on local llama have ran 70b or 235B locally. 235B just required more system RAM. Static weights go to GPU.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0imk68/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751253159,"author_flair_text":null,"treatment_tags":[],"created_utc":1751253159,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-7}}],"before":null}},"user_reports":[],"saved":false,"id":"n0i7iig","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Edzomatic","can_mod_post":false,"created_utc":1751247257,"send_replies":true,"parent_id":"t1_n0hk6bd","score":34,"author_fullname":"t2_9ist3ny0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Come close means running deepseek which is most likely not gonna happen. But 70b at a decent quant can serve you well depending on the use case","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0i7iig","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Come close means running deepseek which is most likely not gonna happen. But 70b at a decent quant can serve you well depending on the use case&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0i7iig/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751247257,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":34}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0j1tw3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"BlueSwordM","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0irxsl","score":7,"author_fullname":"t2_qhqon","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You use the 3090 to greatly boost prompt processing and context.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0j1tw3","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You use the 3090 to greatly boost prompt processing and context.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0j1tw3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751260164,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1751260164,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0izxk6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0irxsl","score":11,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Prompt processing.GPU massively speeds it up, even if inference us 100% on cpu.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0izxk6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Prompt processing.GPU massively speeds it up, even if inference us 100% on cpu.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0izxk6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751259212,"author_flair_text":null,"treatment_tags":[],"created_utc":1751259212,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0iztan","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ixip8","score":7,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"No gpu = non existent PP speed","edited":false,"author_flair_css_class":null,"name":"t1_n0iztan","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No gpu = non existent PP speed&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0iztan/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751259154,"author_flair_text":null,"collapsed":false,"created_utc":1751259154,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0juvkr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Threatening-Silence-","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ixip8","score":0,"author_fullname":"t2_15wqsifdjf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Prompt processing speed on CPU only is about the same as inference token speed on CPU only. A couple of tokens per second. \\n\\nRoo Code's initial prompt seems to be around 12,000 tokens. \\n\\nMy 3090 can do 77t/s processing that. So I get my first reply in about two minutes (Deepseek R1 0528 - IQ3_XXS, ubatch 1024).\\n\\nThe same prompt on CPU only would take about 40 minutes until you get your first reply 💀","edited":false,"author_flair_css_class":null,"name":"t1_n0juvkr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Prompt processing speed on CPU only is about the same as inference token speed on CPU only. A couple of tokens per second. &lt;/p&gt;\\n\\n&lt;p&gt;Roo Code&amp;#39;s initial prompt seems to be around 12,000 tokens. &lt;/p&gt;\\n\\n&lt;p&gt;My 3090 can do 77t/s processing that. So I get my first reply in about two minutes (Deepseek R1 0528 - IQ3_XXS, ubatch 1024).&lt;/p&gt;\\n\\n&lt;p&gt;The same prompt on CPU only would take about 40 minutes until you get your first reply 💀&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0juvkr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751277153,"author_flair_text":null,"collapsed":false,"created_utc":1751277153,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ixip8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ambitious_Subject108","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0irxsl","score":0,"author_fullname":"t2_iaffzj2q","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It'll hurt more than it helps. CPU to memory latency is an order of magnitude faster than CPU to GPU.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ixip8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;ll hurt more than it helps. CPU to memory latency is an order of magnitude faster than CPU to GPU.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0ixip8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751258044,"author_flair_text":null,"treatment_tags":[],"created_utc":1751258044,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n0irxsl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"gigaflops_","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0iivh7","score":6,"author_fullname":"t2_1t2n2s9f","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Question: what good would the 3090 do in this build? My understanding is that since the vast majority of the model (~85% of it) is stored on system RAM, it'll be computer almost entirely on the CPU. Unless of course you configure it to run entirely on GPU and transfer hundreds of gigabytes across the pcie lane every token, which in my experience is slower than CPU inference.\\n\\nI'm trying to understand this stuff better, so lmk where my thinking is wrong.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0irxsl","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Question: what good would the 3090 do in this build? My understanding is that since the vast majority of the model (~85% of it) is stored on system RAM, it&amp;#39;ll be computer almost entirely on the CPU. Unless of course you configure it to run entirely on GPU and transfer hundreds of gigabytes across the pcie lane every token, which in my experience is slower than CPU inference.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m trying to understand this stuff better, so lmk where my thinking is wrong.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0irxsl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751255472,"author_flair_text":null,"treatment_tags":[],"created_utc":1751255472,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jft2p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0jfhea","score":1,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"~$300 for 256 and ~$500 for 512","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jft2p","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;~$300 for 256 and ~$500 for 512&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jft2p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751267921,"author_flair_text":null,"treatment_tags":[],"created_utc":1751267921,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0kdhi5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RickyRickC137","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0jfhea","score":1,"author_fullname":"t2_mhdt7ir5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You'll get repeated customer discounts","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0kdhi5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You&amp;#39;ll get repeated customer discounts&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0kdhi5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751286146,"author_flair_text":null,"treatment_tags":[],"created_utc":1751286146,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0jfhea","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"forhorglingrads","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0iivh7","score":2,"author_fullname":"t2_4pqxe","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"why is the 2nd half of the ram $100 cheaper than the first half","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0jfhea","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;why is the 2nd half of the ram $100 cheaper than the first half&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jfhea/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751267728,"author_flair_text":null,"treatment_tags":[],"created_utc":1751267728,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0iivh7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"created_utc":1751251652,"send_replies":true,"parent_id":"t1_n0hk6bd","score":-3,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"At full speed? Yeah, you can’t come close to Claude at home.\\n\\nSlowly? You can buy an old workstation on ebay for $300, spend $300 to drop in 256gb of RAM, put a 3090 in, and run Deepseek R1 0528 quantized to Q2 pretty easily. You’ll just get only 5 tokens/sec. \\n\\nIf Q2 is too small for you, spend $200 more and you’ll get 512gb RAM, and that lets you run Deepseek at Q5 which is pretty close to full quality.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0iivh7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;At full speed? Yeah, you can’t come close to Claude at home.&lt;/p&gt;\\n\\n&lt;p&gt;Slowly? You can buy an old workstation on ebay for $300, spend $300 to drop in 256gb of RAM, put a 3090 in, and run Deepseek R1 0528 quantized to Q2 pretty easily. You’ll just get only 5 tokens/sec. &lt;/p&gt;\\n\\n&lt;p&gt;If Q2 is too small for you, spend $200 more and you’ll get 512gb RAM, and that lets you run Deepseek at Q5 which is pretty close to full quality.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0iivh7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751251652,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hk6bd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Current-Ticket4214","can_mod_post":false,"created_utc":1751238618,"send_replies":true,"parent_id":"t3_1lnsax9","score":107,"author_fullname":"t2_6yvd3nyy","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You won’t match Claude with a local LLM running on high end consumer hardware. You need a commercial grade server with a ton of compute to even come close.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hk6bd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You won’t match Claude with a local LLM running on high end consumer hardware. You need a commercial grade server with a ton of compute to even come close.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hk6bd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751238618,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":107}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jayyn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Effect3325","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0iai45","score":3,"author_fullname":"t2_kkv4u2t2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I use serverless Runpod to run my own model. Upload your model to HuggingFace and then you can run your model from Runpod serverless taking the model from the HuggingFace. It costs you per second of model inference (I use flex worker)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0jayyn","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I use serverless Runpod to run my own model. Upload your model to HuggingFace and then you can run your model from Runpod serverless taking the model from the HuggingFace. It costs you per second of model inference (I use flex worker)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jayyn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751265114,"author_flair_text":null,"treatment_tags":[],"created_utc":1751265114,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0iyasl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Karyo_Ten","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0iigj4","score":5,"author_fullname":"t2_tbdqg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt;\\"Provably secure\\" depends on your threat model but it's also a well solved problem.\\n\\nI would be out of a job if that was the case (cryptography).\\n\\nBut yeah, if you're asking, it's good enough for you.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0iyasl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;&amp;quot;Provably secure&amp;quot; depends on your threat model but it&amp;#39;s also a well solved problem.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I would be out of a job if that was the case (cryptography).&lt;/p&gt;\\n\\n&lt;p&gt;But yeah, if you&amp;#39;re asking, it&amp;#39;s good enough for you.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0iyasl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751258423,"author_flair_text":null,"treatment_tags":[],"created_utc":1751258423,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n0iigj4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Steve_Streza","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0iai45","score":4,"author_fullname":"t2_35vw6osy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Probably not too long. A weekend if you know what you're doing, a few days or a week or so if you aren't familiar with the... quirks of AWS or GCM or Azure or whatever.\\n\\nOnce you get the tools set up, it's really not much different from setting up a virtual machine. You'd just SSH into it and set up your software and model. Shut it down when you're not using it (important! maybe set up a \\"shut down automatically after inactivity\\" script), and it'll probably only take a couple minutes (maybe a little longer for a model of that size) to boot up again.\\n\\n\\"Provably secure\\" depends on your threat model but it's also a well solved problem.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0iigj4","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Probably not too long. A weekend if you know what you&amp;#39;re doing, a few days or a week or so if you aren&amp;#39;t familiar with the... quirks of AWS or GCM or Azure or whatever.&lt;/p&gt;\\n\\n&lt;p&gt;Once you get the tools set up, it&amp;#39;s really not much different from setting up a virtual machine. You&amp;#39;d just SSH into it and set up your software and model. Shut it down when you&amp;#39;re not using it (important! maybe set up a &amp;quot;shut down automatically after inactivity&amp;quot; script), and it&amp;#39;ll probably only take a couple minutes (maybe a little longer for a model of that size) to boot up again.&lt;/p&gt;\\n\\n&lt;p&gt;&amp;quot;Provably secure&amp;quot; depends on your threat model but it&amp;#39;s also a well solved problem.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0iigj4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751251487,"author_flair_text":null,"treatment_tags":[],"created_utc":1751251487,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jouun","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Daemontatox","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0iai45","score":1,"author_fullname":"t2_1rm9syq1nb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Depends really on how complicated you want it to be , \\n\\nFor example , aws and azure ? Couple of minutes , the instances come preinstalled with torch , so its a matter of getting vllm /lightingai or torch serve.\\n\\nYou can get cheaper gpus on vastai or lambda but they might take abit longer to setup.\\n\\nAll and all its not too long of  a setup , so i will probably say couple of hours if its bare metal to a weekend depending on how experienced you are.\\n\\nNote: there are alot of guides that walk you through it , so dont worry about getting lost.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0jouun","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Depends really on how complicated you want it to be , &lt;/p&gt;\\n\\n&lt;p&gt;For example , aws and azure ? Couple of minutes , the instances come preinstalled with torch , so its a matter of getting vllm /lightingai or torch serve.&lt;/p&gt;\\n\\n&lt;p&gt;You can get cheaper gpus on vastai or lambda but they might take abit longer to setup.&lt;/p&gt;\\n\\n&lt;p&gt;All and all its not too long of  a setup , so i will probably say couple of hours if its bare metal to a weekend depending on how experienced you are.&lt;/p&gt;\\n\\n&lt;p&gt;Note: there are alot of guides that walk you through it , so dont worry about getting lost.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jouun/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751273503,"author_flair_text":null,"treatment_tags":[],"created_utc":1751273503,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0iai45","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ElectronSpiderwort","can_mod_post":false,"created_utc":1751248392,"send_replies":true,"parent_id":"t1_n0hocvs","score":10,"author_fullname":"t2_mxbu5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm curious what is the time investment for this; I know GPUs can be had for like $1.50/gpu/hour, but I don't know how long it takes to spin up a container or whatever that has the right software and is provably secure etc. etc..  How long does it take, starting from nothing, to get to hosting a quite large model (say Qwen 235b for instance) on a rented GPU cluster?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0iai45","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m curious what is the time investment for this; I know GPUs can be had for like $1.50/gpu/hour, but I don&amp;#39;t know how long it takes to spin up a container or whatever that has the right software and is provably secure etc. etc..  How long does it take, starting from nothing, to get to hosting a quite large model (say Qwen 235b for instance) on a rented GPU cluster?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0iai45/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751248392,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0joeje","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Daemontatox","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0imbmv","score":7,"author_fullname":"t2_1rm9syq1nb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Well openrouter isn't really an accurate representation to the gpu , codts and model setup , they wont know how much it costs , how hard it is , which models they will be able to realisticaly host locally .\\n\\nTheir goal is getting a gpu , not just making calls to free models.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0joeje","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Well openrouter isn&amp;#39;t really an accurate representation to the gpu , codts and model setup , they wont know how much it costs , how hard it is , which models they will be able to realisticaly host locally .&lt;/p&gt;\\n\\n&lt;p&gt;Their goal is getting a gpu , not just making calls to free models.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0joeje/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751273225,"author_flair_text":null,"treatment_tags":[],"created_utc":1751273225,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}}],"before":null}},"user_reports":[],"saved":false,"id":"n0imbmv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"TheRealGentlefox","can_mod_post":false,"created_utc":1751253061,"send_replies":true,"parent_id":"t1_n0hocvs","score":4,"author_fullname":"t2_f471r","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"No reason to rent a GPU, they can set up OpenRouter in literally 5 minutes to try out every model known to man. No configuring needed, just run them all in chat completion mode.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0imbmv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No reason to rent a GPU, they can set up OpenRouter in literally 5 minutes to try out every model known to man. No configuring needed, just run them all in chat completion mode.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0imbmv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751253061,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hocvs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Daemontatox","can_mod_post":false,"created_utc":1751240112,"send_replies":true,"parent_id":"t3_1lnsax9","score":50,"author_fullname":"t2_1rm9syq1nb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"How about renting a gpu online and trying out the models you think will do great in use case before committing to buying a gpu.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hocvs","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How about renting a gpu online and trying out the models you think will do great in use case before committing to buying a gpu.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hocvs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751240112,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":50}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0i96k6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Secure_Reflection409","can_mod_post":false,"created_utc":1751247890,"send_replies":true,"parent_id":"t3_1lnsax9","score":27,"author_fullname":"t2_by77ogdhr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You won't match the cloud for speed, cost or competence.\\n\\n\\nThe cloud will never match local for privacy.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0i96k6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You won&amp;#39;t match the cloud for speed, cost or competence.&lt;/p&gt;\\n\\n&lt;p&gt;The cloud will never match local for privacy.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0i96k6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751247890,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":27}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hrkvg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Terminator857","can_mod_post":false,"created_utc":1751241278,"send_replies":true,"parent_id":"t3_1lnsax9","score":15,"author_fullname":"t2_m40tjcn","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Maybe next year it won't cost too much to run deepseek at home.  As soon as the hardware costs drop to under $5K for reasonable speed I'm buying.  \\n\\nMaybe software improvements like diffusion for LLMs will make this more probable.    [https://www.reddit.com/r/LocalLLM/comments/1ljbajp/diffusion\\\\_language\\\\_models\\\\_will\\\\_cut\\\\_the\\\\_cost\\\\_of/](https://www.reddit.com/r/LocalLLM/comments/1ljbajp/diffusion_language_models_will_cut_the_cost_of/)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hrkvg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Maybe next year it won&amp;#39;t cost too much to run deepseek at home.  As soon as the hardware costs drop to under $5K for reasonable speed I&amp;#39;m buying.  &lt;/p&gt;\\n\\n&lt;p&gt;Maybe software improvements like diffusion for LLMs will make this more probable.    &lt;a href=\\"https://www.reddit.com/r/LocalLLM/comments/1ljbajp/diffusion_language_models_will_cut_the_cost_of/\\"&gt;https://www.reddit.com/r/LocalLLM/comments/1ljbajp/diffusion_language_models_will_cut_the_cost_of/&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hrkvg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751241278,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":15}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0j0s90","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Karyo_Ten","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0j0d9v","score":2,"author_fullname":"t2_tbdqg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"https://www.youtube.com/watch?v=hSoCmAoIMOU","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0j0s90","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://www.youtube.com/watch?v=hSoCmAoIMOU\\"&gt;https://www.youtube.com/watch?v=hSoCmAoIMOU&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0j0s90/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751259635,"author_flair_text":null,"treatment_tags":[],"created_utc":1751259635,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0j0d9v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"moncallikta","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0iygcb","score":3,"author_fullname":"t2_15ju4l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Can it run Doom?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0j0d9v","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Can it run Doom?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0j0d9v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751259427,"author_flair_text":null,"treatment_tags":[],"created_utc":1751259427,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0iygcb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Karyo_Ten","can_mod_post":false,"created_utc":1751258496,"send_replies":true,"parent_id":"t1_n0htp47","score":3,"author_fullname":"t2_tbdqg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"If we follow your reasoning, OP should buy a dam instead.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0iygcb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If we follow your reasoning, OP should buy a dam instead.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0iygcb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751258496,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0htp47","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FunnyAsparagus1253","can_mod_post":false,"created_utc":1751242066,"send_replies":true,"parent_id":"t3_1lnsax9","score":27,"author_fullname":"t2_i6c8tay3w","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Dude just buy a 3090 for fun. Then when the apocalypse hits and nobody has internet any more you’ll still have someone smart to chat to. Run quantized mistral small or whatever - sure it’s not as smart or good at coding as claude but so what? :)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0htp47","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Dude just buy a 3090 for fun. Then when the apocalypse hits and nobody has internet any more you’ll still have someone smart to chat to. Run quantized mistral small or whatever - sure it’s not as smart or good at coding as claude but so what? :)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0htp47/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751242066,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":27}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0kbjhn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"photodesignch","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0k7vke","score":1,"author_fullname":"t2_dnfi3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Cline is multi-agents.  Same with continue. But performance is pretty bad.  If I need code analysis I can probably run a deepseek and wait for a couple of hours for it to reply.  Otherwise, it’s impossible to get the Claude quality to be honest.  To code a “hello world” any SLM can do it. But to do real coding, I only found Claude and Gemini can do the task now.  Copilot is okay, but far from accurate.\\n\\nAs you code a lot, the answer is pretty obvious.  local LLMs can’t match remote.\\n\\nHowever! One I suggest to do is running SLM on autocomplete. That you can do easily and it’s fairly accurately.  Just not writing code for you and give you runnable completed answers.\\n\\nSLM can do code analysis decently though.\\n\\nWhat remote LLM can do is to go through the whole project and write up the design architecture, api swagger fairly easily.  Thats super helpful for documentation tasks and remote LLM do it nicely and quick.  Local LLM might take ages to do it and the result is somewhat 50/50.\\n\\nWhat local SLM does for me is to consolidate and give fast general informations like summarize what a repository of code does.  I often use fabric ai with SLM to summarize code for me into a few sentences or bullet points.  Great for presentation PPT.","edited":false,"author_flair_css_class":null,"name":"t1_n0kbjhn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Cline is multi-agents.  Same with continue. But performance is pretty bad.  If I need code analysis I can probably run a deepseek and wait for a couple of hours for it to reply.  Otherwise, it’s impossible to get the Claude quality to be honest.  To code a “hello world” any SLM can do it. But to do real coding, I only found Claude and Gemini can do the task now.  Copilot is okay, but far from accurate.&lt;/p&gt;\\n\\n&lt;p&gt;As you code a lot, the answer is pretty obvious.  local LLMs can’t match remote.&lt;/p&gt;\\n\\n&lt;p&gt;However! One I suggest to do is running SLM on autocomplete. That you can do easily and it’s fairly accurately.  Just not writing code for you and give you runnable completed answers.&lt;/p&gt;\\n\\n&lt;p&gt;SLM can do code analysis decently though.&lt;/p&gt;\\n\\n&lt;p&gt;What remote LLM can do is to go through the whole project and write up the design architecture, api swagger fairly easily.  Thats super helpful for documentation tasks and remote LLM do it nicely and quick.  Local LLM might take ages to do it and the result is somewhat 50/50.&lt;/p&gt;\\n\\n&lt;p&gt;What local SLM does for me is to consolidate and give fast general informations like summarize what a repository of code does.  I often use fabric ai with SLM to summarize code for me into a few sentences or bullet points.  Great for presentation PPT.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0kbjhn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751285358,"author_flair_text":null,"collapsed":false,"created_utc":1751285358,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0k7vke","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Subject_Ratio6842","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0jfcs8","score":1,"author_fullname":"t2_y8qz0s2mk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I use cline with a local llm , and they can modify and create documents using the mcp.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0k7vke","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I use cline with a local llm , and they can modify and create documents using the mcp.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0k7vke/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751283792,"author_flair_text":null,"treatment_tags":[],"created_utc":1751283792,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0jfcs8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"photodesignch","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0i0yrd","score":4,"author_fullname":"t2_dnfi3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"LLM do not directly modify your file system.  Claude does.  When it presents a code, one agent goes into your file I/O to do file open and file save. When editing current file, there is one agent does file diff so it automatically marks diff code and you can apply over current file. Etc etc… it’s a multi-agent task from the cloud services under the hood.\\n\\nNormally LLM just takes input and report back output to you. It can’t interact with your system with separate agents with tools.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0jfcs8","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;LLM do not directly modify your file system.  Claude does.  When it presents a code, one agent goes into your file I/O to do file open and file save. When editing current file, there is one agent does file diff so it automatically marks diff code and you can apply over current file. Etc etc… it’s a multi-agent task from the cloud services under the hood.&lt;/p&gt;\\n\\n&lt;p&gt;Normally LLM just takes input and report back output to you. It can’t interact with your system with separate agents with tools.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jfcs8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751267652,"author_flair_text":null,"treatment_tags":[],"created_utc":1751267652,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n0i0yrd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"pineh2","can_mod_post":false,"created_utc":1751244798,"send_replies":true,"parent_id":"t1_n0hk8tr","score":-7,"author_fullname":"t2_fifter7i","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This is awesome. \\n\\nCan you tell us about Claude being  a a multi agents system behind it?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0i0yrd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is awesome. &lt;/p&gt;\\n\\n&lt;p&gt;Can you tell us about Claude being  a a multi agents system behind it?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0i0yrd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751244798,"author_flair_text":null,"treatment_tags":[],"collapsed":true,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0iu05c","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"hapliniste","can_mod_post":false,"created_utc":1751256404,"send_replies":true,"parent_id":"t1_n0hk8tr","score":-7,"author_fullname":"t2_fc7rd","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Who upvote this? 🤨","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0iu05c","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Who upvote this? 🤨&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0iu05c/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751256404,"author_flair_text":null,"treatment_tags":[],"collapsed":true,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-7}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hk8tr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"photodesignch","can_mod_post":false,"created_utc":1751238642,"send_replies":true,"parent_id":"t3_1lnsax9","score":35,"author_fullname":"t2_dnfi3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Don’t. You won’t get the same performance locally. Invest gpu only if you need to run normal LLM.  Claude is not just LLM itself. It’s multi agents system behind it which isn’t gonna to be replaced locally easily.  There is a point why cloud services are popular.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hk8tr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Don’t. You won’t get the same performance locally. Invest gpu only if you need to run normal LLM.  Claude is not just LLM itself. It’s multi agents system behind it which isn’t gonna to be replaced locally easily.  There is a point why cloud services are popular.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hk8tr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751238642,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":35}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"body":"Which M processor do you have and what is the generation speed?","subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0k9fyz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BumbleSlob","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0j2cbe","score":1,"author_fullname":"t2_1j7fhlcqkp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Qwen3 30B MoE: around 50tps\\n\\nQwen3 32B: probably closer to 17Tps\\n\\nFor me, M2 Max 64Gb I bought new like 2 years ago for ~$4k","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0k9fyz","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen3 30B MoE: around 50tps&lt;/p&gt;\\n\\n&lt;p&gt;Qwen3 32B: probably closer to 17Tps&lt;/p&gt;\\n\\n&lt;p&gt;For me, M2 Max 64Gb I bought new like 2 years ago for ~$4k&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0k9fyz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751284476,"author_flair_text":null,"treatment_tags":[],"created_utc":1751284476,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0j2cbe","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"edeltoaster","can_mod_post":false,"created_utc":1751260426,"send_replies":true,"parent_id":"t1_n0hl8fe","score":1,"author_fullname":"t2_c7rlb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"author_cakeday":true,"edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0j2cbe","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Which M processor do you have and what is the generation speed?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0j2cbe/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751260426,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hl8fe","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"mantafloppy","can_mod_post":false,"created_utc":1751238998,"send_replies":true,"parent_id":"t3_1lnsax9","score":16,"author_fullname":"t2_co2hf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I bought a Mac with 64gb ram/VRAM to play with LLM.\\n\\n\\nI can run 70b q4k_m guff model.\\n\\n\\n\\nMy main use is coding.\\n\\n\\nI pay for Claude for when i need serious help coding.\\n\\n\\nThe only \\"local\\" model that comes close to closed source model are the 600b+ model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hl8fe","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I bought a Mac with 64gb ram/VRAM to play with LLM.&lt;/p&gt;\\n\\n&lt;p&gt;I can run 70b q4k_m guff model.&lt;/p&gt;\\n\\n&lt;p&gt;My main use is coding.&lt;/p&gt;\\n\\n&lt;p&gt;I pay for Claude for when i need serious help coding.&lt;/p&gt;\\n\\n&lt;p&gt;The only &amp;quot;local&amp;quot; model that comes close to closed source model are the 600b+ model.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hl8fe/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751238998,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":16}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hs1li","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"GustaveVonZarovich","can_mod_post":false,"created_utc":1751241450,"send_replies":true,"parent_id":"t3_1lnsax9","score":8,"author_fullname":"t2_ftfadtol","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"size issue: cloud llms are huge, near 1t, local llms are generally around 32b ~ 70b, 14b makes more mistakes compared to 32b, and 32b makes more mistakes compared to 70b, you get the picture,\\n\\nspeed issue: local llms are like snails compared to cloud llms, if it's ok to wait 4 hrs for a response, go local, but remember that a cloud llm can generate that response in 4 minutes,\\n\\nfor a local llm qwen3 is good, but demands a good setup with 256gb ram/vram and that's expensive, \\n\\nand it's always too early to make a final decision, everyday everything is changing rapidly, tomorrow we might be lucky enough to run a great llm with only 32gb ram with no gpu","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hs1li","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;size issue: cloud llms are huge, near 1t, local llms are generally around 32b ~ 70b, 14b makes more mistakes compared to 32b, and 32b makes more mistakes compared to 70b, you get the picture,&lt;/p&gt;\\n\\n&lt;p&gt;speed issue: local llms are like snails compared to cloud llms, if it&amp;#39;s ok to wait 4 hrs for a response, go local, but remember that a cloud llm can generate that response in 4 minutes,&lt;/p&gt;\\n\\n&lt;p&gt;for a local llm qwen3 is good, but demands a good setup with 256gb ram/vram and that&amp;#39;s expensive, &lt;/p&gt;\\n\\n&lt;p&gt;and it&amp;#39;s always too early to make a final decision, everyday everything is changing rapidly, tomorrow we might be lucky enough to run a great llm with only 32gb ram with no gpu&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hs1li/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751241450,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jzeru","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"stoppableDissolution","can_mod_post":false,"created_utc":1751279703,"send_replies":true,"parent_id":"t1_n0hytpp","score":1,"author_fullname":"t2_1n0su21k4z","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Not quite that high yet, but #metoo, lol. Zero regret tho. My only issue is that I turned my workstation into AI rig, so its inconvenient to keep the model loaded 24/7, gotta move it into separate server.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jzeru","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not quite that high yet, but #metoo, lol. Zero regret tho. My only issue is that I turned my workstation into AI rig, so its inconvenient to keep the model loaded 24/7, gotta move it into separate server.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jzeru/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751279703,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hytpp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"grim-432","can_mod_post":false,"created_utc":1751243998,"send_replies":true,"parent_id":"t3_1lnsax9","score":15,"author_fullname":"t2_l3u9f39ym","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Spent $10,000 on my local llm rig.\\n\\nStill use cloud every day.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hytpp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Spent $10,000 on my local llm rig.&lt;/p&gt;\\n\\n&lt;p&gt;Still use cloud every day.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hytpp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751243998,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":15}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hzg2v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"CMDR-Bugsbunny","can_mod_post":false,"created_utc":1751244230,"send_replies":true,"parent_id":"t1_n0hu8ri","score":17,"author_fullname":"t2_lj6an","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah, it really depends on the use case. Anyone that says that model x is better than model y without context is clueless.\\n\\nQwen 32b (and others) with Context7 can be run locally and would have very solid coding capabilities. If I was using Claude for coding this would come close. \\n\\nIf you wanted writing capabilities, then there are many choices Mixtral, Gemma 3, Llama and combined with RAG can create a serious domain specific workspace. You can easily use OpenRouter suggested by No-Refrigerator-1672 and use a vector database and n8n for RAG.\\n\\nDon't want to figure out how to use n8n, vector database, etc. Then get AnythingLLM and drop a bunch of PDFs and viola - a smarter local LLM with specific domain knowledge.\\n\\nI dropped 50,000+ pages of PDF on a business topic with Gemma 3 27b QAT and it performs near ChatGPT/Claude/Gemini on this domain. I used AnythingLLM with LM Studio and used over 1GB of PDFs. It took about 10-15 minutes and I was prompting against a vast library.\\n\\nSo it really depends.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hzg2v","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, it really depends on the use case. Anyone that says that model x is better than model y without context is clueless.&lt;/p&gt;\\n\\n&lt;p&gt;Qwen 32b (and others) with Context7 can be run locally and would have very solid coding capabilities. If I was using Claude for coding this would come close. &lt;/p&gt;\\n\\n&lt;p&gt;If you wanted writing capabilities, then there are many choices Mixtral, Gemma 3, Llama and combined with RAG can create a serious domain specific workspace. You can easily use OpenRouter suggested by No-Refrigerator-1672 and use a vector database and n8n for RAG.&lt;/p&gt;\\n\\n&lt;p&gt;Don&amp;#39;t want to figure out how to use n8n, vector database, etc. Then get AnythingLLM and drop a bunch of PDFs and viola - a smarter local LLM with specific domain knowledge.&lt;/p&gt;\\n\\n&lt;p&gt;I dropped 50,000+ pages of PDF on a business topic with Gemma 3 27b QAT and it performs near ChatGPT/Claude/Gemini on this domain. I used AnythingLLM with LM Studio and used over 1GB of PDFs. It took about 10-15 minutes and I was prompting against a vast library.&lt;/p&gt;\\n\\n&lt;p&gt;So it really depends.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hzg2v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751244230,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":17}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hu8ri","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No-Refrigerator-1672","can_mod_post":false,"created_utc":1751242269,"send_replies":true,"parent_id":"t3_1lnsax9","score":17,"author_fullname":"t2_baavelp5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Whoah, so much comments and none gave you the correct answer. Most of the locally-runnable models are awailable on OpenRouter; go ahead, spend 10 bucks on credits, try them out and determine it for yourself. Other commenters keep telling you how non-local models are smarter; and while it is for the flagship ones, it's actually possible that your particular tasks don't require this flagship intelligence, so you really should to your own tests instead of relying on crowd instinct. And should you find out that locals actually can do your tasks, then you can just divide the price of the GPU by your monthly credit spending and use that number to get objective judgement what's actually better for you.","edited":1751242471,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hu8ri","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Whoah, so much comments and none gave you the correct answer. Most of the locally-runnable models are awailable on OpenRouter; go ahead, spend 10 bucks on credits, try them out and determine it for yourself. Other commenters keep telling you how non-local models are smarter; and while it is for the flagship ones, it&amp;#39;s actually possible that your particular tasks don&amp;#39;t require this flagship intelligence, so you really should to your own tests instead of relying on crowd instinct. And should you find out that locals actually can do your tasks, then you can just divide the price of the GPU by your monthly credit spending and use that number to get objective judgement what&amp;#39;s actually better for you.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hu8ri/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751242269,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":17}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hkd9i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No_Reveal_7826","can_mod_post":false,"created_utc":1751238685,"send_replies":true,"parent_id":"t3_1lnsax9","score":4,"author_fullname":"t2_1i5q2gxarw","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Some find local good enough, but nothing can compare to online models. At least when it comes to what can load in 24 GB GPU memory which is what I have.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hkd9i","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Some find local good enough, but nothing can compare to online models. At least when it comes to what can load in 24 GB GPU memory which is what I have.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hkd9i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751238685,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ixauk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Space__Whiskey","can_mod_post":false,"created_utc":1751257941,"send_replies":true,"parent_id":"t3_1lnsax9","score":3,"author_fullname":"t2_1cuwlonegr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"run comfyUI on it, and go on a comfyUI bender. That will pay it off.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ixauk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;run comfyUI on it, and go on a comfyUI bender. That will pay it off.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0ixauk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751257941,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0j19l9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"entsnack","can_mod_post":false,"created_utc":1751259876,"send_replies":true,"parent_id":"t3_1lnsax9","score":4,"author_fullname":"t2_1a48h7vf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Don't buy one for LLMs. Buy one for ComfyUI. No good APIs for the image/video generation stuff yet.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0j19l9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Don&amp;#39;t buy one for LLMs. Buy one for ComfyUI. No good APIs for the image/video generation stuff yet.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0j19l9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751259876,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hpq0w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"loyalekoinu88","can_mod_post":false,"created_utc":1751240605,"send_replies":true,"parent_id":"t3_1lnsax9","score":2,"author_fullname":"t2_1x5p0ubz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Tiny Qwen3 models can do tool calling. Coding doesn’t have the best options right now.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hpq0w","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Tiny Qwen3 models can do tool calling. Coding doesn’t have the best options right now.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hpq0w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751240605,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0j4xvq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"redditisunproductive","can_mod_post":false,"created_utc":1751261790,"send_replies":true,"parent_id":"t3_1lnsax9","score":2,"author_fullname":"t2_19353jsswd","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Finetuned local models can outperform SOTA models on very narrow tasks.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0j4xvq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Finetuned local models can outperform SOTA models on very narrow tasks.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0j4xvq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751261790,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0j6u3p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Noiselexer","can_mod_post":false,"created_utc":1751262816,"send_replies":true,"parent_id":"t3_1lnsax9","score":2,"author_fullname":"t2_n4xpz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I try models, never use them and use cloud for all my coding stuff.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0j6u3p","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I try models, never use them and use cloud for all my coding stuff.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0j6u3p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751262816,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jfi1n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"madaradess007","can_mod_post":false,"created_utc":1751267738,"send_replies":true,"parent_id":"t3_1lnsax9","score":2,"author_fullname":"t2_79slapln","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"no, local cant compare to cloud, but you can build workflows locally that will scale if you make a switch to external APIs","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jfi1n","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;no, local cant compare to cloud, but you can build workflows locally that will scale if you make a switch to external APIs&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jfi1n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751267738,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jqw9l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Fox-Lopsided","can_mod_post":false,"created_utc":1751274752,"send_replies":true,"parent_id":"t3_1lnsax9","score":2,"author_fullname":"t2_7ivwbs3t","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Nothing comes Close to Gemini and Claude especially because of the context window :(\\nBut who knows what the future will Bring","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jqw9l","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nothing comes Close to Gemini and Claude especially because of the context window :(\\nBut who knows what the future will Bring&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jqw9l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751274752,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0js5l8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"vinylger","can_mod_post":false,"created_utc":1751275529,"send_replies":true,"parent_id":"t3_1lnsax9","score":2,"author_fullname":"t2_f1nbx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Huh, is this me writing on another account w/o being aware of it?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0js5l8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Huh, is this me writing on another account w/o being aware of it?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0js5l8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751275529,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0k7vsc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"thallazar","can_mod_post":false,"created_utc":1751283794,"send_replies":true,"parent_id":"t3_1lnsax9","score":2,"author_fullname":"t2_4m3wz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Have never seen a local model that can perform as good as the remote, and generally at a fraction of the price. APIs are dirt cheap. Local has some utility, especially in privacy matters but for sheer performance, nothing compares.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0k7vsc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Have never seen a local model that can perform as good as the remote, and generally at a fraction of the price. APIs are dirt cheap. Local has some utility, especially in privacy matters but for sheer performance, nothing compares.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0k7vsc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751283794,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hq1jx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Federal_Order4324","can_mod_post":false,"created_utc":1751240722,"send_replies":true,"parent_id":"t1_n0hkbc2","score":0,"author_fullname":"t2_rlhobztpn","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"No one realistically does.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hq1jx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No one realistically does.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hq1jx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751240722,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hvnpk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Koksny","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0htyqi","score":17,"author_fullname":"t2_olk3n","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"\\\\* For coding only  \\n\\\\*\\\\* Requires 400GB of DDR  \\n\\\\*\\\\*\\\\* At Q4  \\n\\\\*\\\\*\\\\*\\\\* OoM@&gt;8k context.","edited":1751242997,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0hvnpk","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;* For coding only&lt;br/&gt;\\n** Requires 400GB of DDR&lt;br/&gt;\\n*** At Q4&lt;br/&gt;\\n**** OoM@&amp;gt;8k context.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hvnpk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751242799,"author_flair_text":null,"treatment_tags":[],"created_utc":1751242799,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":17}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0i1ab3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullstackSensei","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0hxflk","score":3,"author_fullname":"t2_17n3nqtj56","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The CPUs are cheap. It's the DDR5 RAM and motherboard where you'll spend a lot.\\n\\nAnd if you need more than 8k context, you'll need a second 24GB GPU","edited":false,"author_flair_css_class":null,"name":"t1_n0i1ab3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The CPUs are cheap. It&amp;#39;s the DDR5 RAM and motherboard where you&amp;#39;ll spend a lot.&lt;/p&gt;\\n\\n&lt;p&gt;And if you need more than 8k context, you&amp;#39;ll need a second 24GB GPU&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0i1ab3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751244918,"author_flair_text":null,"collapsed":false,"created_utc":1751244918,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hxflk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Agreeable_Patience47","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0hw4vd","score":-2,"author_fullname":"t2_8levrzbi","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It’s pretty cheap in my country and it's not required, it's just faster","edited":1751246718,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hxflk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It’s pretty cheap in my country and it&amp;#39;s not required, it&amp;#39;s just faster&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hxflk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751243475,"author_flair_text":null,"treatment_tags":[],"created_utc":1751243475,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hw4vd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"netvyper","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0htyqi","score":2,"author_fullname":"t2_v6nk8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"But doesn't that require an AMX compatible CPU, which is more than the 24GB GPU at this point?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0hw4vd","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;But doesn&amp;#39;t that require an AMX compatible CPU, which is more than the 24GB GPU at this point?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hw4vd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751242982,"author_flair_text":null,"treatment_tags":[],"created_utc":1751242982,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0htyqi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"Agreeable_Patience47","can_mod_post":false,"created_utc":1751242166,"send_replies":true,"parent_id":"t1_n0hkbc2","score":-5,"author_fullname":"t2_8levrzbi","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Your information is outdated. R1 actually runs on a single 24G GPU. https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/DeepseekR1_V3_tutorial.md","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0htyqi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Your information is outdated. R1 actually runs on a single 24G GPU. &lt;a href=\\"https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/DeepseekR1_V3_tutorial.md\\"&gt;https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/DeepseekR1_V3_tutorial.md&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0htyqi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751242166,"author_flair_text":null,"treatment_tags":[],"collapsed":true,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-5}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hkbc2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"z_3454_pfk","can_mod_post":false,"created_utc":1751238667,"send_replies":true,"parent_id":"t3_1lnsax9","score":3,"author_fullname":"t2_askwa","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"no local model can really match claude for swe. r1 0528 is decent but requires too much VRAM (unless you have that kind of money)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hkbc2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;no local model can really match claude for swe. r1 0528 is decent but requires too much VRAM (unless you have that kind of money)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hkbc2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751238667,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0k08x4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FunnyAsparagus1253","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0jqxqw","score":2,"author_fullname":"t2_i6c8tay3w","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Omg you’re straight back to ‘spending thousands to goon’ and ‘addiction’ 😅\\n\\nIt’s not a linear spectrum. It’s a big multidimensional reality not a two-ended stick. There are many different individuals who do different things for different reasons with different levels of spending and different levels of nfsw-ness, different levels of ‘taking it seriously’ different amounts of tech knowledge, different resources available, different life situations, different personal histories, different aims, etc etc etc etc etc.\\n\\nThis hasn’t been the worst conversation I’ve had on reddit, not even close. I don’t disagree with you that the cloud models are cheaper and better overall. Have a nice day, MengerianMango. I don’t want to argue about this anymore- I have tidying to do and bunnies to look after. Take care :) 🫡🖖","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0k08x4","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Omg you’re straight back to ‘spending thousands to goon’ and ‘addiction’ 😅&lt;/p&gt;\\n\\n&lt;p&gt;It’s not a linear spectrum. It’s a big multidimensional reality not a two-ended stick. There are many different individuals who do different things for different reasons with different levels of spending and different levels of nfsw-ness, different levels of ‘taking it seriously’ different amounts of tech knowledge, different resources available, different life situations, different personal histories, different aims, etc etc etc etc etc.&lt;/p&gt;\\n\\n&lt;p&gt;This hasn’t been the worst conversation I’ve had on reddit, not even close. I don’t disagree with you that the cloud models are cheaper and better overall. Have a nice day, MengerianMango. I don’t want to argue about this anymore- I have tidying to do and bunnies to look after. Take care :) 🫡🖖&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0k08x4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751280145,"author_flair_text":null,"treatment_tags":[],"created_utc":1751280145,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0jqxqw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MengerianMango","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0jjbch","score":0,"author_fullname":"t2_mcvyi","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Using 3090 sized models for work feels like trying to prove the infinite monkey theorem empirically. It sounds hyperbolic but it's not. Even the best closed models are still pretty annoying rn. It comes down to success rate. Low success rates are just really annoying. You end up feeling you'd have been better off just writing the code yourself. I'm assuming OP already knows how to code.\\n\\nI'm perplexed how it's a point of debate that sexual obsessions are bad for you. There's a lot of room on the spectrum between techbro and gooner. Objectively, we are overdressed monkeys. Tech today is like cocaine in the 1920s. It's designed to hijack our instincts and create addiction. Spending thousands to goon on custom erotica is some late stage capitalism level addictive consumption","edited":false,"author_flair_css_class":null,"name":"t1_n0jqxqw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Using 3090 sized models for work feels like trying to prove the infinite monkey theorem empirically. It sounds hyperbolic but it&amp;#39;s not. Even the best closed models are still pretty annoying rn. It comes down to success rate. Low success rates are just really annoying. You end up feeling you&amp;#39;d have been better off just writing the code yourself. I&amp;#39;m assuming OP already knows how to code.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m perplexed how it&amp;#39;s a point of debate that sexual obsessions are bad for you. There&amp;#39;s a lot of room on the spectrum between techbro and gooner. Objectively, we are overdressed monkeys. Tech today is like cocaine in the 1920s. It&amp;#39;s designed to hijack our instincts and create addiction. Spending thousands to goon on custom erotica is some late stage capitalism level addictive consumption&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jqxqw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751274777,"author_flair_text":null,"collapsed":false,"created_utc":1751274777,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n0jjbch","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FunnyAsparagus1253","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0jdfhv","score":4,"author_fullname":"t2_i6c8tay3w","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You’re using the word ‘objectively’ a lot while sharing a lot of your own opinions.  Really unpleasant judgemental opinions IMO. Calling it ‘illegal’, using phrases like ‘your mom’s basement’, ‘any sane person’ - and using ‘…..’ in place of the word ‘erotic’. A *lot* of people use LLMs for things other than ‘putting money in the bank’. It’s not *objectively wrong* no matter what you think.\\n\\nOP has said they want assistance with coding and tool-calling. It wouldn’t be the worst thing in the world to get a used 3090 and keep a coding model on hand for times when their internet is down, or for when Anthropic is experiencing an outage. Not everything has to be the ultra best 1 trillion parameters, 10xing your productivity and getting you ahead in the techbro capitalist rat race. This is r/LocalLLaMa, dude. We have all sorts here. Don’t be such a hater. Peace 🫶✌️","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jjbch","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You’re using the word ‘objectively’ a lot while sharing a lot of your own opinions.  Really unpleasant judgemental opinions IMO. Calling it ‘illegal’, using phrases like ‘your mom’s basement’, ‘any sane person’ - and using ‘…..’ in place of the word ‘erotic’. A &lt;em&gt;lot&lt;/em&gt; of people use LLMs for things other than ‘putting money in the bank’. It’s not &lt;em&gt;objectively wrong&lt;/em&gt; no matter what you think.&lt;/p&gt;\\n\\n&lt;p&gt;OP has said they want assistance with coding and tool-calling. It wouldn’t be the worst thing in the world to get a used 3090 and keep a coding model on hand for times when their internet is down, or for when Anthropic is experiencing an outage. Not everything has to be the ultra best 1 trillion parameters, 10xing your productivity and getting you ahead in the techbro capitalist rat race. This is &lt;a href=\\"/r/LocalLLaMa\\"&gt;r/LocalLLaMa&lt;/a&gt;, dude. We have all sorts here. Don’t be such a hater. Peace 🫶✌️&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jjbch/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751270069,"author_flair_text":null,"treatment_tags":[],"created_utc":1751270069,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n0jdfhv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MengerianMango","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0jchum","score":-3,"author_fullname":"t2_mcvyi","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Bro i like fancy toys too but OP is asking for objectivity. Literal fact is that you're not getting your money's worth out of your hardware. Unless you live in your mom's basement, assuming you have a job and arent running the tits off your gpus 34/7, you could rent better for the same amount of actual usage/time for way less than you paid. I could too. I mean i do get it. Fancy toys are nice and nothing gets me harder than caressing my 6000, but yk, ion need it and it's not putting money in my bank account. It's not paying my rent. Except it kinda might. I got the thing bc pnl gets me a profit bonus. Excessive success in the project I'm working on could easily net me a 10x return on my money.\\n\\nObjectively, any sane person will tell you that you shouldn't spend 5k+ on an item that doesn't generate some kinda return for you. Unless you're making 400k, in which case why even ask?\\n\\nBut yeah the amount of people judging models by their...... fiction writing abilities. It might not be illegal but its certainly not a use of time or money that I respect very much","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0jdfhv","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Bro i like fancy toys too but OP is asking for objectivity. Literal fact is that you&amp;#39;re not getting your money&amp;#39;s worth out of your hardware. Unless you live in your mom&amp;#39;s basement, assuming you have a job and arent running the tits off your gpus 34/7, you could rent better for the same amount of actual usage/time for way less than you paid. I could too. I mean i do get it. Fancy toys are nice and nothing gets me harder than caressing my 6000, but yk, ion need it and it&amp;#39;s not putting money in my bank account. It&amp;#39;s not paying my rent. Except it kinda might. I got the thing bc pnl gets me a profit bonus. Excessive success in the project I&amp;#39;m working on could easily net me a 10x return on my money.&lt;/p&gt;\\n\\n&lt;p&gt;Objectively, any sane person will tell you that you shouldn&amp;#39;t spend 5k+ on an item that doesn&amp;#39;t generate some kinda return for you. Unless you&amp;#39;re making 400k, in which case why even ask?&lt;/p&gt;\\n\\n&lt;p&gt;But yeah the amount of people judging models by their...... fiction writing abilities. It might not be illegal but its certainly not a use of time or money that I respect very much&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jdfhv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751266524,"author_flair_text":null,"treatment_tags":[],"created_utc":1751266524,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0jchum","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FunnyAsparagus1253","can_mod_post":false,"created_utc":1751265992,"send_replies":true,"parent_id":"t1_n0hll17","score":4,"author_fullname":"t2_i6c8tay3w","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Saying that the only real use case for having a home setup is *illegal porn* is a really weird response given that we’re in r/LocalLLaMa . Is that *seriously* what you think? Maybe you’re in the wrong place 🤨","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jchum","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Saying that the only real use case for having a home setup is &lt;em&gt;illegal porn&lt;/em&gt; is a really weird response given that we’re in &lt;a href=\\"/r/LocalLLaMa\\"&gt;r/LocalLLaMa&lt;/a&gt; . Is that &lt;em&gt;seriously&lt;/em&gt; what you think? Maybe you’re in the wrong place 🤨&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jchum/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751265992,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0iey5z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MerlinTrashMan","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0i64gu","score":1,"author_fullname":"t2_7ghjkhuo","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It should be a lot of fun to play with the data.  I found the first step for me was to study the correlated items and build better signals from them and then use uncorrelated values to either scale the new signals or invalidate them.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0iey5z","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It should be a lot of fun to play with the data.  I found the first step for me was to study the correlated items and build better signals from them and then use uncorrelated values to either scale the new signals or invalidate them.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0iey5z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751250097,"author_flair_text":null,"treatment_tags":[],"created_utc":1751250097,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0i64gu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MengerianMango","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0i0r4o","score":0,"author_fullname":"t2_mcvyi","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Meta modeling. We already have awesome models. But they're generally simple and derived from first principles. To greatly oversimplify, we basically have built a pretty successful fund out of simply adding 500ish simple models. I want to use NNs to distill a better signal from the model soup than we're currently doing by just summing.\\n\\nIn theory, I should be using xgboost for this. But we'll see. Anything uncorrelated is valuable.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0i64gu","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Meta modeling. We already have awesome models. But they&amp;#39;re generally simple and derived from first principles. To greatly oversimplify, we basically have built a pretty successful fund out of simply adding 500ish simple models. I want to use NNs to distill a better signal from the model soup than we&amp;#39;re currently doing by just summing.&lt;/p&gt;\\n\\n&lt;p&gt;In theory, I should be using xgboost for this. But we&amp;#39;ll see. Anything uncorrelated is valuable.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0i64gu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751246728,"author_flair_text":null,"treatment_tags":[],"created_utc":1751246728,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n0i0r4o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MerlinTrashMan","can_mod_post":false,"created_utc":1751244721,"send_replies":true,"parent_id":"t1_n0hll17","score":1,"author_fullname":"t2_7ghjkhuo","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Id love to chat on your GPU use for quant finance.  I have been writing kernels for option portfolio optimization that run on a 3090 but I am running out of vram.  I am curious what you plan to do.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0i0r4o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Id love to chat on your GPU use for quant finance.  I have been writing kernels for option portfolio optimization that run on a 3090 but I am running out of vram.  I am curious what you plan to do.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0i0r4o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751244721,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ibq0a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MengerianMango","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0i773r","score":2,"author_fullname":"t2_mcvyi","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"PP=prompt processing=prefill.\\n\\nThanks for all the info. I'm super jelly ngl. I'm a straight man but id give my booty for a dual epyc system. Unfortunately this is a side project so I dont have company funding for it. The 6000 could directly yield ROI for me in terms of performance bonus but I dont think id feel the same positive impact in my wallet from a dual epyc, as much as id love have one.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n0ibq0a","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;PP=prompt processing=prefill.&lt;/p&gt;\\n\\n&lt;p&gt;Thanks for all the info. I&amp;#39;m super jelly ngl. I&amp;#39;m a straight man but id give my booty for a dual epyc system. Unfortunately this is a side project so I dont have company funding for it. The 6000 could directly yield ROI for me in terms of performance bonus but I dont think id feel the same positive impact in my wallet from a dual epyc, as much as id love have one.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0ibq0a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751248852,"author_flair_text":null,"treatment_tags":[],"created_utc":1751248852,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0i773r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"createthiscom","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0i5ngq","score":1,"author_fullname":"t2_ozxxf","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I use Open Hands AI. I’m not sure what you mean by PP. I get 22 tok/s starting out. prefill is between 18 and 45 tok/s depending on the input. By 50k context I’m usually down to 18 tok/s. I’m quite happy with that performance for V3. It’s a little slow for R1 with the extra thinking, but I use R1 sometimes anyway for more complex tasks and sometimes it’s worthwhile. I used to prefer ktransformers because their ktransformer backend had zero delay for long context. The prefill cache was perfect because there was only one session. However, I get a lot of crashes with ktransformers and I have a lot of available VRAM with the 6000 pro, so I use llama.cpp now because it’s so reliable. Check the video I linked for examples.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0i773r","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I use Open Hands AI. I’m not sure what you mean by PP. I get 22 tok/s starting out. prefill is between 18 and 45 tok/s depending on the input. By 50k context I’m usually down to 18 tok/s. I’m quite happy with that performance for V3. It’s a little slow for R1 with the extra thinking, but I use R1 sometimes anyway for more complex tasks and sometimes it’s worthwhile. I used to prefer ktransformers because their ktransformer backend had zero delay for long context. The prefill cache was perfect because there was only one session. However, I get a lot of crashes with ktransformers and I have a lot of available VRAM with the 6000 pro, so I use llama.cpp now because it’s so reliable. Check the video I linked for examples.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0i773r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751247137,"author_flair_text":null,"treatment_tags":[],"created_utc":1751247137,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0i5ngq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MengerianMango","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0i06d4","score":1,"author_fullname":"t2_mcvyi","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What tps do you get tho, especially for PP? Personally, I'm pretty sensitive to delay for \\"vibe\\"/agentic coding. It would need to be something close to as fast as the APIs to feel useful. Do you use it for agentic coding (eg cline/aider) or just chat?","edited":false,"author_flair_css_class":null,"name":"t1_n0i5ngq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What tps do you get tho, especially for PP? Personally, I&amp;#39;m pretty sensitive to delay for &amp;quot;vibe&amp;quot;/agentic coding. It would need to be something close to as fast as the APIs to feel useful. Do you use it for agentic coding (eg cline/aider) or just chat?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0i5ngq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751246553,"author_flair_text":null,"collapsed":false,"created_utc":1751246553,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0iot40","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"createthiscom","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ih8sa","score":1,"author_fullname":"t2_ozxxf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yeah I haven’t noticed R1’s code to be better than V3’s. It tends to not follow instructions quite as faithfully too. I keep using it now and then trying to find a niche I like it in though.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n0iot40","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah I haven’t noticed R1’s code to be better than V3’s. It tends to not follow instructions quite as faithfully too. I keep using it now and then trying to find a niche I like it in though.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0iot40/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751254108,"author_flair_text":null,"treatment_tags":[],"created_utc":1751254108,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ih8sa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MengerianMango","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0igv0u","score":3,"author_fullname":"t2_mcvyi","approved_by":null,"mod_note":null,"all_awardings":[],"body":"r1 (and thinking models in general) tend to be worse at just doing stuff. Like v3 will outperform at editing code given the same quality instructions. People often use r1 as a master model to command v3 for ex. r1 will do the high level architectural design and hand off to v3 to do the grunt work.\\n\\nI think for ppl who already know how to code, v3 is more useful. I generally can do the architectural stuff myself as well or better than r1. I just want a \\"dumb\\" grunt to do the work 10x faster than I could","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0ih8sa","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;r1 (and thinking models in general) tend to be worse at just doing stuff. Like v3 will outperform at editing code given the same quality instructions. People often use r1 as a master model to command v3 for ex. r1 will do the high level architectural design and hand off to v3 to do the grunt work.&lt;/p&gt;\\n\\n&lt;p&gt;I think for ppl who already know how to code, v3 is more useful. I generally can do the architectural stuff myself as well or better than r1. I just want a &amp;quot;dumb&amp;quot; grunt to do the work 10x faster than I could&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0ih8sa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751251004,"author_flair_text":null,"treatment_tags":[],"created_utc":1751251004,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0igv0u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Few-Yam9901","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0i06d4","score":1,"author_fullname":"t2_1rhlf3bcfk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Isn’t r1 0528 better? I run it with mixed Nvidia gpus 2bit, 3-400 pp ts and 30-40 gen ts. Its Good","edited":false,"author_flair_css_class":null,"name":"t1_n0igv0u","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Isn’t r1 0528 better? I run it with mixed Nvidia gpus 2bit, 3-400 pp ts and 30-40 gen ts. Its Good&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lnsax9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0igv0u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751250850,"author_flair_text":null,"collapsed":false,"created_utc":1751250850,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0i06d4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"createthiscom","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0hzefi","score":3,"author_fullname":"t2_ozxxf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I don’t know if threadripper’s memory bandwidth is sufficient, but yes. I have a dual CPU EPYC system that was roughly 14k without the GPU. However, I can literally run V3 all day for only about $30/mo in electricity.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0i06d4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don’t know if threadripper’s memory bandwidth is sufficient, but yes. I have a dual CPU EPYC system that was roughly 14k without the GPU. However, I can literally run V3 all day for only about $30/mo in electricity.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0i06d4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751244505,"author_flair_text":null,"treatment_tags":[],"created_utc":1751244505,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hzefi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MengerianMango","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0hz6y4","score":3,"author_fullname":"t2_mcvyi","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"So roughly another 10k for a ddr5 threadripper or epyc build. You're still talking 9k gpu + ~10k host.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0hzefi","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So roughly another 10k for a ddr5 threadripper or epyc build. You&amp;#39;re still talking 9k gpu + ~10k host.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hzefi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751244213,"author_flair_text":null,"treatment_tags":[],"created_utc":1751244213,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jd4lw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"sc166","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0hz6y4","score":1,"author_fullname":"t2_twa0i","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Do you mind sharing instructions? I have 7985x with 768gb (8 channel 6000mt) and single 6000 pro (waiting for 2nd one). I don’t think my system has as high memory bandwidth but would like to test how close I can in terms of tok/s. Thanks","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0jd4lw","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Do you mind sharing instructions? I have 7985x with 768gb (8 channel 6000mt) and single 6000 pro (waiting for 2nd one). I don’t think my system has as high memory bandwidth but would like to test how close I can in terms of tok/s. Thanks&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jd4lw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751266352,"author_flair_text":null,"treatment_tags":[],"created_utc":1751266352,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hz6y4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"createthiscom","can_mod_post":false,"created_utc":1751244135,"send_replies":true,"parent_id":"t1_n0hll17","score":1,"author_fullname":"t2_ozxxf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You can run V3 with considerably less than a single blackwell 6000 pro, but with it I get about 22 tok/s. The host needs about 400gb ram and 720gb/s of bandwidth to achieve those speeds. Video evidence: https://youtu.be/vfi9LRJxgHs?si=FJ7NLiaDos1wOhIW","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hz6y4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can run V3 with considerably less than a single blackwell 6000 pro, but with it I get about 22 tok/s. The host needs about 400gb ram and 720gb/s of bandwidth to achieve those speeds. Video evidence: &lt;a href=\\"https://youtu.be/vfi9LRJxgHs?si=FJ7NLiaDos1wOhIW\\"&gt;https://youtu.be/vfi9LRJxgHs?si=FJ7NLiaDos1wOhIW&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hz6y4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751244135,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hll17","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MengerianMango","can_mod_post":false,"created_utc":1751239122,"send_replies":true,"parent_id":"t3_1lnsax9","score":3,"author_fullname":"t2_mcvyi","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I bought a 6000 Blackwell and I still can't even run Qwen3 235b in 4 bit. I can run in 2 bit with 64k context, even 128k context is too much.\\n\\nAnd really you want to run Deepseek v3 for real work. For that you need roughly 3 6000 Blackwells, so roughly $27k. And you'd still be disappointed vs Claude.\\n\\nI knew this going in tho. I work in quant finance and intend to use the GPU for non-llm work. I wouldnt have bought it just for llms.\\n\\nGPUs dont really make sense unless you're generating illegal porn and really dont want any trace of your crimes on other people's servers or maybe if you're discussing incredibly valuable IP. I talk with qwen3 about the internal secrets from my firm, play with new ideas, etc. Obv I dont trust OpenAI to talk with gpt about such things. I could leak million dollar secrets if they lie about not training on my chats. Other than a few very limited circumstances, just use openrouter.","edited":1751239307,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hll17","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I bought a 6000 Blackwell and I still can&amp;#39;t even run Qwen3 235b in 4 bit. I can run in 2 bit with 64k context, even 128k context is too much.&lt;/p&gt;\\n\\n&lt;p&gt;And really you want to run Deepseek v3 for real work. For that you need roughly 3 6000 Blackwells, so roughly $27k. And you&amp;#39;d still be disappointed vs Claude.&lt;/p&gt;\\n\\n&lt;p&gt;I knew this going in tho. I work in quant finance and intend to use the GPU for non-llm work. I wouldnt have bought it just for llms.&lt;/p&gt;\\n\\n&lt;p&gt;GPUs dont really make sense unless you&amp;#39;re generating illegal porn and really dont want any trace of your crimes on other people&amp;#39;s servers or maybe if you&amp;#39;re discussing incredibly valuable IP. I talk with qwen3 about the internal secrets from my firm, play with new ideas, etc. Obv I dont trust OpenAI to talk with gpt about such things. I could leak million dollar secrets if they lie about not training on my chats. Other than a few very limited circumstances, just use openrouter.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hll17/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751239122,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hthcl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"KDCreerStudios","can_mod_post":false,"created_utc":1751241986,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_1qfbu2cvzc","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"With tool callings and fine-tuned specialization it can beat out Claude in certain tasks. But a general purpose one, no, not really.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hthcl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;With tool callings and fine-tuned specialization it can beat out Claude in certain tasks. But a general purpose one, no, not really.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hthcl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751241986,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hxrlv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Syzeon","can_mod_post":false,"created_utc":1751243603,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_82nlaimd","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"rent a runpod or something, try out a few local you have in mind, evaluate yourself if local model is what you're after. Make your decision after you tried it out","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hxrlv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;rent a runpod or something, try out a few local you have in mind, evaluate yourself if local model is what you&amp;#39;re after. Make your decision after you tried it out&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hxrlv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751243603,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hy8zc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"evilbarron2","can_mod_post":false,"created_utc":1751243784,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_gr2fr79s1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"No, they can’t, but they can still do a lot. They just have far less “presence” and adaptability. I’ve been using frontier for dev and local for production - more work but I can sleep at night not worrying about a huge bill","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hy8zc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No, they can’t, but they can still do a lot. They just have far less “presence” and adaptability. I’ve been using frontier for dev and local for production - more work but I can sleep at night not worrying about a huge bill&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hy8zc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751243784,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0i30ep","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"makinggrace","can_mod_post":false,"created_utc":1751245563,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_z9vf1vr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Try before you buy. \\n\\nLocal LLM isn't (yet) IMHO a great tool for execution unless the use case is extremely specific and cannot be performed in cloud for reasons. It is too slow and too expensive to do tasks locally that can be done in cloud.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0i30ep","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Try before you buy. &lt;/p&gt;\\n\\n&lt;p&gt;Local LLM isn&amp;#39;t (yet) IMHO a great tool for execution unless the use case is extremely specific and cannot be performed in cloud for reasons. It is too slow and too expensive to do tasks locally that can be done in cloud.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0i30ep/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751245563,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0i4pyh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"robberviet","can_mod_post":false,"created_utc":1751246206,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_jxc5a","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"No, just use services. You don't get that performance selfhost.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0i4pyh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No, just use services. You don&amp;#39;t get that performance selfhost.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0i4pyh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751246206,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jgl1e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"godndiogoat","can_mod_post":false,"created_utc":1751268387,"send_replies":true,"parent_id":"t1_n0i7zlk","score":1,"author_fullname":"t2_1o8b7or53v","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Time a full day of your prompts on a rented GPU before buying one yourself. Spin up a Llama-3-8B or Mistral-7B on RunPod for a few cents, or just Ollama on CPU overnight, and log latency + token cost. If the math says you’d save more than the GPU price in six months, grab the card; if not, stay cloud-side. In my case, the break-even for a 4090 was nine months, but the local model still missed about 20% of Claude’s coding fixes, so I stuck with the API. I keep a cheap RTX 3060 only for fast unit-test stubs. I also tried Spellbook and Replicate for comparison, yet ended up tracking them through APIWrapper.ai because it lets me swap local models against cloud calls without rewriting my stack. Same idea might save you cash long-term.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jgl1e","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Time a full day of your prompts on a rented GPU before buying one yourself. Spin up a Llama-3-8B or Mistral-7B on RunPod for a few cents, or just Ollama on CPU overnight, and log latency + token cost. If the math says you’d save more than the GPU price in six months, grab the card; if not, stay cloud-side. In my case, the break-even for a 4090 was nine months, but the local model still missed about 20% of Claude’s coding fixes, so I stuck with the API. I keep a cheap RTX 3060 only for fast unit-test stubs. I also tried Spellbook and Replicate for comparison, yet ended up tracking them through APIWrapper.ai because it lets me swap local models against cloud calls without rewriting my stack. Same idea might save you cash long-term.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jgl1e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751268387,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0i7zlk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Top-Winter938","can_mod_post":false,"created_utc":1751247436,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_c1i53eiz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Try out some of the smaller models you could run locally via OpenRouter first. If they’re good enough for what you need, figure out your break-even point: is it cheaper to just keep using the API, or does it make sense to buy a GPU? Will you still be using the model after you hit that break-even? That usually gives you your answer. \\n\\nAs you’re using it for coding and used to claude, you’ll probably be frustrated by these smaller models. They are good for smaller, specific tasks","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0i7zlk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Try out some of the smaller models you could run locally via OpenRouter first. If they’re good enough for what you need, figure out your break-even point: is it cheaper to just keep using the API, or does it make sense to buy a GPU? Will you still be using the model after you hit that break-even? That usually gives you your answer. &lt;/p&gt;\\n\\n&lt;p&gt;As you’re using it for coding and used to claude, you’ll probably be frustrated by these smaller models. They are good for smaller, specific tasks&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0i7zlk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751247436,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0i90lt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kironlau","can_mod_post":false,"created_utc":1751247828,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_tb0dz2ds","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Go to openrouter, get a free trial. In field, will get you a answer.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0i90lt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Go to openrouter, get a free trial. In field, will get you a answer.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0i90lt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751247828,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0idned","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Little-Parfait-423","can_mod_post":false,"created_utc":1751249589,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_1i51wbv406","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I have a rtx 3060 and a rtx 3090. The VRAM constraint of 12GB vs 24GB between them allows me to run larger models (barely) but at significant loss in context and speed. I use it to debug code, quick scripts, look at logs, review output from tests, and prepare prompts for larger models over API. In all honestly you get decent results with 8b models but no tool use. If you prompt well and add context you get tool use (ie edit for you) at 14b models you can sometimes get tool use for small files. 24-30b models are smarter but even with 24gb VRAM there is no context window to load/edit files anyway. My advice to anyone is get a used rtx3060 12gb vram ~200-300 USD (or rtx 4060 16gb if you can find one) and run 8b/14b models to prep your work for cloud based models. Don’t mind the gap between local LLM with beefy/expensive cards it’s not worth being just as frustrated and broke on top of it :2cents:","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0idned","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have a rtx 3060 and a rtx 3090. The VRAM constraint of 12GB vs 24GB between them allows me to run larger models (barely) but at significant loss in context and speed. I use it to debug code, quick scripts, look at logs, review output from tests, and prepare prompts for larger models over API. In all honestly you get decent results with 8b models but no tool use. If you prompt well and add context you get tool use (ie edit for you) at 14b models you can sometimes get tool use for small files. 24-30b models are smarter but even with 24gb VRAM there is no context window to load/edit files anyway. My advice to anyone is get a used rtx3060 12gb vram ~200-300 USD (or rtx 4060 16gb if you can find one) and run 8b/14b models to prep your work for cloud based models. Don’t mind the gap between local LLM with beefy/expensive cards it’s not worth being just as frustrated and broke on top of it :2cents:&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0idned/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751249589,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ii0i2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Violaze27","can_mod_post":false,"created_utc":1751251311,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_fe4u4syr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For games absolutely","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ii0i2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For games absolutely&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0ii0i2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751251311,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ikgln","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ardalok","can_mod_post":false,"created_utc":1751252293,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_vgnewja","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The only locally deployable model that can compete with Claude, GPT, and similar large language models is DeepSeek, and it cannot even be hosted on a single GPU.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ikgln","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The only locally deployable model that can compete with Claude, GPT, and similar large language models is DeepSeek, and it cannot even be hosted on a single GPU.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0ikgln/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751252293,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0j2lrt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cguy1234","can_mod_post":false,"created_utc":1751260561,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_mhjuy","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Claude seems way ahead of any local LLM that I’ve tried in terms of quality of outputs and also just dragging a big PDF into it and getting quite good analysis. Even with a good GPU, I haven’t seen a local model that has the same quality.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0j2lrt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Claude seems way ahead of any local LLM that I’ve tried in terms of quality of outputs and also just dragging a big PDF into it and getting quite good analysis. Even with a good GPU, I haven’t seen a local model that has the same quality.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0j2lrt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751260561,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0j6foc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Elderberry_9132","can_mod_post":false,"created_utc":1751262600,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_byh4ysuoh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Well, here is an answer, you don’t understand how this AI magic works, mainly LLM models perform particular task of either helping to reason, or generate text, their purpose is to guess the next token a user expects, it is just guessing it with enough correct answers for you to believe it is actually intelligent. \\n\\nSo, keeping this in mind, “AI” is a huge pile of different algorithms, services, stages and pipelines. \\n\\nYou won’t be able to run it locally unless you have the time to developer it and deploy, and manage it","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0j6foc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Well, here is an answer, you don’t understand how this AI magic works, mainly LLM models perform particular task of either helping to reason, or generate text, their purpose is to guess the next token a user expects, it is just guessing it with enough correct answers for you to believe it is actually intelligent. &lt;/p&gt;\\n\\n&lt;p&gt;So, keeping this in mind, “AI” is a huge pile of different algorithms, services, stages and pipelines. &lt;/p&gt;\\n\\n&lt;p&gt;You won’t be able to run it locally unless you have the time to developer it and deploy, and manage it&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0j6foc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751262600,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jamiu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Spiritual-Spend8187","can_mod_post":false,"created_utc":1751264917,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_4aevhf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Closest you will get to cloud level on local is deepseek but that is still very expensive on consumer hardware possible but insane.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jamiu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Closest you will get to cloud level on local is deepseek but that is still very expensive on consumer hardware possible but insane.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jamiu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751264917,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jdg5y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Fast-Satisfaction482","can_mod_post":false,"created_utc":1751266535,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_9ceux4xp","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I have access to a pretty decent workstation with dual 4090 at work and while it's all fun and games as long as your company pays for it, it's by far not as good as OpenAI or Anthropic's offerings.\\n\\n\\nOf course unless your use case is against the terms of service for the big cloud services, then it's the only option. (Or cloud is against regulation at your job ) ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jdg5y","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have access to a pretty decent workstation with dual 4090 at work and while it&amp;#39;s all fun and games as long as your company pays for it, it&amp;#39;s by far not as good as OpenAI or Anthropic&amp;#39;s offerings.&lt;/p&gt;\\n\\n&lt;p&gt;Of course unless your use case is against the terms of service for the big cloud services, then it&amp;#39;s the only option. (Or cloud is against regulation at your job ) &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jdg5y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751266535,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jegxi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Liringlass","can_mod_post":false,"created_utc":1751267131,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_dfewdhav","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Not only anything local can only come close, not reach Claude for coding.\\n\\nBut anything local won't upgrade magically. What you buy today will still have the same capacity in 2 years, and while same size LLMs improve over time you won't be able to upgrade your hardware \\"for free\\".\\n\\nLike someone else said Deepseek (the big model) might be the closest one but you either need a big ram/cpu server (just a few K$) but it's gonna be very very slow or you  need multiple 10/20k$ GPUs (and even here i'm not sure how fast it will be compared to Claude on subscription services).\\n\\nPeople mention 70b models which could be fine for some help but don't expect Cursor / GHCP / Cline / RooCode agent mode level of help, it will be more basic.\\n\\nOne more thing is that subscription services seem very cheap to me compared to the cost of doing it yourself, might be due to competition? Last time I checked cursor was USD 20 a month and that's really cheap for all the work you can get done on big models.\\n\\nLastly, anything local or cloud based need work to setup and maintain. That's also something to take into account if you look at it in a ROI perspective.\\n\\nThe only cases where I would advise local LLMs are: \\n\\n\\\\- When confidentiality is a must and deserves the added cost / lower level of service. And even then you might be better off renting the equipment in the cloud, and running your own private API, rather than buying 20 or 40k USD worth of equipment.\\n\\n\\\\- When you want to DIY and learn, focusing on that journey rather than on the direct result, and added cost is not a problem","edited":1751267314,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jegxi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not only anything local can only come close, not reach Claude for coding.&lt;/p&gt;\\n\\n&lt;p&gt;But anything local won&amp;#39;t upgrade magically. What you buy today will still have the same capacity in 2 years, and while same size LLMs improve over time you won&amp;#39;t be able to upgrade your hardware &amp;quot;for free&amp;quot;.&lt;/p&gt;\\n\\n&lt;p&gt;Like someone else said Deepseek (the big model) might be the closest one but you either need a big ram/cpu server (just a few K$) but it&amp;#39;s gonna be very very slow or you  need multiple 10/20k$ GPUs (and even here i&amp;#39;m not sure how fast it will be compared to Claude on subscription services).&lt;/p&gt;\\n\\n&lt;p&gt;People mention 70b models which could be fine for some help but don&amp;#39;t expect Cursor / GHCP / Cline / RooCode agent mode level of help, it will be more basic.&lt;/p&gt;\\n\\n&lt;p&gt;One more thing is that subscription services seem very cheap to me compared to the cost of doing it yourself, might be due to competition? Last time I checked cursor was USD 20 a month and that&amp;#39;s really cheap for all the work you can get done on big models.&lt;/p&gt;\\n\\n&lt;p&gt;Lastly, anything local or cloud based need work to setup and maintain. That&amp;#39;s also something to take into account if you look at it in a ROI perspective.&lt;/p&gt;\\n\\n&lt;p&gt;The only cases where I would advise local LLMs are: &lt;/p&gt;\\n\\n&lt;p&gt;- When confidentiality is a must and deserves the added cost / lower level of service. And even then you might be better off renting the equipment in the cloud, and running your own private API, rather than buying 20 or 40k USD worth of equipment.&lt;/p&gt;\\n\\n&lt;p&gt;- When you want to DIY and learn, focusing on that journey rather than on the direct result, and added cost is not a problem&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jegxi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751267131,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jnm9c","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Cergorach","can_mod_post":false,"created_utc":1751272739,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_cs4w88d2l","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt;Can any local LLM compare with cloud models? \\n\\nWhile technically the can, when the model is available (like DeepSeek), but when you run the full model you need such ridiculous hardware that it's not realistic or practical. You can run the smaller quantized models easily, but these tend to be lobotomized models. Now, depending on your task, those lobotomized models might work perfectly fine for you...\\n\\nIt's also not just buying an expensive GPU once, it needs ridiculous amounts of power to run (possibly requiring you to upgrade your power supply as well) and even at idle (not in use) they use quite a bit of power. Almost all that power is converted into heat, so you would need to use even more power to cool that with AC... A modern 14900k + 5090 pulls as much idle as a Mac Mini pulls at full power...","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jnm9c","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Can any local LLM compare with cloud models? &lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;While technically the can, when the model is available (like DeepSeek), but when you run the full model you need such ridiculous hardware that it&amp;#39;s not realistic or practical. You can run the smaller quantized models easily, but these tend to be lobotomized models. Now, depending on your task, those lobotomized models might work perfectly fine for you...&lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s also not just buying an expensive GPU once, it needs ridiculous amounts of power to run (possibly requiring you to upgrade your power supply as well) and even at idle (not in use) they use quite a bit of power. Almost all that power is converted into heat, so you would need to use even more power to cool that with AC... A modern 14900k + 5090 pulls as much idle as a Mac Mini pulls at full power...&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jnm9c/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751272739,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jpxvk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MelodicRecognition7","can_mod_post":false,"created_utc":1751274162,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_1eex9ug5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I agree with other posters - local LLM do not and probably never will match the cloud services, you'll have to spend like $50k to come close to the cloud. You'll want to run local only if you have to - obliged by law or NDA or just don't want your horny chats get published some day lol.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jpxvk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I agree with other posters - local LLM do not and probably never will match the cloud services, you&amp;#39;ll have to spend like $50k to come close to the cloud. You&amp;#39;ll want to run local only if you have to - obliged by law or NDA or just don&amp;#39;t want your horny chats get published some day lol.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jpxvk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751274162,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jq4h2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Round_Mixture_7541","can_mod_post":false,"created_utc":1751274273,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_114cnblv7x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"How about mistral's new model designed for tool calls and coding tasks? Has anyone set it up?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jq4h2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How about mistral&amp;#39;s new model designed for tool calls and coding tasks? Has anyone set it up?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0jq4h2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751274273,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0k5w6f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ylsid","can_mod_post":false,"created_utc":1751282895,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_6lmlc","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"DeepSeek R1 will compete well with Claude, but good luck running it locally. Even Claude runs at cost","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0k5w6f","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;DeepSeek R1 will compete well with Claude, but good luck running it locally. Even Claude runs at cost&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0k5w6f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751282895,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0k934m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BumbleSlob","can_mod_post":false,"created_utc":1751284323,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_1j7fhlcqkp","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I run Apple silicon with Qwen3 30B and it’s great for most things at 50+ tps. Consumer hardware is not yet at a point where we can run the extremely large models (like Claude’s size) locally at good speeds, but it’ll probably get there in next 5 years.\\n\\nI’d recommend you get something in your budget and use it as much as you can and flip to Claude only when you need it. ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0k934m","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I run Apple silicon with Qwen3 30B and it’s great for most things at 50+ tps. Consumer hardware is not yet at a point where we can run the extremely large models (like Claude’s size) locally at good speeds, but it’ll probably get there in next 5 years.&lt;/p&gt;\\n\\n&lt;p&gt;I’d recommend you get something in your budget and use it as much as you can and flip to Claude only when you need it. &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0k934m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751284323,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hkpsv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"polandtown","can_mod_post":false,"created_utc":1751238812,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_g1ws1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Ok. Want to really learn how to use LLMs at the enterprise level? That's done though the cloud: let that be on an on-premise vlan or a 'traditional' cloud instance. You get to learn the required networking side ontop of everything else, making your portfolio stronger.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hkpsv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ok. Want to really learn how to use LLMs at the enterprise level? That&amp;#39;s done though the cloud: let that be on an on-premise vlan or a &amp;#39;traditional&amp;#39; cloud instance. You get to learn the required networking side ontop of everything else, making your portfolio stronger.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hkpsv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751238812,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hwsnb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fizzy1242","can_mod_post":false,"created_utc":1751243233,"send_replies":true,"parent_id":"t1_n0hw3au","score":3,"author_fullname":"t2_16zcsx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"waste for you, invaluable for others","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hwsnb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;waste for you, invaluable for others&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hwsnb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751243233,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hw3au","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"INtuitiveTJop","can_mod_post":false,"created_utc":1751242966,"send_replies":true,"parent_id":"t3_1lnsax9","score":1,"author_fullname":"t2_u16k63kl","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I paid 2k only to end up not using it. Don’t waste the money","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hw3au","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I paid 2k only to end up not using it. Don’t waste the money&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hw3au/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751242966,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hvitu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"digiwiggles","can_mod_post":false,"created_utc":1751242748,"send_replies":true,"parent_id":"t1_n0hr3lg","score":3,"author_fullname":"t2_5284u","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I've been asking around, and can't get anyone to answer.   So please forgive me, I have to try with you as well.  What have you had good results with R1-0528?  I can't get anything decent code or any work based task out of it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hvitu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve been asking around, and can&amp;#39;t get anyone to answer.   So please forgive me, I have to try with you as well.  What have you had good results with R1-0528?  I can&amp;#39;t get anything decent code or any work based task out of it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnsax9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hvitu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751242748,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hr3lg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Daniel_H212","can_mod_post":false,"created_utc":1751241103,"send_replies":true,"parent_id":"t3_1lnsax9","score":0,"author_fullname":"t2_1vi6fut","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yes, R1-0528 is on a similar level to Claude.\\n\\nNo, you won't be running it locally with any reasonable speed unless you drop enough money to buy a car.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hr3lg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, R1-0528 is on a similar level to Claude.&lt;/p&gt;\\n\\n&lt;p&gt;No, you won&amp;#39;t be running it locally with any reasonable speed unless you drop enough money to buy a car.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsax9/please_convince_me_not_to_get_a_gpu_i_dont_need/n0hr3lg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751241103,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsax9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}}]`);export{e as default};
