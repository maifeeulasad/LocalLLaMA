import{j as e}from"./index-xfnGEtuL.js";import{R as t}from"./RedditPostRenderer-DAZRIZZK.js";import"./index-BaNn5-RR.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Im trying to decide which cpu to get in a mini pc, but am on a budget.  Im okay shelling out for 880m over 780m, but getting mixed messages on performance in llms.\\n\\nI'd like to toss 64 or more ram into the system and run some llms, but i can't tell what if any igpus have support. I can only find the 395max which is way out of my budget.  From people actually running gpu-less, is it reasonable to do this, or is it kinda pointless still?  I will be using windows.\\n\\nIm getting a occulink capable minipc, for potential gpu options in yhe future, but dont want that to be my only option.\\n\\nEdit- im mostly curious about larger models.  I can already run a slow 8b model on my phone.  So I'd be most curious about 30b and 70b models.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"What can I expect from current amd igpu performance?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lw72q8","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.66,"author_flair_background_color":null,"subreddit_type":"public","ups":3,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_ovn8y","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":3,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1752135697,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752134455,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Im trying to decide which cpu to get in a mini pc, but am on a budget.  Im okay shelling out for 880m over 780m, but getting mixed messages on performance in llms.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;d like to toss 64 or more ram into the system and run some llms, but i can&amp;#39;t tell what if any igpus have support. I can only find the 395max which is way out of my budget.  From people actually running gpu-less, is it reasonable to do this, or is it kinda pointless still?  I will be using windows.&lt;/p&gt;\\n\\n&lt;p&gt;Im getting a occulink capable minipc, for potential gpu options in yhe future, but dont want that to be my only option.&lt;/p&gt;\\n\\n&lt;p&gt;Edit- im mostly curious about larger models.  I can already run a slow 8b model on my phone.  So I&amp;#39;d be most curious about 30b and 70b models.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lw72q8","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"plzdonforgetthisname","discussion_type":null,"num_comments":3,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lw72q8/what_can_i_expect_from_current_amd_igpu/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lw72q8/what_can_i_expect_from_current_amd_igpu/","subreddit_subscribers":497354,"created_utc":1752134455,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ceff5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"plzdonforgetthisname","can_mod_post":false,"created_utc":1752146342,"send_replies":true,"parent_id":"t1_n2burv0","score":1,"author_fullname":"t2_ovn8y","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"See i see constantly conflicting info on this.  What the heck is ROCm then?  I see several people, and confirmed info that the 395m ryzen cpu can combine its ram as vram and get incredible results.  So why do I see some people saying igpu does nothing, and others saying its a huge improvement...  I've also seen talks of using Vulcan vs rocm.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ceff5","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;See i see constantly conflicting info on this.  What the heck is ROCm then?  I see several people, and confirmed info that the 395m ryzen cpu can combine its ram as vram and get incredible results.  So why do I see some people saying igpu does nothing, and others saying its a huge improvement...  I&amp;#39;ve also seen talks of using Vulcan vs rocm.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lw72q8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw72q8/what_can_i_expect_from_current_amd_igpu/n2ceff5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752146342,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2burv0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1752135902,"send_replies":true,"parent_id":"t3_1lw72q8","score":1,"author_fullname":"t2_uz37qfx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I doubt if there'd be any performance boost at prompt processing, and there is certainly no boost at token generation, compared to CPU.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2burv0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I doubt if there&amp;#39;d be any performance boost at prompt processing, and there is certainly no boost at token generation, compared to CPU.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw72q8/what_can_i_expect_from_current_amd_igpu/n2burv0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752135902,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lw72q8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2cszeq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Calm-Start-5945","can_mod_post":false,"created_utc":1752151990,"send_replies":true,"parent_id":"t3_1lw72q8","score":1,"author_fullname":"t2_pl6nh0s3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"[https://github.com/ggml-org/llama.cpp/discussions/10879](https://github.com/ggml-org/llama.cpp/discussions/10879) mentions a few iGPUs.\\n\\nAnd  the performance benefit against CPU inference strongly depends on the specific system. For instance, my own 3400G benefits \\\\*a lot\\\\* from Vulkan (\\\\~7x prompt processing, \\\\~3x token generation, and less power usage), but it's an older CPU paired with a comparatively stronger iGPU. The Vulkan driver may also matter: amdvlk has consistently much better pp and worse tg than radv.\\n\\nLarger models also tend to be more sensitive to memory bottlenecks, so I'd expect the difference to get smaller. But even Qwen3-A3B-30B gets a solid \\\\~2x tg speedup in my case.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2cszeq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://github.com/ggml-org/llama.cpp/discussions/10879\\"&gt;https://github.com/ggml-org/llama.cpp/discussions/10879&lt;/a&gt; mentions a few iGPUs.&lt;/p&gt;\\n\\n&lt;p&gt;And  the performance benefit against CPU inference strongly depends on the specific system. For instance, my own 3400G benefits *a lot* from Vulkan (~7x prompt processing, ~3x token generation, and less power usage), but it&amp;#39;s an older CPU paired with a comparatively stronger iGPU. The Vulkan driver may also matter: amdvlk has consistently much better pp and worse tg than radv.&lt;/p&gt;\\n\\n&lt;p&gt;Larger models also tend to be more sensitive to memory bottlenecks, so I&amp;#39;d expect the difference to get smaller. But even Qwen3-A3B-30B gets a solid ~2x tg speedup in my case.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw72q8/what_can_i_expect_from_current_amd_igpu/n2cszeq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752151990,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lw72q8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),s=()=>e.jsx(t,{data:l});export{s as default};
