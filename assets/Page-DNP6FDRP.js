import{j as e}from"./index-xfnGEtuL.js";import{R as t}from"./RedditPostRenderer-DAZRIZZK.js";import"./index-BaNn5-RR.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hi everyone,\\n\\nI'm currently working on a project to fine-tune multilingual embedding models to improve document retrieval within a company's RAG system. The dataset consists of German and English documents related to industrial products, so multilingual support is essential. The dataset has a query-passage format with synthetic generated queries from the given documens.\\n\\n \\n\\nRequirements:\\n\\n* Multilingual (German &amp; English)\\n* Max. 7B parameters\\n* Preferably compatible with Sentence-Transformers\\n* Open-source\\n\\n \\n\\nModels based on MTEB Retrieval performance:\\n\\n[http://mteb-leaderboard.hf.space/?benchmark\\\\_name=MTEB%28Multilingual%2C+v2%29](http://mteb-leaderboard.hf.space/?benchmark_name=MTEB%28Multilingual%2C+v2%29)\\n\\n* Qwen Embedding 8B / 4B\\n* SFR-Embedding-Mistral\\n* E5-mistral-7b-instruct\\n* Snowflake-arctic-embed-m-v2.0\\n\\n \\n\\nI also read some papers and found that the following models were frequently used for fine-tuning embedding models for closed-domain use cases:\\n\\n* BGE (all variants)\\n* mE5\\n* All-MiniLM-L6-v1.5\\n* Text-Embedding-3-Large (often used as a baseline)\\n\\n \\n\\nWould love to hear your thoughts or experiences, especially if you've worked on similar multilingual or domain-specific retrieval systems!\\n\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Fine-Tuning Multilingual Embedding Models for Industrial RAG System","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m68elw","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.9,"author_flair_background_color":null,"subreddit_type":"public","ups":8,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_k6flr0wx","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":8,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1753172119,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m currently working on a project to fine-tune multilingual embedding models to improve document retrieval within a company&amp;#39;s RAG system. The dataset consists of German and English documents related to industrial products, so multilingual support is essential. The dataset has a query-passage format with synthetic generated queries from the given documens.&lt;/p&gt;\\n\\n&lt;p&gt; &lt;/p&gt;\\n\\n&lt;p&gt;Requirements:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Multilingual (German &amp;amp; English)&lt;/li&gt;\\n&lt;li&gt;Max. 7B parameters&lt;/li&gt;\\n&lt;li&gt;Preferably compatible with Sentence-Transformers&lt;/li&gt;\\n&lt;li&gt;Open-source&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt; &lt;/p&gt;\\n\\n&lt;p&gt;Models based on MTEB Retrieval performance:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"http://mteb-leaderboard.hf.space/?benchmark_name=MTEB%28Multilingual%2C+v2%29\\"&gt;http://mteb-leaderboard.hf.space/?benchmark_name=MTEB%28Multilingual%2C+v2%29&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Qwen Embedding 8B / 4B&lt;/li&gt;\\n&lt;li&gt;SFR-Embedding-Mistral&lt;/li&gt;\\n&lt;li&gt;E5-mistral-7b-instruct&lt;/li&gt;\\n&lt;li&gt;Snowflake-arctic-embed-m-v2.0&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt; &lt;/p&gt;\\n\\n&lt;p&gt;I also read some papers and found that the following models were frequently used for fine-tuning embedding models for closed-domain use cases:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;BGE (all variants)&lt;/li&gt;\\n&lt;li&gt;mE5&lt;/li&gt;\\n&lt;li&gt;All-MiniLM-L6-v1.5&lt;/li&gt;\\n&lt;li&gt;Text-Embedding-3-Large (often used as a baseline)&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt; &lt;/p&gt;\\n\\n&lt;p&gt;Would love to hear your thoughts or experiences, especially if you&amp;#39;ve worked on similar multilingual or domain-specific retrieval systems!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/b3_nc9eUM96LdvrtRkpsiSfLCjhmgpLRDj18BCf7ynE.jpeg?auto=webp&amp;s=1c3659b9b728ad6e80974340870a81fcaca748a0","width":2473,"height":1280},"resolutions":[{"url":"https://external-preview.redd.it/b3_nc9eUM96LdvrtRkpsiSfLCjhmgpLRDj18BCf7ynE.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2e4403396b3271cb41e3343fd3daf2f432ae3c37","width":108,"height":55},{"url":"https://external-preview.redd.it/b3_nc9eUM96LdvrtRkpsiSfLCjhmgpLRDj18BCf7ynE.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=afdae3f5636f14bbbbdb4a9c9476beaba37383bf","width":216,"height":111},{"url":"https://external-preview.redd.it/b3_nc9eUM96LdvrtRkpsiSfLCjhmgpLRDj18BCf7ynE.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bcf9a9a3b91639d396787d20b125c83d45d2dd41","width":320,"height":165},{"url":"https://external-preview.redd.it/b3_nc9eUM96LdvrtRkpsiSfLCjhmgpLRDj18BCf7ynE.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5bb4fac65e86637fc3c5896156424954a65818fc","width":640,"height":331},{"url":"https://external-preview.redd.it/b3_nc9eUM96LdvrtRkpsiSfLCjhmgpLRDj18BCf7ynE.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1321b3e29e18141a9cf9837f0ec1dea4f236eabf","width":960,"height":496},{"url":"https://external-preview.redd.it/b3_nc9eUM96LdvrtRkpsiSfLCjhmgpLRDj18BCf7ynE.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=271a28cd1ea8c4260dcf2d7f9dcf3842182678c4","width":1080,"height":558}],"variants":{},"id":"b3_nc9eUM96LdvrtRkpsiSfLCjhmgpLRDj18BCf7ynE"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1m68elw","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Maddin187","discussion_type":null,"num_comments":4,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m68elw/finetuning_multilingual_embedding_models_for/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m68elw/finetuning_multilingual_embedding_models_for/","subreddit_subscribers":502981,"created_utc":1753172119,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4hrjig","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"chenwu777","can_mod_post":false,"created_utc":1753174477,"send_replies":true,"parent_id":"t3_1m68elw","score":3,"author_fullname":"t2_108424lc","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What specific problem are you trying to solve? Is multilingual embedding the main issue causing low relevance in document retrieval? \\n\\nThere are many approaches to document retrieval - embedding is just one of them. You could combine it with keyword search methods to improve success rates, or use rerank models to slightly boost accuracy. \\n\\nAlternatively, you could simply have all documents translated by an LLM before putting them into the knowledge base. \\n\\nAll these approaches would be simpler than fine-tuning your own model.","edited":1753175458,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4hrjig","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What specific problem are you trying to solve? Is multilingual embedding the main issue causing low relevance in document retrieval? &lt;/p&gt;\\n\\n&lt;p&gt;There are many approaches to document retrieval - embedding is just one of them. You could combine it with keyword search methods to improve success rates, or use rerank models to slightly boost accuracy. &lt;/p&gt;\\n\\n&lt;p&gt;Alternatively, you could simply have all documents translated by an LLM before putting them into the knowledge base. &lt;/p&gt;\\n\\n&lt;p&gt;All these approaches would be simpler than fine-tuning your own model.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m68elw/finetuning_multilingual_embedding_models_for/n4hrjig/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753174477,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m68elw","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4hsjes","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HistorianPotential48","can_mod_post":false,"created_utc":1753175069,"send_replies":true,"parent_id":"t3_1m68elw","score":1,"author_fullname":"t2_4dzthia7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"[https://zenn.dev/microsoft/articles/rag\\\\_textbook#rag-%E3%81%AE%E7%B2%BE%E5%BA%A6%E6%94%B9%E5%96%84%E3%81%AE%E9%80%B2%E3%82%81%E6%96%B9](https://zenn.dev/microsoft/articles/rag_textbook#rag-%E3%81%AE%E7%B2%BE%E5%BA%A6%E6%94%B9%E5%96%84%E3%81%AE%E9%80%B2%E3%82%81%E6%96%B9)\\n\\nGreat post for RAG refinement. Use translator if you don't know Japanese. One issue is that is finetuning really necessary? Did you already refined every other steps to best?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4hsjes","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://zenn.dev/microsoft/articles/rag_textbook#rag-%E3%81%AE%E7%B2%BE%E5%BA%A6%E6%94%B9%E5%96%84%E3%81%AE%E9%80%B2%E3%82%81%E6%96%B9\\"&gt;https://zenn.dev/microsoft/articles/rag_textbook#rag-%E3%81%AE%E7%B2%BE%E5%BA%A6%E6%94%B9%E5%96%84%E3%81%AE%E9%80%B2%E3%82%81%E6%96%B9&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Great post for RAG refinement. Use translator if you don&amp;#39;t know Japanese. One issue is that is finetuning really necessary? Did you already refined every other steps to best?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m68elw/finetuning_multilingual_embedding_models_for/n4hsjes/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753175069,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m68elw","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"fe89e94a-13f2-11f0-a9de-6262c74956cf","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4hvjrl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Asleep-Ratio7535","can_mod_post":false,"created_utc":1753176823,"send_replies":true,"parent_id":"t3_1m68elw","score":1,"author_fullname":"t2_1lfyddwf0c","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Qwen3? I have no luck with them... But maybe gguf problem. You can try jina, cohere, they are quite focusing on embedding too, especially multilingual ones. I hope you can post your findings later ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4hvjrl","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 4"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen3? I have no luck with them... But maybe gguf problem. You can try jina, cohere, they are quite focusing on embedding too, especially multilingual ones. I hope you can post your findings later &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m68elw/finetuning_multilingual_embedding_models_for/n4hvjrl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753176823,"author_flair_text":"Llama 4","treatment_tags":[],"link_id":"t3_1m68elw","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#b0ae9b","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4iey5b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"balerion20","can_mod_post":false,"created_utc":1753185995,"send_replies":true,"parent_id":"t3_1m68elw","score":1,"author_fullname":"t2_weyoo","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I actually working on a similar project. Only issue was I didn’t have the necessary dataset but I generated a synthetic one and finetuned first bge-m3 then qwen3 4b. Both finetuning improved the performance of retrieval on synthetic dataset but I am not sure I saw genuine improvement on real project. I am still assessing, however I am pretty sure it would have genuine improvement with proper dataset. \\n\\nQwen 3 finetuning a bit harder because framework was little bit different but it is doable\\n\\nI followed [this](https://docs.llamaindex.ai/en/stable/examples/finetuning/embeddings/finetune_embedding/) guide from llamaindex.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4iey5b","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I actually working on a similar project. Only issue was I didn’t have the necessary dataset but I generated a synthetic one and finetuned first bge-m3 then qwen3 4b. Both finetuning improved the performance of retrieval on synthetic dataset but I am not sure I saw genuine improvement on real project. I am still assessing, however I am pretty sure it would have genuine improvement with proper dataset. &lt;/p&gt;\\n\\n&lt;p&gt;Qwen 3 finetuning a bit harder because framework was little bit different but it is doable&lt;/p&gt;\\n\\n&lt;p&gt;I followed &lt;a href=\\"https://docs.llamaindex.ai/en/stable/examples/finetuning/embeddings/finetune_embedding/\\"&gt;this&lt;/a&gt; guide from llamaindex.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m68elw/finetuning_multilingual_embedding_models_for/n4iey5b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753185995,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m68elw","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),i=()=>e.jsx(t,{data:l});export{i as default};
