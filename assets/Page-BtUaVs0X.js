import{j as e}from"./index-F0NXdzZX.js";import{R as l}from"./RedditPostRenderer-CoEZB1dt.js";import"./index-DrtravXm.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"After reading all the amazing post of yours, I've bought in. About to offer my management a localized coding agent, to prevent code and API keys leaks. From 20 to 50 people coding at any moment.\\n\\nLocally I'd need a used 3080+ card. But what type of the hardware I'm looking for to provide for 20+ folks?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Help needed: 20+ devs on the local model","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m00yn1","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.14,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_31gku","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752532847,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;After reading all the amazing post of yours, I&amp;#39;ve bought in. About to offer my management a localized coding agent, to prevent code and API keys leaks. From 20 to 50 people coding at any moment.&lt;/p&gt;\\n\\n&lt;p&gt;Locally I&amp;#39;d need a used 3080+ card. But what type of the hardware I&amp;#39;m looking for to provide for 20+ folks?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m00yn1","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"3dom","discussion_type":null,"num_comments":13,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m00yn1/help_needed_20_devs_on_the_local_model/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m00yn1/help_needed_20_devs_on_the_local_model/","subreddit_subscribers":499295,"created_utc":1752532847,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n36r5d0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LA_rent_Aficionado","can_mod_post":false,"send_replies":true,"parent_id":"t1_n36qqr2","score":1,"author_fullname":"t2_t8zbiflk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Vllm is definitely the GOAT for parallel requests and optimization ðŸ’¯ I assumed theyâ€™re thinking coding - thereâ€™s definitely a lot of promise on the horizon when it comes to agenetic coding local models.","edited":false,"author_flair_css_class":null,"name":"t1_n36r5d0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Vllm is definitely the GOAT for parallel requests and optimization ðŸ’¯ I assumed theyâ€™re thinking coding - thereâ€™s definitely a lot of promise on the horizon when it comes to agenetic coding local models.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m00yn1","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m00yn1/help_needed_20_devs_on_the_local_model/n36r5d0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752544210,"author_flair_text":null,"collapsed":false,"created_utc":1752544210,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n36qqr2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Capable-Ad-7494","can_mod_post":false,"send_replies":true,"parent_id":"t1_n36nzs9","score":2,"author_fullname":"t2_9so78ol2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"thatâ€™s the limits of local coding, just telling him what he needs for what he specified though, for a large serving environment.\\n\\ncan also recommend lm cache if you want to save on hardware and have kv cache get offloaded nicely ish on vllm","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n36qqr2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;thatâ€™s the limits of local coding, just telling him what he needs for what he specified though, for a large serving environment.&lt;/p&gt;\\n\\n&lt;p&gt;can also recommend lm cache if you want to save on hardware and have kv cache get offloaded nicely ish on vllm&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m00yn1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m00yn1/help_needed_20_devs_on_the_local_model/n36qqr2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752544068,"author_flair_text":null,"treatment_tags":[],"created_utc":1752544068,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n36nzs9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LA_rent_Aficionado","can_mod_post":false,"send_replies":true,"parent_id":"t1_n36fz5b","score":2,"author_fullname":"t2_t8zbiflk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Depends on the workflow, sounds like they'd be using agentic coding which frankly not many local models perform well at.  Even with 128GB vram and Qwen3 235B cline hits a wall as you start to climb up the context ladder, that's less pronounced with enterprise models - not even considering the fewer iterations you'd get with a smarter model and the knowledge gap.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n36nzs9","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Depends on the workflow, sounds like they&amp;#39;d be using agentic coding which frankly not many local models perform well at.  Even with 128GB vram and Qwen3 235B cline hits a wall as you start to climb up the context ladder, that&amp;#39;s less pronounced with enterprise models - not even considering the fewer iterations you&amp;#39;d get with a smarter model and the knowledge gap.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m00yn1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m00yn1/help_needed_20_devs_on_the_local_model/n36nzs9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752543104,"author_flair_text":null,"treatment_tags":[],"created_utc":1752543104,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n36fz5b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Capable-Ad-7494","can_mod_post":false,"created_utc":1752540326,"send_replies":true,"parent_id":"t1_n369j0p","score":2,"author_fullname":"t2_9so78ol2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"i like my 5090, think iâ€™ve managed to batch out over 12m tokens from qwen 32b so far, far cry from the 9 billion tokenâ€™s iâ€™d need to pay for it in llm api usage, but whatever","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n36fz5b","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;i like my 5090, think iâ€™ve managed to batch out over 12m tokens from qwen 32b so far, far cry from the 9 billion tokenâ€™s iâ€™d need to pay for it in llm api usage, but whatever&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m00yn1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m00yn1/help_needed_20_devs_on_the_local_model/n36fz5b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752540326,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n369j0p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Pale_Ad_6029","can_mod_post":false,"created_utc":1752538102,"send_replies":true,"parent_id":"t3_1m00yn1","score":7,"author_fullname":"t2_d8fd8nuk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The amount ur spending on local hardware, will fund those ppls AI costs for 2-4 years. Your hardware will become obsolete in the next year when more vRam infused cards come out and more hungry LLMs are out","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n369j0p","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The amount ur spending on local hardware, will fund those ppls AI costs for 2-4 years. Your hardware will become obsolete in the next year when more vRam infused cards come out and more hungry LLMs are out&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m00yn1/help_needed_20_devs_on_the_local_model/n369j0p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752538102,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m00yn1","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n36flr1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Capable-Ad-7494","can_mod_post":false,"send_replies":true,"parent_id":"t1_n35v0kk","score":2,"author_fullname":"t2_9so78ol2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah i would get a 3090 or two. 1.2k, run it in vllm in tensor parallel, each card will give enough context after weights to have most userâ€™s not notice any kv cache swaps should the available kv cache get saturated","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n36flr1","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah i would get a 3090 or two. 1.2k, run it in vllm in tensor parallel, each card will give enough context after weights to have most userâ€™s not notice any kv cache swaps should the available kv cache get saturated&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m00yn1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m00yn1/help_needed_20_devs_on_the_local_model/n36flr1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752540198,"author_flair_text":null,"treatment_tags":[],"created_utc":1752540198,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n37has3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Shivacious","can_mod_post":false,"send_replies":true,"parent_id":"t1_n35v0kk","score":2,"author_fullname":"t2_chxnc83m9","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ask the management for the budget first. i would suggest a mi325x  (good for inference only tuning recently added) or a rtx 6000 either pro. Goes long way good","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n37has3","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ask the management for the budget first. i would suggest a mi325x  (good for inference only tuning recently added) or a rtx 6000 either pro. Goes long way good&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m00yn1","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m00yn1/help_needed_20_devs_on_the_local_model/n37has3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752554465,"author_flair_text":"Llama 405B","treatment_tags":[],"created_utc":1752554465,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n37q77x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"perelmanych","can_mod_post":false,"send_replies":true,"parent_id":"t1_n35v0kk","score":2,"author_fullname":"t2_63q8kong","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"7-8b model will be good only for auto compete. Qwen3 32b q6+ will be ok for chat. You may even try it for agentic use, but don't expect much. So if you are limited in budget then I would say at least two 3090, the more the better. If you have some money to spend get 2+ of 6000 Pro.","edited":1752559168,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n37q77x","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;7-8b model will be good only for auto compete. Qwen3 32b q6+ will be ok for chat. You may even try it for agentic use, but don&amp;#39;t expect much. So if you are limited in budget then I would say at least two 3090, the more the better. If you have some money to spend get 2+ of 6000 Pro.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m00yn1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m00yn1/help_needed_20_devs_on_the_local_model/n37q77x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752558834,"author_flair_text":null,"treatment_tags":[],"created_utc":1752558834,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n35v0kk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"3dom","can_mod_post":false,"created_utc":1752533348,"send_replies":true,"parent_id":"t1_n35u2f9","score":1,"author_fullname":"t2_31gku","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"7-8B 4Q minimum, 32k context up to 128k+\\n\\nCode base size if about 5Mb (couple Bibles), four languages are in use.\\n\\nRAM/CPU size doesn't really matter, we have a unique situation where we can scale x50 daily no problem.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n35v0kk","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;7-8B 4Q minimum, 32k context up to 128k+&lt;/p&gt;\\n\\n&lt;p&gt;Code base size if about 5Mb (couple Bibles), four languages are in use.&lt;/p&gt;\\n\\n&lt;p&gt;RAM/CPU size doesn&amp;#39;t really matter, we have a unique situation where we can scale x50 daily no problem.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m00yn1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m00yn1/help_needed_20_devs_on_the_local_model/n35v0kk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752533348,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n35u2f9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Shivacious","can_mod_post":false,"created_utc":1752533043,"send_replies":true,"parent_id":"t3_1m00yn1","score":4,"author_fullname":"t2_chxnc83m9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Model parameter size? Well slap it ob a h100","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n35u2f9","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Model parameter size? Well slap it ob a h100&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m00yn1/help_needed_20_devs_on_the_local_model/n35u2f9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752533043,"author_flair_text":"Llama 405B","treatment_tags":[],"link_id":"t3_1m00yn1","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n362r5b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"3dom","can_mod_post":false,"created_utc":1752535846,"send_replies":true,"parent_id":"t1_n35vzdv","score":1,"author_fullname":"t2_31gku","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; What is happening with the way people ask questions &amp; why wouldn't you use free OpenRouter bleeding edge 10X better model?\\n\\nThanks for the direction! ... I don't understand what OR does exactly and how is it useful? Ive seen LLM caches working exactly like OR, but for \\"free\\" (on my own hardware, they can provide x3 boost)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n362r5b","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;What is happening with the way people ask questions &amp;amp; why wouldn&amp;#39;t you use free OpenRouter bleeding edge 10X better model?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Thanks for the direction! ... I don&amp;#39;t understand what OR does exactly and how is it useful? Ive seen LLM caches working exactly like OR, but for &amp;quot;free&amp;quot; (on my own hardware, they can provide x3 boost)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m00yn1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m00yn1/help_needed_20_devs_on_the_local_model/n362r5b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752535846,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n35vzdv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Johnroberts95000","can_mod_post":false,"created_utc":1752533657,"send_replies":true,"parent_id":"t3_1m00yn1","score":1,"author_fullname":"t2_x4n1v","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What is happening with the way people ask questions &amp; why wouldn't you use free OpenRouter bleeding edge 10X better model?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n35vzdv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What is happening with the way people ask questions &amp;amp; why wouldn&amp;#39;t you use free OpenRouter bleeding edge 10X better model?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m00yn1/help_needed_20_devs_on_the_local_model/n35vzdv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752533657,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m00yn1","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n363zcd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LA_rent_Aficionado","can_mod_post":false,"created_utc":1752536245,"send_replies":true,"parent_id":"t3_1m00yn1","score":2,"author_fullname":"t2_t8zbiflk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For any decent model that competes with APIs with max context Iâ€™d recommend at least dropping 36-40k on 4x RTX 6000 and a server board with at least 512GB of RAM - go big or go home!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n363zcd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For any decent model that competes with APIs with max context Iâ€™d recommend at least dropping 36-40k on 4x RTX 6000 and a server board with at least 512GB of RAM - go big or go home!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m00yn1/help_needed_20_devs_on_the_local_model/n363zcd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752536245,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m00yn1","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),n=()=>e.jsx(l,{data:a});export{n as default};
