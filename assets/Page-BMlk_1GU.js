import{j as e}from"./index-Bqs-ekb2.js";import{R as l}from"./RedditPostRenderer-DUVdf0-i.js";import"./index-D52ORTDm.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"If we can make some models that can \\"reason\\" very well but lack a lot of knowledge, isnt it generaly cheaper to just have a small model + added context from a web search api? \\n\\nAre there some pipelines that exist on github or somewhere of such a project? \\n\\nI wanted to try out something like qwen3-8b-r1 + web search and possibly python scripts tool calling to have a solid model even with limited internal knowledge. ","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Very small high scores models + web search?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lp5lu3","quarantine":false,"link_flair_text_color":"light","upvote_ratio":1,"author_flair_background_color":null,"subreddit_type":"public","ups":1,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1p50pl73j2","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":1,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751385944,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;If we can make some models that can &amp;quot;reason&amp;quot; very well but lack a lot of knowledge, isnt it generaly cheaper to just have a small model + added context from a web search api? &lt;/p&gt;\\n\\n&lt;p&gt;Are there some pipelines that exist on github or somewhere of such a project? &lt;/p&gt;\\n\\n&lt;p&gt;I wanted to try out something like qwen3-8b-r1 + web search and possibly python scripts tool calling to have a solid model even with limited internal knowledge. &lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lp5lu3","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"GreenTreeAndBlueSky","discussion_type":null,"num_comments":6,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lp5lu3/very_small_high_scores_models_web_search/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lp5lu3/very_small_high_scores_models_web_search/","subreddit_subscribers":493457,"created_utc":1751385944,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0s92ue","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MotorNetwork380","can_mod_post":false,"created_utc":1751387545,"send_replies":true,"parent_id":"t3_1lp5lu3","score":5,"author_fullname":"t2_1rff20s0ca","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"this supports ollama and tavily search api: [https://github.com/mdillondc/terminal-ai](https://github.com/mdillondc/terminal-ai)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0s92ue","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;this supports ollama and tavily search api: &lt;a href=\\"https://github.com/mdillondc/terminal-ai\\"&gt;https://github.com/mdillondc/terminal-ai&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp5lu3/very_small_high_scores_models_web_search/n0s92ue/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751387545,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp5lu3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0sy6f3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Zigtronik","can_mod_post":false,"created_utc":1751394417,"send_replies":true,"parent_id":"t3_1lp5lu3","score":2,"author_fullname":"t2_t5cnu","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It will likely work in many respects, but at this point we can clearly see that LLMs develop world models and relational concepts that I don't think in context learning can alleviate. So sacrificing generalized use cases for size and speed.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0sy6f3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It will likely work in many respects, but at this point we can clearly see that LLMs develop world models and relational concepts that I don&amp;#39;t think in context learning can alleviate. So sacrificing generalized use cases for size and speed.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp5lu3/very_small_high_scores_models_web_search/n0sy6f3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751394417,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp5lu3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0vsgu1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"eggs-benedryl","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0vlj46","score":1,"author_fullname":"t2_8nlxwtdi","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"i litearlly only know the 2 models that do that, i'd be happy to hear about more models. every other model i've tried can do the hand off but not anything more","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0vsgu1","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;i litearlly only know the 2 models that do that, i&amp;#39;d be happy to hear about more models. every other model i&amp;#39;ve tried can do the hand off but not anything more&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp5lu3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp5lu3/very_small_high_scores_models_web_search/n0vsgu1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751427503,"author_flair_text":null,"treatment_tags":[],"created_utc":1751427503,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0vlj46","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Voxandr","can_mod_post":false,"created_utc":1751424790,"send_replies":true,"parent_id":"t1_n0tbayv","score":1,"author_fullname":"t2_86dk0gye","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Another jan-Nano ads? It's really bad...","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0vlj46","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Another jan-Nano ads? It&amp;#39;s really bad...&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp5lu3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp5lu3/very_small_high_scores_models_web_search/n0vlj46/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751424790,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0tbayv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"eggs-benedryl","can_mod_post":false,"created_utc":1751398160,"send_replies":true,"parent_id":"t3_1lp5lu3","score":1,"author_fullname":"t2_8nlxwtdi","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"A lot of models support tool calling. Until recently, and as far as I'm aware you needed to design workflows with various agents.\\n\\nRecently there's two models i'm aware of Arc and Jan-Nano that can call tools, read the info and repeat the process as necessary all autonomously, choosing when to do what. \\n\\nYou can hook these up to MCP agents and do this. I have a wikipedia agent, a web search, a lyric search and so on, on my PC. \\n\\nI use Ollama\\n\\nJan-Nano\\n\\nWitsy for UI and MCP managment","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0tbayv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;A lot of models support tool calling. Until recently, and as far as I&amp;#39;m aware you needed to design workflows with various agents.&lt;/p&gt;\\n\\n&lt;p&gt;Recently there&amp;#39;s two models i&amp;#39;m aware of Arc and Jan-Nano that can call tools, read the info and repeat the process as necessary all autonomously, choosing when to do what. &lt;/p&gt;\\n\\n&lt;p&gt;You can hook these up to MCP agents and do this. I have a wikipedia agent, a web search, a lyric search and so on, on my PC. &lt;/p&gt;\\n\\n&lt;p&gt;I use Ollama&lt;/p&gt;\\n\\n&lt;p&gt;Jan-Nano&lt;/p&gt;\\n\\n&lt;p&gt;Witsy for UI and MCP managment&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp5lu3/very_small_high_scores_models_web_search/n0tbayv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751398160,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp5lu3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0vxdaq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"godndiogoat","can_mod_post":false,"created_utc":1751429564,"send_replies":true,"parent_id":"t3_1lp5lu3","score":1,"author_fullname":"t2_1o8b7or53v","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Running a slim 8B model with real-time retrieval beats throwing VRAM at a huge checkpoint. I wire up qwen-7B with a DuckDuckGo scrape, chunk the top docs with a simple sentence splitter, embed them, then feed the best 10 chunks back as context; latency stays under two seconds on a 3060 while answers jump a couple of eval points. LangChain’s RetrievalQA template or Haystack’s WebRetriever node can get you there with almost no code. If you want tool calls, LlamaIndex’s agent runner already chains search -&gt; python -&gt; model. For a lighter footprint I moved the orchestration to Flyte and squeezed another 200ms off; APIWrapper.ai let me swap between OpenRouter and a local GGUF without rewriting logic. A tuned retrieval pipeline will squeeze surprising mileage out of that little model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0vxdaq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Running a slim 8B model with real-time retrieval beats throwing VRAM at a huge checkpoint. I wire up qwen-7B with a DuckDuckGo scrape, chunk the top docs with a simple sentence splitter, embed them, then feed the best 10 chunks back as context; latency stays under two seconds on a 3060 while answers jump a couple of eval points. LangChain’s RetrievalQA template or Haystack’s WebRetriever node can get you there with almost no code. If you want tool calls, LlamaIndex’s agent runner already chains search -&amp;gt; python -&amp;gt; model. For a lighter footprint I moved the orchestration to Flyte and squeezed another 200ms off; APIWrapper.ai let me swap between OpenRouter and a local GGUF without rewriting logic. A tuned retrieval pipeline will squeeze surprising mileage out of that little model.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp5lu3/very_small_high_scores_models_web_search/n0vxdaq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751429564,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp5lu3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(l,{data:a});export{n as default};
