import{j as e}from"./index-CmSyeZDT.js";import{R as l}from"./RedditPostRenderer-C2Zg39IK.js";import"./index-CiTZuv6Z.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Would an AMD Ryzen 7 5700G with 32, 64 or 128 GB be enough for initial experiments with local LLMs?  Just to study and practice the technology, without expectations about performance. Thank you.\\n\\n--\\n\\n**EDIT:** I'd also have the option to add a GPU card later for more demanding tasks.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"AMD 5700G for experimenting with local LLMs?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1loywkt","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.5,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_e9hdj","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1751369276,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751368344,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Would an AMD Ryzen 7 5700G with 32, 64 or 128 GB be enough for initial experiments with local LLMs?  Just to study and practice the technology, without expectations about performance. Thank you.&lt;/p&gt;\\n\\n&lt;h2&gt;&lt;/h2&gt;\\n\\n&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; I&amp;#39;d also have the option to add a GPU card later for more demanding tasks.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1loywkt","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Taikal","discussion_type":null,"num_comments":8,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1loywkt/amd_5700g_for_experimenting_with_local_llms/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1loywkt/amd_5700g_for_experimenting_with_local_llms/","subreddit_subscribers":493458,"created_utc":1751368344,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0qpe3o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MrHumanist","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0qoi2o","score":1,"author_fullname":"t2_6ao2t6i0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"There are many small models you can easily run, but the inference speed will be slow like 3-5 words a sec. But good enough for learning.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0qpe3o","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There are many small models you can easily run, but the inference speed will be slow like 3-5 words a sec. But good enough for learning.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1loywkt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loywkt/amd_5700g_for_experimenting_with_local_llms/n0qpe3o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751369809,"author_flair_text":null,"treatment_tags":[],"created_utc":1751369809,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0qoi2o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Taikal","can_mod_post":false,"created_utc":1751369415,"send_replies":true,"parent_id":"t1_n0qnpld","score":1,"author_fullname":"t2_e9hdj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I've edited my question to add that I'd also have the option to add a GPU card later for more demanding tasks.  Would that allow to experiment with graphics capabilities?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0qoi2o","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve edited my question to add that I&amp;#39;d also have the option to add a GPU card later for more demanding tasks.  Would that allow to experiment with graphics capabilities?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1loywkt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loywkt/amd_5700g_for_experimenting_with_local_llms/n0qoi2o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751369415,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0qnpld","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MrHumanist","can_mod_post":false,"created_utc":1751369064,"send_replies":true,"parent_id":"t3_1loywkt","score":1,"author_fullname":"t2_6ao2t6i0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yes.. 3-8 B parameter models will run fine.. but models can't use their graphics capabilities. So, as a cpu this will be quite slow for llm. Better go for 7700x/9700x with a 5060/5070 12gb if you are building a new pc under 2000$.","edited":1751369321,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0qnpld","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes.. 3-8 B parameter models will run fine.. but models can&amp;#39;t use their graphics capabilities. So, as a cpu this will be quite slow for llm. Better go for 7700x/9700x with a 5060/5070 12gb if you are building a new pc under 2000$.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loywkt/amd_5700g_for_experimenting_with_local_llms/n0qnpld/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751369064,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1loywkt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0qq94l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MDT-49","can_mod_post":false,"created_utc":1751370182,"send_replies":true,"parent_id":"t3_1loywkt","score":1,"author_fullname":"t2_h8yrica5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Sure! If you don't care about performance, you can run anything that fits into the RAM. I think a MoE like Qwen3-30B-A3 would be the optimal LLM right now for your specs. \\n\\nYou could also experiment using the iGPU (using Vulkan back-end in llama.cpp) instead of the CPU. As far as I know, using the iGPU is often faster for prompt processing compared to using the CPU. I don't think there would be much of a difference for token generation as the bottleneck would be the limited RAM bandwidth.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0qq94l","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sure! If you don&amp;#39;t care about performance, you can run anything that fits into the RAM. I think a MoE like Qwen3-30B-A3 would be the optimal LLM right now for your specs. &lt;/p&gt;\\n\\n&lt;p&gt;You could also experiment using the iGPU (using Vulkan back-end in llama.cpp) instead of the CPU. As far as I know, using the iGPU is often faster for prompt processing compared to using the CPU. I don&amp;#39;t think there would be much of a difference for token generation as the bottleneck would be the limited RAM bandwidth.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loywkt/amd_5700g_for_experimenting_with_local_llms/n0qq94l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751370182,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1loywkt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0vl6ry","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"coolestmage","can_mod_post":false,"created_utc":1751424661,"send_replies":true,"parent_id":"t3_1loywkt","score":1,"author_fullname":"t2_6dtdz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You can get the igpu in the 5700g working with ROCM, you just have to set the correct override. I've done it, you should be able to run some smaller models fine.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0vl6ry","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can get the igpu in the 5700g working with ROCM, you just have to set the correct override. I&amp;#39;ve done it, you should be able to run some smaller models fine.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loywkt/amd_5700g_for_experimenting_with_local_llms/n0vl6ry/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751424661,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1loywkt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0qz53l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AliNT77","can_mod_post":false,"created_utc":1751373705,"send_replies":true,"parent_id":"t3_1loywkt","score":1,"author_fullname":"t2_66tlmx2l","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"5700g has a strong memory controller, meaning you can run very high clocks on your ram. You should be able to run 4200mhz+ on it quite easily. With those speeds you can get around 27 tokens/s with qwen3 30B-A3B which is more than usable.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0qz53l","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;5700g has a strong memory controller, meaning you can run very high clocks on your ram. You should be able to run 4200mhz+ on it quite easily. With those speeds you can get around 27 tokens/s with qwen3 30B-A3B which is more than usable.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loywkt/amd_5700g_for_experimenting_with_local_llms/n0qz53l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751373705,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1loywkt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0sagj1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Taikal","can_mod_post":false,"created_utc":1751387935,"send_replies":true,"parent_id":"t1_n0qsfc0","score":1,"author_fullname":"t2_e9hdj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; what exactly would you like to \\"study\\"?\\n\\nWell, I still know nothing about this subject.  I'd like to gain some confidence with this technology by giving a look \\"under the hood\\" by running my own LLM. I doubt that I'll ever reach the level where one programs LLMs.\\n\\n&gt; if you need a PC you can add GPUs to, get whatever with DDR5\\n\\nUnfortunately, a more modern system with DDR5 would go way over budget.  If I can only run llama.cpp, I'll make do with it. If, due to circumstances I can't foresee now, I need or want to go beyond that, I'll make the required investment.\\n\\nThank you for chiming in.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0sagj1","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;what exactly would you like to &amp;quot;study&amp;quot;?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Well, I still know nothing about this subject.  I&amp;#39;d like to gain some confidence with this technology by giving a look &amp;quot;under the hood&amp;quot; by running my own LLM. I doubt that I&amp;#39;ll ever reach the level where one programs LLMs.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;if you need a PC you can add GPUs to, get whatever with DDR5&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Unfortunately, a more modern system with DDR5 would go way over budget.  If I can only run llama.cpp, I&amp;#39;ll make do with it. If, due to circumstances I can&amp;#39;t foresee now, I need or want to go beyond that, I&amp;#39;ll make the required investment.&lt;/p&gt;\\n\\n&lt;p&gt;Thank you for chiming in.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1loywkt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loywkt/amd_5700g_for_experimenting_with_local_llms/n0sagj1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751387935,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0qsfc0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"gpupoor","can_mod_post":false,"created_utc":1751371091,"send_replies":true,"parent_id":"t3_1loywkt","score":0,"author_fullname":"t2_1hcyral852","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"what exactly would you like to \\"study\\"? if you want to learn GPU programming an AMD platform is a very bad choice, documentation and courses are much more sparse than with CUDA, and you probably can't do anything since the 5700G has never been supporred by ROCm.\\n\\n\\nif you're only looking to learn how to use inference engines, it's a bad choice all the same because you literally can't use anything but llama.cpp.\\n\\n\\nalso, its performance is way too low, you can just run small models like Qwen3 30B-A3B and similar, so you'd have no use for 128GB, or even 64.\\n\\n\\nuse the money you'd spend for a 128GB 5700G setup on the cheapest Ampere card you can find. if you need a PC you can add GPUs to, get whatever with DDR5","edited":1751381458,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0qsfc0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;what exactly would you like to &amp;quot;study&amp;quot;? if you want to learn GPU programming an AMD platform is a very bad choice, documentation and courses are much more sparse than with CUDA, and you probably can&amp;#39;t do anything since the 5700G has never been supporred by ROCm.&lt;/p&gt;\\n\\n&lt;p&gt;if you&amp;#39;re only looking to learn how to use inference engines, it&amp;#39;s a bad choice all the same because you literally can&amp;#39;t use anything but llama.cpp.&lt;/p&gt;\\n\\n&lt;p&gt;also, its performance is way too low, you can just run small models like Qwen3 30B-A3B and similar, so you&amp;#39;d have no use for 128GB, or even 64.&lt;/p&gt;\\n\\n&lt;p&gt;use the money you&amp;#39;d spend for a 128GB 5700G setup on the cheapest Ampere card you can find. if you need a PC you can add GPUs to, get whatever with DDR5&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loywkt/amd_5700g_for_experimenting_with_local_llms/n0qsfc0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751371091,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1loywkt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}}]`),r=()=>e.jsx(l,{data:t});export{r as default};
