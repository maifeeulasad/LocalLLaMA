import{j as e}from"./index-DLSqWzaI.js";import{R as t}from"./RedditPostRenderer-CysRo2D_.js";import"./index-COXiL3Lo.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I'm trying to make a fetch\\\\_url tool using MCP:  \\n[https://github.com/modelcontextprotocol](https://github.com/modelcontextprotocol)\\n\\nSetup: LMStudio + Qwen32b / Gemma27b / Gemma12b / DeepSeek R1 (Qwen3 distil)\\n\\nWhen I ask the model to get a URL, it successfully calls the fetch\\\\_url function (and gets a correct response). However, it doesn't understand that it has to stop and keeps calling the same tool again and again. \\n\\nI also have another add\\\\_num function (copied from the docs) which works perfectly. I've tested this on Qwen32b, Gemma 27b (and below) and all have the same issue. \\n\\nAnyone has had this issue? Is there some hidden flag that tells the model to stop calling a tool repeatedly -- even if it was a success? ","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"MCP tool development -- repeated calls with no further processing","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lobqvc","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.4,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1gdl5ph","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1751300210,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m trying to make a fetch_url tool using MCP:&lt;br/&gt;\\n&lt;a href=\\"https://github.com/modelcontextprotocol\\"&gt;https://github.com/modelcontextprotocol&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Setup: LMStudio + Qwen32b / Gemma27b / Gemma12b / DeepSeek R1 (Qwen3 distil)&lt;/p&gt;\\n\\n&lt;p&gt;When I ask the model to get a URL, it successfully calls the fetch_url function (and gets a correct response). However, it doesn&amp;#39;t understand that it has to stop and keeps calling the same tool again and again. &lt;/p&gt;\\n\\n&lt;p&gt;I also have another add_num function (copied from the docs) which works perfectly. I&amp;#39;ve tested this on Qwen32b, Gemma 27b (and below) and all have the same issue. &lt;/p&gt;\\n\\n&lt;p&gt;Anyone has had this issue? Is there some hidden flag that tells the model to stop calling a tool repeatedly -- even if it was a success? &lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/8lBwgtz0GifbCTXgYXAe7KwHqB6r9d6n6NtOb1minSs.png?auto=webp&amp;s=686c93b395f20d312be0379d158ae4e2d731b944","width":280,"height":280},"resolutions":[{"url":"https://external-preview.redd.it/8lBwgtz0GifbCTXgYXAe7KwHqB6r9d6n6NtOb1minSs.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3ad6a7b78d99e504bc515c83cbb4a5e75372c770","width":108,"height":108},{"url":"https://external-preview.redd.it/8lBwgtz0GifbCTXgYXAe7KwHqB6r9d6n6NtOb1minSs.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6bdd63c35158b4c37a8f8551569f704d14850df4","width":216,"height":216}],"variants":{},"id":"8lBwgtz0GifbCTXgYXAe7KwHqB6r9d6n6NtOb1minSs"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lobqvc","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"nuketro0p3r","discussion_type":null,"num_comments":8,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lobqvc/mcp_tool_development_repeated_calls_with_no/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lobqvc/mcp_tool_development_repeated_calls_with_no/","subreddit_subscribers":493240,"created_utc":1751300210,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"d2642412-d9ce-11ed-ae30-32b11309f5bd","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0m9khw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"nuketro0p3r","can_mod_post":false,"created_utc":1751306810,"send_replies":true,"parent_id":"t1_n0ly5fs","score":1,"author_fullname":"t2_1gdl5ph","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I prepend the following to the model response and it still keep on repeating it's message and calling the tool again and again\\n\\n\\"The fetch\\\\_url\\\\_text\\\\_content returned the HTML successfully.  \\n        Do not call this tool again for this specific request.  \\n        This successful result is enough to begin processing.  \\n        Following is the HTML that was retrieved.\\"","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0m9khw","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I prepend the following to the model response and it still keep on repeating it&amp;#39;s message and calling the tool again and again&lt;/p&gt;\\n\\n&lt;p&gt;&amp;quot;The fetch_url_text_content returned the HTML successfully.&lt;br/&gt;\\n        Do not call this tool again for this specific request.&lt;br/&gt;\\n        This successful result is enough to begin processing.&lt;br/&gt;\\n        Following is the HTML that was retrieved.&amp;quot;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lobqvc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lobqvc/mcp_tool_development_repeated_calls_with_no/n0m9khw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751306810,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ly5fs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Eisenstein","can_mod_post":false,"created_utc":1751303689,"send_replies":true,"parent_id":"t3_1lobqvc","score":3,"author_fullname":"t2_5aiux","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You need to return a string that tells it that it worked, along with the results. MCP is a way to populate the model's context. If it doesn't get information back, it has no idea what happened.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ly5fs","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Alpaca"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You need to return a string that tells it that it worked, along with the results. MCP is a way to populate the model&amp;#39;s context. If it doesn&amp;#39;t get information back, it has no idea what happened.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lobqvc/mcp_tool_development_repeated_calls_with_no/n0ly5fs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751303689,"author_flair_text":"Alpaca","treatment_tags":[],"link_id":"t3_1lobqvc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bd9e9e","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0nev68","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"nuketro0p3r","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0n6c3a","score":1,"author_fullname":"t2_1gdl5ph","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Actually that's it! If the response has &gt;5k characters then the model is always confused. If I reduce it for debugging -- even Gemma 4b works. Wierd...\\n\\nFirst success -- Thanks a lot :D\\n\\n\\"Okay, I have the content of the Wikipedia page. Now, what would you like me to do with it? Do you want me to summarize it, extract specific information, or something else?\\"","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0nev68","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Actually that&amp;#39;s it! If the response has &amp;gt;5k characters then the model is always confused. If I reduce it for debugging -- even Gemma 4b works. Wierd...&lt;/p&gt;\\n\\n&lt;p&gt;First success -- Thanks a lot :D&lt;/p&gt;\\n\\n&lt;p&gt;&amp;quot;Okay, I have the content of the Wikipedia page. Now, what would you like me to do with it? Do you want me to summarize it, extract specific information, or something else?&amp;quot;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lobqvc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lobqvc/mcp_tool_development_repeated_calls_with_no/n0nev68/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751318858,"author_flair_text":null,"treatment_tags":[],"created_utc":1751318858,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0nndtl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"nuketro0p3r","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0n6c3a","score":1,"author_fullname":"t2_1gdl5ph","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It turns out that I didn't increase the model context from 4k before this. When I change it to 64k, it all works.\\n\\nThanks a lot for the hint. \\n\\nI'm sorry that it was that dumb from my part","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0nndtl","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It turns out that I didn&amp;#39;t increase the model context from 4k before this. When I change it to 64k, it all works.&lt;/p&gt;\\n\\n&lt;p&gt;Thanks a lot for the hint. &lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m sorry that it was that dumb from my part&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lobqvc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lobqvc/mcp_tool_development_repeated_calls_with_no/n0nndtl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751321518,"author_flair_text":null,"treatment_tags":[],"created_utc":1751321518,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0n6c3a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"noage","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0m9dc9","score":2,"author_fullname":"t2_5ao30","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm using the same mcp via lm studio and haven't run into a problem with this. But it is notable that there is a truncated return to 5000 characters which the llm is informed about. If it thinks it needs more data it can repeatedly call different portions of data from the site in chunks","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0n6c3a","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m using the same mcp via lm studio and haven&amp;#39;t run into a problem with this. But it is notable that there is a truncated return to 5000 characters which the llm is informed about. If it thinks it needs more data it can repeatedly call different portions of data from the site in chunks&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"link_id":"t3_1lobqvc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lobqvc/mcp_tool_development_repeated_calls_with_no/n0n6c3a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751316367,"author_flair_text":null,"treatment_tags":[],"created_utc":1751316367,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0m9dc9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"nuketro0p3r","can_mod_post":false,"created_utc":1751306754,"send_replies":true,"parent_id":"t1_n0lxj5e","score":2,"author_fullname":"t2_1gdl5ph","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm reading the fetch url server on the original repo, but they seem to be using some other decorators -- and they also add additional context like you said. Mine looks like the basic example, but even if I append the context to the top of the message, it's just not working. I'm starting to think that there's probably some specific way to telling the model that it's done!\\n\\nHere's the prefix of the text that I return:  \\n\\"The fetch\\\\_url\\\\_text\\\\_content returned the HTML successfully.  \\n        Do not call this tool again for this specific request.  \\n        This successful result is enough to begin processing.  \\n        Following is the HTML that was retrieved.\\"\\n\\nAlso, I tried writing a system prompt that explicitly forbids repeated calling, but it seems to be ignored. could be that the model passes control to the LMStudio for this -- idk","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0m9dc9","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m reading the fetch url server on the original repo, but they seem to be using some other decorators -- and they also add additional context like you said. Mine looks like the basic example, but even if I append the context to the top of the message, it&amp;#39;s just not working. I&amp;#39;m starting to think that there&amp;#39;s probably some specific way to telling the model that it&amp;#39;s done!&lt;/p&gt;\\n\\n&lt;p&gt;Here&amp;#39;s the prefix of the text that I return:&lt;br/&gt;\\n&amp;quot;The fetch_url_text_content returned the HTML successfully.&lt;br/&gt;\\n        Do not call this tool again for this specific request.&lt;br/&gt;\\n        This successful result is enough to begin processing.&lt;br/&gt;\\n        Following is the HTML that was retrieved.&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;Also, I tried writing a system prompt that explicitly forbids repeated calling, but it seems to be ignored. could be that the model passes control to the LMStudio for this -- idk&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lobqvc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lobqvc/mcp_tool_development_repeated_calls_with_no/n0m9dc9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751306754,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0lxj5e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SM8085","can_mod_post":false,"created_utc":1751303519,"send_replies":true,"parent_id":"t3_1lobqvc","score":2,"author_fullname":"t2_14vikjao97","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt;Anyone has had this issue?\\n\\nYou're using the new [built-in MCP use in LMStudio](https://lmstudio.ai/docs/app/plugins/mcp)?  Might be an issue within that.\\n\\n&gt;Is there some hidden flag that tells the model to stop calling a tool repeatedly -- even if it was a success?\\n\\nNot that I've seen.  I've made a few MCP's specifically for the Goose AI agent.  It's just a matter of writing a tool  that returns text and exposing it with the MCP framework.\\n\\nMaybe it needs a system prompt explaining how to use tools?  Maybe that's something agents like Goose do?  \\"Hey, don't call the tool more than you need to, bruh.\\"","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0lxj5e","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Anyone has had this issue?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;You&amp;#39;re using the new &lt;a href=\\"https://lmstudio.ai/docs/app/plugins/mcp\\"&gt;built-in MCP use in LMStudio&lt;/a&gt;?  Might be an issue within that.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Is there some hidden flag that tells the model to stop calling a tool repeatedly -- even if it was a success?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Not that I&amp;#39;ve seen.  I&amp;#39;ve made a few MCP&amp;#39;s specifically for the Goose AI agent.  It&amp;#39;s just a matter of writing a tool  that returns text and exposing it with the MCP framework.&lt;/p&gt;\\n\\n&lt;p&gt;Maybe it needs a system prompt explaining how to use tools?  Maybe that&amp;#39;s something agents like Goose do?  &amp;quot;Hey, don&amp;#39;t call the tool more than you need to, bruh.&amp;quot;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lobqvc/mcp_tool_development_repeated_calls_with_no/n0lxj5e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751303519,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lobqvc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0mn8tr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Zc5Gwu","can_mod_post":false,"created_utc":1751310755,"send_replies":true,"parent_id":"t3_1lobqvc","score":1,"author_fullname":"t2_67qrvlir","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I know that openhands has a “finish tool” that the LLM can call that hands stuff off back to the user. I’m not too familiar with llmstudio though.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0mn8tr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I know that openhands has a “finish tool” that the LLM can call that hands stuff off back to the user. I’m not too familiar with llmstudio though.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lobqvc/mcp_tool_development_repeated_calls_with_no/n0mn8tr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751310755,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lobqvc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),s=()=>e.jsx(t,{data:l});export{s as default};
