import{j as e}from"./index-CmSyeZDT.js";import{R as l}from"./RedditPostRenderer-C2Zg39IK.js";import"./index-CiTZuv6Z.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Why are other people having such good luck with ai coding agents and I can't even get mine to write a simple comment block at the top of a 400 line file?\\n\\nThe common refrain is it's like having a junior engineer to pass a coding task off to...well, I've never had a junior engineer scroll 1/3rd of the way through a file and then decide it's too big for it to work with. It frequently just gets stuck in a loop reading through the file looking for where it's supposed to edit and then giving up part way through and saying it's reached a token limit. How many tokens do I need for a 300-500 line C/C++ file? Most of mine are about this big, I try to split them up if they get much bigger because even my own brain can't fathom my old 20k line files very well anymore...\\n\\nTell me what I'm doing wrong?\\n\\n- LM Studio on a Mac M4 max with 128 gigglebytes of RAM\\n- Qwen3 30b A3B, supports up to 40k tokens\\n- VS Code with Continue extension pointed to the local LM Studio instance (I've also tried through OpenWebUI's OpenAI endpoint in case API differences were the culprit)\\n\\nDo I need a beefier model? Something with more tokens? Different extension? More gigglebytes? Why can't I just give it 10 million tokens if I otherwise have enough RAM?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"AI coding agents...what am I doing wrong?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lnin1x","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.8,"author_flair_background_color":null,"subreddit_type":"public","ups":25,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_fx58l","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":25,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751213952,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Why are other people having such good luck with ai coding agents and I can&amp;#39;t even get mine to write a simple comment block at the top of a 400 line file?&lt;/p&gt;\\n\\n&lt;p&gt;The common refrain is it&amp;#39;s like having a junior engineer to pass a coding task off to...well, I&amp;#39;ve never had a junior engineer scroll 1/3rd of the way through a file and then decide it&amp;#39;s too big for it to work with. It frequently just gets stuck in a loop reading through the file looking for where it&amp;#39;s supposed to edit and then giving up part way through and saying it&amp;#39;s reached a token limit. How many tokens do I need for a 300-500 line C/C++ file? Most of mine are about this big, I try to split them up if they get much bigger because even my own brain can&amp;#39;t fathom my old 20k line files very well anymore...&lt;/p&gt;\\n\\n&lt;p&gt;Tell me what I&amp;#39;m doing wrong?&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;LM Studio on a Mac M4 max with 128 gigglebytes of RAM&lt;/li&gt;\\n&lt;li&gt;Qwen3 30b A3B, supports up to 40k tokens&lt;/li&gt;\\n&lt;li&gt;VS Code with Continue extension pointed to the local LM Studio instance (I&amp;#39;ve also tried through OpenWebUI&amp;#39;s OpenAI endpoint in case API differences were the culprit)&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Do I need a beefier model? Something with more tokens? Different extension? More gigglebytes? Why can&amp;#39;t I just give it 10 million tokens if I otherwise have enough RAM?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lnin1x","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"furyfuryfury","discussion_type":null,"num_comments":49,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/","subreddit_subscribers":493240,"created_utc":1751213952,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0fjbuz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"furyfuryfury","can_mod_post":false,"created_utc":1751215083,"send_replies":true,"parent_id":"t1_n0fh5q3","score":3,"author_fullname":"t2_fx58l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks! I hadn't heard of them yet. I'll give those a shot","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0fjbuz","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks! I hadn&amp;#39;t heard of them yet. I&amp;#39;ll give those a shot&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnin1x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/n0fjbuz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751215083,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0fh5q3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"bick_nyers","can_mod_post":false,"created_utc":1751214385,"send_replies":true,"parent_id":"t3_1lnin1x","score":19,"author_fullname":"t2_6nwld4d3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Try RooCode with GLM-4 32B (the newest one, 0414 or something) and see if that feels better.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0fh5q3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Try RooCode with GLM-4 32B (the newest one, 0414 or something) and see if that feels better.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/n0fh5q3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751214385,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnin1x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":19}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0fkrqu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"dkeiz","can_mod_post":false,"created_utc":1751215545,"send_replies":true,"parent_id":"t3_1lnin1x","score":35,"author_fullname":"t2_1d1fsbe7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"\\\\&gt;Qwen3 30b A3B  \\nthat is a problem. \\n\\ntry any coder, like qwen 2.5 7-14B coder.   \\nI tried with LMstudio and Ollama and works fine for me even with 4B models.   \\nLanguage important as well.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0fkrqu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&amp;gt;Qwen3 30b A3B&lt;br/&gt;\\nthat is a problem. &lt;/p&gt;\\n\\n&lt;p&gt;try any coder, like qwen 2.5 7-14B coder.&lt;br/&gt;\\nI tried with LMstudio and Ollama and works fine for me even with 4B models.&lt;br/&gt;\\nLanguage important as well.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/n0fkrqu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751215545,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnin1x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":35}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0kx8en","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CheatCodesOfLife","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0kve6t","score":1,"author_fullname":"t2_32el727b","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Cool, if you do get around to trying it, let me know how it goes.\\n\\nI had it we pretty good for me today refactoring a small codebase.\\n\\nI'd be surprised if it's as good as V3 for anything complex. I mostly tried it because it stays coherent at &gt; 60k context and it's good at following instructions.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n0kx8en","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Cool, if you do get around to trying it, let me know how it goes.&lt;/p&gt;\\n\\n&lt;p&gt;I had it we pretty good for me today refactoring a small codebase.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;d be surprised if it&amp;#39;s as good as V3 for anything complex. I mostly tried it because it stays coherent at &amp;gt; 60k context and it&amp;#39;s good at following instructions.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lnin1x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/n0kx8en/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751292975,"author_flair_text":null,"treatment_tags":[],"created_utc":1751292975,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0kve6t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"createthiscom","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ixaa5","score":2,"author_fullname":"t2_ozxxf","approved_by":null,"mod_note":null,"all_awardings":[],"body":"You got me curious about Command-A. I have a few more applications in this current project that I need to process. I'll try Command-A today and see if it's as good as V3.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0kve6t","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You got me curious about Command-A. I have a few more applications in this current project that I need to process. I&amp;#39;ll try Command-A today and see if it&amp;#39;s as good as V3.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lnin1x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/n0kve6t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751292398,"author_flair_text":null,"treatment_tags":[],"created_utc":1751292398,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ixaa5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CheatCodesOfLife","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ip3h4","score":1,"author_fullname":"t2_32el727b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I tried Qwen3 FP8 32b and the large MoE Q4_K but they didn't work well with roo for me (buggy writing files)\\n\\nI've actually just had some good luck with Command-A (AWQ) for the past hour or so. (To my surprise since I've never seen it recommended for this)\\n\\n&gt; V3 and R1\\n\\nYeah thought so. V3 is hard to run fast on my hardware. I will try it though if Command-A lets me down.\\n\\nThanks for the suggestions.","edited":false,"author_flair_css_class":null,"name":"t1_n0ixaa5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I tried Qwen3 FP8 32b and the large MoE Q4_K but they didn&amp;#39;t work well with roo for me (buggy writing files)&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ve actually just had some good luck with Command-A (AWQ) for the past hour or so. (To my surprise since I&amp;#39;ve never seen it recommended for this)&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;V3 and R1&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Yeah thought so. V3 is hard to run fast on my hardware. I will try it though if Command-A lets me down.&lt;/p&gt;\\n\\n&lt;p&gt;Thanks for the suggestions.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lnin1x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/n0ixaa5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751257933,"author_flair_text":null,"collapsed":false,"created_utc":1751257933,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ip3h4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"createthiscom","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0iokgh","score":1,"author_fullname":"t2_ozxxf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"V3 and R1. Qwen3 is ok too, but not quite as good in my experience. Video examples: https://youtu.be/vfi9LRJxgHs","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ip3h4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;V3 and R1. Qwen3 is ok too, but not quite as good in my experience. Video examples: &lt;a href=\\"https://youtu.be/vfi9LRJxgHs\\"&gt;https://youtu.be/vfi9LRJxgHs&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnin1x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/n0ip3h4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751254233,"author_flair_text":null,"treatment_tags":[],"created_utc":1751254233,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0iokgh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CheatCodesOfLife","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0fsz36","score":2,"author_fullname":"t2_32el727b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Any recommendations (for local models on such hardware)?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0iokgh","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Any recommendations (for local models on such hardware)?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnin1x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/n0iokgh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751254004,"author_flair_text":null,"treatment_tags":[],"created_utc":1751254004,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0fsz36","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"createthiscom","can_mod_post":false,"created_utc":1751218071,"send_replies":true,"parent_id":"t1_n0fibne","score":11,"author_fullname":"t2_ozxxf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"\\\\\`local models\\\\\` work fine. You just need really beefy hardware. Like, in the 14k to 30k USD range.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0fsz36","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;\`local models\` work fine. You just need really beefy hardware. Like, in the 14k to 30k USD range.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnin1x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/n0fsz36/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751218071,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}}],"before":null}},"user_reports":[],"saved":false,"id":"n0fibne","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Trotskyist","can_mod_post":false,"created_utc":1751214758,"send_replies":true,"parent_id":"t3_1lnin1x","score":43,"author_fullname":"t2_350qe","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"In my experience local models just aren't good enough for full agentic coding for all but the most trivial of tasks.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0fibne","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;In my experience local models just aren&amp;#39;t good enough for full agentic coding for all but the most trivial of tasks.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/n0fibne/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751214758,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnin1x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":43}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0flwq7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"LocoMod","can_mod_post":false,"created_utc":1751215896,"send_replies":true,"parent_id":"t3_1lnin1x","score":8,"author_fullname":"t2_6uuoq","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Agentic workflows don’t generally tend to operate with huge context per “task turn”. I’m assuming part of the problem here is your LMStudio params or Continue configuration. I don’t use either so I’m not sure how they manage context. Generally speaking, it’s better to set a low ctx for the agent so it doesn’t have to process unnecessary context when it should be solving bite sized tasks. You should have a “coordinator” agent that distills your goal into small steps. This agent should then invoke other agents whose entire purpose is to call a tool or related set of tools. For example, a CLI agent. This agent should then be able to use the CLI tool to issue any necessary commands to understand a particular file. It should be able to read the file in chunks and find what needs to be refactored for THIS file in THIS step only. It should then report back to the coordinator agent so the next task can proceed. \\n\\nFor more complex workflows, you can have the coordinator agent intelligently determine the task dependency tree (this task must wait for the results of this task before executing), and run tasks that are not dependent in parallel. \\n\\nYou’re also going to want some form of web RAG. So agents can go reference the latest docs for a tool, or research topics. You need to augment the local LLMs with some external knowledge base. \\n\\nSo review your parameters, your system prompts or instructions, where those are ideally configured in the tools you use, and how to connect a RAG solution. \\n\\nAlso, try using a good public model from OpenAI, Google or Anthropic. See if those models solve your problem using your current configuration. If they don’t, there’s a good chance the problem is with your setup. If they can, it’s a good indication the local model you are using isn’t up to the task, is not configured with the proper tool calling template, or other parameters need to be adjusted for the use-case.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0flwq7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Agentic workflows don’t generally tend to operate with huge context per “task turn”. I’m assuming part of the problem here is your LMStudio params or Continue configuration. I don’t use either so I’m not sure how they manage context. Generally speaking, it’s better to set a low ctx for the agent so it doesn’t have to process unnecessary context when it should be solving bite sized tasks. You should have a “coordinator” agent that distills your goal into small steps. This agent should then invoke other agents whose entire purpose is to call a tool or related set of tools. For example, a CLI agent. This agent should then be able to use the CLI tool to issue any necessary commands to understand a particular file. It should be able to read the file in chunks and find what needs to be refactored for THIS file in THIS step only. It should then report back to the coordinator agent so the next task can proceed. &lt;/p&gt;\\n\\n&lt;p&gt;For more complex workflows, you can have the coordinator agent intelligently determine the task dependency tree (this task must wait for the results of this task before executing), and run tasks that are not dependent in parallel. &lt;/p&gt;\\n\\n&lt;p&gt;You’re also going to want some form of web RAG. So agents can go reference the latest docs for a tool, or research topics. You need to augment the local LLMs with some external knowledge base. &lt;/p&gt;\\n\\n&lt;p&gt;So review your parameters, your system prompts or instructions, where those are ideally configured in the tools you use, and how to connect a RAG solution. &lt;/p&gt;\\n\\n&lt;p&gt;Also, try using a good public model from OpenAI, Google or Anthropic. See if those models solve your problem using your current configuration. If they don’t, there’s a good chance the problem is with your setup. If they can, it’s a good indication the local model you are using isn’t up to the task, is not configured with the proper tool calling template, or other parameters need to be adjusted for the use-case.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/n0flwq7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751215896,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnin1x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ggvr1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ElectronSpiderwort","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0fj6kl","score":4,"author_fullname":"t2_mxbu5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Test Qwen-2.5-32b-coder also; it sets a pretty high bar for understanding and not ruining your code. It's not up to date on modern tool calls but for single-shot changes it's startlingly good for a local model","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0ggvr1","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Test Qwen-2.5-32b-coder also; it sets a pretty high bar for understanding and not ruining your code. It&amp;#39;s not up to date on modern tool calls but for single-shot changes it&amp;#39;s startlingly good for a local model&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnin1x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/n0ggvr1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751225609,"author_flair_text":null,"treatment_tags":[],"created_utc":1751225609,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n0fj6kl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"furyfuryfury","can_mod_post":false,"created_utc":1751215036,"send_replies":true,"parent_id":"t1_n0fhpvy","score":3,"author_fullname":"t2_fx58l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"As far as I'm concerned, it can be slow as molasses if its output is good. I hadn't thought to try llama3.3-70b for coding yet, but I do have it on the machine, so I'll give that a shot. I also have Qwen3-32b. Thanks!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0fj6kl","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;As far as I&amp;#39;m concerned, it can be slow as molasses if its output is good. I hadn&amp;#39;t thought to try llama3.3-70b for coding yet, but I do have it on the machine, so I&amp;#39;ll give that a shot. I also have Qwen3-32b. Thanks!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnin1x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/n0fj6kl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751215036,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0fhpvy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ForsookComparison","can_mod_post":false,"created_utc":1751214564,"send_replies":true,"parent_id":"t3_1lnin1x","score":9,"author_fullname":"t2_on5es7pe3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt; Qwen3 30b A3B\\n\\nWhile the inference speed is tempting, this model for me always falls off quickly as context gets larger and larger.\\n\\nSome things I'd recommend that I'd seen others recommend here:\\n\\n- if you still want the MoE speed boost, try Qwen3-30b-a6b-Extreme. It does a fair bit better at large contexts and is still really fast\\n\\n- try Qwen3-32B or even Qwen3-14b, both do much better with a lot of text\\n\\n- try Llama 3.3 70B (even lower quants)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0fhpvy","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Qwen3 30b A3B&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;While the inference speed is tempting, this model for me always falls off quickly as context gets larger and larger.&lt;/p&gt;\\n\\n&lt;p&gt;Some things I&amp;#39;d recommend that I&amp;#39;d seen others recommend here:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;p&gt;if you still want the MoE speed boost, try Qwen3-30b-a6b-Extreme. It does a fair bit better at large contexts and is still really fast&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;try Qwen3-32B or even Qwen3-14b, both do much better with a lot of text&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;try Llama 3.3 70B (even lower quants)&lt;/p&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/n0fhpvy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751214564,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lnin1x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0fl36i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"zenmatrix83","can_mod_post":false,"created_utc":1751215643,"send_replies":true,"parent_id":"t3_1lnin1x","score":3,"author_fullname":"t2_3d4w4qp7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"* Qwen3 30b A3B, supports up to 40k tokens  \\n\\ncheck the actual settings, should be advanced or something if I remember right, I'd make sure its set to 40k and not like 8k. I'd also play around deekseek r1 has done ok and devstral as well, I use moslty claude code online now, but have had minor sucess with local models.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0fl36i","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;ul&gt;\\n&lt;li&gt;Qwen3 30b A3B, supports up to 40k tokens&lt;br/&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;check the actual settings, should be advanced or something if I remember right, I&amp;#39;d make sure its set to 40k and not like 8k. I&amp;#39;d also play around deekseek r1 has done ok and devstral as well, I use moslty claude code online now, but have had minor sucess with local models.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/n0fl36i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751215643,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnin1x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0i35u6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"maverick_soul_143747","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0hy407","score":1,"author_fullname":"t2_1af9q3qa0b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Probably you are right. I have not given very long context and stick to the 14B models","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0i35u6","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Probably you are right. I have not given very long context and stick to the 14B models&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnin1x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/n0i35u6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751245620,"author_flair_text":null,"treatment_tags":[],"created_utc":1751245620,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hy407","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SkyFeistyLlama8","can_mod_post":false,"created_utc":1751243732,"send_replies":true,"parent_id":"t1_n0flxew","score":3,"author_fullname":"t2_1hgbaqgbnq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"With thinking turned off. Qwen 3 models tend to ramble on and on if given long contexts.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hy407","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;With thinking turned off. Qwen 3 models tend to ramble on and on if given long contexts.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnin1x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/n0hy407/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751243732,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0flxew","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"maverick_soul_143747","can_mod_post":false,"created_utc":1751215902,"send_replies":true,"parent_id":"t3_1lnin1x","score":4,"author_fullname":"t2_1af9q3qa0b","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Try the Qwen 3 14B and that is the one I use. The local models can help to an extent. My use case involves working with local model for boilerplate review to start with and then using Claude cli and Gemini cli to enhance it further","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0flxew","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Try the Qwen 3 14B and that is the one I use. The local models can help to an extent. My use case involves working with local model for boilerplate review to start with and then using Claude cli and Gemini cli to enhance it further&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/n0flxew/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751215902,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnin1x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0h2st0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ilintar","can_mod_post":false,"created_utc":1751232626,"send_replies":true,"parent_id":"t3_1lnin1x","score":4,"author_fullname":"t2_cctud","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I guess it has something to do with this: \\n\\n[LMStudio file context in prompts gets truncated · Issue #4491 · continuedev/continue](https://github.com/continuedev/continue/issues/4491)\\n\\nI filed it in March, it hasn't even been touched. Continue's approach to development (adding features / commercialization while not even acknowledging completely breaking bugs like this one) is one reason I'm really disappointed in them.\\n\\nJust use RooCode ([Roo Code – Your AI-Powered Dev Team in VS Code](https://roocode.com/)) - fully open source, bugs get fixed quickly. For local coding, you can pair it with i.e. Qwen3 30B-A3B, hell, I've had a lot of success pairing it with Polaris 4B Q8\\\\_0.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0h2st0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I guess it has something to do with this: &lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/continuedev/continue/issues/4491\\"&gt;LMStudio file context in prompts gets truncated · Issue #4491 · continuedev/continue&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;I filed it in March, it hasn&amp;#39;t even been touched. Continue&amp;#39;s approach to development (adding features / commercialization while not even acknowledging completely breaking bugs like this one) is one reason I&amp;#39;m really disappointed in them.&lt;/p&gt;\\n\\n&lt;p&gt;Just use RooCode (&lt;a href=\\"https://roocode.com/\\"&gt;Roo Code – Your AI-Powered Dev Team in VS Code&lt;/a&gt;) - fully open source, bugs get fixed quickly. For local coding, you can pair it with i.e. Qwen3 30B-A3B, hell, I&amp;#39;ve had a lot of success pairing it with Polaris 4B Q8_0.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/n0h2st0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751232626,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnin1x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0g1tm1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"IndianaNetworkAdmin","can_mod_post":false,"created_utc":1751220789,"send_replies":true,"parent_id":"t3_1lnin1x","score":3,"author_fullname":"t2_15styv","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Instead of doing direct integration, I give detailed instructions of what I need. Have a model develop pseudocode first, then feed that in and have them build individual functions. When using smaller models it's better to make things as modular as possible.\\n\\n\\"I need logic to accomplish the following: Accept two variables, add them together, and return the value. The function should work for any numeric value, float or otherwise. The function should throw an error if a non-numeric value is provided. The returned value should be a float.\\"\\n\\nTake that, and then:\\n\\n\\"I need the following psudocode rendered in Python 3.11 using only native Python capabilities: &lt;Logic from prior response&gt;\\"\\n\\nI've done this when I've been too lazy to reinvent the wheel. The pseudocode pass also gives you a chance to review the logic before it's turned to code.\\n\\nYou can also do a second pass of pseudocode -\\n\\n\\"Evaluate the following pseudocode, and determine if there is a more optimal approach with the assumption it will be rendered with Python 3.11:\\"\\n\\nThis lets it determine if there is a better or faster way of doing things. For example if the sorting method provided could be more easily accomplished with another method.\\n\\nI don't use a local model at the moment, I'm waiting to see how things look in another six months before adding a dedicated LLM microtower to my cluster. But the above process does phenomenally well on Gemini at the moment. And by doing pseudocode first and limiting to individual components, you can work with smaller context limits.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0g1tm1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Instead of doing direct integration, I give detailed instructions of what I need. Have a model develop pseudocode first, then feed that in and have them build individual functions. When using smaller models it&amp;#39;s better to make things as modular as possible.&lt;/p&gt;\\n\\n&lt;p&gt;&amp;quot;I need logic to accomplish the following: Accept two variables, add them together, and return the value. The function should work for any numeric value, float or otherwise. The function should throw an error if a non-numeric value is provided. The returned value should be a float.&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;Take that, and then:&lt;/p&gt;\\n\\n&lt;p&gt;&amp;quot;I need the following psudocode rendered in Python 3.11 using only native Python capabilities: &amp;lt;Logic from prior response&amp;gt;&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ve done this when I&amp;#39;ve been too lazy to reinvent the wheel. The pseudocode pass also gives you a chance to review the logic before it&amp;#39;s turned to code.&lt;/p&gt;\\n\\n&lt;p&gt;You can also do a second pass of pseudocode -&lt;/p&gt;\\n\\n&lt;p&gt;&amp;quot;Evaluate the following pseudocode, and determine if there is a more optimal approach with the assumption it will be rendered with Python 3.11:&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;This lets it determine if there is a better or faster way of doing things. For example if the sorting method provided could be more easily accomplished with another method.&lt;/p&gt;\\n\\n&lt;p&gt;I don&amp;#39;t use a local model at the moment, I&amp;#39;m waiting to see how things look in another six months before adding a dedicated LLM microtower to my cluster. But the above process does phenomenally well on Gemini at the moment. And by doing pseudocode first and limiting to individual components, you can work with smaller context limits.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/n0g1tm1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751220789,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnin1x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0fl5sf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kneeanderthul","can_mod_post":false,"created_utc":1751215666,"send_replies":true,"parent_id":"t3_1lnin1x","score":1,"author_fullname":"t2_nz7b4uav","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you've tasked the prompt window with other data prior to coding, it might simply not know which you find important.\\n\\nI'd simply ask the prompt window, it knows why it can't and can do. Work within its limitations. Maybe even make the task a bit more management so it doesn't brick. Build it one step at a time instead of all in one go. Collaboration is key\\n\\nAll the best","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0fl5sf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you&amp;#39;ve tasked the prompt window with other data prior to coding, it might simply not know which you find important.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;d simply ask the prompt window, it knows why it can&amp;#39;t and can do. Work within its limitations. Maybe even make the task a bit more management so it doesn&amp;#39;t brick. Build it one step at a time instead of all in one go. Collaboration is key&lt;/p&gt;\\n\\n&lt;p&gt;All the best&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/n0fl5sf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751215666,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnin1x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0gwol3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"phaetto","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0gmxm1","score":1,"author_fullname":"t2_ywgf7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yeap, a very comprehensive one","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0gwol3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeap, a very comprehensive one&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnin1x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/n0gwol3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751230641,"author_flair_text":null,"treatment_tags":[],"created_utc":1751230641,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0gmxm1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"false79","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0fzp78","score":1,"author_fullname":"t2_wn888","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Do you have a system prompt that declares the standard you want?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0gmxm1","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Do you have a system prompt that declares the standard you want?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnin1x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/n0gmxm1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751227566,"author_flair_text":null,"treatment_tags":[],"created_utc":1751227566,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0fzp78","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"phaetto","can_mod_post":false,"created_utc":1751220130,"send_replies":true,"parent_id":"t1_n0ftc92","score":1,"author_fullname":"t2_ywgf7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I had really tough time with Deepseek R1 to generate C# code with correct coding standards. I tried deepseek-coder-v2:236b and deepseek-r1:70b from Ollama. Do you have any tips if you did anything special for their setup?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0fzp78","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I had really tough time with Deepseek R1 to generate C# code with correct coding standards. I tried deepseek-coder-v2:236b and deepseek-r1:70b from Ollama. Do you have any tips if you did anything special for their setup?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnin1x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/n0fzp78/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751220130,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ftc92","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Threatening-Silence-","can_mod_post":false,"created_utc":1751218184,"send_replies":true,"parent_id":"t3_1lnin1x","score":1,"author_fullname":"t2_15wqsifdjf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'm running Roo Code with a local agent. 32k tokens is the absolute bare minimum for context. 64k is more reasonable and my current 85k is comfortable. \\n\\nTo be honest I wouldn't try any local model lower than Qwen3 235B. I currently use Deepseek R1 and it's solid.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ftc92","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m running Roo Code with a local agent. 32k tokens is the absolute bare minimum for context. 64k is more reasonable and my current 85k is comfortable. &lt;/p&gt;\\n\\n&lt;p&gt;To be honest I wouldn&amp;#39;t try any local model lower than Qwen3 235B. I currently use Deepseek R1 and it&amp;#39;s solid.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/n0ftc92/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751218184,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnin1x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0fy8nt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No-Mountain3817","can_mod_post":false,"created_utc":1751219683,"send_replies":true,"parent_id":"t3_1lnin1x","score":1,"author_fullname":"t2_hylfch6q5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"[https://huggingface.co/models?sort=downloads&amp;search=qwen2.5+coder++32b+instruct](https://huggingface.co/models?sort=downloads&amp;search=qwen2.5+coder++32b+instruct)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0fy8nt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://huggingface.co/models?sort=downloads&amp;amp;search=qwen2.5+coder++32b+instruct\\"&gt;https://huggingface.co/models?sort=downloads&amp;amp;search=qwen2.5+coder++32b+instruct&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/n0fy8nt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751219683,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnin1x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0g6gwp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LoSboccacc","can_mod_post":false,"created_utc":1751222262,"send_replies":true,"parent_id":"t3_1lnin1x","score":1,"author_fullname":"t2_dievh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":" \\\\&gt; Qwen3 30b A3B\\n\\n\\\\^ this","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0g6gwp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&amp;gt; Qwen3 30b A3B&lt;/p&gt;\\n\\n&lt;p&gt;^ this&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/n0g6gwp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751222262,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnin1x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0g6vm5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"1ncehost","can_mod_post":false,"created_utc":1751222393,"send_replies":true,"parent_id":"t3_1lnin1x","score":1,"author_fullname":"t2_lrannsv","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I recently compared 4 of the top models on a basic coding task with large context. Overall my favorite was Devstral, but the highest quality was Qwen3 32B. I'd give those a shot if you have alenough VRAM. \\n\\nArticle I made for that (not paywalled): https://medium.com/p/c12a737bab0e\\n\\nIf you don't have enough VRAM Qwen3 14B and the Gemma 3 12B are good options. Gemma 3n is also surprisingly good for its size and can do very basic agentic programming.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0g6vm5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I recently compared 4 of the top models on a basic coding task with large context. Overall my favorite was Devstral, but the highest quality was Qwen3 32B. I&amp;#39;d give those a shot if you have alenough VRAM. &lt;/p&gt;\\n\\n&lt;p&gt;Article I made for that (not paywalled): &lt;a href=\\"https://medium.com/p/c12a737bab0e\\"&gt;https://medium.com/p/c12a737bab0e&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;If you don&amp;#39;t have enough VRAM Qwen3 14B and the Gemma 3 12B are good options. Gemma 3n is also surprisingly good for its size and can do very basic agentic programming.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/n0g6vm5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751222393,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnin1x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0gdc3q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CupcakeSecure4094","can_mod_post":false,"created_utc":1751224464,"send_replies":true,"parent_id":"t3_1lnin1x","score":1,"author_fullname":"t2_9ed3983m","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Try Google AI studio, free and 1M tokens context, also the best a code in my opinion. Upload all relevant files as context and explain the initial request like a github ticket, a fully self contained story.\\nBuild on that context with simple changes as required, always provide compile (etc) errors/fully explain nuances, only change one thing at a time.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0gdc3q","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Try Google AI studio, free and 1M tokens context, also the best a code in my opinion. Upload all relevant files as context and explain the initial request like a github ticket, a fully self contained story.\\nBuild on that context with simple changes as required, always provide compile (etc) errors/fully explain nuances, only change one thing at a time.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/n0gdc3q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751224464,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnin1x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0grn5u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"southpalito","can_mod_post":false,"created_utc":1751229057,"send_replies":true,"parent_id":"t3_1lnin1x","score":1,"author_fullname":"t2_3chu3u3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Most likely, they’re lying and not disclosing the limitations. I saw an instance where the output was garbage. The agent inexplicably changed all the &gt; to &lt;, causing a complete breakdown in the logic further down in the workflow.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0grn5u","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Most likely, they’re lying and not disclosing the limitations. I saw an instance where the output was garbage. The agent inexplicably changed all the &amp;gt; to &amp;lt;, causing a complete breakdown in the logic further down in the workflow.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/n0grn5u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751229057,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnin1x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hfbql","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"12bitmisfit","can_mod_post":false,"created_utc":1751236896,"send_replies":true,"parent_id":"t3_1lnin1x","score":1,"author_fullname":"t2_3phzie41","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'd highly recommend using unsloths 128k variants for some extra context length.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hfbql","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;d highly recommend using unsloths 128k variants for some extra context length.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/n0hfbql/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751236896,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnin1x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0kohxg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"created_utc":1751290169,"send_replies":true,"parent_id":"t3_1lnin1x","score":0,"author_fullname":"t2_1a48h7vf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Use a model that performs well on the full SWE-bench, it's a difficult benchmark: https://www.swebench.com.\\n\\nClaude is universally liked for this.\\n\\nNo one uses DeepSeek v3/r1, it's a \\"leaderboard-only\\" model that shills love to say they use but you'll never see evidence beyond a few hype videos and posts. Ask someone to show you a Github issue closed successfully by DeepSeek.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0kohxg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Use a model that performs well on the full SWE-bench, it&amp;#39;s a difficult benchmark: &lt;a href=\\"https://www.swebench.com\\"&gt;https://www.swebench.com&lt;/a&gt;.&lt;/p&gt;\\n\\n&lt;p&gt;Claude is universally liked for this.&lt;/p&gt;\\n\\n&lt;p&gt;No one uses DeepSeek v3/r1, it&amp;#39;s a &amp;quot;leaderboard-only&amp;quot; model that shills love to say they use but you&amp;#39;ll never see evidence beyond a few hype videos and posts. Ask someone to show you a Github issue closed successfully by DeepSeek.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/n0kohxg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751290169,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnin1x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0gwjtz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"amranu","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0gw3om","score":1,"author_fullname":"t2_3yvyd","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Huh okay, I didn't bother investigating that far. Fair enough.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0gwjtz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Huh okay, I didn&amp;#39;t bother investigating that far. Fair enough.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnin1x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/n0gwjtz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751230599,"author_flair_text":null,"treatment_tags":[],"created_utc":1751230599,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0gw3om","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"createthiscom","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0gtyue","score":1,"author_fullname":"t2_ozxxf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I've found the #1 reason why they usually generate unusable unified diffs is because they can't count and hallucinate the chunk header line counts. That's an easy thing to fix, so my tool does that automagically.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0gw3om","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve found the #1 reason why they usually generate unusable unified diffs is because they can&amp;#39;t count and hallucinate the chunk header line counts. That&amp;#39;s an easy thing to fix, so my tool does that automagically.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnin1x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/n0gw3om/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751230456,"author_flair_text":null,"treatment_tags":[],"created_utc":1751230456,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0gtyue","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"amranu","can_mod_post":false,"created_utc":1751229787,"send_replies":true,"parent_id":"t1_n0fsbx1","score":1,"author_fullname":"t2_3yvyd","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I find AIs, even top tier ones, are terrible at unified diffs. I tried adding a unified diff edit command to my agentic framework [cli-agent](https://github.com/amranu/cli-agent) and it did not go well, so I removed it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0gtyue","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I find AIs, even top tier ones, are terrible at unified diffs. I tried adding a unified diff edit command to my agentic framework &lt;a href=\\"https://github.com/amranu/cli-agent\\"&gt;cli-agent&lt;/a&gt; and it did not go well, so I removed it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnin1x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/n0gtyue/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751229787,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0kw6d9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"createthiscom","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0jyit4","score":1,"author_fullname":"t2_ozxxf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I have 768gb of RAM, but you only need about 400. I have 720gb/s system ram bandwidth, which is uncommonly high and also very important. I have a 96gb blackwell 6000 pro, but a 48gb card would work fine. I only use about 30% of the GPU VRAM, and only about 60% of the GPU because I'm bottlenecked on system RAM throughput. Full details and demo runs in the video: [https://youtu.be/vfi9LRJxgHs](https://youtu.be/vfi9LRJxgHs) Linked in the description are two more videos, one showing performance on a 3090, and one showing performance on CPU ONLY, in addition to the PC build. HTH.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0kw6d9","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have 768gb of RAM, but you only need about 400. I have 720gb/s system ram bandwidth, which is uncommonly high and also very important. I have a 96gb blackwell 6000 pro, but a 48gb card would work fine. I only use about 30% of the GPU VRAM, and only about 60% of the GPU because I&amp;#39;m bottlenecked on system RAM throughput. Full details and demo runs in the video: &lt;a href=\\"https://youtu.be/vfi9LRJxgHs\\"&gt;https://youtu.be/vfi9LRJxgHs&lt;/a&gt; Linked in the description are two more videos, one showing performance on a 3090, and one showing performance on CPU ONLY, in addition to the PC build. HTH.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnin1x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/n0kw6d9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751292645,"author_flair_text":null,"treatment_tags":[],"created_utc":1751292645,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0jyit4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cantgetthistowork","can_mod_post":false,"created_utc":1751279219,"send_replies":true,"parent_id":"t1_n0fsbx1","score":1,"author_fullname":"t2_j1i0o","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Specs for getting 128K on V3? Have you tried Devstral?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jyit4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Specs for getting 128K on V3? Have you tried Devstral?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnin1x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/n0jyit4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751279219,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0fsbx1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"createthiscom","can_mod_post":false,"created_utc":1751217872,"send_replies":true,"parent_id":"t3_1lnin1x","score":1,"author_fullname":"t2_ozxxf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"First of all, you want a system that can handle 128k or more context if you want to work agentically. You may be able to work with 50k or less, but it's going to be annoying.\\n\\nSecond, you want to use top tier models. 671b or larger. Qwen3-235B-A22B-128K isn't as good as DeepSeek-V3-0324:671b-q4\\\\_k\\\\_xl. I run the latter locally on a traditional PC (not a datacenter), but I have a really beefy server style system.\\n\\nThird, I'm the author of a free project called \\\\\`diffcalculia\\\\_mcp\\\\\` that provides an MCP server designed to help AIs edit large files effectively using unified diffs: [https://github.com/createthis/diffcalculia\\\\_mcp](https://github.com/createthis/diffcalculia_mcp)\\n\\nIt's not perfect. Sometimes it still fails, but in my experience it works better than some of the other solutions out there. AIs struggle with large file editing. Check it out if you're interested.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0fsbx1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;First of all, you want a system that can handle 128k or more context if you want to work agentically. You may be able to work with 50k or less, but it&amp;#39;s going to be annoying.&lt;/p&gt;\\n\\n&lt;p&gt;Second, you want to use top tier models. 671b or larger. Qwen3-235B-A22B-128K isn&amp;#39;t as good as DeepSeek-V3-0324:671b-q4_k_xl. I run the latter locally on a traditional PC (not a datacenter), but I have a really beefy server style system.&lt;/p&gt;\\n\\n&lt;p&gt;Third, I&amp;#39;m the author of a free project called \`diffcalculia_mcp\` that provides an MCP server designed to help AIs edit large files effectively using unified diffs: &lt;a href=\\"https://github.com/createthis/diffcalculia_mcp\\"&gt;https://github.com/createthis/diffcalculia_mcp&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s not perfect. Sometimes it still fails, but in my experience it works better than some of the other solutions out there. AIs struggle with large file editing. Check it out if you&amp;#39;re interested.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/n0fsbx1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751217872,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnin1x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0fyfe6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"phaetto","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0fk3xh","score":0,"author_fullname":"t2_ywgf7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You have to set an environment variable with OLLAMA\\\\_KEEP\\\\_ALIVE to something like 8h to avoid this problem.  \\nApart from that, I had only success with gemma3:27b been able to understand what I need so far with coding tasks.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0fyfe6","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You have to set an environment variable with OLLAMA_KEEP_ALIVE to something like 8h to avoid this problem.&lt;br/&gt;\\nApart from that, I had only success with gemma3:27b been able to understand what I need so far with coding tasks.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnin1x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/n0fyfe6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751219740,"author_flair_text":null,"treatment_tags":[],"created_utc":1751219740,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n0fk3xh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"furyfuryfury","can_mod_post":false,"created_utc":1751215334,"send_replies":true,"parent_id":"t1_n0fir08","score":1,"author_fullname":"t2_fx58l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What other LLM hosts have you had good luck with? I tried ollama first but it doesn't keep models in RAM so every single prompt has the additional delay of waiting for the model to load","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0fk3xh","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What other LLM hosts have you had good luck with? I tried ollama first but it doesn&amp;#39;t keep models in RAM so every single prompt has the additional delay of waiting for the model to load&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnin1x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/n0fk3xh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751215334,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0fka2w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AlwaysInconsistant","can_mod_post":false,"created_utc":1751215389,"send_replies":true,"parent_id":"t1_n0fir08","score":1,"author_fullname":"t2_65e1spw7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I have found it to be among the better options on Mac.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0fka2w","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have found it to be among the better options on Mac.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnin1x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/n0fka2w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751215389,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0fir08","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Physical-Citron5153","can_mod_post":false,"created_utc":1751214896,"send_replies":true,"parent_id":"t3_1lnin1x","score":0,"author_fullname":"t2_clhgguip","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I really don’t know if what I am saying is correct or not, but using LM Studio as the endpoint always performed poorly for me, whereas others worked so much better.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0fir08","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I really don’t know if what I am saying is correct or not, but using LM Studio as the endpoint always performed poorly for me, whereas others worked so much better.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/n0fir08/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751214896,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnin1x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0j08lr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Any_Pressure4251","can_mod_post":false,"created_utc":1751259363,"send_replies":true,"parent_id":"t1_n0g05ex","score":1,"author_fullname":"t2_6yisgee3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I know some people just like doing really stupid things!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0j08lr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I know some people just like doing really stupid things!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnin1x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/n0j08lr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751259363,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0g05ex","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Low-Opening25","can_mod_post":false,"created_utc":1751220269,"send_replies":true,"parent_id":"t3_1lnin1x","score":-2,"author_fullname":"t2_ebfjvj5t","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"you are using local models\\nfor coding? 🤣🤣🤣🤣 ok…. good luck","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0g05ex","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;you are using local models\\nfor coding? 🤣🤣🤣🤣 ok…. good luck&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/n0g05ex/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751220269,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnin1x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0fkcs9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"PrimaryRequirement49","can_mod_post":false,"created_utc":1751215413,"send_replies":true,"parent_id":"t3_1lnin1x","score":-2,"author_fullname":"t2_1liv96532t","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Local models are pretty trash when it comes to writing code. Just not worth the fuss in the slightest. If you are serious about writing code Claude Code is the best you can use.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0fkcs9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Local models are pretty trash when it comes to writing code. Just not worth the fuss in the slightest. If you are serious about writing code Claude Code is the best you can use.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/n0fkcs9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751215413,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnin1x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ps9p8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FieldProgrammable","can_mod_post":false,"created_utc":1751351266,"send_replies":true,"parent_id":"t3_1lnin1x","score":1,"author_fullname":"t2_moet0t","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I only use continue for local autocomplete (which needs specialised FIM models with low latency). For local agent stuff I would recommend Cline or Roo Code. For a model I would recommend Devstral, in a high quant (unsloth q5_k_xl or better). You can get away with less context provided your tasking client can properly isolate the functions to be edited. Having the full 128k context available is more an insurance.\\n\\nFor edits like you are wanting you need to describe where you want the code injected in a way that the model will be able to gwnerate a regex query with a chance of success. E.g. give it the function name. Once the model has used regex to grab the function then abother step would be to understand the line to edit, this can be more vague because now your whole function is in context.\\n\\nYou can have Cline run in plan mode first to plan the edit before switching to act, this avoids cleaning up incorrect edits.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ps9p8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I only use continue for local autocomplete (which needs specialised FIM models with low latency). For local agent stuff I would recommend Cline or Roo Code. For a model I would recommend Devstral, in a high quant (unsloth q5_k_xl or better). You can get away with less context provided your tasking client can properly isolate the functions to be edited. Having the full 128k context available is more an insurance.&lt;/p&gt;\\n\\n&lt;p&gt;For edits like you are wanting you need to describe where you want the code injected in a way that the model will be able to gwnerate a regex query with a chance of success. E.g. give it the function name. Once the model has used regex to grab the function then abother step would be to understand the line to edit, this can be more vague because now your whole function is in context.&lt;/p&gt;\\n\\n&lt;p&gt;You can have Cline run in plan mode first to plan the edit before switching to act, this avoids cleaning up incorrect edits.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnin1x/ai_coding_agentswhat_am_i_doing_wrong/n0ps9p8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751351266,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnin1x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(l,{data:t});export{r as default};
