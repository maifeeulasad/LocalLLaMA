import{j as e}from"./index-Cd3v0jxz.js";import{R as l}from"./RedditPostRenderer-cI96YLhy.js";import"./index-DGBoKyQm.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hello,\\n\\nI'm interested in running local LLMs but it's not super clear to me if it's better to have Nvidia over AMD for this use case.\\n\\nThe main idea would be to run local LLMs to hook them up to Cursor/Cline/Roo/etc for programming work.\\n\\nThe budget is fairly limited, I guess maybe 1000â‚¬ for GPUs (which I guess could get me about 32GB of VRAM in 2 GPUs).\\n\\nI know that Nvidia is king of the hill for data centers, but that's another world. Does CUDA matter for local LLMs?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Is there a reason to prefer Nvidia over AMD for programming use cases?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m4zpqt","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.43,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_9a80o","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753044589,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m interested in running local LLMs but it&amp;#39;s not super clear to me if it&amp;#39;s better to have Nvidia over AMD for this use case.&lt;/p&gt;\\n\\n&lt;p&gt;The main idea would be to run local LLMs to hook them up to Cursor/Cline/Roo/etc for programming work.&lt;/p&gt;\\n\\n&lt;p&gt;The budget is fairly limited, I guess maybe 1000â‚¬ for GPUs (which I guess could get me about 32GB of VRAM in 2 GPUs).&lt;/p&gt;\\n\\n&lt;p&gt;I know that Nvidia is king of the hill for data centers, but that&amp;#39;s another world. Does CUDA matter for local LLMs?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m4zpqt","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"oblio-","discussion_type":null,"num_comments":14,"send_replies":false,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m4zpqt/is_there_a_reason_to_prefer_nvidia_over_amd_for/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m4zpqt/is_there_a_reason_to_prefer_nvidia_over_amd_for/","subreddit_subscribers":502273,"created_utc":1753044589,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n48btp7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"oblio-","can_mod_post":false,"created_utc":1753045612,"send_replies":true,"parent_id":"t1_n48bauh","score":3,"author_fullname":"t2_9a80o","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Inference 95%, I would say, for my use case.\\n\\nI wasn't super sure what Nvidia's moat was (that led to that huge market cap), so it's actually training those huge models. Regular news articles don't write that out since regular folks don't care, I imagine ðŸ™‚\\n\\nThank you for the detailed explanation!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n48btp7","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Inference 95%, I would say, for my use case.&lt;/p&gt;\\n\\n&lt;p&gt;I wasn&amp;#39;t super sure what Nvidia&amp;#39;s moat was (that led to that huge market cap), so it&amp;#39;s actually training those huge models. Regular news articles don&amp;#39;t write that out since regular folks don&amp;#39;t care, I imagine ðŸ™‚&lt;/p&gt;\\n\\n&lt;p&gt;Thank you for the detailed explanation!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4zpqt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4zpqt/is_there_a_reason_to_prefer_nvidia_over_amd_for/n48btp7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753045612,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n48bauh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ForsookComparison","can_mod_post":false,"created_utc":1753045448,"send_replies":true,"parent_id":"t3_1m4zpqt","score":5,"author_fullname":"t2_on5es7pe3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you're doing inference and can follow basic instructions (Llama CPP Hipblas build instructions) then there's very little reason not to pick AMD if it's better price-vs-performance where you're buying.\\n\\nNvidia has an edge on prompt-processing (especially if you go for older Vega-based Instinct cards for AMD), but it's not the end of the world unless latency is super-critical to whatever this workstation will be doing.\\n\\nIf you're training and haven't wrestled with rocm+pytorch docker images with success before, then just consider it Nvidia's world for now.","edited":1753045696,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n48bauh","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you&amp;#39;re doing inference and can follow basic instructions (Llama CPP Hipblas build instructions) then there&amp;#39;s very little reason not to pick AMD if it&amp;#39;s better price-vs-performance where you&amp;#39;re buying.&lt;/p&gt;\\n\\n&lt;p&gt;Nvidia has an edge on prompt-processing (especially if you go for older Vega-based Instinct cards for AMD), but it&amp;#39;s not the end of the world unless latency is super-critical to whatever this workstation will be doing.&lt;/p&gt;\\n\\n&lt;p&gt;If you&amp;#39;re training and haven&amp;#39;t wrestled with rocm+pytorch docker images with success before, then just consider it Nvidia&amp;#39;s world for now.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4zpqt/is_there_a_reason_to_prefer_nvidia_over_amd_for/n48bauh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753045448,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m4zpqt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n48af28","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"fizzy1242","can_mod_post":false,"created_utc":1753045176,"send_replies":true,"parent_id":"t3_1m4zpqt","score":4,"author_fullname":"t2_16zcsx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"from what I understand, modern amd gpus are fine for inferencing llms with vulkan and rocm/zluda. but for finetuning, you'll probably want nvidia","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n48af28","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;from what I understand, modern amd gpus are fine for inferencing llms with vulkan and rocm/zluda. but for finetuning, you&amp;#39;ll probably want nvidia&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4zpqt/is_there_a_reason_to_prefer_nvidia_over_amd_for/n48af28/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753045176,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4zpqt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n48blmj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fp4guru","can_mod_post":false,"created_utc":1753045542,"send_replies":true,"parent_id":"t3_1m4zpqt","score":2,"author_fullname":"t2_1tp8zldw5g","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Mi50 is probably the most affordable one. You have to live with the inconvenience.","edited":1753049248,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n48blmj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Mi50 is probably the most affordable one. You have to live with the inconvenience.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4zpqt/is_there_a_reason_to_prefer_nvidia_over_amd_for/n48blmj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753045542,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4zpqt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n48bo0o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mpasila","can_mod_post":false,"created_utc":1753045562,"send_replies":true,"parent_id":"t3_1m4zpqt","score":2,"author_fullname":"t2_lhhagpdw","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For LLMs AMD will probably work fine. For anything else you may not have as much luck with AMD. Some more popular stuff like stable-diffusion I think probably will work but you kinda have to check it for yourself.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n48bo0o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For LLMs AMD will probably work fine. For anything else you may not have as much luck with AMD. Some more popular stuff like stable-diffusion I think probably will work but you kinda have to check it for yourself.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4zpqt/is_there_a_reason_to_prefer_nvidia_over_amd_for/n48bo0o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753045562,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4zpqt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n48j3e2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"thetaFAANG","can_mod_post":false,"created_utc":1753047934,"send_replies":true,"parent_id":"t3_1m4zpqt","score":2,"author_fullname":"t2_da5i8ajs","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"At this point it only matters if you are trying to make a return on investment based on purchase price, performance and energy consumption\\n\\notherwise is basically doesnt matter, just get whatever card you have access to and find economical to purchase","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n48j3e2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;At this point it only matters if you are trying to make a return on investment based on purchase price, performance and energy consumption&lt;/p&gt;\\n\\n&lt;p&gt;otherwise is basically doesnt matter, just get whatever card you have access to and find economical to purchase&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4zpqt/is_there_a_reason_to_prefer_nvidia_over_amd_for/n48j3e2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753047934,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4zpqt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n48usg0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ttkciar","can_mod_post":false,"created_utc":1753051923,"send_replies":true,"parent_id":"t3_1m4zpqt","score":2,"author_fullname":"t2_cpegz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"AMD gives you more bang for your buck, **if** your inference stack supports it.\\n\\nllama.cpp/vulkan works great with AMD, so I'm happy with it (using a 32GB MI60 in one system, 16GB V340 in another).  I have heard pytorch also jfw with AMD these days, too, but I can't speak from personal experience.\\n\\nWhatever framework you prefer to program with, see if it supports AMD.  If it does, go with AMD.  By \\"more bang for your buck\\" I mean it's cheaper for a given capability relative to Nvidia.","edited":1753054861,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n48usg0","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;AMD gives you more bang for your buck, &lt;strong&gt;if&lt;/strong&gt; your inference stack supports it.&lt;/p&gt;\\n\\n&lt;p&gt;llama.cpp/vulkan works great with AMD, so I&amp;#39;m happy with it (using a 32GB MI60 in one system, 16GB V340 in another).  I have heard pytorch also jfw with AMD these days, too, but I can&amp;#39;t speak from personal experience.&lt;/p&gt;\\n\\n&lt;p&gt;Whatever framework you prefer to program with, see if it supports AMD.  If it does, go with AMD.  By &amp;quot;more bang for your buck&amp;quot; I mean it&amp;#39;s cheaper for a given capability relative to Nvidia.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4zpqt/is_there_a_reason_to_prefer_nvidia_over_amd_for/n48usg0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753051923,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m4zpqt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n48ltz7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"oblio-","can_mod_post":false,"created_utc":1753048838,"send_replies":true,"parent_id":"t1_n48bp7z","score":2,"author_fullname":"t2_9a80o","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Great point about testing, thank you!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n48ltz7","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Great point about testing, thank you!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4zpqt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4zpqt/is_there_a_reason_to_prefer_nvidia_over_amd_for/n48ltz7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753048838,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n48bp7z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"offlinesir","can_mod_post":false,"created_utc":1753045573,"send_replies":true,"parent_id":"t3_1m4zpqt","score":2,"author_fullname":"t2_jn5ft2le","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I, along with many others, would go NvidiaÂ just due to support from programs. CUDA is often required or just in general better overall (cuda is an NvidiaÂ GPU exclusive feature) when running AI tasks. Ollama and llama.cpp do support AMD gpu's, but you are better off with Nvidia.\\n\\nAs for your end goal:\\n\\n&gt;The main idea would be to run local LLMs to hook them up to Cursor/Cline/Roo/etc for programming work.\\n\\nRemember that 32gb of vram isn't going to be enough for a coding model that can do \\"agent\\" style interactions you may be used to. Before you fork over $1000 for this setup, I would recomend you use openrouter API with free models to test if those local models you are considering would actually do what you need.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n48bp7z","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I, along with many others, would go NvidiaÂ just due to support from programs. CUDA is often required or just in general better overall (cuda is an NvidiaÂ GPU exclusive feature) when running AI tasks. Ollama and llama.cpp do support AMD gpu&amp;#39;s, but you are better off with Nvidia.&lt;/p&gt;\\n\\n&lt;p&gt;As for your end goal:&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;The main idea would be to run local LLMs to hook them up to Cursor/Cline/Roo/etc for programming work.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Remember that 32gb of vram isn&amp;#39;t going to be enough for a coding model that can do &amp;quot;agent&amp;quot; style interactions you may be used to. Before you fork over $1000 for this setup, I would recomend you use openrouter API with free models to test if those local models you are considering would actually do what you need.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4zpqt/is_there_a_reason_to_prefer_nvidia_over_amd_for/n48bp7z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753045573,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4zpqt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n48sx9f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"shaolinmaru","can_mod_post":false,"send_replies":true,"parent_id":"t1_n48agle","score":2,"author_fullname":"t2_17jq0u","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I think it will depends of the target SO too.\\n\\nAssuming Linux usage (because ROCm compatibility) the extra VRAM could make the advantage tilt to AMD side.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n48sx9f","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think it will depends of the target SO too.&lt;/p&gt;\\n\\n&lt;p&gt;Assuming Linux usage (because ROCm compatibility) the extra VRAM could make the advantage tilt to AMD side.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4zpqt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4zpqt/is_there_a_reason_to_prefer_nvidia_over_amd_for/n48sx9f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753051272,"author_flair_text":null,"treatment_tags":[],"created_utc":1753051272,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4af55t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Rich_Repeat_22","can_mod_post":false,"send_replies":true,"parent_id":"t1_n48agle","score":2,"author_fullname":"t2_viufiki6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"So you are talking about mid-low end cards. AMD 16GB then. Going from 8B (12GB GPU) to 12B model (16GB GPU) is big gap. \\n\\nIf you buy 7000/9000 series, we have ROCm 7 coming out with full windows support, not only Linux. \\n\\nIf you also want to play with ComfyUI there is a guide that takes 10 mins to follow and works like a charm on Windows.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4af55t","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So you are talking about mid-low end cards. AMD 16GB then. Going from 8B (12GB GPU) to 12B model (16GB GPU) is big gap. &lt;/p&gt;\\n\\n&lt;p&gt;If you buy 7000/9000 series, we have ROCm 7 coming out with full windows support, not only Linux. &lt;/p&gt;\\n\\n&lt;p&gt;If you also want to play with ComfyUI there is a guide that takes 10 mins to follow and works like a charm on Windows.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4zpqt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4zpqt/is_there_a_reason_to_prefer_nvidia_over_amd_for/n4af55t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753073674,"author_flair_text":null,"treatment_tags":[],"created_utc":1753073674,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n48agle","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"oblio-","can_mod_post":false,"created_utc":1753045189,"send_replies":true,"parent_id":"t1_n489vcp","score":5,"author_fullname":"t2_9a80o","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It's not a troll post. What's the performance difference, exactly?\\n\\nFor example, assuming the prices are comparable, is it better to have Nvidia 12GB or AMD 16GB?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n48agle","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s not a troll post. What&amp;#39;s the performance difference, exactly?&lt;/p&gt;\\n\\n&lt;p&gt;For example, assuming the prices are comparable, is it better to have Nvidia 12GB or AMD 16GB?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4zpqt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4zpqt/is_there_a_reason_to_prefer_nvidia_over_amd_for/n48agle/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753045189,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n489vcp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"mayo551","can_mod_post":false,"created_utc":1753045004,"send_replies":true,"parent_id":"t3_1m4zpqt","score":-6,"author_fullname":"t2_vsz5kd9o","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":true,"body":"I'm unsure if this is a troll post or not.\\n\\nDoes it matter? YES.\\n\\nCan you use AMD? SOMETIMES.\\n\\nHave fun.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n489vcp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m unsure if this is a troll post or not.&lt;/p&gt;\\n\\n&lt;p&gt;Does it matter? YES.&lt;/p&gt;\\n\\n&lt;p&gt;Can you use AMD? SOMETIMES.&lt;/p&gt;\\n\\n&lt;p&gt;Have fun.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4zpqt/is_there_a_reason_to_prefer_nvidia_over_amd_for/n489vcp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753045004,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4zpqt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-6}}],"before":null}}]`),n=()=>e.jsx(l,{data:t});export{n as default};
