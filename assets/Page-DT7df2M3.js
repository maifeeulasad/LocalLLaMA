import{j as e}from"./index-DQXiEb7D.js";import{R as t}from"./RedditPostRenderer-BjndLgq8.js";import"./index-B-ILyjT1.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hey guys, me and some friends are working on a project for the summer just to get our feet a little wet in the field. We are freshman uni students with a good amount of coding experience. Just wanted y’all’s thoughts about the project and its usability/feasibility along with anything else yall got. \\n\\n\\nProject Info: \\n\\nUse ai to detect bias in text. We’ve identified 4 different categories that help make up bias and are fine tuning a model and want to use it as a multi label classifier  to label bias among those 4 categories. Then make the model accessible via a chrome extension. The idea is to use it when reading news articles to see what types of bias are present in what you’re reading. Eventually we want to expand it to the writing side of things as well with a “writing mode” where the same core model detects the biases in your text and then offers more neutral text to replace it. So kinda like grammarly but for bias. \\n\\n\\nAgain appreciate any and all thoughts","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Wanted y’all’s thoughts on a project","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m2glqg","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.4,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_6q96hy8v","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752779906,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey guys, me and some friends are working on a project for the summer just to get our feet a little wet in the field. We are freshman uni students with a good amount of coding experience. Just wanted y’all’s thoughts about the project and its usability/feasibility along with anything else yall got. &lt;/p&gt;\\n\\n&lt;p&gt;Project Info: &lt;/p&gt;\\n\\n&lt;p&gt;Use ai to detect bias in text. We’ve identified 4 different categories that help make up bias and are fine tuning a model and want to use it as a multi label classifier  to label bias among those 4 categories. Then make the model accessible via a chrome extension. The idea is to use it when reading news articles to see what types of bias are present in what you’re reading. Eventually we want to expand it to the writing side of things as well with a “writing mode” where the same core model detects the biases in your text and then offers more neutral text to replace it. So kinda like grammarly but for bias. &lt;/p&gt;\\n\\n&lt;p&gt;Again appreciate any and all thoughts&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m2glqg","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"King-Ninja-OG","discussion_type":null,"num_comments":2,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m2glqg/wanted_yalls_thoughts_on_a_project/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m2glqg/wanted_yalls_thoughts_on_a_project/","subreddit_subscribers":500897,"created_utc":1752779906,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3pexm3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"King-Ninja-OG","can_mod_post":false,"created_utc":1752787459,"send_replies":true,"parent_id":"t1_n3p22fg","score":2,"author_fullname":"t2_6q96hy8v","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This is written out greatly. Thanks so much.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3pexm3","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is written out greatly. Thanks so much.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2glqg","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2glqg/wanted_yalls_thoughts_on_a_project/n3pexm3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752787459,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3p22fg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RedKnightRG","can_mod_post":false,"created_utc":1752783769,"send_replies":true,"parent_id":"t3_1m2glqg","score":4,"author_fullname":"t2_tlq31","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"In my mind, this is a good example of a problem where you can get mostly there very cheaply but the last 20% will be extremely costly.  It looks to me like you have two goals: bias characterization/detection, and a writing assistant to remove bias from text.\\n\\nFor the first goal, if you search the academic literature you'll see that this is a problem that's been considered since well before the AI boom.  I think you can probably detect most of the obvious bias with pre-LLM ML techniques like NLP.  With relatively little effort you can train a classifier on your 4 categories and I bet, without that much training data or time, classify most articles relatively quickly - most of the obvious bias you'd want to detect is, well, obvious.   (Check out [scikit-learn](https://scikit-learn.org/1.4/tutorial/text_analytics/working_with_text_data.html)'s NLP tutorial page for some decent ideas on where to start if you wanted to go down this road.)  \\n\\nI realize this is an LLM board and you probably want LLM answers but at your age I think you would benefit from looking at the set of techniques we were using 10 years ago as not every problem calls for a hammer.  (Plus you can compare and contrast how effective your classifier, with limited data, is to a big generalist LLM.)  That said if you specifically want to use LLMs you could get a decent demo by tuning one prompt for each of the bias categories and then feeding the whole article into the LLM 4 times, one for each bias category with the appropriate prompt, and seeing what comes out.   With a little bit of prompt tuning (for example, ask the LLM to pay attention to how many positive adjectives are being applied to certain nouns or how many negative ones - ask it to focus on whatever signatures you have identified for your 4 categories) I'm confident you could get an LLM to identify the most obvious bias.\\n\\nFor the second goal, if you want to make a webpage or a writing assistance app you can vibe code a simple webpage that uses openrouter calls to some free LLM system to write new text to the right of input text being written on the left.   The prompt construction should look like \\\\[users's text\\\\] + \\\\[detected category of bias\\\\] + \\\\[prompt with hints on how to remove that kind of bias and general language on otherwise reproducing the author's central ideas and tone, etc.\\\\].\\n\\nSo your final pipeline would look like:\\n\\n\\\\[Input Text\\\\] -&gt; Classify for 4 bias categories (or use an LLM) -&gt; Pass Text + Identified categories + bias remedy to LLM -&gt; corrected text.\\n\\nWith 4 defined categories you can identify signatures for those categories by hand.  So as long as you accept that you will miss subtle bias + hallucinate false positives of bias  this is a doable project.   \\n\\nIf you want to attack the 20% that is subtle+difficult you are now in another world entirely.   I bet an LLM will be great at ferreting out some kinds of bias like dog whistle adjectives (e.g., the usage of 'urban' in certain contexts) simply because they will have been trained on plenty of text with these dog whistles(\\\\*) and so those probability associations should already be baked into the weights.   On the other hand, if you want to see if someone has, say, quoted a set of statistics in a way to make a biased point, this is almost certainly beyond the capabilities of current models, even to us as human readers, as you would need access to the full data set they were slicing from to identify this.  I could imagine a whole complex RAG pipeline where you start scraping the internet to try and build evidence for an LLM to identify subtle bias signatures and, while that's definitely an interesting project, I think you're likely to invest 10x the time to get that working compared to the rest of the project and you will only catch 20% more bias in total quantity terms than far easier approaches.\\n\\nSo yeah, keep your focus limited and your bias categories defined with clear signatures for positive detection and this is totally viable as a demo at least.\\n\\n\\n\\n\\\\* As an aside, I wonder how much your prompts will get hijacked by all the work the frontier firms are doing to prevent their models from saying objectionable things.(\\\\*\\\\*)   Prompting a model that it is a researcher looking for biased content and that you want it to explain how biased certain content is is very adjacent to prompts designed to jailbreak models to produce actually biased content.   You might need to shop models a bit if you find the model rejecting your queries or otherwise scurrying away from the subject matter.\\n\\n  \\n\\\\*\\\\* For fun, you could actually use grok or uncensored open models as outlier detectors in both directions.    Given the variance in training/fine-tuning/prompting, you could actually build an entire signal detection engine by training a model to predict bias probability based solely on how different models react to the same text.   Out of scope for your project and something that might, again, take a long time to build up but I can see it being hilarious to play with  (And enlightening about what 'bias' might actually mean!).","edited":1752785205,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3p22fg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;In my mind, this is a good example of a problem where you can get mostly there very cheaply but the last 20% will be extremely costly.  It looks to me like you have two goals: bias characterization/detection, and a writing assistant to remove bias from text.&lt;/p&gt;\\n\\n&lt;p&gt;For the first goal, if you search the academic literature you&amp;#39;ll see that this is a problem that&amp;#39;s been considered since well before the AI boom.  I think you can probably detect most of the obvious bias with pre-LLM ML techniques like NLP.  With relatively little effort you can train a classifier on your 4 categories and I bet, without that much training data or time, classify most articles relatively quickly - most of the obvious bias you&amp;#39;d want to detect is, well, obvious.   (Check out &lt;a href=\\"https://scikit-learn.org/1.4/tutorial/text_analytics/working_with_text_data.html\\"&gt;scikit-learn&lt;/a&gt;&amp;#39;s NLP tutorial page for some decent ideas on where to start if you wanted to go down this road.)  &lt;/p&gt;\\n\\n&lt;p&gt;I realize this is an LLM board and you probably want LLM answers but at your age I think you would benefit from looking at the set of techniques we were using 10 years ago as not every problem calls for a hammer.  (Plus you can compare and contrast how effective your classifier, with limited data, is to a big generalist LLM.)  That said if you specifically want to use LLMs you could get a decent demo by tuning one prompt for each of the bias categories and then feeding the whole article into the LLM 4 times, one for each bias category with the appropriate prompt, and seeing what comes out.   With a little bit of prompt tuning (for example, ask the LLM to pay attention to how many positive adjectives are being applied to certain nouns or how many negative ones - ask it to focus on whatever signatures you have identified for your 4 categories) I&amp;#39;m confident you could get an LLM to identify the most obvious bias.&lt;/p&gt;\\n\\n&lt;p&gt;For the second goal, if you want to make a webpage or a writing assistance app you can vibe code a simple webpage that uses openrouter calls to some free LLM system to write new text to the right of input text being written on the left.   The prompt construction should look like [users&amp;#39;s text] + [detected category of bias] + [prompt with hints on how to remove that kind of bias and general language on otherwise reproducing the author&amp;#39;s central ideas and tone, etc.].&lt;/p&gt;\\n\\n&lt;p&gt;So your final pipeline would look like:&lt;/p&gt;\\n\\n&lt;p&gt;[Input Text] -&amp;gt; Classify for 4 bias categories (or use an LLM) -&amp;gt; Pass Text + Identified categories + bias remedy to LLM -&amp;gt; corrected text.&lt;/p&gt;\\n\\n&lt;p&gt;With 4 defined categories you can identify signatures for those categories by hand.  So as long as you accept that you will miss subtle bias + hallucinate false positives of bias  this is a doable project.   &lt;/p&gt;\\n\\n&lt;p&gt;If you want to attack the 20% that is subtle+difficult you are now in another world entirely.   I bet an LLM will be great at ferreting out some kinds of bias like dog whistle adjectives (e.g., the usage of &amp;#39;urban&amp;#39; in certain contexts) simply because they will have been trained on plenty of text with these dog whistles(*) and so those probability associations should already be baked into the weights.   On the other hand, if you want to see if someone has, say, quoted a set of statistics in a way to make a biased point, this is almost certainly beyond the capabilities of current models, even to us as human readers, as you would need access to the full data set they were slicing from to identify this.  I could imagine a whole complex RAG pipeline where you start scraping the internet to try and build evidence for an LLM to identify subtle bias signatures and, while that&amp;#39;s definitely an interesting project, I think you&amp;#39;re likely to invest 10x the time to get that working compared to the rest of the project and you will only catch 20% more bias in total quantity terms than far easier approaches.&lt;/p&gt;\\n\\n&lt;p&gt;So yeah, keep your focus limited and your bias categories defined with clear signatures for positive detection and this is totally viable as a demo at least.&lt;/p&gt;\\n\\n&lt;p&gt;* As an aside, I wonder how much your prompts will get hijacked by all the work the frontier firms are doing to prevent their models from saying objectionable things.(**)   Prompting a model that it is a researcher looking for biased content and that you want it to explain how biased certain content is is very adjacent to prompts designed to jailbreak models to produce actually biased content.   You might need to shop models a bit if you find the model rejecting your queries or otherwise scurrying away from the subject matter.&lt;/p&gt;\\n\\n&lt;p&gt;** For fun, you could actually use grok or uncensored open models as outlier detectors in both directions.    Given the variance in training/fine-tuning/prompting, you could actually build an entire signal detection engine by training a model to predict bias probability based solely on how different models react to the same text.   Out of scope for your project and something that might, again, take a long time to build up but I can see it being hilarious to play with  (And enlightening about what &amp;#39;bias&amp;#39; might actually mean!).&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2glqg/wanted_yalls_thoughts_on_a_project/n3p22fg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752783769,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2glqg","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}}]`),l=()=>e.jsx(t,{data:a});export{l as default};
