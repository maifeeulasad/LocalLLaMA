import{j as e}from"./index-Bu7qcPAU.js";import{R as l}from"./RedditPostRenderer-CbHA7O5q.js";import"./index-BKgbfxhf.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I have 32 GB of ram and a 4060 TI 16 GB. What's the best model I can run right now?\\n\\nIs there a website where you can just enter your specs and it spits out compatible models?\\n\\nWhat's the best local UI right now? LM Studio?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"It's been a while, I'm out of date, suggest me a model","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1ly0jnx","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.5,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_67hxg","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752327096,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I have 32 GB of ram and a 4060 TI 16 GB. What&amp;#39;s the best model I can run right now?&lt;/p&gt;\\n\\n&lt;p&gt;Is there a website where you can just enter your specs and it spits out compatible models?&lt;/p&gt;\\n\\n&lt;p&gt;What&amp;#39;s the best local UI right now? LM Studio?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1ly0jnx","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"mmmm_frietjes","discussion_type":null,"num_comments":12,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1ly0jnx/its_been_a_while_im_out_of_date_suggest_me_a_model/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1ly0jnx/its_been_a_while_im_out_of_date_suggest_me_a_model/","subreddit_subscribers":498345,"created_utc":1752327096,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2qeoo6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mmmm_frietjes","can_mod_post":false,"created_utc":1752330681,"send_replies":true,"parent_id":"t1_n2qdvf1","score":1,"author_fullname":"t2_67hxg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Perfect. Thank you","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2qeoo6","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Perfect. Thank you&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ly0jnx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ly0jnx/its_been_a_while_im_out_of_date_suggest_me_a_model/n2qeoo6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752330681,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2qsyy3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CommunityTough1","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2qm24h","score":2,"author_fullname":"t2_1iuzpxw7eg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You'll get around 1 token per second (maybe, if that) with something like LLaMA 3.3 70B since it'll be mostly offloaded to system RAM. I mean, it'll run, but barely.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2qsyy3","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You&amp;#39;ll get around 1 token per second (maybe, if that) with something like LLaMA 3.3 70B since it&amp;#39;ll be mostly offloaded to system RAM. I mean, it&amp;#39;ll run, but barely.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ly0jnx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ly0jnx/its_been_a_while_im_out_of_date_suggest_me_a_model/n2qsyy3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752335200,"author_flair_text":null,"treatment_tags":[],"created_utc":1752335200,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2r0cg2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mmmm_frietjes","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2qt3mq","score":1,"author_fullname":"t2_67hxg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Slow is fine. I can just do something else while I wait.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2r0cg2","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Slow is fine. I can just do something else while I wait.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ly0jnx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ly0jnx/its_been_a_while_im_out_of_date_suggest_me_a_model/n2r0cg2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752337491,"author_flair_text":null,"treatment_tags":[],"created_utc":1752337491,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2qt3mq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"lly0571","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2qm24h","score":1,"author_fullname":"t2_70vzcleel","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"70B dense models like Llama-3.3-70B or Qwen2.5-72B might still better than newer \\\\~30B models, but they would run intolerably slow (maybe 2t/s) as most of the model are loaded on CPU.\\n\\nIf you run moe at this size(like Hunyuan-A13B-80B), you can get some t/s if you are using DDR5. I got 80-100t/s prefill and \\\\~10t/s decode with llamacpp on a DDR5 PC with 4060TI. Maybe you can try Hunyuan-A13B-Q3(\\\\~36GB), but I don't think the model is that much better than Qwen3-32B.\\n\\nHere is Hunyuan Q4\\\\_K\\\\_XL on my PC:\\n\\nhttps://preview.redd.it/2xblhh98sgcf1.png?width=1783&amp;format=png&amp;auto=webp&amp;s=772116ea34e1b60bb9e7942a98ecd397eb36361e","edited":1752335466,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2qt3mq","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;70B dense models like Llama-3.3-70B or Qwen2.5-72B might still better than newer ~30B models, but they would run intolerably slow (maybe 2t/s) as most of the model are loaded on CPU.&lt;/p&gt;\\n\\n&lt;p&gt;If you run moe at this size(like Hunyuan-A13B-80B), you can get some t/s if you are using DDR5. I got 80-100t/s prefill and ~10t/s decode with llamacpp on a DDR5 PC with 4060TI. Maybe you can try Hunyuan-A13B-Q3(~36GB), but I don&amp;#39;t think the model is that much better than Qwen3-32B.&lt;/p&gt;\\n\\n&lt;p&gt;Here is Hunyuan Q4_K_XL on my PC:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/2xblhh98sgcf1.png?width=1783&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=772116ea34e1b60bb9e7942a98ecd397eb36361e\\"&gt;https://preview.redd.it/2xblhh98sgcf1.png?width=1783&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=772116ea34e1b60bb9e7942a98ecd397eb36361e&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ly0jnx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ly0jnx/its_been_a_while_im_out_of_date_suggest_me_a_model/n2qt3mq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752335241,"media_metadata":{"2xblhh98sgcf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":128,"x":108,"u":"https://preview.redd.it/2xblhh98sgcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=43bbd9e72bf91c4a6b3e4f207441e94d1c08e126"},{"y":257,"x":216,"u":"https://preview.redd.it/2xblhh98sgcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=67517b8540a3669a24dfa83a01dcd933464f6b18"},{"y":381,"x":320,"u":"https://preview.redd.it/2xblhh98sgcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a4cfda1adaf802272ca3c7a439b1115c1860144c"},{"y":763,"x":640,"u":"https://preview.redd.it/2xblhh98sgcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2d35d3ebe5ba60e9e2789deedba3fff590c11bc3"},{"y":1144,"x":960,"u":"https://preview.redd.it/2xblhh98sgcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=48b8e8f23e9ef8e7e37b618be59d0740ecbddc9d"},{"y":1287,"x":1080,"u":"https://preview.redd.it/2xblhh98sgcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=90b78abb633094f095346d3bb6ecb14fa060338d"}],"s":{"y":2126,"x":1783,"u":"https://preview.redd.it/2xblhh98sgcf1.png?width=1783&amp;format=png&amp;auto=webp&amp;s=772116ea34e1b60bb9e7942a98ecd397eb36361e"},"id":"2xblhh98sgcf1"}},"author_flair_text":null,"treatment_tags":[],"created_utc":1752335241,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2qm24h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mmmm_frietjes","can_mod_post":false,"created_utc":1752333046,"send_replies":true,"parent_id":"t1_n2qdvf1","score":1,"author_fullname":"t2_67hxg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What about 70b models when I combine gpu and normal ram?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2qm24h","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What about 70b models when I combine gpu and normal ram?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ly0jnx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ly0jnx/its_been_a_while_im_out_of_date_suggest_me_a_model/n2qm24h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752333046,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2tzle9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mucha_gordo","can_mod_post":false,"created_utc":1752373631,"send_replies":true,"parent_id":"t1_n2qdvf1","score":1,"author_fullname":"t2_2h3v47gv","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yooo great list I’ll have to try some with my anything llm setup rn","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2tzle9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yooo great list I’ll have to try some with my anything llm setup rn&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ly0jnx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ly0jnx/its_been_a_while_im_out_of_date_suggest_me_a_model/n2tzle9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752373631,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2qdvf1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"lly0571","can_mod_post":false,"created_utc":1752330413,"send_replies":true,"parent_id":"t3_1ly0jnx","score":12,"author_fullname":"t2_70vzcleel","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"A few models:\\n\\n* Mistral Small 3.2: Decent mid sized model with multimodal support. Q4 quants fits tightly with 16GB VRAM, you can get around 20 t/s with a 4060Ti. I think the model is slightly better than Gemma3-27B in 16GB vRAM devices. You can also try Magistral-Small-2506, but I think Qwen3/QwQ is better overall.\\n* Gemma3-27B-qat: Good mid sized model with multimodal support, more stable(less self repetition) and has a better tokenizer than Mistral, but more censored.\\n* Qwen3-32B: Larger model with more aggressive quant and better reasoning capability than mistral, you can get around 10-15 t/s with Q3K quants.\\n* GLM4-0414-32B: Larger model with more aggressive quant, might better than Qwen3(nothink) in some knowledge or programming tasks. You can reach around 15 t/s with Q3K quants.\\n* Qwen3-30B-A3B: Fast model(maybe 50+ t/s, related to RAM and offload config) with acceptable quality(with llamacpp \`-ot\`). You can also consider using Qwen3-14B-AWQ or Qwen3-8B-FP8 with vllm if you need throughput.\\n* Gemma3-4/12B-qat: Smaller everyday model with multimodal support. Gemma has qat which allows fewer performance degrade after quantization.\\n* Qwen2.5-VL-3/7B: Small multimodal LLM with better high resolution support than Gemma. There are some quality OCR finetunes based on Qwen2.5-VL series thanks to AI2(for OLMOCR dataset) and Qwen.\\n\\nHF has a built in \`Hardware compatibility\` which shows whether the model can run on your hardware.\\n\\nFor UI, I am using self hosted openwebui now.","edited":1752330594,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2qdvf1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;A few models:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Mistral Small 3.2: Decent mid sized model with multimodal support. Q4 quants fits tightly with 16GB VRAM, you can get around 20 t/s with a 4060Ti. I think the model is slightly better than Gemma3-27B in 16GB vRAM devices. You can also try Magistral-Small-2506, but I think Qwen3/QwQ is better overall.&lt;/li&gt;\\n&lt;li&gt;Gemma3-27B-qat: Good mid sized model with multimodal support, more stable(less self repetition) and has a better tokenizer than Mistral, but more censored.&lt;/li&gt;\\n&lt;li&gt;Qwen3-32B: Larger model with more aggressive quant and better reasoning capability than mistral, you can get around 10-15 t/s with Q3K quants.&lt;/li&gt;\\n&lt;li&gt;GLM4-0414-32B: Larger model with more aggressive quant, might better than Qwen3(nothink) in some knowledge or programming tasks. You can reach around 15 t/s with Q3K quants.&lt;/li&gt;\\n&lt;li&gt;Qwen3-30B-A3B: Fast model(maybe 50+ t/s, related to RAM and offload config) with acceptable quality(with llamacpp &lt;code&gt;-ot&lt;/code&gt;). You can also consider using Qwen3-14B-AWQ or Qwen3-8B-FP8 with vllm if you need throughput.&lt;/li&gt;\\n&lt;li&gt;Gemma3-4/12B-qat: Smaller everyday model with multimodal support. Gemma has qat which allows fewer performance degrade after quantization.&lt;/li&gt;\\n&lt;li&gt;Qwen2.5-VL-3/7B: Small multimodal LLM with better high resolution support than Gemma. There are some quality OCR finetunes based on Qwen2.5-VL series thanks to AI2(for OLMOCR dataset) and Qwen.&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;HF has a built in &lt;code&gt;Hardware compatibility&lt;/code&gt; which shows whether the model can run on your hardware.&lt;/p&gt;\\n\\n&lt;p&gt;For UI, I am using self hosted openwebui now.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ly0jnx/its_been_a_while_im_out_of_date_suggest_me_a_model/n2qdvf1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752330413,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ly0jnx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2q60xf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"m1tm0","can_mod_post":false,"created_utc":1752327724,"send_replies":true,"parent_id":"t3_1ly0jnx","score":3,"author_fullname":"t2_mcazknoi","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Smaller qwen or gemma probably, llama.cpp webui pretty good","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2q60xf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Smaller qwen or gemma probably, llama.cpp webui pretty good&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ly0jnx/its_been_a_while_im_out_of_date_suggest_me_a_model/n2q60xf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752327724,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ly0jnx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2qd4ul","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ii_social","can_mod_post":false,"created_utc":1752330168,"send_replies":true,"parent_id":"t3_1ly0jnx","score":2,"author_fullname":"t2_tohvxz80x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Qwen, Gemma, and Mistral are good, the latest ones which you can fit. \\n\\nAnd he’s LM studio is great! Better than ollama IMO, but no batching :O","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2qd4ul","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen, Gemma, and Mistral are good, the latest ones which you can fit. &lt;/p&gt;\\n\\n&lt;p&gt;And he’s LM studio is great! Better than ollama IMO, but no batching :O&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ly0jnx/its_been_a_while_im_out_of_date_suggest_me_a_model/n2qd4ul/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752330168,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ly0jnx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2qr8az","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SillyLilBear","can_mod_post":false,"created_utc":1752334663,"send_replies":true,"parent_id":"t3_1ly0jnx","score":2,"author_fullname":"t2_wjjtz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Qwen3 30B or 32B depending if you want MoE or not.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2qr8az","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen3 30B or 32B depending if you want MoE or not.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ly0jnx/its_been_a_while_im_out_of_date_suggest_me_a_model/n2qr8az/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752334663,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ly0jnx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2qkwby","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AaronFeng47","can_mod_post":false,"created_utc":1752332681,"send_replies":true,"parent_id":"t3_1ly0jnx","score":1,"author_fullname":"t2_4gc7hf3m","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Qwen3 14B iq4-xs","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2qkwby","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen3 14B iq4-xs&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ly0jnx/its_been_a_while_im_out_of_date_suggest_me_a_model/n2qkwby/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752332681,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1ly0jnx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2r021o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Total_Activity_7550","can_mod_post":false,"created_utc":1752337401,"send_replies":true,"parent_id":"t3_1ly0jnx","score":2,"author_fullname":"t2_nwfj64go","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What is your use case?\\nBe it coding chat - choose Qwen3 quant that fits in.\\nCode completion - Qwen2.5-Coder-14B-Base quant.\\nVision - GLM-4.1V 9B or Gemma 12B QAT or Gemma-27B low quant.\\n\\n\\nBest frontend - Open WebUI.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2r021o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What is your use case?\\nBe it coding chat - choose Qwen3 quant that fits in.\\nCode completion - Qwen2.5-Coder-14B-Base quant.\\nVision - GLM-4.1V 9B or Gemma 12B QAT or Gemma-27B low quant.&lt;/p&gt;\\n\\n&lt;p&gt;Best frontend - Open WebUI.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ly0jnx/its_been_a_while_im_out_of_date_suggest_me_a_model/n2r021o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752337401,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ly0jnx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),o=()=>e.jsx(l,{data:t});export{o as default};
