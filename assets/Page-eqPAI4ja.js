import{j as e}from"./index-xfnGEtuL.js";import{R as l}from"./RedditPostRenderer-DAZRIZZK.js";import"./index-BaNn5-RR.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Sure, the full Deepseek R1 model loads, but the tokens per second are still way too slow to be useful.  \\n\\n\\nSo I’m just curious: for those of you who spent $10K+ on that nice little box, what are you actually doing with it?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"People with a Mac Studio 512G: what are you doing with it?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lxgi3j","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.76,"author_flair_background_color":null,"subreddit_type":"public","ups":20,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_5vmxq95xe","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":20,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752263334,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Sure, the full Deepseek R1 model loads, but the tokens per second are still way too slow to be useful.  &lt;/p&gt;\\n\\n&lt;p&gt;So I’m just curious: for those of you who spent $10K+ on that nice little box, what are you actually doing with it?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lxgi3j","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Dangerous-Yak3976","discussion_type":null,"num_comments":31,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lxgi3j/people_with_a_mac_studio_512g_what_are_you_doing/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lxgi3j/people_with_a_mac_studio_512g_what_are_you_doing/","subreddit_subscribers":498115,"created_utc":1752263334,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2m86he","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"jzn21","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2m7ssj","score":8,"author_fullname":"t2_2oh4fbzf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes, it’s much better. Especially Unsloth models.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2m86he","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, it’s much better. Especially Unsloth models.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"link_id":"t3_1lxgi3j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgi3j/people_with_a_mac_studio_512g_what_are_you_doing/n2m86he/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752266994,"author_flair_text":null,"treatment_tags":[],"created_utc":1752266994,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2n86vv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"nomorebuttsplz","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2mqsma","score":1,"author_fullname":"t2_syq52","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Can you give an example of the type of prompt that you find Qwen excels in?","edited":false,"author_flair_css_class":null,"name":"t1_n2n86vv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Can you give an example of the type of prompt that you find Qwen excels in?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lxgi3j","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgi3j/people_with_a_mac_studio_512g_what_are_you_doing/n2n86vv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752278861,"author_flair_text":null,"collapsed":false,"created_utc":1752278861,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2mqsma","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"madaradess007","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2mj61j","score":2,"author_fullname":"t2_79slapln","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"i dunno, i find qwen much more intelligent than deepseek","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2mqsma","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;i dunno, i find qwen much more intelligent than deepseek&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxgi3j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgi3j/people_with_a_mac_studio_512g_what_are_you_doing/n2mqsma/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752272852,"author_flair_text":null,"treatment_tags":[],"created_utc":1752272852,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2mj61j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"nomorebuttsplz","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2m7ssj","score":2,"author_fullname":"t2_syq52","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes, but Qwen 235b is also quite good for math stuff. And LLama 3.3 70b fine tunes have a place still for creative instruction following ability.\\n\\nDeepseek R1 is if you need o3 level responses (without the web research by default) with sensitive data. Qwen 235b is o3 mini level. Smaller models can be great for agentic stuff. Or even something like Maverick with good prompt processing speeds.","edited":1752270645,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2mj61j","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, but Qwen 235b is also quite good for math stuff. And LLama 3.3 70b fine tunes have a place still for creative instruction following ability.&lt;/p&gt;\\n\\n&lt;p&gt;Deepseek R1 is if you need o3 level responses (without the web research by default) with sensitive data. Qwen 235b is o3 mini level. Smaller models can be great for agentic stuff. Or even something like Maverick with good prompt processing speeds.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxgi3j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgi3j/people_with_a_mac_studio_512g_what_are_you_doing/n2mj61j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752270370,"author_flair_text":null,"treatment_tags":[],"created_utc":1752270370,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2m7ssj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"false79","can_mod_post":false,"created_utc":1752266881,"send_replies":true,"parent_id":"t1_n2m57ob","score":2,"author_fullname":"t2_wn888","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Do you find you can get what you want from Deepseek 671b+ in one shot  compared to lower param models, e.g. Lllama 3 70b?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2m7ssj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Do you find you can get what you want from Deepseek 671b+ in one shot  compared to lower param models, e.g. Lllama 3 70b?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxgi3j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgi3j/people_with_a_mac_studio_512g_what_are_you_doing/n2m7ssj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752266881,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2n89z8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RateOk8628","can_mod_post":false,"created_utc":1752278893,"send_replies":true,"parent_id":"t1_n2m57ob","score":2,"author_fullname":"t2_abcbufokg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Are you using a quantized model?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2n89z8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Are you using a quantized model?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxgi3j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgi3j/people_with_a_mac_studio_512g_what_are_you_doing/n2n89z8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752278893,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2prgu5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jzn21","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2otj40","score":1,"author_fullname":"t2_2oh4fbzf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I use Q4 MLX","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2prgu5","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I use Q4 MLX&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"link_id":"t3_1lxgi3j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgi3j/people_with_a_mac_studio_512g_what_are_you_doing/n2prgu5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752322033,"author_flair_text":null,"treatment_tags":[],"created_utc":1752322033,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2otj40","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kaxapi","can_mod_post":false,"created_utc":1752303083,"send_replies":true,"parent_id":"t1_n2m57ob","score":2,"author_fullname":"t2_yejfw","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"20 t/s is quite fast, what quants and context size do you use?\\n\\nalso, do you run it with something like LM Studio or directly with mlx\\\\_lm library?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2otj40","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;20 t/s is quite fast, what quants and context size do you use?&lt;/p&gt;\\n\\n&lt;p&gt;also, do you run it with something like LM Studio or directly with mlx_lm library?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxgi3j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgi3j/people_with_a_mac_studio_512g_what_are_you_doing/n2otj40/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752303083,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2m57ob","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"jzn21","can_mod_post":false,"created_utc":1752266119,"send_replies":true,"parent_id":"t3_1lxgi3j","score":24,"author_fullname":"t2_2oh4fbzf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I run Deepseek V3 MLX and use it when privacy matters. It works excellent with 20 t/s. Also Maverick does an great job. Couldn’t be happier.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2m57ob","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I run Deepseek V3 MLX and use it when privacy matters. It works excellent with 20 t/s. Also Maverick does an great job. Couldn’t be happier.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgi3j/people_with_a_mac_studio_512g_what_are_you_doing/n2m57ob/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752266119,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxgi3j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":24}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2n5hss","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"NotSeanStrickland","can_mod_post":false,"created_utc":1752277899,"send_replies":true,"parent_id":"t1_n2mke5l","score":3,"author_fullname":"t2_1hssjiqz5k","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"R1 is no good for agents, all it does is benchmark well, and it is way too wordy. It just burns token for no reason. It basically thinks too much.\\n\\nUse V3, and in the system prompt, ask it to think about the users question in great detail, then call any tools needed to gather information required to complete the request.\\n\\nAfter the tool call is fed back in, the model will now see its own chain of thought in the input messages, giving more positive feedback to push the model down the right path, and it will become quite good for agentic use. It's not quite as intelligent as R1, but plans well, tool calls better, maintains context better, and runs at least 2x faster\\n\\nThank me later or message me if you can't get it working","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2n5hss","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;R1 is no good for agents, all it does is benchmark well, and it is way too wordy. It just burns token for no reason. It basically thinks too much.&lt;/p&gt;\\n\\n&lt;p&gt;Use V3, and in the system prompt, ask it to think about the users question in great detail, then call any tools needed to gather information required to complete the request.&lt;/p&gt;\\n\\n&lt;p&gt;After the tool call is fed back in, the model will now see its own chain of thought in the input messages, giving more positive feedback to push the model down the right path, and it will become quite good for agentic use. It&amp;#39;s not quite as intelligent as R1, but plans well, tool calls better, maintains context better, and runs at least 2x faster&lt;/p&gt;\\n\\n&lt;p&gt;Thank me later or message me if you can&amp;#39;t get it working&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxgi3j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgi3j/people_with_a_mac_studio_512g_what_are_you_doing/n2n5hss/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752277899,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2mke5l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"theDreamCome","can_mod_post":false,"created_utc":1752270756,"send_replies":true,"parent_id":"t3_1lxgi3j","score":6,"author_fullname":"t2_a1t4cb9y","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I’ve been trying multiple recipes and found my self leaning towards the 4-bit quantization version of most the models. \\n\\n\\n\\nMulti-model scenarios:\\n\\n—-\\n- Gemma / Gemma n models work pretty good.\\n- Qwen 3 family works great as well \\n- Mistral new generation models are now my preferred ones. \\n\\n\\nHigh memory models:\\n\\n—-\\n- Llama 4 models work good with vision and tool call capabilities. \\n- MoE Qwen 3 models work nicely!\\n- Deepseek R1: Everybody’s must-try model but for me doesn’t seem to work that good when it comes to a normal agentic flow. Time to first token reaches up to 30-40 seconds. Then, for the first response I achieve 19tokens/second response but after the first response it will take forever to process the subsequent prompt and sometimes fail to respond completely. \\n\\n\\nCouple of highlights:\\n\\n—-\\n- I use MLX model versions. \\n- I’ve gotten rid of the unified memory OS limitations making sure to use all the available memory. \\n- I’m using LM studio. \\n\\n\\nOther use cases:\\n\\n—-\\n\\n- Diffusion Models for Image generation\\n- Video Processing","edited":1752271658,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2mke5l","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I’ve been trying multiple recipes and found my self leaning towards the 4-bit quantization version of most the models. &lt;/p&gt;\\n\\n&lt;p&gt;Multi-model scenarios:&lt;/p&gt;\\n\\n&lt;p&gt;—-\\n- Gemma / Gemma n models work pretty good.\\n- Qwen 3 family works great as well \\n- Mistral new generation models are now my preferred ones. &lt;/p&gt;\\n\\n&lt;p&gt;High memory models:&lt;/p&gt;\\n\\n&lt;p&gt;—-\\n- Llama 4 models work good with vision and tool call capabilities. \\n- MoE Qwen 3 models work nicely!\\n- Deepseek R1: Everybody’s must-try model but for me doesn’t seem to work that good when it comes to a normal agentic flow. Time to first token reaches up to 30-40 seconds. Then, for the first response I achieve 19tokens/second response but after the first response it will take forever to process the subsequent prompt and sometimes fail to respond completely. &lt;/p&gt;\\n\\n&lt;p&gt;Couple of highlights:&lt;/p&gt;\\n\\n&lt;p&gt;—-\\n- I use MLX model versions. \\n- I’ve gotten rid of the unified memory OS limitations making sure to use all the available memory. \\n- I’m using LM studio. &lt;/p&gt;\\n\\n&lt;p&gt;Other use cases:&lt;/p&gt;\\n\\n&lt;p&gt;—-&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Diffusion Models for Image generation&lt;/li&gt;\\n&lt;li&gt;Video Processing&lt;/li&gt;\\n&lt;/ul&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgi3j/people_with_a_mac_studio_512g_what_are_you_doing/n2mke5l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752270756,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxgi3j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2m0841","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"panchovix","can_mod_post":false,"created_utc":1752264655,"send_replies":true,"parent_id":"t3_1lxgi3j","score":5,"author_fullname":"t2_j1kqr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I don't have a Mac but I guess the 512G one should be quite decent for TG t/s? I think the issue would be PP t/s, as that is compute bound.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2m0841","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t have a Mac but I guess the 512G one should be quite decent for TG t/s? I think the issue would be PP t/s, as that is compute bound.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgi3j/people_with_a_mac_studio_512g_what_are_you_doing/n2m0841/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752264655,"author_flair_text":"Llama 405B","treatment_tags":[],"link_id":"t3_1lxgi3j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2m04u8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"maglat","can_mod_post":false,"created_utc":1752264629,"send_replies":true,"parent_id":"t3_1lxgi3j","score":4,"author_fullname":"t2_clzcglj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"rule 34","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2m04u8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;rule 34&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgi3j/people_with_a_mac_studio_512g_what_are_you_doing/n2m04u8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752264629,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxgi3j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2p5775","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Front_Eagle739","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2msfre","score":2,"author_fullname":"t2_1jjg3a05mm","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"300 t/s or so. So few seconds for short context few thousand context but if you are loading in 50k fresh context at a time itll chug for a few minutes","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2p5775","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;300 t/s or so. So few seconds for short context few thousand context but if you are loading in 50k fresh context at a time itll chug for a few minutes&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxgi3j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgi3j/people_with_a_mac_studio_512g_what_are_you_doing/n2p5775/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752309903,"author_flair_text":null,"treatment_tags":[],"created_utc":1752309903,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2n64sd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Turbulent_Pin7635","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2msfre","score":1,"author_fullname":"t2_1hra1kibwa","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"From a few seconds to 1 minute","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2n64sd","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;From a few seconds to 1 minute&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxgi3j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgi3j/people_with_a_mac_studio_512g_what_are_you_doing/n2n64sd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752278125,"author_flair_text":null,"treatment_tags":[],"created_utc":1752278125,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2msfre","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Dry-Influence9","can_mod_post":false,"created_utc":1752273416,"send_replies":true,"parent_id":"t1_n2mqysn","score":1,"author_fullname":"t2_vep8zxxd","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"how long does the prompt processing take?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2msfre","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;how long does the prompt processing take?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxgi3j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgi3j/people_with_a_mac_studio_512g_what_are_you_doing/n2msfre/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752273416,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2mqysn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Turbulent_Pin7635","can_mod_post":false,"created_utc":1752272909,"send_replies":true,"parent_id":"t3_1lxgi3j","score":1,"author_fullname":"t2_1hra1kibwa","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Even the R1 goes between 18-20 t/s. Just a bit slow to start the prompt. But, nothing that kills","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2mqysn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Even the R1 goes between 18-20 t/s. Just a bit slow to start the prompt. But, nothing that kills&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgi3j/people_with_a_mac_studio_512g_what_are_you_doing/n2mqysn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752272909,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxgi3j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2n12oy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"acasto","can_mod_post":false,"created_utc":1752276368,"send_replies":true,"parent_id":"t3_1lxgi3j","score":1,"author_fullname":"t2_1itasc0o","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"That's why when I got the M2 Ultra I went with 128GB on the reasoning that if I actually needed to use more than that I should probably use something else and am still happy with it. And it's a range that I could get in MBP if I wanted or feasibly build out in a PC w/ GPUs if warranted.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2n12oy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s why when I got the M2 Ultra I went with 128GB on the reasoning that if I actually needed to use more than that I should probably use something else and am still happy with it. And it&amp;#39;s a range that I could get in MBP if I wanted or feasibly build out in a PC w/ GPUs if warranted.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgi3j/people_with_a_mac_studio_512g_what_are_you_doing/n2n12oy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752276368,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxgi3j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2nvgao","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"allenasm","can_mod_post":false,"created_utc":1752287385,"send_replies":true,"parent_id":"t3_1lxgi3j","score":1,"author_fullname":"t2_fouwt","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"MOE for many different things.  Running multiple models at once for different tasks.  Having very high precision for local llms. Tons of research on various models including some small amount of training which I'm doing more of lately.  Soon planning to get more deeply into rag / graphrag to add more of that to my local models. etc.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2nvgao","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;MOE for many different things.  Running multiple models at once for different tasks.  Having very high precision for local llms. Tons of research on various models including some small amount of training which I&amp;#39;m doing more of lately.  Soon planning to get more deeply into rag / graphrag to add more of that to my local models. etc.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgi3j/people_with_a_mac_studio_512g_what_are_you_doing/n2nvgao/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752287385,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxgi3j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2oid5k","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MachinaVerum","can_mod_post":false,"created_utc":1752297123,"send_replies":true,"parent_id":"t3_1lxgi3j","score":1,"author_fullname":"t2_6ybfgv5i","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"selling it and buying a blackwell","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2oid5k","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;selling it and buying a blackwell&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgi3j/people_with_a_mac_studio_512g_what_are_you_doing/n2oid5k/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752297123,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxgi3j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ooav9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Marksta","can_mod_post":false,"created_utc":1752300200,"send_replies":true,"parent_id":"t3_1lxgi3j","score":1,"author_fullname":"t2_559a1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Awaiting my first prompt to finish processing 💀","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ooav9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Awaiting my first prompt to finish processing 💀&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgi3j/people_with_a_mac_studio_512g_what_are_you_doing/n2ooav9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752300200,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxgi3j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2mtcv5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Mundane_Ad8936","can_mod_post":false,"created_utc":1752273732,"send_replies":false,"parent_id":"t1_n2m82r3","score":4,"author_fullname":"t2_j26ktvd7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":" Let's not pretend that the majority of people share that sentiment.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2mtcv5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Let&amp;#39;s not pretend that the majority of people share that sentiment.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxgi3j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgi3j/people_with_a_mac_studio_512g_what_are_you_doing/n2mtcv5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752273732,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2pbwnh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Forgot_Password_Dude","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2n5uhs","score":1,"author_fullname":"t2_g8xg6sut","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Meaning of life?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2pbwnh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Meaning of life?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxgi3j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgi3j/people_with_a_mac_studio_512g_what_are_you_doing/n2pbwnh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752314002,"author_flair_text":null,"treatment_tags":[],"created_utc":1752314002,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2n5uhs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ThisWillPass","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2moo46","score":0,"author_fullname":"t2_4mmtr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"42","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2n5uhs","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;42&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxgi3j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgi3j/people_with_a_mac_studio_512g_what_are_you_doing/n2n5uhs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752278024,"author_flair_text":null,"treatment_tags":[],"created_utc":1752278024,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2nrfu8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LocoMod","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2niryl","score":2,"author_fullname":"t2_6uuoq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I've been very tempted to get one but it's in this weird spot where I dont think its worth it. There isn't a model that is that much better that can fit in 96GB than what can fit in my 5090RTX 32GB. What is out there between Qwen3-32B and DeepSeek-R1 that can run in 96GB and beat Qwen3-32B at ~~Q8~~ Q6 in real world utility and not benchmaxing?","edited":false,"author_flair_css_class":null,"name":"t1_n2nrfu8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve been very tempted to get one but it&amp;#39;s in this weird spot where I dont think its worth it. There isn&amp;#39;t a model that is that much better that can fit in 96GB than what can fit in my 5090RTX 32GB. What is out there between Qwen3-32B and DeepSeek-R1 that can run in 96GB and beat Qwen3-32B at &lt;del&gt;Q8&lt;/del&gt; Q6 in real world utility and not benchmaxing?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lxgi3j","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgi3j/people_with_a_mac_studio_512g_what_are_you_doing/n2nrfu8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752285891,"author_flair_text":null,"collapsed":false,"created_utc":1752285891,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ol1j7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Expensive-Award1965","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2niryl","score":1,"author_fullname":"t2_qwfqrjbj5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"that's \\\\~20k for a system... from what i could find? not even close","edited":false,"author_flair_css_class":null,"name":"t1_n2ol1j7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;that&amp;#39;s ~20k for a system... from what i could find? not even close&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lxgi3j","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgi3j/people_with_a_mac_studio_512g_what_are_you_doing/n2ol1j7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752298486,"author_flair_text":null,"collapsed":false,"created_utc":1752298486,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2niryl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"false79","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2n92v5","score":5,"author_fullname":"t2_wn888","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The new Blackwell 6000 Pro cards. They are the current speed demon but limit your VRAM only to 96GB which only 18% the capacity of top of the line m3 ultra.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2niryl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The new Blackwell 6000 Pro cards. They are the current speed demon but limit your VRAM only to 96GB which only 18% the capacity of top of the line m3 ultra.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxgi3j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgi3j/people_with_a_mac_studio_512g_what_are_you_doing/n2niryl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752282724,"author_flair_text":null,"treatment_tags":[],"created_utc":1752282724,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2nqxeh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LocoMod","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2n92v5","score":1,"author_fullname":"t2_6uuoq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Oh I dont know. It doesnt matter in the context of the debate. The Mac Studio is still too slow when running inference on super large models. I own a 64GB M2 MAX MacBook Pro and 128GB M3 MAX Macbook Pro so im not hating on the Studio. I was in the market for it when it first came out and did my research and concluded that for most use cases, a top tier 32B or larger MoE local model will suffice. If all you're doing is back and forth chat, in a non highly technical domain, and you supplement it with RAG, then most mid tier setups suffice.\\n\\nIf you're running agent workflows or more complex things where your prompts balloon in size, and you're doing some sort of \\"frontier\\" work where you expect the model to infer something \\"new\\", then you need large models with plenty of memory to spare for large context, plus super fast prompt processing. The Apple hardware leaves a lot to be desired in that regard. I wish that wasnt the case but it is.\\n\\nYou can drop $100 in the OpenAI API and it will last you weeks if not months if you offload the hard tasks to it when a local model isnt sufficient. This is what I do in Manifold.\\n\\nNot trying to promote as this is just a hobby project. Stick with Ollama or whatever.\\n\\nhttps://preview.redd.it/fsdmu08yoccf1.png?width=3456&amp;format=png&amp;auto=webp&amp;s=d1c01f91d0b3e9be0b639646918f52844f1b76ee\\n\\n[https://github.com/intelligencedev/manifold](https://github.com/intelligencedev/manifold)","edited":1752286798,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2nqxeh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Oh I dont know. It doesnt matter in the context of the debate. The Mac Studio is still too slow when running inference on super large models. I own a 64GB M2 MAX MacBook Pro and 128GB M3 MAX Macbook Pro so im not hating on the Studio. I was in the market for it when it first came out and did my research and concluded that for most use cases, a top tier 32B or larger MoE local model will suffice. If all you&amp;#39;re doing is back and forth chat, in a non highly technical domain, and you supplement it with RAG, then most mid tier setups suffice.&lt;/p&gt;\\n\\n&lt;p&gt;If you&amp;#39;re running agent workflows or more complex things where your prompts balloon in size, and you&amp;#39;re doing some sort of &amp;quot;frontier&amp;quot; work where you expect the model to infer something &amp;quot;new&amp;quot;, then you need large models with plenty of memory to spare for large context, plus super fast prompt processing. The Apple hardware leaves a lot to be desired in that regard. I wish that wasnt the case but it is.&lt;/p&gt;\\n\\n&lt;p&gt;You can drop $100 in the OpenAI API and it will last you weeks if not months if you offload the hard tasks to it when a local model isnt sufficient. This is what I do in Manifold.&lt;/p&gt;\\n\\n&lt;p&gt;Not trying to promote as this is just a hobby project. Stick with Ollama or whatever.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/fsdmu08yoccf1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d1c01f91d0b3e9be0b639646918f52844f1b76ee\\"&gt;https://preview.redd.it/fsdmu08yoccf1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d1c01f91d0b3e9be0b639646918f52844f1b76ee&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/intelligencedev/manifold\\"&gt;https://github.com/intelligencedev/manifold&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxgi3j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgi3j/people_with_a_mac_studio_512g_what_are_you_doing/n2nqxeh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752285704,"media_metadata":{"fsdmu08yoccf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":61,"x":108,"u":"https://preview.redd.it/fsdmu08yoccf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c48a4b0653ee6c6cef9d3125ff42a2ec0d3d3c0b"},{"y":123,"x":216,"u":"https://preview.redd.it/fsdmu08yoccf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=cab1ea7395c493fee0eeca05b35b74685175a19e"},{"y":183,"x":320,"u":"https://preview.redd.it/fsdmu08yoccf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4dfd15907379ed0f77c1d3261de580de28af9756"},{"y":367,"x":640,"u":"https://preview.redd.it/fsdmu08yoccf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f8d4a61bcc61e104fad4dc44dca2ddb3052fff93"},{"y":550,"x":960,"u":"https://preview.redd.it/fsdmu08yoccf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=12d93339a211ea11a2c8458bdabe24091d7782c0"},{"y":619,"x":1080,"u":"https://preview.redd.it/fsdmu08yoccf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d89b319fffc650c2d8006f8509b6970ba2e11dd5"}],"s":{"y":1982,"x":3456,"u":"https://preview.redd.it/fsdmu08yoccf1.png?width=3456&amp;format=png&amp;auto=webp&amp;s=d1c01f91d0b3e9be0b639646918f52844f1b76ee"},"id":"fsdmu08yoccf1"}},"author_flair_text":null,"treatment_tags":[],"created_utc":1752285704,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2n92v5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Expensive-Award1965","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2moo46","score":-1,"author_fullname":"t2_qwfqrjbj5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"what goes faster than that for \\\\~10k?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2n92v5","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;what goes faster than that for ~10k?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxgi3j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgi3j/people_with_a_mac_studio_512g_what_are_you_doing/n2n92v5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752279183,"author_flair_text":null,"treatment_tags":[],"created_utc":1752279183,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2moo46","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LocoMod","can_mod_post":false,"created_utc":1752272140,"send_replies":true,"parent_id":"t1_n2m82r3","score":4,"author_fullname":"t2_6uuoq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Not by choice. You can justify dropping ~10k on a machine to infer at 2tks all you want. But if that’s all you’re getting out of it then it’s perfectly reasonable to think you wasted both time AND money. \\n\\nMost people spend money to save time. \\n\\n“But it’s private”\\n\\nSure. It’s still slow.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2moo46","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not by choice. You can justify dropping ~10k on a machine to infer at 2tks all you want. But if that’s all you’re getting out of it then it’s perfectly reasonable to think you wasted both time AND money. &lt;/p&gt;\\n\\n&lt;p&gt;Most people spend money to save time. &lt;/p&gt;\\n\\n&lt;p&gt;“But it’s private”&lt;/p&gt;\\n\\n&lt;p&gt;Sure. It’s still slow.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxgi3j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgi3j/people_with_a_mac_studio_512g_what_are_you_doing/n2moo46/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752272140,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n2m82r3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"segmond","can_mod_post":false,"created_utc":1752266964,"send_replies":true,"parent_id":"t3_1lxgi3j","score":-8,"author_fullname":"t2_ah13x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":true,"body":"Slow for who?  For you?   You need to stop projecting your problems unto others.   Some folks are happy to infer at 2 tk/sec.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2m82r3","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Slow for who?  For you?   You need to stop projecting your problems unto others.   Some folks are happy to infer at 2 tk/sec.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxgi3j/people_with_a_mac_studio_512g_what_are_you_doing/n2m82r3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752266964,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lxgi3j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-8}}],"before":null}}]`),n=()=>e.jsx(l,{data:t});export{n as default};
