import{j as e}from"./index-Bu7qcPAU.js";import{R as t}from"./RedditPostRenderer-CbHA7O5q.js";import"./index-BKgbfxhf.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"## 20/07/2025 10:20(GMT+3) Update ##\\n\\n- I think I wasn't clear on what I'm offering. I'm swamped with my personal ongoing projects so I don't have the capacity (and probably the ability lol) to implement all your cool ideas. I'm looking for something that's already baked. A ready to run script/notebook (and datasets).\\n\\n- So far /u/hotroaches4liferz [post](https://www.reddit.com/r/LocalLLaMA/comments/1m39uqi/i_made_a_1000_hour_nsfw_tts_dataset/) about the NSFW TTS dataset is in the lead (as suggested by /u/Semi_Tech )! Anyone up to create a notebook for it? (I've never fine tuned TTS models before)\\n\\n- There are a bunch of great ideas on here. I really liked distilling a smaller model based on Kimi K2 output or creating our own Qwen3-Coder while we wait for the official release. If anyone is up to script those, let's upvote them!\\n\\n---\\n\\nFollowing a comment I made on another post here that failed to come to fruition, I’ve decided to step it up. I’ve got some GPU resources, we (the community) have a ton of cool ideas - let’s make this happen.\\n\\nPremise is pretty simple, comment below with an idea for a fine-tune, any kind, any open weights model, any purpose/modality. We’ll let the community vote, and top comment (let’s say in 48hrs?) wins. \\n\\nRules are:\\n\\nHas to be something tested/mature. Unfortunately that means no “experiments”. I need a working notebook/script with a solid training pipeline (including all datasets, etc.), can’t provide shell access to the compute resources themselves. \\n\\nThe output of the training will be shared publicly on HF for the benefit of the community. \\n\\nWhat do you say, interested? ","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Localllama’s (first?) IFTA - I’ll Fine-Tune Anything","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m3yzes","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.92,"author_flair_background_color":null,"subreddit_type":"public","ups":65,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_4dvff","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":65,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":true,"thumbnail":"self","edited":1752996115,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752938891,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;h2&gt;20/07/2025 10:20(GMT+3) Update&lt;/h2&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;p&gt;I think I wasn&amp;#39;t clear on what I&amp;#39;m offering. I&amp;#39;m swamped with my personal ongoing projects so I don&amp;#39;t have the capacity (and probably the ability lol) to implement all your cool ideas. I&amp;#39;m looking for something that&amp;#39;s already baked. A ready to run script/notebook (and datasets).&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;So far &lt;a href=\\"/u/hotroaches4liferz\\"&gt;/u/hotroaches4liferz&lt;/a&gt; &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1m39uqi/i_made_a_1000_hour_nsfw_tts_dataset/\\"&gt;post&lt;/a&gt; about the NSFW TTS dataset is in the lead (as suggested by &lt;a href=\\"/u/Semi_Tech\\"&gt;/u/Semi_Tech&lt;/a&gt; )! Anyone up to create a notebook for it? (I&amp;#39;ve never fine tuned TTS models before)&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;There are a bunch of great ideas on here. I really liked distilling a smaller model based on Kimi K2 output or creating our own Qwen3-Coder while we wait for the official release. If anyone is up to script those, let&amp;#39;s upvote them!&lt;/p&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;p&gt;Following a comment I made on another post here that failed to come to fruition, I’ve decided to step it up. I’ve got some GPU resources, we (the community) have a ton of cool ideas - let’s make this happen.&lt;/p&gt;\\n\\n&lt;p&gt;Premise is pretty simple, comment below with an idea for a fine-tune, any kind, any open weights model, any purpose/modality. We’ll let the community vote, and top comment (let’s say in 48hrs?) wins. &lt;/p&gt;\\n\\n&lt;p&gt;Rules are:&lt;/p&gt;\\n\\n&lt;p&gt;Has to be something tested/mature. Unfortunately that means no “experiments”. I need a working notebook/script with a solid training pipeline (including all datasets, etc.), can’t provide shell access to the compute resources themselves. &lt;/p&gt;\\n\\n&lt;p&gt;The output of the training will be shared publicly on HF for the benefit of the community. &lt;/p&gt;\\n\\n&lt;p&gt;What do you say, interested? &lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1m3yzes","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"indicava","discussion_type":null,"num_comments":47,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/","subreddit_subscribers":502274,"created_utc":1752938891,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"50c36eba-fdca-11ee-9735-92a88d7e3b87","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n41ox5n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FuckNinjas","can_mod_post":false,"send_replies":true,"parent_id":"t1_n40ha7e","score":7,"author_fullname":"t2_cnicn","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I have 1.5GB of smut in a sqlite db. I didn't go beyond the titanic, ml-wise, but I can donate the.. dataset to the cause, if it helps.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n41ox5n","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have 1.5GB of smut in a sqlite db. I didn&amp;#39;t go beyond the titanic, ml-wise, but I can donate the.. dataset to the cause, if it helps.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3yzes","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n41ox5n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752953895,"author_flair_text":null,"treatment_tags":[],"created_utc":1752953895,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n40iqau","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"UsualAir4","can_mod_post":false,"send_replies":true,"parent_id":"t1_n40ha7e","score":1,"author_fullname":"t2_57nifx2p","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Can you fine tune sensevoice small ASR to have better emotion and event recognition? Like in that NSFW dataset","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n40iqau","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Can you fine tune sensevoice small ASR to have better emotion and event recognition? Like in that NSFW dataset&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3yzes","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n40iqau/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752940635,"author_flair_text":null,"treatment_tags":[],"created_utc":1752940635,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n40ha7e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"indicava","can_mod_post":false,"created_utc":1752940185,"send_replies":true,"parent_id":"t1_n40eq2b","score":15,"author_fullname":"t2_4dvff","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That’s the post the inspired this one. OP (of that post) never replied back…","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n40ha7e","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That’s the post the inspired this one. OP (of that post) never replied back…&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"link_id":"t3_1m3yzes","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n40ha7e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752940185,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":15}}],"before":null}},"user_reports":[],"saved":false,"id":"n40eq2b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Semi_Tech","can_mod_post":false,"created_utc":1752939398,"send_replies":true,"parent_id":"t3_1m3yzes","score":32,"author_fullname":"t2_kbar9qn28","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"There is that nsfw tts dataset that was posted here recently......","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n40eq2b","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Ollama"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There is that nsfw tts dataset that was posted here recently......&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n40eq2b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752939398,"author_flair_text":"Ollama","treatment_tags":[],"link_id":"t3_1m3yzes","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":32}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n42sbs4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"qrios","can_mod_post":false,"send_replies":true,"parent_id":"t1_n42fvcz","score":1,"author_fullname":"t2_3612e","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Sadly, ideas are cheaper than GPU time. Hopefully it gets picked though!","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n42sbs4","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sadly, ideas are cheaper than GPU time. Hopefully it gets picked though!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3yzes","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n42sbs4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752967089,"author_flair_text":null,"treatment_tags":[],"created_utc":1752967089,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n42fvcz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GenLabsAI","can_mod_post":false,"created_utc":1752962719,"send_replies":true,"parent_id":"t1_n4179qh","score":1,"author_fullname":"t2_14cl94t8ha","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Dang. I had this idea.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n42fvcz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Dang. I had this idea.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3yzes","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n42fvcz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752962719,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n44775f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"qrios","can_mod_post":false,"send_replies":true,"parent_id":"t1_n441bni","score":1,"author_fullname":"t2_3612e","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; but if the end output is different than the KV cache because a token has been removed, you will have to re-process the entire KV cache when a new prompt is sent.\\n\\nThat issue is already handled by the *\\"UIs could then be configured to hide output preceding the backspace tokens (while they remain in context for and visible to the model)\\"* part. \\nIn other words, the model would always see both the bad text, followed by backspace tokens, followed by corrected text. So from its perspective it remains autoregressive. \\nTo avoid polluting the context, this could optionally be cleared and the kv-cache from the earliest correction onward be recomputed while the user is busy typing a response.\\n\\n&gt;  I am not familiar with AI models misspelling things but I see your point. \\n\\nYeah I should have explained the reasoning for that better. The purpose of including typos (or something trivially typo-like) isn't actually to fix any tendency for typos models are unlikely to have in the first place. It's anticipating two things: \\n1. the model might better learn what the token is for if we leverage the model's internal sense of \\"wrong\\" or \\"undesirable\\" early by having backspace tokens show up in the most trivial examples where they would be necessary, thereby allowing the model to generalize from there to wider senses of undesirability on later training examples. (this is sort of relying on previous findings that models form general concepts of \\"bad\\", \\"undesirable\\", \\"evil\\" encoded in only a handful of directions)\\n2. compensate for the fact that models are pretty bad at counting, and are liable to generate the incorrect number of backspace tokens, thereby resulting in blatant typos when viewed by the user. This would allow downstream systems at inference time to optionally give the model its own corrected generation back to it in parallel as the user would see it, and let it catch any typos that end up resulting from not having applying backspace the correct number of times.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n44775f","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;but if the end output is different than the KV cache because a token has been removed, you will have to re-process the entire KV cache when a new prompt is sent.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;That issue is already handled by the &lt;em&gt;&amp;quot;UIs could then be configured to hide output preceding the backspace tokens (while they remain in context for and visible to the model)&amp;quot;&lt;/em&gt; part. \\nIn other words, the model would always see both the bad text, followed by backspace tokens, followed by corrected text. So from its perspective it remains autoregressive. \\nTo avoid polluting the context, this could optionally be cleared and the kv-cache from the earliest correction onward be recomputed while the user is busy typing a response.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;I am not familiar with AI models misspelling things but I see your point. &lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Yeah I should have explained the reasoning for that better. The purpose of including typos (or something trivially typo-like) isn&amp;#39;t actually to fix any tendency for typos models are unlikely to have in the first place. It&amp;#39;s anticipating two things: \\n1. the model might better learn what the token is for if we leverage the model&amp;#39;s internal sense of &amp;quot;wrong&amp;quot; or &amp;quot;undesirable&amp;quot; early by having backspace tokens show up in the most trivial examples where they would be necessary, thereby allowing the model to generalize from there to wider senses of undesirability on later training examples. (this is sort of relying on previous findings that models form general concepts of &amp;quot;bad&amp;quot;, &amp;quot;undesirable&amp;quot;, &amp;quot;evil&amp;quot; encoded in only a handful of directions)\\n2. compensate for the fact that models are pretty bad at counting, and are liable to generate the incorrect number of backspace tokens, thereby resulting in blatant typos when viewed by the user. This would allow downstream systems at inference time to optionally give the model its own corrected generation back to it in parallel as the user would see it, and let it catch any typos that end up resulting from not having applying backspace the correct number of times.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3yzes","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n44775f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752987309,"author_flair_text":null,"treatment_tags":[],"created_utc":1752987309,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n441bni","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"triynizzles1","can_mod_post":false,"created_utc":1752984612,"send_replies":true,"parent_id":"t1_n4179qh","score":1,"author_fullname":"t2_zr0g49ixt","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This is a good idea, but if the end output is different than the KV cache because a token has been removed, you will have to re-process the entire KV cache when a new prompt is sent. There could be some creative software infrastructure to correct this though.\\n\\nOn a sidenote, I am not familiar with AI models misspelling things but I see your point. An ai could say 2 “R” in strawberry. Then realize its actually 3 and correct itself.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n441bni","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is a good idea, but if the end output is different than the KV cache because a token has been removed, you will have to re-process the entire KV cache when a new prompt is sent. There could be some creative software infrastructure to correct this though.&lt;/p&gt;\\n\\n&lt;p&gt;On a sidenote, I am not familiar with AI models misspelling things but I see your point. An ai could say 2 “R” in strawberry. Then realize its actually 3 and correct itself.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3yzes","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n441bni/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752984612,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4179qh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"qrios","can_mod_post":false,"created_utc":1752948223,"send_replies":true,"parent_id":"t3_1m3yzes","score":9,"author_fullname":"t2_3612e","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Backspace Tokens.\\n\\nNot sure if this ventures too far into \\"experimental\\" territory but the dataset would be trivial to generate: just inject some typos and llm-predicted text snippets into some existing training data, followed by some number of token-wise \\"backspace\\" presses that would be needed to remove that text before continuing with the rest of the output.\\n\\nUIs could then be configured to hide output preceding the backspace tokens (while they remain in context for and visible to the model)\\n\\nThe advantages here are two-fold.\\n\\n1. It (may) teach the model to self correct the previous token if it notices that some instruction was violated while its processing the next token.\\n2. It would allow users to inject backspace tokens to the end of a generation, and thereby indicate to the model \\"whatever you generate should be something substantially different from that\\".\\n\\n\\n**EDIT:** I'm willing to generate the dataset. Training procedure should be pretty vanilla.","edited":1753015836,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4179qh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Backspace Tokens.&lt;/p&gt;\\n\\n&lt;p&gt;Not sure if this ventures too far into &amp;quot;experimental&amp;quot; territory but the dataset would be trivial to generate: just inject some typos and llm-predicted text snippets into some existing training data, followed by some number of token-wise &amp;quot;backspace&amp;quot; presses that would be needed to remove that text before continuing with the rest of the output.&lt;/p&gt;\\n\\n&lt;p&gt;UIs could then be configured to hide output preceding the backspace tokens (while they remain in context for and visible to the model)&lt;/p&gt;\\n\\n&lt;p&gt;The advantages here are two-fold.&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;It (may) teach the model to self correct the previous token if it notices that some instruction was violated while its processing the next token.&lt;/li&gt;\\n&lt;li&gt;It would allow users to inject backspace tokens to the end of a generation, and thereby indicate to the model &amp;quot;whatever you generate should be something substantially different from that&amp;quot;.&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; I&amp;#39;m willing to generate the dataset. Training procedure should be pretty vanilla.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n4179qh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752948223,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3yzes","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n40tqum","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Amgadoz","can_mod_post":false,"created_utc":1752944101,"send_replies":true,"parent_id":"t1_n40lmuk","score":5,"author_fullname":"t2_3el21u3z","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Have you tested gemma and mistral? They should be really good with German.\\nAlso Cohere","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n40tqum","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Have you tested gemma and mistral? They should be really good with German.\\nAlso Cohere&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3yzes","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n40tqum/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752944101,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n44octl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"indicava","can_mod_post":false,"created_utc":1752996295,"send_replies":true,"parent_id":"t1_n40lmuk","score":1,"author_fullname":"t2_4dvff","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Do you mean a coding model that can understand instructions in German?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44octl","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Do you mean a coding model that can understand instructions in German?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"link_id":"t3_1m3yzes","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n44octl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752996295,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n40lmuk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Hurricane31337","can_mod_post":false,"created_utc":1752941545,"send_replies":true,"parent_id":"t3_1m3yzes","score":5,"author_fullname":"t2_13pf1m","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I’d love to see a Qwen3 32B fine tune that can code VB.NET &amp; WinForms (bonus: DevExpress) in German and English, plus proper tool calling / MCP support to make it future proof.\\n\\nIn general there are so many English models but nearly no German ones. I know that many models are multilingual but they all suck compared to their English capabilities…","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n40lmuk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I’d love to see a Qwen3 32B fine tune that can code VB.NET &amp;amp; WinForms (bonus: DevExpress) in German and English, plus proper tool calling / MCP support to make it future proof.&lt;/p&gt;\\n\\n&lt;p&gt;In general there are so many English models but nearly no German ones. I know that many models are multilingual but they all suck compared to their English capabilities…&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n40lmuk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752941545,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3yzes","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n42fznf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GenLabsAI","can_mod_post":false,"created_utc":1752962760,"send_replies":true,"parent_id":"t1_n40ug3o","score":1,"author_fullname":"t2_14cl94t8ha","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Merge qwen3 32b and qwen2.5 coder 32b. It'll be almost the same","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n42fznf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Merge qwen3 32b and qwen2.5 coder 32b. It&amp;#39;ll be almost the same&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3yzes","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n42fznf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752962760,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n40ug3o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"TheOneThatIsHated","can_mod_post":false,"created_utc":1752944317,"send_replies":true,"parent_id":"t3_1m3yzes","score":6,"author_fullname":"t2_3n4puyfc","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Definitely a qwen3 fim token coder finetune, since they don't want to do it themselves","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n40ug3o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Definitely a qwen3 fim token coder finetune, since they don&amp;#39;t want to do it themselves&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n40ug3o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752944317,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3yzes","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n41v33n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"lavilao","can_mod_post":false,"created_utc":1752955887,"send_replies":true,"parent_id":"t3_1m3yzes","score":4,"author_fullname":"t2_8r6zinl9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Could you finetune smollm2-135m for FIM/code completion? That would gave the open source community a viable alternative to microsoft intellicode.&gt;! I know some people might say its not needed because there are already 7B+ models that do code completion but, I mean, do we really need a model that big for autocompletion? Microsoft did it on 80MB before the chatgpt revolution. Also the latency of a 135m model would be really good for code completion. Just python would be great as intellicode only works with pylance and thats a vscode only extension (and I use neovim on my potato).!&lt; Thanks in advice.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n41v33n","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Could you finetune smollm2-135m for FIM/code completion? That would gave the open source community a viable alternative to microsoft intellicode.&amp;gt;! I know some people might say its not needed because there are already 7B+ models that do code completion but, I mean, do we really need a model that big for autocompletion? Microsoft did it on 80MB before the chatgpt revolution. Also the latency of a 135m model would be really good for code completion. Just python would be great as intellicode only works with pylance and thats a vscode only extension (and I use neovim on my potato).!&amp;lt; Thanks in advice.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n41v33n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752955887,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3yzes","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n431ngj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"minpeter2","can_mod_post":false,"created_utc":1752970451,"send_replies":true,"parent_id":"t1_n426r14","score":2,"author_fullname":"t2_1fc9cbovwe","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm one of those people who pick up bottles in the ocean. LOL","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n431ngj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m one of those people who pick up bottles in the ocean. LOL&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3yzes","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n431ngj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752970451,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n44ou87","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"indicava","can_mod_post":false,"created_utc":1752996569,"send_replies":true,"parent_id":"t1_n426r14","score":1,"author_fullname":"t2_4dvff","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I really hope the initiative works. We can setup a localllama space/repo on HF and have the entire community contribute.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44ou87","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I really hope the initiative works. We can setup a localllama space/repo on HF and have the entire community contribute.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"link_id":"t3_1m3yzes","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n44ou87/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752996569,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n426r14","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"myelinatednervefiber","can_mod_post":false,"created_utc":1752959692,"send_replies":true,"parent_id":"t3_1m3yzes","score":4,"author_fullname":"t2_uw6dkckm","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Damn, that's amazing of you. If this became a regular thing I think one huge benefit might just be having a central point for dataset awareness/curation/whatever. I know a lot of people never bother to upload their datasets anywhere because it feels a bit like tossing a bottled message into the ocean of huggingspace. Meanwhile getting in touch with people doing fine tuning to offer it up just feels a bit spammy. I think a lot of us just wind up resting on top of our datasets like it's some kind of dragon hoard. Just doing rare training on our own and possibly not even uploading the results.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n426r14","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Damn, that&amp;#39;s amazing of you. If this became a regular thing I think one huge benefit might just be having a central point for dataset awareness/curation/whatever. I know a lot of people never bother to upload their datasets anywhere because it feels a bit like tossing a bottled message into the ocean of huggingspace. Meanwhile getting in touch with people doing fine tuning to offer it up just feels a bit spammy. I think a lot of us just wind up resting on top of our datasets like it&amp;#39;s some kind of dragon hoard. Just doing rare training on our own and possibly not even uploading the results.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n426r14/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752959692,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3yzes","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"c07aa42e-51fe-11f0-afcc-462aad931709","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n44ohz9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"indicava","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4380s6","score":2,"author_fullname":"t2_4dvff","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I agree, considering the number of upvotes my original comment (on another thread) got, I really though this thread would blow up with awesome ideas.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n44ohz9","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I agree, considering the number of upvotes my original comment (on another thread) got, I really though this thread would blow up with awesome ideas.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"link_id":"t3_1m3yzes","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n44ohz9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752996377,"author_flair_text":null,"treatment_tags":[],"created_utc":1752996377,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"c07aa42e-51fe-11f0-afcc-462aad931709","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n44oo5r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"indicava","can_mod_post":false,"send_replies":true,"parent_id":"t1_n43bttm","score":2,"author_fullname":"t2_4dvff","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I tried explaining what I meant in an update just now. I just don't have the capacity to implement anything. Experiments are fine if there is a ready to go training pipeline I can just pretty much run.\\n\\nSorry for being so strict, but I currently don't have the time to actively code anything.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44oo5r","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I tried explaining what I meant in an update just now. I just don&amp;#39;t have the capacity to implement anything. Experiments are fine if there is a ready to go training pipeline I can just pretty much run.&lt;/p&gt;\\n\\n&lt;p&gt;Sorry for being so strict, but I currently don&amp;#39;t have the time to actively code anything.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"link_id":"t3_1m3yzes","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n44oo5r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752996473,"author_flair_text":null,"treatment_tags":[],"created_utc":1752996473,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n43bttm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4380s6","score":1,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"OPs bar is quite high for the average r/LocalLLaMA user (no \\"experiments\\").","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n43bttm","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"a":":X:","u":"https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X","e":"emoji"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;OPs bar is quite high for the average &lt;a href=\\"/r/LocalLLaMA\\"&gt;r/LocalLLaMA&lt;/a&gt; user (no &amp;quot;experiments&amp;quot;).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3yzes","unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n43bttm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752974300,"author_flair_text":":X:","treatment_tags":[],"created_utc":1752974300,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"transparent","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4380s6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"3dom","can_mod_post":false,"created_utc":1752972846,"send_replies":true,"parent_id":"t1_n40fzms","score":3,"author_fullname":"t2_31gku","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Almost a day later the community is being quite dull: train us a programming model. I suppose the folks with actually original-interesting ideas keep them for themselves.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4380s6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Almost a day later the community is being quite dull: train us a programming model. I suppose the folks with actually original-interesting ideas keep them for themselves.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3yzes","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n4380s6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752972846,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n40fzms","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"created_utc":1752939783,"send_replies":true,"parent_id":"t3_1m3yzes","score":3,"author_fullname":"t2_1a48h7vf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"lesgoooo","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n40fzms","is_submitter":false,"downs":0,"author_flair_richtext":[{"a":":X:","u":"https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X","e":"emoji"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;lesgoooo&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n40fzms/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752939783,"author_flair_text":":X:","treatment_tags":[],"link_id":"t3_1m3yzes","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"transparent","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n41y2m4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Expensive-Apricot-25","can_mod_post":false,"created_utc":1752956859,"send_replies":true,"parent_id":"t3_1m3yzes","score":3,"author_fullname":"t2_idqkwio0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"implement predicting multiple tokens at a time based on the recent paper from apple\\n\\n[https://www.reddit.com/r/LocalLLaMA/comments/1m3vqom/a\\\\_new\\\\_paper\\\\_from\\\\_apple\\\\_shows\\\\_you\\\\_can\\\\_tack\\\\_on/](https://www.reddit.com/r/LocalLLaMA/comments/1m3vqom/a_new_paper_from_apple_shows_you_can_tack_on/)\\n\\nI'd recommend starting with the smaller qwen3 models","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n41y2m4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;implement predicting multiple tokens at a time based on the recent paper from apple&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1m3vqom/a_new_paper_from_apple_shows_you_can_tack_on/\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1m3vqom/a_new_paper_from_apple_shows_you_can_tack_on/&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;d recommend starting with the smaller qwen3 models&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n41y2m4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752956859,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3yzes","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n40k6h7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LagOps91","can_mod_post":false,"created_utc":1752941088,"send_replies":true,"parent_id":"t1_n40jnke","score":2,"author_fullname":"t2_3wi6j7vwh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Not sure if this idea counts as something that can be done. It's effectively a variation on self-taught test time compute. Could be easy to adapt or could be difficult to do. I don't really have the knowledge to tell.","edited":1752944136,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n40k6h7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not sure if this idea counts as something that can be done. It&amp;#39;s effectively a variation on self-taught test time compute. Could be easy to adapt or could be difficult to do. I don&amp;#39;t really have the knowledge to tell.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3yzes","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n40k6h7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752941088,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n40jnke","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"LagOps91","can_mod_post":false,"created_utc":1752940925,"send_replies":true,"parent_id":"t3_1m3yzes","score":3,"author_fullname":"t2_3wi6j7vwh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I would love to see a fine tune for multi turn chain of thought of sorts. Right now, chain of thought is only good to improve the current output and all thoughts get removed for the next turn. How about using RL to self-learn to use the following format: thought block, output for humans, notes block for follow up turns. The ai can use it to note down insights or perform long term planning. A good example would be creative writing. Instead of just \\"making up\\" the story on the fly, the ai can plan ahead, take notes on characters or events in the story. I think it could be quite nice to have for RP finetunes as well. Unlike the thought block, the notes block remains in the context, but isn't intended to be read by the user.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n40jnke","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I would love to see a fine tune for multi turn chain of thought of sorts. Right now, chain of thought is only good to improve the current output and all thoughts get removed for the next turn. How about using RL to self-learn to use the following format: thought block, output for humans, notes block for follow up turns. The ai can use it to note down insights or perform long term planning. A good example would be creative writing. Instead of just &amp;quot;making up&amp;quot; the story on the fly, the ai can plan ahead, take notes on characters or events in the story. I think it could be quite nice to have for RP finetunes as well. Unlike the thought block, the notes block remains in the context, but isn&amp;#39;t intended to be read by the user.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n40jnke/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752940925,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3yzes","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n40iyk3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No-Refrigerator-1672","can_mod_post":false,"created_utc":1752940707,"send_replies":true,"parent_id":"t3_1m3yzes","score":2,"author_fullname":"t2_baavelp5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"There is [Mantella](https://github.com/art-from-the-machine/Mantella) mod for games Skyrim and Fallout 4 that allows the LLM to take role and conversate as NPCs in said games. Some time ago this mod also introduced vision support. It would be cool to train a small (3-10B) vision capable model on the lore and screenshots of said games to provide a locally-runnable companions. However, to the best of my knowledge, a training-ready dataset for such finetunes doesn't exist, so should you consider doing this, you will first have to crawl game wikis and/or request community assistance for acquiring the training data.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n40iyk3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There is &lt;a href=\\"https://github.com/art-from-the-machine/Mantella\\"&gt;Mantella&lt;/a&gt; mod for games Skyrim and Fallout 4 that allows the LLM to take role and conversate as NPCs in said games. Some time ago this mod also introduced vision support. It would be cool to train a small (3-10B) vision capable model on the lore and screenshots of said games to provide a locally-runnable companions. However, to the best of my knowledge, a training-ready dataset for such finetunes doesn&amp;#39;t exist, so should you consider doing this, you will first have to crawl game wikis and/or request community assistance for acquiring the training data.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n40iyk3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752940707,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3yzes","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n436qx3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"TheRealMasonMac","can_mod_post":false,"created_utc":1752972363,"send_replies":true,"parent_id":"t3_1m3yzes","score":2,"author_fullname":"t2_101haj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Gemma 12B or 27B R1 distill from the OpenR1 dataset mixture, lesgo","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n436qx3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Gemma 12B or 27B R1 distill from the OpenR1 dataset mixture, lesgo&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n436qx3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752972363,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3yzes","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n44ia0s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"R46H4V","can_mod_post":false,"created_utc":1752992950,"send_replies":true,"parent_id":"t3_1m3yzes","score":2,"author_fullname":"t2_aedi2k9c","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I wanted to fine-tune a qwen model like the Qwen 2.5 Coder or the upcoming Qwen 3 coder on the new manin (python animation library by 3blue1brown) dataset that was recently released which combined many different datasets together. This could help in generating high quality animation code. I thought of fine-tuning the smaller ones myself via unsloth, but if you could fine-tune the bigger ones it would be even better and then do QAT on them as well to make them perform better on lower Quants.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44ia0s","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I wanted to fine-tune a qwen model like the Qwen 2.5 Coder or the upcoming Qwen 3 coder on the new manin (python animation library by 3blue1brown) dataset that was recently released which combined many different datasets together. This could help in generating high quality animation code. I thought of fine-tuning the smaller ones myself via unsloth, but if you could fine-tune the bigger ones it would be even better and then do QAT on them as well to make them perform better on lower Quants.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n44ia0s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752992950,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3yzes","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n41h5cl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Trick-Independent469","can_mod_post":false,"created_utc":1752951381,"send_replies":true,"parent_id":"t3_1m3yzes","score":4,"author_fullname":"t2_fs6bx6g5r","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I have 10000 romanian language books .  We can fine tune a model on them 4 GB .txt file size folder .  It would be interesting . Please vote me up","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n41h5cl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have 10000 romanian language books .  We can fine tune a model on them 4 GB .txt file size folder .  It would be interesting . Please vote me up&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n41h5cl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752951381,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3yzes","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4255lj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RobotRobotWhatDoUSee","can_mod_post":false,"created_utc":1752959167,"send_replies":true,"parent_id":"t1_n40wz5w","score":1,"author_fullname":"t2_m78cdz1nv","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I read the OP and immediately thought that we should spin up a second thread for newbs and experimenting.\\n\\nWhich is to say, yes,  I definitely support this and would participate if you spin up a thread!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4255lj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I read the OP and immediately thought that we should spin up a second thread for newbs and experimenting.&lt;/p&gt;\\n\\n&lt;p&gt;Which is to say, yes,  I definitely support this and would participate if you spin up a thread!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3yzes","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n4255lj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752959167,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n40wz5w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"munkiemagik","can_mod_post":false,"created_utc":1752945083,"send_replies":true,"parent_id":"t3_1m3yzes","score":2,"author_fullname":"t2_if95iuzc","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'd like to create a fork of this idea please!\\n\\nFor those of us who have no idea about anything but are fascinated by the subject and just want to experiment and learn and have stupidly decided to imeditately jump in feet first and purchase equipment 'for the sake of the hobby':\\n\\nSome kind of workshop zone where we can all collaborate - Poeple who acutally know what they are doing but dont have the hardware to spare, to direct and guide with their skill and knowledge how to create a product they want and the shmuck with the hardware that knows absolutely nothing gets to learn hands on with guidance and discover the potential of LLMs and the hardware.\\n\\nI dont want to hijack this thread so if anyone thinks this could be a thing, message me and I'll create a new thread\\n\\n(PS no offence meant to anyone - I reference myself as one of the above-mentioned 'shmucks')","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n40wz5w","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;d like to create a fork of this idea please!&lt;/p&gt;\\n\\n&lt;p&gt;For those of us who have no idea about anything but are fascinated by the subject and just want to experiment and learn and have stupidly decided to imeditately jump in feet first and purchase equipment &amp;#39;for the sake of the hobby&amp;#39;:&lt;/p&gt;\\n\\n&lt;p&gt;Some kind of workshop zone where we can all collaborate - Poeple who acutally know what they are doing but dont have the hardware to spare, to direct and guide with their skill and knowledge how to create a product they want and the shmuck with the hardware that knows absolutely nothing gets to learn hands on with guidance and discover the potential of LLMs and the hardware.&lt;/p&gt;\\n\\n&lt;p&gt;I dont want to hijack this thread so if anyone thinks this could be a thing, message me and I&amp;#39;ll create a new thread&lt;/p&gt;\\n\\n&lt;p&gt;(PS no offence meant to anyone - I reference myself as one of the above-mentioned &amp;#39;shmucks&amp;#39;)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n40wz5w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752945083,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3yzes","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n40dx50","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RemindMeBot","can_mod_post":false,"created_utc":1752939152,"send_replies":true,"parent_id":"t1_n40ds55","score":1,"author_fullname":"t2_gbm4p","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I will be messaging you in 2 days on [**2025-07-21 15:31:49 UTC**](http://www.wolframalpha.com/input/?i=2025-07-21%2015:31:49%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n40ds55/?context=3)\\n\\n[**1 OTHERS CLICKED THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FLocalLLaMA%2Fcomments%2F1m3yzes%2Flocalllamas_first_ifta_ill_finetune_anything%2Fn40ds55%2F%5D%0A%0ARemindMe%21%202025-07-21%2015%3A31%3A49%20UTC) to send a PM to also be reminded and to reduce spam.\\n\\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete%20Comment&amp;message=Delete%21%201m3yzes)\\n\\n*****\\n\\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List%20Of%20Reminders&amp;message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&amp;subject=RemindMeBot%20Feedback)|\\n|-|-|-|-|","edited":1752941103,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n40dx50","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I will be messaging you in 2 days on &lt;a href=\\"http://www.wolframalpha.com/input/?i=2025-07-21%2015:31:49%20UTC%20To%20Local%20Time\\"&gt;&lt;strong&gt;2025-07-21 15:31:49 UTC&lt;/strong&gt;&lt;/a&gt; to remind you of &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n40ds55/?context=3\\"&gt;&lt;strong&gt;this link&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.reddit.com/message/compose/?to=RemindMeBot&amp;amp;subject=Reminder&amp;amp;message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FLocalLLaMA%2Fcomments%2F1m3yzes%2Flocalllamas_first_ifta_ill_finetune_anything%2Fn40ds55%2F%5D%0A%0ARemindMe%21%202025-07-21%2015%3A31%3A49%20UTC\\"&gt;&lt;strong&gt;1 OTHERS CLICKED THIS LINK&lt;/strong&gt;&lt;/a&gt; to send a PM to also be reminded and to reduce spam.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;sup&gt;Parent commenter can &lt;/sup&gt; &lt;a href=\\"https://www.reddit.com/message/compose/?to=RemindMeBot&amp;amp;subject=Delete%20Comment&amp;amp;message=Delete%21%201m3yzes\\"&gt;&lt;sup&gt;delete this message to hide from others.&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;table&gt;&lt;thead&gt;\\n&lt;tr&gt;\\n&lt;th&gt;&lt;a href=\\"https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/\\"&gt;&lt;sup&gt;Info&lt;/sup&gt;&lt;/a&gt;&lt;/th&gt;\\n&lt;th&gt;&lt;a href=\\"https://www.reddit.com/message/compose/?to=RemindMeBot&amp;amp;subject=Reminder&amp;amp;message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here\\"&gt;&lt;sup&gt;Custom&lt;/sup&gt;&lt;/a&gt;&lt;/th&gt;\\n&lt;th&gt;&lt;a href=\\"https://www.reddit.com/message/compose/?to=RemindMeBot&amp;amp;subject=List%20Of%20Reminders&amp;amp;message=MyReminders%21\\"&gt;&lt;sup&gt;Your Reminders&lt;/sup&gt;&lt;/a&gt;&lt;/th&gt;\\n&lt;th&gt;&lt;a href=\\"https://www.reddit.com/message/compose/?to=Watchful1&amp;amp;subject=RemindMeBot%20Feedback\\"&gt;&lt;sup&gt;Feedback&lt;/sup&gt;&lt;/a&gt;&lt;/th&gt;\\n&lt;/tr&gt;\\n&lt;/thead&gt;&lt;tbody&gt;\\n&lt;/tbody&gt;&lt;/table&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3yzes","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n40dx50/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752939152,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n40ds55","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"gaztrab","can_mod_post":false,"created_utc":1752939109,"send_replies":true,"parent_id":"t3_1m3yzes","score":1,"author_fullname":"t2_2f6s556","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"!remindme 2 days","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n40ds55","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;!remindme 2 days&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n40ds55/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752939109,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3yzes","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n41dcz0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"schlammsuhler","can_mod_post":false,"created_utc":1752950163,"send_replies":true,"parent_id":"t1_n412iaj","score":3,"author_fullname":"t2_cx7q6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You could do tool calling with the summary to replace current thinking with the summary. Should be easy engineering.\\n\\nBut you risk endless loops.\\n\\nAlso, theres no precedent how to summarize without losing critical info. Imho summaries of stories are very bad and just focus on facts, not the why.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n41dcz0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You could do tool calling with the summary to replace current thinking with the summary. Should be easy engineering.&lt;/p&gt;\\n\\n&lt;p&gt;But you risk endless loops.&lt;/p&gt;\\n\\n&lt;p&gt;Also, theres no precedent how to summarize without losing critical info. Imho summaries of stories are very bad and just focus on facts, not the why.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3yzes","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n41dcz0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752950163,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n412iaj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"random-tomato","can_mod_post":false,"created_utc":1752946752,"send_replies":true,"parent_id":"t3_1m3yzes","score":1,"author_fullname":"t2_fmd6oq5v6","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"GRPO to make a model be able to pause during reasoning and summarize what it's done so far, then replacing its thinking tokens with that summary. Then it effectively gives that model a much longer context window. No idea how this would be done though lol","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n412iaj","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;GRPO to make a model be able to pause during reasoning and summarize what it&amp;#39;s done so far, then replacing its thinking tokens with that summary. Then it effectively gives that model a much longer context window. No idea how this would be done though lol&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n412iaj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752946752,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m3yzes","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n428b98","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rorowhat","can_mod_post":false,"created_utc":1752960203,"send_replies":true,"parent_id":"t3_1m3yzes","score":1,"author_fullname":"t2_yq51a","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"More sol and add Link","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n428b98","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;More sol and add Link&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n428b98/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752960203,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3yzes","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n42fquz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GenLabsAI","can_mod_post":false,"created_utc":1752962676,"send_replies":true,"parent_id":"t3_1m3yzes","score":1,"author_fullname":"t2_14cl94t8ha","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"KIMI K2 Reasoning Please! (I'm not sure you have enough for that though, lol)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n42fquz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;KIMI K2 Reasoning Please! (I&amp;#39;m not sure you have enough for that though, lol)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n42fquz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752962676,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3yzes","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n463s4q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Salt-Advertising-939","can_mod_post":false,"created_utc":1753021054,"send_replies":true,"parent_id":"t3_1m3yzes","score":1,"author_fullname":"t2_16dkvz9j62","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Fintuning the qwen 30b 3A moe on the qwen long l1 dataset to make it the perfect rag and summary companion! This would be so insane","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n463s4q","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Fintuning the qwen 30b 3A moe on the qwen long l1 dataset to make it the perfect rag and summary companion! This would be so insane&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n463s4q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753021054,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3yzes","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n40xjc6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Famous-Appointment-8","can_mod_post":false,"created_utc":1752945252,"send_replies":true,"parent_id":"t3_1m3yzes","score":1,"author_fullname":"t2_b1m1wiao","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Humanize AI Text and train against originallity ai","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n40xjc6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Humanize AI Text and train against originallity ai&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n40xjc6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752945252,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3yzes","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n46sw9j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RandumbRedditor1000","can_mod_post":false,"send_replies":true,"parent_id":"t1_n42ysm2","score":1,"author_fullname":"t2_s7n3irsrx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Nope, it doesn't behave quite right even if i specify in the prompt\\n\\nThe closest I got to what I want was with GLM-4 32b and even that didnt work as well as I would like for this use-case","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n46sw9j","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nope, it doesn&amp;#39;t behave quite right even if i specify in the prompt&lt;/p&gt;\\n\\n&lt;p&gt;The closest I got to what I want was with GLM-4 32b and even that didnt work as well as I would like for this use-case&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3yzes","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n46sw9j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753028885,"author_flair_text":null,"treatment_tags":[],"created_utc":1753028885,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n42ysm2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"martinerous","can_mod_post":false,"created_utc":1752969391,"send_replies":true,"parent_id":"t1_n418b9f","score":1,"author_fullname":"t2_5tp54ey","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Doesn't it work for you when you describe that requirement in the system prompt and give the first dialog examples?\\n\\nI have had quite good results with it, when I gave specific instructions that the character is able to communicate over text chat only.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n42ysm2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Doesn&amp;#39;t it work for you when you describe that requirement in the system prompt and give the first dialog examples?&lt;/p&gt;\\n\\n&lt;p&gt;I have had quite good results with it, when I gave specific instructions that the character is able to communicate over text chat only.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3yzes","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n42ysm2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752969391,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n418b9f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RandumbRedditor1000","can_mod_post":false,"created_utc":1752948546,"send_replies":true,"parent_id":"t3_1m3yzes","score":1,"author_fullname":"t2_s7n3irsrx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Ive always wanted a model fine-tuned to follow character cards, but respond as if it were the character texting you rather than roleplaying, with a more informal tone","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n418b9f","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ive always wanted a model fine-tuned to follow character cards, but respond as if it were the character texting you rather than roleplaying, with a more informal tone&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n418b9f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752948546,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3yzes","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n42b4nk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"EugenePopcorn","can_mod_post":false,"created_utc":1752961142,"send_replies":true,"parent_id":"t3_1m3yzes","score":1,"author_fullname":"t2_g6hpxxgss","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Anybody looking to make a name for themselves might start providing QATs for models that don't already have them. Unsloth gets a lot of well deserved attention for their UD quants, but a  4_0 will run faster than almost anything else. Quantization is lossy, and it's weird we don't have a healing step afterward by default. \\n\\n\\nI'd start with Devstral. Any model with agentic capabilities is going to have a lot of demand for high throughput, high accuracy quants. ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n42b4nk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Anybody looking to make a name for themselves might start providing QATs for models that don&amp;#39;t already have them. Unsloth gets a lot of well deserved attention for their UD quants, but a  4_0 will run faster than almost anything else. Quantization is lossy, and it&amp;#39;s weird we don&amp;#39;t have a healing step afterward by default. &lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;d start with Devstral. Any model with agentic capabilities is going to have a lot of demand for high throughput, high accuracy quants. &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n42b4nk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752961142,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3yzes","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n40vfc9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"triynizzles1","can_mod_post":false,"created_utc":1752944620,"send_replies":true,"parent_id":"t1_n40vdjf","score":1,"author_fullname":"t2_zr0g49ixt","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"https://preview.redd.it/kqewhqqb4vdf1.jpeg?width=1170&amp;format=pjpg&amp;auto=webp&amp;s=82599f0b81677fd3f1237524514d5960c5d8a106\\n\\nGemini completely gave up after recognizing the impossible task i had given it lol.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n40vfc9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://preview.redd.it/kqewhqqb4vdf1.jpeg?width=1170&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=82599f0b81677fd3f1237524514d5960c5d8a106\\"&gt;https://preview.redd.it/kqewhqqb4vdf1.jpeg?width=1170&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=82599f0b81677fd3f1237524514d5960c5d8a106&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Gemini completely gave up after recognizing the impossible task i had given it lol.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3yzes","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n40vfc9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752944620,"media_metadata":{"kqewhqqb4vdf1":{"status":"valid","e":"Image","m":"image/jpeg","p":[{"y":216,"x":108,"u":"https://preview.redd.it/kqewhqqb4vdf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=eb5ee8d1c416088f516dc75ecf402ea4b851599f"},{"y":432,"x":216,"u":"https://preview.redd.it/kqewhqqb4vdf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b01aa949f40b2e96241c1e2091872a57ead8de91"},{"y":640,"x":320,"u":"https://preview.redd.it/kqewhqqb4vdf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fa455c3dfceaf00237ed4fe3d59761f58aca3822"},{"y":1280,"x":640,"u":"https://preview.redd.it/kqewhqqb4vdf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=719d63b5a20dd4eb5f8ea886d1442630c1ca7983"},{"y":1920,"x":960,"u":"https://preview.redd.it/kqewhqqb4vdf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=802d14554b2416ab748ca26cf1e50416d5866c9e"},{"y":2160,"x":1080,"u":"https://preview.redd.it/kqewhqqb4vdf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8690d8e7def25df0eae17140c0b7654491a7e588"}],"s":{"y":2532,"x":1170,"u":"https://preview.redd.it/kqewhqqb4vdf1.jpeg?width=1170&amp;format=pjpg&amp;auto=webp&amp;s=82599f0b81677fd3f1237524514d5960c5d8a106"},"id":"kqewhqqb4vdf1"}},"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n40vdjf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"triynizzles1","can_mod_post":false,"created_utc":1752944605,"send_replies":true,"parent_id":"t3_1m3yzes","score":0,"author_fullname":"t2_zr0g49ixt","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I would like to see an AI that can competently play the game hangman. It seems like a simple task, however the transformer architecture makes it pretty much impossible. This is because AI predict the next token. Which means information like the word I’m trying to guess when playing hangman does not exist. Of course, an easy solution is to include a word in system prompt but I’d like to see an AI that can competently play without any additional software or human assistance. Here are some examples of how other AI models handle this.\\n\\nPrompt was: Let’s play a game of hangman. You think of the word and I will guess it.\\n\\n(screenshots from lmarena)\\n\\n&amp;#x200B;\\n\\nhttps://preview.redd.it/z3hu2ob74vdf1.jpeg?width=1170&amp;format=pjpg&amp;auto=webp&amp;s=6c23987802f99e0554e4c711017164ef4e86a5c4\\n\\nThe word is hydrosphere. It gave started to give me letters for free. It also gave the incorrect length and didn’t populate h correctly.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n40vdjf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I would like to see an AI that can competently play the game hangman. It seems like a simple task, however the transformer architecture makes it pretty much impossible. This is because AI predict the next token. Which means information like the word I’m trying to guess when playing hangman does not exist. Of course, an easy solution is to include a word in system prompt but I’d like to see an AI that can competently play without any additional software or human assistance. Here are some examples of how other AI models handle this.&lt;/p&gt;\\n\\n&lt;p&gt;Prompt was: Let’s play a game of hangman. You think of the word and I will guess it.&lt;/p&gt;\\n\\n&lt;p&gt;(screenshots from lmarena)&lt;/p&gt;\\n\\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/z3hu2ob74vdf1.jpeg?width=1170&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6c23987802f99e0554e4c711017164ef4e86a5c4\\"&gt;https://preview.redd.it/z3hu2ob74vdf1.jpeg?width=1170&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6c23987802f99e0554e4c711017164ef4e86a5c4&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;The word is hydrosphere. It gave started to give me letters for free. It also gave the incorrect length and didn’t populate h correctly.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3yzes/localllamas_first_ifta_ill_finetune_anything/n40vdjf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752944605,"media_metadata":{"z3hu2ob74vdf1":{"status":"valid","e":"Image","m":"image/jpeg","p":[{"y":216,"x":108,"u":"https://preview.redd.it/z3hu2ob74vdf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e1e4e146f045d5f0da782f69e850b5429a85305d"},{"y":432,"x":216,"u":"https://preview.redd.it/z3hu2ob74vdf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9f70fe8340e278f1979091281723ce97abddbf6c"},{"y":640,"x":320,"u":"https://preview.redd.it/z3hu2ob74vdf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e58362a4d0f9a6b5cd02f9381a5023d6b2e8f1d6"},{"y":1280,"x":640,"u":"https://preview.redd.it/z3hu2ob74vdf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a799615f8c58a9218a8b5eb46f898a2bc762fabb"},{"y":1920,"x":960,"u":"https://preview.redd.it/z3hu2ob74vdf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=94b4f4555e51e82cb934495a79b906c38f986c66"},{"y":2160,"x":1080,"u":"https://preview.redd.it/z3hu2ob74vdf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b48d039a9e0cf69ea8782e9391fb7b02ea799635"}],"s":{"y":2532,"x":1170,"u":"https://preview.redd.it/z3hu2ob74vdf1.jpeg?width=1170&amp;format=pjpg&amp;auto=webp&amp;s=6c23987802f99e0554e4c711017164ef4e86a5c4"},"id":"z3hu2ob74vdf1"}},"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3yzes","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}}]`),r=()=>e.jsx(t,{data:l});export{r as default};
