import{j as e}from"./index-DLSqWzaI.js";import{R as t}from"./RedditPostRenderer-CysRo2D_.js";import"./index-COXiL3Lo.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hi everyone,\\n\\nI'm working on a project where I want to fine-tune a language model to mimic a user‚Äôs personal writing style ‚Äî specifically by training on their own email history (with full consent and access via API).\\n\\nThe goal is to generate email replies that sound like the user actually wrote them.\\n\\n# I‚Äôm curious to know:\\n\\n* Has anyone here tried something similar using **LoRA adapters** or **QLoRA**?\\n* What would the training dataset look like in practice? Just the raw email threads, or should I include metadata like recipient, subject, or response time?\\n* What‚Äôs the most **practical open-source LLM** for this use case that can be trained with **48GB of VRAM**?\\n   * I‚Äôve been considering **LLaMA 3 8B**, **Qwen 2.5 14B**, and **Vicuna 13B**.\\n   * I know LLaMA 70B is out of scope for my setup.\\n\\nAny recommendations, lessons learned, or repo links would be really helpful!\\n\\nThanks in advance üôè\\n\\nr/LocalLLaMA","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"LoRA adapter on emails to mimic users style of writing from their emails","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m2acb8","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.9,"author_flair_background_color":null,"subreddit_type":"public","ups":8,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_fo6z1svnu","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":8,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752765579,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m working on a project where I want to fine-tune a language model to mimic a user‚Äôs personal writing style ‚Äî specifically by training on their own email history (with full consent and access via API).&lt;/p&gt;\\n\\n&lt;p&gt;The goal is to generate email replies that sound like the user actually wrote them.&lt;/p&gt;\\n\\n&lt;h1&gt;I‚Äôm curious to know:&lt;/h1&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Has anyone here tried something similar using &lt;strong&gt;LoRA adapters&lt;/strong&gt; or &lt;strong&gt;QLoRA&lt;/strong&gt;?&lt;/li&gt;\\n&lt;li&gt;What would the training dataset look like in practice? Just the raw email threads, or should I include metadata like recipient, subject, or response time?&lt;/li&gt;\\n&lt;li&gt;What‚Äôs the most &lt;strong&gt;practical open-source LLM&lt;/strong&gt; for this use case that can be trained with &lt;strong&gt;48GB of VRAM&lt;/strong&gt;?\\n\\n&lt;ul&gt;\\n&lt;li&gt;I‚Äôve been considering &lt;strong&gt;LLaMA 3 8B&lt;/strong&gt;, &lt;strong&gt;Qwen 2.5 14B&lt;/strong&gt;, and &lt;strong&gt;Vicuna 13B&lt;/strong&gt;.&lt;/li&gt;\\n&lt;li&gt;I know LLaMA 70B is out of scope for my setup.&lt;/li&gt;\\n&lt;/ul&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Any recommendations, lessons learned, or repo links would be really helpful!&lt;/p&gt;\\n\\n&lt;p&gt;Thanks in advance üôè&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"/r/LocalLLaMA\\"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1m2acb8","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Mindless_Paint6516","discussion_type":null,"num_comments":4,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m2acb8/lora_adapter_on_emails_to_mimic_users_style_of/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m2acb8/lora_adapter_on_emails_to_mimic_users_style_of/","subreddit_subscribers":500897,"created_utc":1752765579,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3rqolg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Mindless_Paint6516","can_mod_post":false,"created_utc":1752818900,"send_replies":true,"parent_id":"t1_n3pjg4r","score":1,"author_fullname":"t2_fo6z1svnu","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks you. \\nHow would you prepare a training datasets after cleaning and preprocessing all those emails so that you can also provide context via vector database in a prompt. \\nAlso what would be better approach, to train with instruction style format or chat- style format?\\nI personally think instruction style format is the way to go but would like to see what you think","edited":1752819503,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3rqolg","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks you. \\nHow would you prepare a training datasets after cleaning and preprocessing all those emails so that you can also provide context via vector database in a prompt. \\nAlso what would be better approach, to train with instruction style format or chat- style format?\\nI personally think instruction style format is the way to go but would like to see what you think&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2acb8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2acb8/lora_adapter_on_emails_to_mimic_users_style_of/n3rqolg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752818900,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3pjg4r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Environmental-Metal9","can_mod_post":false,"created_utc":1752788804,"send_replies":true,"parent_id":"t3_1m2acb8","score":3,"author_fullname":"t2_6x9o42az","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You‚Äôll need a lot of data for this. Check out DoRA adapter in the PEFT library for an option that might give you better results with less data. I got good results with rank 64 (after trying 16 and 32), alpha at 2x the rank, and dropout at 0.\\n\\nA lot of data here is above thousands of samples. If you have that, and your dataset is of a good quality (deduplicated, well formatted, a variety of ways of asking the same thing without it being just noise, etc), then LoRA will work really well. With only hundreds of samples, you can still achieve similar results, so long as your expectations aren‚Äôt really high (it won‚Äôt replace you, but it will be good enough that your work will become editing, not writing from scratch)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3pjg4r","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You‚Äôll need a lot of data for this. Check out DoRA adapter in the PEFT library for an option that might give you better results with less data. I got good results with rank 64 (after trying 16 and 32), alpha at 2x the rank, and dropout at 0.&lt;/p&gt;\\n\\n&lt;p&gt;A lot of data here is above thousands of samples. If you have that, and your dataset is of a good quality (deduplicated, well formatted, a variety of ways of asking the same thing without it being just noise, etc), then LoRA will work really well. With only hundreds of samples, you can still achieve similar results, so long as your expectations aren‚Äôt really high (it won‚Äôt replace you, but it will be good enough that your work will become editing, not writing from scratch)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2acb8/lora_adapter_on_emails_to_mimic_users_style_of/n3pjg4r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752788804,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2acb8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3o8cc7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"justicecurcian","can_mod_post":false,"created_utc":1752775252,"send_replies":true,"parent_id":"t3_1m2acb8","score":2,"author_fullname":"t2_1ij6vq4d","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I've made something similar using prompt engineering - I've threw few examples and asked the llm to make a prompt that will yield similar results, it worked for me\\n\\nAlso I would use qwen 3 if I were to make a lora, probably smaller ones will be enough if it's just to rewrite the email","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3o8cc7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve made something similar using prompt engineering - I&amp;#39;ve threw few examples and asked the llm to make a prompt that will yield similar results, it worked for me&lt;/p&gt;\\n\\n&lt;p&gt;Also I would use qwen 3 if I were to make a lora, probably smaller ones will be enough if it&amp;#39;s just to rewrite the email&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2acb8/lora_adapter_on_emails_to_mimic_users_style_of/n3o8cc7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752775252,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2acb8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),n=()=>e.jsx(t,{data:l});export{n as default};
