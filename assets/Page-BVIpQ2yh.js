import{j as e}from"./index-xfnGEtuL.js";import{R as l}from"./RedditPostRenderer-DAZRIZZK.js";import"./index-BaNn5-RR.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hi,  \\nI have 2x 7900 XTX and not getting any model run with them in a docker.\\n\\n    docker pull rocm/vllm:rocm6.4.1_vllm_0.9.1_20250702\\n    \\n    docker run -it \\\\\\n      --dns=1.1.1.1 \\\\\\n      --dns=8.8.8.8 \\\\\\n      --network=host \\\\\\n      --group-add=video \\\\\\n      --ipc=host \\\\\\n      --cap-add=SYS_PTRACE \\\\\\n      --security-opt seccomp=unconfined \\\\\\n      --privileged \\\\\\n      --device /dev/kfd \\\\\\n      --device /dev/dri \\\\\\n      -e ROCM_VISIBLE_DEVICES=0,1,2,3 \\\\\\n      -e HIP_VISIBLE_DEVICES=0,1,2,3 \\\\\\n      -e CUDA_VISIBLE_DEVICES=0,1,2,3 \\\\\\n      -e VLLM_USE_TRITON_FLASH_ATTN=0 \\\\\\n      -e PYTORCH_TUNABLEOP_ENABLED=1 \\\\\\n      -e HSA_OVERRIDE_GFX_VERSION=11.0.0 \\\\\\n      -e PYTORCH_ROCM_ARCH=\\"gfx1100\\" \\\\\\n      -e GPU_MAX_HW_QUEUES=1 \\\\\\n      -v /home/ubuntu/vllm_models:/workspace/models \\\\\\n      rocm/vllm:rocm6.4.1_vllm_0.9.1_20250702 bash\\n\\napt update &amp;&amp; apt install -y git build-essential\\n\\npip install ninja\\n\\npip3 install -U xformers --index-url [https://download.pytorch.org/whl/rocm6.3](https://download.pytorch.org/whl/rocm6.3)\\n\\n    vllm serve /workspace/models/DeepSeek-R1-Distill-Qwen-14B/ \\\\\\n      --dtype float16 \\\\\\n      --kv-cache-dtype auto \\\\\\n      --tensor-parallel-size 2 \\\\\\n      --trust-remote-code \\\\\\n      --tokenizer_mode auto \\\\\\n      --port 8000 \\\\\\n      --host 0.0.0.0 \\n\\nHave tried different models, some may go to a point where vllm says \\"started and waiting\\" but then when trying to chat with it all crashes.\\n\\nHow this is so hard? What is AMD doing for this? Or are we dummer meant to fall back to Ollama? AMD makes me very sad.\\n\\nEDIT: I trusted you AMD, I really did....","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Getting a model run with vLLM and 7900 XTX","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m75i0b","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":1,"author_flair_background_color":null,"subreddit_type":"public","ups":4,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1jk2ep8a52","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":4,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1753272524,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753266053,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi,&lt;br/&gt;\\nI have 2x 7900 XTX and not getting any model run with them in a docker.&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;docker pull rocm/vllm:rocm6.4.1_vllm_0.9.1_20250702\\n\\ndocker run -it \\\\\\n  --dns=1.1.1.1 \\\\\\n  --dns=8.8.8.8 \\\\\\n  --network=host \\\\\\n  --group-add=video \\\\\\n  --ipc=host \\\\\\n  --cap-add=SYS_PTRACE \\\\\\n  --security-opt seccomp=unconfined \\\\\\n  --privileged \\\\\\n  --device /dev/kfd \\\\\\n  --device /dev/dri \\\\\\n  -e ROCM_VISIBLE_DEVICES=0,1,2,3 \\\\\\n  -e HIP_VISIBLE_DEVICES=0,1,2,3 \\\\\\n  -e CUDA_VISIBLE_DEVICES=0,1,2,3 \\\\\\n  -e VLLM_USE_TRITON_FLASH_ATTN=0 \\\\\\n  -e PYTORCH_TUNABLEOP_ENABLED=1 \\\\\\n  -e HSA_OVERRIDE_GFX_VERSION=11.0.0 \\\\\\n  -e PYTORCH_ROCM_ARCH=&amp;quot;gfx1100&amp;quot; \\\\\\n  -e GPU_MAX_HW_QUEUES=1 \\\\\\n  -v /home/ubuntu/vllm_models:/workspace/models \\\\\\n  rocm/vllm:rocm6.4.1_vllm_0.9.1_20250702 bash\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;apt update &amp;amp;&amp;amp; apt install -y git build-essential&lt;/p&gt;\\n\\n&lt;p&gt;pip install ninja&lt;/p&gt;\\n\\n&lt;p&gt;pip3 install -U xformers --index-url &lt;a href=\\"https://download.pytorch.org/whl/rocm6.3\\"&gt;https://download.pytorch.org/whl/rocm6.3&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;vllm serve /workspace/models/DeepSeek-R1-Distill-Qwen-14B/ \\\\\\n  --dtype float16 \\\\\\n  --kv-cache-dtype auto \\\\\\n  --tensor-parallel-size 2 \\\\\\n  --trust-remote-code \\\\\\n  --tokenizer_mode auto \\\\\\n  --port 8000 \\\\\\n  --host 0.0.0.0 \\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;Have tried different models, some may go to a point where vllm says &amp;quot;started and waiting&amp;quot; but then when trying to chat with it all crashes.&lt;/p&gt;\\n\\n&lt;p&gt;How this is so hard? What is AMD doing for this? Or are we dummer meant to fall back to Ollama? AMD makes me very sad.&lt;/p&gt;\\n\\n&lt;p&gt;EDIT: I trusted you AMD, I really did....&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m75i0b","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Rich_Artist_8327","discussion_type":null,"num_comments":3,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m75i0b/getting_a_model_run_with_vllm_and_7900_xtx/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m75i0b/getting_a_model_run_with_vllm_and_7900_xtx/","subreddit_subscribers":503518,"created_utc":1753266053,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4pa6i0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ForsookComparison","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4p9p63","score":1,"author_fullname":"t2_on5es7pe3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"vLLM ROCm does I believe, sorry I edited my post as you replied. I had more luck installing vLLM ROCm directly to my host than I did with Docker.\\n\\nIf you're finding yourself confused, start peeling away layers","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4pa6i0","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;vLLM ROCm does I believe, sorry I edited my post as you replied. I had more luck installing vLLM ROCm directly to my host than I did with Docker.&lt;/p&gt;\\n\\n&lt;p&gt;If you&amp;#39;re finding yourself confused, start peeling away layers&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m75i0b","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m75i0b/getting_a_model_run_with_vllm_and_7900_xtx/n4pa6i0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753274052,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1753274052,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4p9p63","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Rich_Artist_8327","can_mod_post":false,"created_utc":1753273881,"send_replies":true,"parent_id":"t1_n4p9gpp","score":1,"author_fullname":"t2_1jk2ep8a52","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Does either of these do tensor parallelism when having multiple GPUs?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4p9p63","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Does either of these do tensor parallelism when having multiple GPUs?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m75i0b","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m75i0b/getting_a_model_run_with_vllm_and_7900_xtx/n4p9p63/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753273881,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4p9gpp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ForsookComparison","can_mod_post":false,"created_utc":1753273797,"send_replies":true,"parent_id":"t3_1m75i0b","score":5,"author_fullname":"t2_on5es7pe3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt; Or are we dummer meant to fall back to Ollama?\\n\\nOllama is just training wheels for Llama CPP which works so well with AMD on the ROCm or Vulkan builds I'd call it parity.\\n\\nAlso if you're struggling to set something up consider trying it outside of Docker first. Peel away layers of possible confusion, get your workflow down, and *then* move to containers and clean up your host.\\n\\n**Source:** doing all this on two AMD GPUs","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4p9gpp","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Or are we dummer meant to fall back to Ollama?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Ollama is just training wheels for Llama CPP which works so well with AMD on the ROCm or Vulkan builds I&amp;#39;d call it parity.&lt;/p&gt;\\n\\n&lt;p&gt;Also if you&amp;#39;re struggling to set something up consider trying it outside of Docker first. Peel away layers of possible confusion, get your workflow down, and &lt;em&gt;then&lt;/em&gt; move to containers and clean up your host.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; doing all this on two AMD GPUs&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m75i0b/getting_a_model_run_with_vllm_and_7900_xtx/n4p9gpp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753273797,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m75i0b","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}}]`),r=()=>e.jsx(l,{data:t});export{r as default};
