import{j as e}from"./index-Cd3v0jxz.js";import{R as l}from"./RedditPostRenderer-cI96YLhy.js";import"./index-DGBoKyQm.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I'd like to run models locally (at my workplaces) and also refine models, and fortunately I'm not paying!  I plan to get a Mac Studio with 80 core GPU and 256GB RAM. Is there any strong case that I'm missing for going with 512GB RAM?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Strong case for a 512GB Mac Studio?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m5uu0t","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.53,"author_flair_background_color":null,"subreddit_type":"public","ups":1,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_sk7nmjrs","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":1,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753131529,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;d like to run models locally (at my workplaces) and also refine models, and fortunately I&amp;#39;m not paying!  I plan to get a Mac Studio with 80 core GPU and 256GB RAM. Is there any strong case that I&amp;#39;m missing for going with 512GB RAM?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m5uu0t","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"ChevChance","discussion_type":null,"num_comments":24,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m5uu0t/strong_case_for_a_512gb_mac_studio/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m5uu0t/strong_case_for_a_512gb_mac_studio/","subreddit_subscribers":502981,"created_utc":1753131529,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4fbrrj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HumanAppointment5","can_mod_post":false,"created_utc":1753137587,"send_replies":true,"parent_id":"t3_1m5uu0t","score":13,"author_fullname":"t2_1tt3gtupry","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Currently you will miss out completely on DeepSeek V3 and R1, plus Kimi K2. DeepSeek V3 is really good for translations. R1 for coding.\\n\\nQwen 235B just got a new version today. Where DeepSeek and Kimi K2 were trained at FP8, Qwen 235B was trained at BF16. On the 512GB you can run the full model instead of a quant.\\n\\nThat's right now. Who knows what models are coming out next week where you wish you had \\"just a tad more RAM\\".\\n\\nLike u/Baldur-Norddahl said, \\"If you are not paying, there are zero reasons for not going with 512GB! :-)\\"","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4fbrrj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Currently you will miss out completely on DeepSeek V3 and R1, plus Kimi K2. DeepSeek V3 is really good for translations. R1 for coding.&lt;/p&gt;\\n\\n&lt;p&gt;Qwen 235B just got a new version today. Where DeepSeek and Kimi K2 were trained at FP8, Qwen 235B was trained at BF16. On the 512GB you can run the full model instead of a quant.&lt;/p&gt;\\n\\n&lt;p&gt;That&amp;#39;s right now. Who knows what models are coming out next week where you wish you had &amp;quot;just a tad more RAM&amp;quot;.&lt;/p&gt;\\n\\n&lt;p&gt;Like &lt;a href=\\"/u/Baldur-Norddahl\\"&gt;u/Baldur-Norddahl&lt;/a&gt; said, &amp;quot;If you are not paying, there are zero reasons for not going with 512GB! :-)&amp;quot;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5uu0t/strong_case_for_a_512gb_mac_studio/n4fbrrj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753137587,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5uu0t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":13}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ffylp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ParaboloidalCrest","can_mod_post":false,"created_utc":1753138951,"send_replies":true,"parent_id":"t3_1m5uu0t","score":12,"author_fullname":"t2_nc2u4f7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Is that a question? Not paying = the biggest you could get.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ffylp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Is that a question? Not paying = the biggest you could get.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5uu0t/strong_case_for_a_512gb_mac_studio/n4ffylp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753138951,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5uu0t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4hnnqt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Baldur-Norddahl","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4fjdex","score":2,"author_fullname":"t2_bvqb8ng0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"They are surprisingly fast due to the use of MoE. The models are huge, but the active parameters per token are small. You can expect something in the range 10-20 tps on the huge MoE models. \\n\\nAnyway, slow but runs beats cannot even load it.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4hnnqt","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They are surprisingly fast due to the use of MoE. The models are huge, but the active parameters per token are small. You can expect something in the range 10-20 tps on the huge MoE models. &lt;/p&gt;\\n\\n&lt;p&gt;Anyway, slow but runs beats cannot even load it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5uu0t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5uu0t/strong_case_for_a_512gb_mac_studio/n4hnnqt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753172195,"author_flair_text":null,"treatment_tags":[],"created_utc":1753172195,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4hdi5s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"k_means_clusterfuck","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4fjdex","score":1,"author_fullname":"t2_4bby1cv5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"They will. Macs are always slower than proper GPUs. But they will run","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4hdi5s","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They will. Macs are always slower than proper GPUs. But they will run&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5uu0t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5uu0t/strong_case_for_a_512gb_mac_studio/n4hdi5s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753166470,"author_flair_text":null,"treatment_tags":[],"created_utc":1753166470,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4fjdex","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ChevChance","can_mod_post":false,"created_utc":1753140068,"send_replies":true,"parent_id":"t1_n4f81yl","score":1,"author_fullname":"t2_sk7nmjrs","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'd heard that the larger models like DeepSeek and Kimi ran slow on the studio.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4fjdex","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;d heard that the larger models like DeepSeek and Kimi ran slow on the studio.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5uu0t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5uu0t/strong_case_for_a_512gb_mac_studio/n4fjdex/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753140068,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4f81yl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Baldur-Norddahl","can_mod_post":false,"created_utc":1753136368,"send_replies":true,"parent_id":"t3_1m5uu0t","score":9,"author_fullname":"t2_bvqb8ng0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you are not paying, there are zero reasons for not going with 512GB! :-)\\n\\n512 GB enables running the serious models such as DeepSeek R1 and Kimi K2 (at 4 bit quantization). It is actually a big deal.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4f81yl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you are not paying, there are zero reasons for not going with 512GB! :-)&lt;/p&gt;\\n\\n&lt;p&gt;512 GB enables running the serious models such as DeepSeek R1 and Kimi K2 (at 4 bit quantization). It is actually a big deal.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5uu0t/strong_case_for_a_512gb_mac_studio/n4f81yl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753136368,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5uu0t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4fk6la","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"chisleu","can_mod_post":false,"created_utc":1753140335,"send_replies":true,"parent_id":"t3_1m5uu0t","score":5,"author_fullname":"t2_cbxyn","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I purchased the 512GB model. If you are going to run LLMs, it's a powerful choice. It can run things by itself that would need many times the price in GPUs alone. Not counting the infra to run them.\\n\\nGet the 512GB of RAM version and get the 4TB SSD upgrade. That will max out the SSD throughput.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4fk6la","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I purchased the 512GB model. If you are going to run LLMs, it&amp;#39;s a powerful choice. It can run things by itself that would need many times the price in GPUs alone. Not counting the infra to run them.&lt;/p&gt;\\n\\n&lt;p&gt;Get the 512GB of RAM version and get the 4TB SSD upgrade. That will max out the SSD throughput.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5uu0t/strong_case_for_a_512gb_mac_studio/n4fk6la/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753140335,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5uu0t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4gcgwc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"synn89","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4fnthq","score":5,"author_fullname":"t2_3jm4t","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Not for a MOE model.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4gcgwc","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not for a MOE model.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5uu0t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5uu0t/strong_case_for_a_512gb_mac_studio/n4gcgwc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753150084,"author_flair_text":null,"treatment_tags":[],"created_utc":1753150084,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n4fnthq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ChevChance","can_mod_post":false,"created_utc":1753141531,"send_replies":true,"parent_id":"t1_n4f8hi2","score":2,"author_fullname":"t2_sk7nmjrs","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Alex Ziskind suggests that the 96GB RAM is the sweet spot\\n\\n[https://www.youtube.com/watch?v=wzPMdp9Qz6Q&amp;t=366s](https://www.youtube.com/watch?v=wzPMdp9Qz6Q&amp;t=366s)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4fnthq","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Alex Ziskind suggests that the 96GB RAM is the sweet spot&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.youtube.com/watch?v=wzPMdp9Qz6Q&amp;amp;t=366s\\"&gt;https://www.youtube.com/watch?v=wzPMdp9Qz6Q&amp;amp;t=366s&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5uu0t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5uu0t/strong_case_for_a_512gb_mac_studio/n4fnthq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753141531,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4f8hi2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"synn89","can_mod_post":false,"created_utc":1753136508,"send_replies":true,"parent_id":"t3_1m5uu0t","score":3,"author_fullname":"t2_3jm4t","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"While it depends on your use case, today, right now Qwen3-235B runs pretty well on Mac, with reasonable sized input context, and you can look at the file sizes at https://huggingface.co/unsloth/Qwen3-235B-A22B-128K-GGUF/tree/main\\n\\nQ8 may be pushing it for 256GB, but a Q6_K fits pretty well and has very little loss at that quant size in 256GB. Now, if you wanted to run MLX, then I think that skips a 6 bit and leaves you with 4 or 8 bit.\\n\\nBut outside of that, the issue is you don't know what's coming down the pipe. Maybe there's a 400-20 MOE that hits which ends up being perfect for the M3 512GB device. But for now, it feels like we're getting really large SOTA models open providers can run(because China lacks inference compute) or 100-300B models for running on smaller systems.\\n\\nThough, really I feel like 512GB won't be great until we get more memory bandwidth. A M3 chip with 256GB sounds about perfect. I know with my 128GB M1 I'd like just a tad more RAM on it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4f8hi2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;While it depends on your use case, today, right now Qwen3-235B runs pretty well on Mac, with reasonable sized input context, and you can look at the file sizes at &lt;a href=\\"https://huggingface.co/unsloth/Qwen3-235B-A22B-128K-GGUF/tree/main\\"&gt;https://huggingface.co/unsloth/Qwen3-235B-A22B-128K-GGUF/tree/main&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Q8 may be pushing it for 256GB, but a Q6_K fits pretty well and has very little loss at that quant size in 256GB. Now, if you wanted to run MLX, then I think that skips a 6 bit and leaves you with 4 or 8 bit.&lt;/p&gt;\\n\\n&lt;p&gt;But outside of that, the issue is you don&amp;#39;t know what&amp;#39;s coming down the pipe. Maybe there&amp;#39;s a 400-20 MOE that hits which ends up being perfect for the M3 512GB device. But for now, it feels like we&amp;#39;re getting really large SOTA models open providers can run(because China lacks inference compute) or 100-300B models for running on smaller systems.&lt;/p&gt;\\n\\n&lt;p&gt;Though, really I feel like 512GB won&amp;#39;t be great until we get more memory bandwidth. A M3 chip with 256GB sounds about perfect. I know with my 128GB M1 I&amp;#39;d like just a tad more RAM on it.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5uu0t/strong_case_for_a_512gb_mac_studio/n4f8hi2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753136508,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5uu0t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4hqyr1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Daemonix00","can_mod_post":false,"created_utc":1753174140,"send_replies":true,"parent_id":"t3_1m5uu0t","score":3,"author_fullname":"t2_h89ec","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":" I have it and I can run all big models, but you talked about \\"refine models\\", this is not easy and memory usage is not the same as \\"running\\" a model.\\n\\nCheck this:  \\n[https://apxml.com/tools/vram-calculator](https://apxml.com/tools/vram-calculator)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4hqyr1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have it and I can run all big models, but you talked about &amp;quot;refine models&amp;quot;, this is not easy and memory usage is not the same as &amp;quot;running&amp;quot; a model.&lt;/p&gt;\\n\\n&lt;p&gt;Check this:&lt;br/&gt;\\n&lt;a href=\\"https://apxml.com/tools/vram-calculator\\"&gt;https://apxml.com/tools/vram-calculator&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5uu0t/strong_case_for_a_512gb_mac_studio/n4hqyr1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753174140,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5uu0t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4g9w57","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"segmond","can_mod_post":false,"created_utc":1753149182,"send_replies":true,"parent_id":"t3_1m5uu0t","score":2,"author_fullname":"t2_ah13x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Even if you are paying, if you can afford it, go as big as you can.  Macs are not upgradeable.  If this was a PC, you can get away by adding ram, GPUs, etc.  My first build from 2 years ago, I started with 64gb of ram and then went to 128gb then at 256gb, and I have additional ram on the way.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4g9w57","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Even if you are paying, if you can afford it, go as big as you can.  Macs are not upgradeable.  If this was a PC, you can get away by adding ram, GPUs, etc.  My first build from 2 years ago, I started with 64gb of ram and then went to 128gb then at 256gb, and I have additional ram on the way.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5uu0t/strong_case_for_a_512gb_mac_studio/n4g9w57/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753149182,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m5uu0t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4fknmf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"chisleu","can_mod_post":false,"created_utc":1753140488,"send_replies":true,"parent_id":"t1_n4fhq6i","score":2,"author_fullname":"t2_cbxyn","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"it's totally a thing, so is distributed llm processing using apple hardware. There are multiple projects going just for that. mx.distributed, exo (exo-labs)... Apple hardware is pretty great for LLMs. You can run the devstral-small bf16 on a macbook pro.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4fknmf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;it&amp;#39;s totally a thing, so is distributed llm processing using apple hardware. There are multiple projects going just for that. mx.distributed, exo (exo-labs)... Apple hardware is pretty great for LLMs. You can run the devstral-small bf16 on a macbook pro.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5uu0t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5uu0t/strong_case_for_a_512gb_mac_studio/n4fknmf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753140488,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4fscqy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"chibop1","can_mod_post":false,"created_utc":1753143074,"send_replies":true,"parent_id":"t1_n4fhq6i","score":0,"author_fullname":"t2_e9jh97s","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You can easily finetune models with MLX.\\n\\nhttps://github.com/ml-explore/mlx-lm/blob/main/mlx_lm/LORA.md","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4fscqy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can easily finetune models with MLX.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/ml-explore/mlx-lm/blob/main/mlx_lm/LORA.md\\"&gt;https://github.com/ml-explore/mlx-lm/blob/main/mlx_lm/LORA.md&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5uu0t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5uu0t/strong_case_for_a_512gb_mac_studio/n4fscqy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753143074,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n4fhq6i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Willing_Landscape_61","can_mod_post":false,"created_utc":1753139531,"send_replies":true,"parent_id":"t3_1m5uu0t","score":2,"author_fullname":"t2_8lvrytgw","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Never heard of fine tuning on Mac.\\nIs that really a thing?\\nAny benchmark available somewhere?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4fhq6i","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Never heard of fine tuning on Mac.\\nIs that really a thing?\\nAny benchmark available somewhere?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5uu0t/strong_case_for_a_512gb_mac_studio/n4fhq6i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753139531,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5uu0t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4f5qlj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ChevChance","can_mod_post":false,"created_utc":1753135616,"send_replies":true,"parent_id":"t1_n4f069l","score":1,"author_fullname":"t2_sk7nmjrs","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Fair question, I anticipate distillation refinement and running models locally, so that company code never leaves the building.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4f5qlj","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Fair question, I anticipate distillation refinement and running models locally, so that company code never leaves the building.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5uu0t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5uu0t/strong_case_for_a_512gb_mac_studio/n4f5qlj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753135616,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4f069l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"National_Meeting_749","can_mod_post":false,"created_utc":1753133875,"send_replies":true,"parent_id":"t3_1m5uu0t","score":1,"author_fullname":"t2_drm5tg5d","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What's your use case? How big of a model do need to run and fine tune?\\n\\nI'm not an expert, but I do know that info will help others help you.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4f069l","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What&amp;#39;s your use case? How big of a model do need to run and fine tune?&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m not an expert, but I do know that info will help others help you.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5uu0t/strong_case_for_a_512gb_mac_studio/n4f069l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753133875,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5uu0t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4h8fbw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HumanAppointment5","can_mod_post":false,"created_utc":1753163743,"send_replies":true,"parent_id":"t3_1m5uu0t","score":1,"author_fullname":"t2_1tt3gtupry","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"We normally think of running one model at a time (due to memory and GPU constraints). But there is lots of value in being able to run multiple models at the same time. Or being able to run agents with your model.\\n\\nSimon Willison mentions this (at https://simonwillison.net/2025/Mar/24/qwen25-vl-32b/) \\"... is a 32B model, which is quickly becoming my personal favourite model size - large enough to have GPT-4-class capabilities, but small enough that on my 64GB Mac there's still enough RAM for me to run other memory-hungry applications like Firefox and VS Code.\\"","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4h8fbw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;We normally think of running one model at a time (due to memory and GPU constraints). But there is lots of value in being able to run multiple models at the same time. Or being able to run agents with your model.&lt;/p&gt;\\n\\n&lt;p&gt;Simon Willison mentions this (at &lt;a href=\\"https://simonwillison.net/2025/Mar/24/qwen25-vl-32b/\\"&gt;https://simonwillison.net/2025/Mar/24/qwen25-vl-32b/&lt;/a&gt;) &amp;quot;... is a 32B model, which is quickly becoming my personal favourite model size - large enough to have GPT-4-class capabilities, but small enough that on my 64GB Mac there&amp;#39;s still enough RAM for me to run other memory-hungry applications like Firefox and VS Code.&amp;quot;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5uu0t/strong_case_for_a_512gb_mac_studio/n4h8fbw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753163743,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5uu0t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4mdc4z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"daaain","can_mod_post":false,"created_utc":1753227941,"send_replies":true,"parent_id":"t1_n4j4p1y","score":1,"author_fullname":"t2_47j85","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The most compelling case isn't to run a huge model, but to fit multiple. But if you want to run any training you're going to be compute bound, so would be better off picking up a few refurb M1/M2 Ultras instead and chaining them up.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4mdc4z","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The most compelling case isn&amp;#39;t to run a huge model, but to fit multiple. But if you want to run any training you&amp;#39;re going to be compute bound, so would be better off picking up a few refurb M1/M2 Ultras instead and chaining them up.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5uu0t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5uu0t/strong_case_for_a_512gb_mac_studio/n4mdc4z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753227941,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4j4p1y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ChevChance","can_mod_post":false,"created_utc":1753194468,"send_replies":true,"parent_id":"t3_1m5uu0t","score":1,"author_fullname":"t2_sk7nmjrs","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Appreciate everyone's input; thanks for taking the trouble to comment! I'm not seeing compelling use cases for a 512GB configuration, aside from running a quantized version of DeepSeek R1, which doesn't seem to run very fast on a 512 M3 Ultra, and hypotheticals about upcoming models which may need the larger RAM. It may not be my money but I need to be realistic about purchases.","edited":1753194754,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4j4p1y","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Appreciate everyone&amp;#39;s input; thanks for taking the trouble to comment! I&amp;#39;m not seeing compelling use cases for a 512GB configuration, aside from running a quantized version of DeepSeek R1, which doesn&amp;#39;t seem to run very fast on a 512 M3 Ultra, and hypotheticals about upcoming models which may need the larger RAM. It may not be my money but I need to be realistic about purchases.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5uu0t/strong_case_for_a_512gb_mac_studio/n4j4p1y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753194468,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5uu0t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4km0tg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Hot-Entrepreneur2934","can_mod_post":false,"created_utc":1753209182,"send_replies":true,"parent_id":"t3_1m5uu0t","score":1,"author_fullname":"t2_1kmdyhhvg4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For a similar price you can get a high end PC with a 5090. It will run everything up to the 32gb memory limit much more quickly. You may not be able to fine tune larger models, but you'll also avoid the compatibility gaps.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4km0tg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For a similar price you can get a high end PC with a 5090. It will run everything up to the 32gb memory limit much more quickly. You may not be able to fine tune larger models, but you&amp;#39;ll also avoid the compatibility gaps.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5uu0t/strong_case_for_a_512gb_mac_studio/n4km0tg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753209182,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5uu0t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4g451u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok_Warning2146","can_mod_post":false,"created_utc":1753147176,"send_replies":true,"parent_id":"t3_1m5uu0t","score":1,"author_fullname":"t2_s6sfw4yy","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"512GB also allows you to run multiple llms at the same time. This allows you to run a workflow that allocates tasks to different llms. For example, gemma 3 is better in writing and qwen3 is better in coding. You can route your question to either one of them to get better result.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4g451u","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;512GB also allows you to run multiple llms at the same time. This allows you to run a workflow that allocates tasks to different llms. For example, gemma 3 is better in writing and qwen3 is better in coding. You can route your question to either one of them to get better result.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5uu0t/strong_case_for_a_512gb_mac_studio/n4g451u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753147176,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5uu0t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4gv9lk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GPTshop_ai","can_mod_post":false,"created_utc":1753157462,"send_replies":true,"parent_id":"t3_1m5uu0t","score":-1,"author_fullname":"t2_rkmud0isr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Anyone who buys apple, does not know what the apple logo really means.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4gv9lk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Anyone who buys apple, does not know what the apple logo really means.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5uu0t/strong_case_for_a_512gb_mac_studio/n4gv9lk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753157462,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5uu0t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4i7fu6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"datbackup","can_mod_post":false,"created_utc":1753182896,"send_replies":true,"parent_id":"t3_1m5uu0t","score":0,"author_fullname":"t2_ielo6","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"\\n&gt;I think between buying a Mac and building a multichannel RAM system, the only big mistake would be buying a Mac that isn’t the 512GB m3 ultra.\\n\\nhttps://www.reddit.com/r/LocalLLaMA/s/6iNGFGQcQ3","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4i7fu6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;I think between buying a Mac and building a multichannel RAM system, the only big mistake would be buying a Mac that isn’t the 512GB m3 ultra.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/s/6iNGFGQcQ3\\"&gt;https://www.reddit.com/r/LocalLLaMA/s/6iNGFGQcQ3&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5uu0t/strong_case_for_a_512gb_mac_studio/n4i7fu6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753182896,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5uu0t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}}]`),n=()=>e.jsx(l,{data:t});export{n as default};
