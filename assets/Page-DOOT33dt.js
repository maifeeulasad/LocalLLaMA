import{j as t}from"./index-BgwOAK4-.js";import{R as e}from"./RedditPostRenderer-BOBjDTFu.js";import"./index-BL22wVg5.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I want to build my own agent system similar to Claude Projects or ChatGPT Projects, where users can:\\n\\n* Upload documents that persist across conversations\\n* Set custom instructions for the agent\\n* Have the AI seamlessly reference uploaded materials\\n\\n**What I'm trying to replicate:**\\n\\n* Upload PDFs, docs, code files as \\"context\\" for an agent\\n* Agent maintains this context across multiple chat sessions\\n* Smooth integration (not obvious \\"searching\\" behavior like traditional RAG)\\n* Custom system instructions that persist\\n\\n**Technical questions for implementation:**\\n\\n1. **Context Management:** Do you think they use traditional RAG with vector search, or just concatenate documents into the prompt? The behavior feels more like extended context than retrieval.\\n2. **Token Limits:** How would you handle large documents exceeding context windows? Smart chunking? Summarization? Hierarchical retrieval?\\n3. **Implementation patterns:** Has anyone built something similar? \\n\\n**Looking for:**\\n\\n* Architecture advice from anyone who's built similar systems\\n* Open source implementations I could learn from\\n* Insights into how the commercial systems might work\\n\\nAny suggestions on approach, tools?\\n\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Building a Claude/ChatGPT Projects-like system: How to implement persistent context with uploaded documents?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1ly3dk9","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.33,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_a0w73wk0","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752334599,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I want to build my own agent system similar to Claude Projects or ChatGPT Projects, where users can:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Upload documents that persist across conversations&lt;/li&gt;\\n&lt;li&gt;Set custom instructions for the agent&lt;/li&gt;\\n&lt;li&gt;Have the AI seamlessly reference uploaded materials&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;&lt;strong&gt;What I&amp;#39;m trying to replicate:&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Upload PDFs, docs, code files as &amp;quot;context&amp;quot; for an agent&lt;/li&gt;\\n&lt;li&gt;Agent maintains this context across multiple chat sessions&lt;/li&gt;\\n&lt;li&gt;Smooth integration (not obvious &amp;quot;searching&amp;quot; behavior like traditional RAG)&lt;/li&gt;\\n&lt;li&gt;Custom system instructions that persist&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Technical questions for implementation:&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;&lt;strong&gt;Context Management:&lt;/strong&gt; Do you think they use traditional RAG with vector search, or just concatenate documents into the prompt? The behavior feels more like extended context than retrieval.&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Token Limits:&lt;/strong&gt; How would you handle large documents exceeding context windows? Smart chunking? Summarization? Hierarchical retrieval?&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Implementation patterns:&lt;/strong&gt; Has anyone built something similar? &lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Looking for:&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Architecture advice from anyone who&amp;#39;s built similar systems&lt;/li&gt;\\n&lt;li&gt;Open source implementations I could learn from&lt;/li&gt;\\n&lt;li&gt;Insights into how the commercial systems might work&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Any suggestions on approach, tools?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1ly3dk9","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Funny-Enthusiasm-610","discussion_type":null,"num_comments":3,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1ly3dk9/building_a_claudechatgpt_projectslike_system_how/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1ly3dk9/building_a_claudechatgpt_projectslike_system_how/","subreddit_subscribers":498345,"created_utc":1752334599,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2qrx6y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok_Doughnut5075","can_mod_post":false,"created_utc":1752334875,"send_replies":true,"parent_id":"t3_1ly3dk9","score":1,"author_fullname":"t2_1gyxgr8g2t","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I would guess that RAG is a big part of what all modern LLM chat products do.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2qrx6y","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I would guess that RAG is a big part of what all modern LLM chat products do.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ly3dk9/building_a_claudechatgpt_projectslike_system_how/n2qrx6y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752334875,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ly3dk9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2qug0e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BidWestern1056","can_mod_post":false,"created_utc":1752335658,"send_replies":true,"parent_id":"t3_1ly3dk9","score":1,"author_fullname":"t2_uzxql7po","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"npcpy has a command history module for keeping track of data associated with a folder and with specific agents/agent teams so would be a good option .\\n\\n\\nhttps://github.com/NPC-Worldwide/npcpy\\n\\n\\nyoud need to build a \\"frontend\\" so to say for which to pull in in your scenario and figure out how to prioritize what contextual items to show but ideally the way this is set up could save you some overhead","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2qug0e","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;npcpy has a command history module for keeping track of data associated with a folder and with specific agents/agent teams so would be a good option .&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/NPC-Worldwide/npcpy\\"&gt;https://github.com/NPC-Worldwide/npcpy&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;youd need to build a &amp;quot;frontend&amp;quot; so to say for which to pull in in your scenario and figure out how to prioritize what contextual items to show but ideally the way this is set up could save you some overhead&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ly3dk9/building_a_claudechatgpt_projectslike_system_how/n2qug0e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752335658,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ly3dk9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2sl4g2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CognitivelyPrismatic","can_mod_post":false,"created_utc":1752355423,"send_replies":true,"parent_id":"t3_1ly3dk9","score":1,"author_fullname":"t2_h795t4jep","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Claude projects literally just puts the entire document in the system prompt","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2sl4g2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Claude projects literally just puts the entire document in the system prompt&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ly3dk9/building_a_claudechatgpt_projectslike_system_how/n2sl4g2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752355423,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ly3dk9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),s=()=>t.jsx(e,{data:l});export{s as default};
