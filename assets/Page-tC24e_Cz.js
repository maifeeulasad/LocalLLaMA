import{j as l}from"./index-xfnGEtuL.js";import{R as e}from"./RedditPostRenderer-DAZRIZZK.js";import"./index-BaNn5-RR.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Or some website that will let me know for a rtx 4090, 32gb ram, what the performance of deepseek-r1 will be? \\n\\nThanks, i don't know where to start. \\n\\nI have an rtx 4080s (16gb graphics ram) with 64gb ram on a 13700k...","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Is there some localllm benchmarking tool to see how well your system will handle a model?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lwp7tv","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.5,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_el036","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752183949,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Or some website that will let me know for a rtx 4090, 32gb ram, what the performance of deepseek-r1 will be? &lt;/p&gt;\\n\\n&lt;p&gt;Thanks, i don&amp;#39;t know where to start. &lt;/p&gt;\\n\\n&lt;p&gt;I have an rtx 4080s (16gb graphics ram) with 64gb ram on a 13700k...&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lwp7tv","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"blackashi","discussion_type":null,"num_comments":11,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lwp7tv/is_there_some_localllm_benchmarking_tool_to_see/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lwp7tv/is_there_some_localllm_benchmarking_tool_to_see/","subreddit_subscribers":497504,"created_utc":1752183949,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2g9e98","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SlowFail2433","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2g1nvc","score":3,"author_fullname":"t2_131eezppgs","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ollama did a number on all of us","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2g9e98","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ollama did a number on all of us&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwp7tv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwp7tv/is_there_some_localllm_benchmarking_tool_to_see/n2g9e98/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752188220,"author_flair_text":null,"treatment_tags":[],"created_utc":1752188220,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2gmb18","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"universenz","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2g9pzw","score":1,"author_fullname":"t2_blo3b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2gmb18","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwp7tv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwp7tv/is_there_some_localllm_benchmarking_tool_to_see/n2gmb18/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752192518,"author_flair_text":null,"treatment_tags":[],"created_utc":1752192518,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2g9pzw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mxmumtuna","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2g1nvc","score":2,"author_fullname":"t2_igbly","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes friend. Good news is that there’s plenty of good models that will run with those specs!","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2g9pzw","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes friend. Good news is that there’s plenty of good models that will run with those specs!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"link_id":"t3_1lwp7tv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwp7tv/is_there_some_localllm_benchmarking_tool_to_see/n2g9pzw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752188325,"author_flair_text":null,"treatment_tags":[],"created_utc":1752188325,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2g1nvc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"blackashi","can_mod_post":false,"created_utc":1752185729,"send_replies":true,"parent_id":"t1_n2g0rsz","score":1,"author_fullname":"t2_el036","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"too big? noooo but ollama said... :(","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2g1nvc","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;too big? noooo but ollama said... :(&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwp7tv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwp7tv/is_there_some_localllm_benchmarking_tool_to_see/n2g1nvc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752185729,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2g0rsz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"universenz","can_mod_post":false,"created_utc":1752185442,"send_replies":true,"parent_id":"t3_1lwp7tv","score":6,"author_fullname":"t2_blo3b","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Blackashi, I’m gonna need you to take a seat as I tell you this. You see Deepseek R1 is…","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2g0rsz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Blackashi, I’m gonna need you to take a seat as I tell you this. You see Deepseek R1 is…&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwp7tv/is_there_some_localllm_benchmarking_tool_to_see/n2g0rsz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752185442,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwp7tv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2i5rdw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"triynizzles1","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2i2xz8","score":2,"author_fullname":"t2_zr0g49ixt","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Complex questions, including coding: QWQ\\nNot so complex coding questions: Qwen 2.5 coder\\nDaily driver: mistral small 3.1. (it has vision capabilities so it needs 28gb to run)\\nHonorable mention: phi 4.\\n\\nI have rtx8000","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2i5rdw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Complex questions, including coding: QWQ\\nNot so complex coding questions: Qwen 2.5 coder\\nDaily driver: mistral small 3.1. (it has vision capabilities so it needs 28gb to run)\\nHonorable mention: phi 4.&lt;/p&gt;\\n\\n&lt;p&gt;I have rtx8000&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwp7tv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwp7tv/is_there_some_localllm_benchmarking_tool_to_see/n2i5rdw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752214634,"author_flair_text":null,"treatment_tags":[],"created_utc":1752214634,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2i2xz8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"blackashi","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2gk08r","score":2,"author_fullname":"t2_el036","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"thanks! i like your formula. what's your go to right now?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2i2xz8","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;thanks! i like your formula. what&amp;#39;s your go to right now?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwp7tv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwp7tv/is_there_some_localllm_benchmarking_tool_to_see/n2i2xz8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752213188,"author_flair_text":null,"treatment_tags":[],"created_utc":1752213188,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2gk08r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"triynizzles1","can_mod_post":false,"created_utc":1752191722,"send_replies":true,"parent_id":"t1_n2gjomd","score":3,"author_fullname":"t2_zr0g49ixt","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":" As others have said the actual deepseek r1 is huge and will not run on a single 4090. There are distilled versions that you can run, but they aren’t as intelligent.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2gk08r","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;As others have said the actual deepseek r1 is huge and will not run on a single 4090. There are distilled versions that you can run, but they aren’t as intelligent.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwp7tv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwp7tv/is_there_some_localllm_benchmarking_tool_to_see/n2gk08r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752191722,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2gjomd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"triynizzles1","can_mod_post":false,"created_utc":1752191613,"send_replies":true,"parent_id":"t3_1lwp7tv","score":3,"author_fullname":"t2_zr0g49ixt","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Take your GPU’s memory bandwidth and divided by the amount of storage the model takes. This will give you a close, rough estimation.\\n\\n4090 has a memory bandwidth of basically 1000 GB/ second\\n\\nA 32 billion parameter model with 6K context window is somewhere around 18 GB of VRAM\\n\\n1000 / 18 is 55.\\n\\n55 tokens per second would be the upper limit of performance. And the real world there will be some additional overhead. In the above described configuration, it would be reasonable to expect 50 to 55 tokens per second.\\n\\nThe math gets a little bit more complex when estimating the speed of an MOE model, you would calculate based on the amount of memory the active parameters take up. The math is also more complicated when layers are shared across multiple GPUs and system memory.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2gjomd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Take your GPU’s memory bandwidth and divided by the amount of storage the model takes. This will give you a close, rough estimation.&lt;/p&gt;\\n\\n&lt;p&gt;4090 has a memory bandwidth of basically 1000 GB/ second&lt;/p&gt;\\n\\n&lt;p&gt;A 32 billion parameter model with 6K context window is somewhere around 18 GB of VRAM&lt;/p&gt;\\n\\n&lt;p&gt;1000 / 18 is 55.&lt;/p&gt;\\n\\n&lt;p&gt;55 tokens per second would be the upper limit of performance. And the real world there will be some additional overhead. In the above described configuration, it would be reasonable to expect 50 to 55 tokens per second.&lt;/p&gt;\\n\\n&lt;p&gt;The math gets a little bit more complex when estimating the speed of an MOE model, you would calculate based on the amount of memory the active parameters take up. The math is also more complicated when layers are shared across multiple GPUs and system memory.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwp7tv/is_there_some_localllm_benchmarking_tool_to_see/n2gjomd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752191613,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwp7tv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2g45ni","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"blackashi","can_mod_post":false,"created_utc":1752186527,"send_replies":true,"parent_id":"t1_n2g29tk","score":1,"author_fullname":"t2_el036","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks that's useful!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2g45ni","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks that&amp;#39;s useful!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwp7tv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwp7tv/is_there_some_localllm_benchmarking_tool_to_see/n2g45ni/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752186527,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2g29tk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Toooooool","can_mod_post":false,"created_utc":1752185924,"send_replies":true,"parent_id":"t3_1lwp7tv","score":1,"author_fullname":"t2_8llornh4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Personally I use:  \\n[https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator](https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator)\\n\\nHelps calculating the right context size and quants too.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2g29tk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Personally I use:&lt;br/&gt;\\n&lt;a href=\\"https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator\\"&gt;https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Helps calculating the right context size and quants too.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwp7tv/is_there_some_localllm_benchmarking_tool_to_see/n2g29tk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752185924,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwp7tv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>l.jsx(e,{data:a});export{n as default};
