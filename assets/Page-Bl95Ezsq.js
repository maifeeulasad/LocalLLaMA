import{j as e}from"./index-BlGsFJYy.js";import{R as t}from"./RedditPostRenderer-B6uvq_Zl.js";import"./index-DDvVtNwD.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Firstly, total disclaimer. About 4 months ago, I knew very little about LLMs, so I am one of those people who went down the rabbit hole and started chatting with AI. But, I'm a chap who does a lot of pattern recognition in the way I work (I can write music for orchestras without reading it) so just sort of tugged on those pattern strings and I think I've found something that's pretty effective (well it has been for me anyway).\\n\\n\\n\\nLong story short, I noticed that all LLMs seem to have their training data steeped in Greek Mythology. So I decided to see if you could use that shared knowledge as compression. Add into that syntax that all LLMs understand (:: for clear key-value assignments, → for causality and progression, etc) and I've combined these two layers to create a DSL that's more token-efficient but also richer and more logically sound.\\n\\n\\n\\nThis isn't a library you need to install; it's just a spec. Any LLM I've tested it on can understand it out of the box. I've documented everything (the full syntax, semantics, philosophy, and benchmarks) on GitHub.\\n\\n\\n\\nI'm sharing this because I think it's a genuinely useful technique, and I'd love to get your feedback to help improve it. Or even someone tell me it already exists and I'll use the proper version!\\n\\n\\n\\nLink to the repo: [https://github.com/elevanaltd/octave](https://github.com/elevanaltd/octave)\\n\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"I've built a spec for LLM-to-LLM comms by combining semantic patterns with structured syntax","link_flair_richtext":[{"e":"text","t":"Resources"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lompd5","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.85,"author_flair_background_color":null,"subreddit_type":"public","ups":13,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_wfcudj1nx","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Resources","can_mod_post":false,"score":13,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1751326260,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Firstly, total disclaimer. About 4 months ago, I knew very little about LLMs, so I am one of those people who went down the rabbit hole and started chatting with AI. But, I&amp;#39;m a chap who does a lot of pattern recognition in the way I work (I can write music for orchestras without reading it) so just sort of tugged on those pattern strings and I think I&amp;#39;ve found something that&amp;#39;s pretty effective (well it has been for me anyway).&lt;/p&gt;\\n\\n&lt;p&gt;Long story short, I noticed that all LLMs seem to have their training data steeped in Greek Mythology. So I decided to see if you could use that shared knowledge as compression. Add into that syntax that all LLMs understand (:: for clear key-value assignments, → for causality and progression, etc) and I&amp;#39;ve combined these two layers to create a DSL that&amp;#39;s more token-efficient but also richer and more logically sound.&lt;/p&gt;\\n\\n&lt;p&gt;This isn&amp;#39;t a library you need to install; it&amp;#39;s just a spec. Any LLM I&amp;#39;ve tested it on can understand it out of the box. I&amp;#39;ve documented everything (the full syntax, semantics, philosophy, and benchmarks) on GitHub.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m sharing this because I think it&amp;#39;s a genuinely useful technique, and I&amp;#39;d love to get your feedback to help improve it. Or even someone tell me it already exists and I&amp;#39;ll use the proper version!&lt;/p&gt;\\n\\n&lt;p&gt;Link to the repo: &lt;a href=\\"https://github.com/elevanaltd/octave\\"&gt;https://github.com/elevanaltd/octave&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/5mRzok8P7sadKGuJL5w4dxeiaZgoDJbskEgknNv52vo.png?auto=webp&amp;s=c7e7eeac9c3a1d0c9a783f1fdac2cbbc985f2e82","width":1200,"height":600},"resolutions":[{"url":"https://external-preview.redd.it/5mRzok8P7sadKGuJL5w4dxeiaZgoDJbskEgknNv52vo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b51ae35d10f2d1ad57bc1bc156d2cc561fdeca52","width":108,"height":54},{"url":"https://external-preview.redd.it/5mRzok8P7sadKGuJL5w4dxeiaZgoDJbskEgknNv52vo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=24c1a8cee395a706e6629f917a530b0df0c80e17","width":216,"height":108},{"url":"https://external-preview.redd.it/5mRzok8P7sadKGuJL5w4dxeiaZgoDJbskEgknNv52vo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=26f4856a8af31503ae033bd40f4dae89dd724ddb","width":320,"height":160},{"url":"https://external-preview.redd.it/5mRzok8P7sadKGuJL5w4dxeiaZgoDJbskEgknNv52vo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=962e6c21187dc5928106ea7593106564d3f67ef4","width":640,"height":320},{"url":"https://external-preview.redd.it/5mRzok8P7sadKGuJL5w4dxeiaZgoDJbskEgknNv52vo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1af2bd863e2a59045c6981fd82b5a2dd006cd35b","width":960,"height":480},{"url":"https://external-preview.redd.it/5mRzok8P7sadKGuJL5w4dxeiaZgoDJbskEgknNv52vo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=014018c0ac83eccb819e608654fc63b71de07233","width":1080,"height":540}],"variants":{},"id":"5mRzok8P7sadKGuJL5w4dxeiaZgoDJbskEgknNv52vo"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#ccac2b","id":"1lompd5","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"sbuswell","discussion_type":null,"num_comments":11,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lompd5/ive_built_a_spec_for_llmtollm_comms_by_combining/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lompd5/ive_built_a_spec_for_llmtollm_comms_by_combining/","subreddit_subscribers":493458,"created_utc":1751326260,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0q15xr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SkyFeistyLlama8","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0psl2w","score":1,"author_fullname":"t2_1hgbaqgbnq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"1. The river Temarc frozen in winter’s grasp like the Styx in Hades’ grip.\\n2. The mind’s loom spinning threads from the looms of old.\\n3. Some patterns are like the Labyrinth, known to the initiate.\\n4. Tales of Shang are as distant as the Hyperboreans.\\n5. The LLM forges meaning from the embers of ancient fires.\\n6. Wintermute and Neuromancer as players in the grand epic.\\n7. The Fates whispering in their ears.\\n8. Words as layered as the scrolls of Homer.\\n\\nI put our previous interaction into Mistral 24B and got it to generate metaphors matching what was discussed. I guess I'm seeing semantic compression here but there's also a trope-izing going on as the LLM tries to generalize from specifics.\\n\\nOne more round of compression and I get: \\"Frozen Styx-river, labyrinth threads, Hyperborean echoes, fire-forged meaning, epic players, Fate’s whispers, Homer’s layered scrolls.\\" Darmok, indeed.","edited":1751356809,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0q15xr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;ol&gt;\\n&lt;li&gt;The river Temarc frozen in winter’s grasp like the Styx in Hades’ grip.&lt;/li&gt;\\n&lt;li&gt;The mind’s loom spinning threads from the looms of old.&lt;/li&gt;\\n&lt;li&gt;Some patterns are like the Labyrinth, known to the initiate.&lt;/li&gt;\\n&lt;li&gt;Tales of Shang are as distant as the Hyperboreans.&lt;/li&gt;\\n&lt;li&gt;The LLM forges meaning from the embers of ancient fires.&lt;/li&gt;\\n&lt;li&gt;Wintermute and Neuromancer as players in the grand epic.&lt;/li&gt;\\n&lt;li&gt;The Fates whispering in their ears.&lt;/li&gt;\\n&lt;li&gt;Words as layered as the scrolls of Homer.&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;I put our previous interaction into Mistral 24B and got it to generate metaphors matching what was discussed. I guess I&amp;#39;m seeing semantic compression here but there&amp;#39;s also a trope-izing going on as the LLM tries to generalize from specifics.&lt;/p&gt;\\n\\n&lt;p&gt;One more round of compression and I get: &amp;quot;Frozen Styx-river, labyrinth threads, Hyperborean echoes, fire-forged meaning, epic players, Fate’s whispers, Homer’s layered scrolls.&amp;quot; Darmok, indeed.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lompd5","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lompd5/ive_built_a_spec_for_llmtollm_comms_by_combining/n0q15xr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751356490,"author_flair_text":null,"treatment_tags":[],"created_utc":1751356490,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0psl2w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"sbuswell","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ordvc","score":1,"author_fullname":"t2_wfcudj1nx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah, I did look at using other cultural references and they can work but I found zero shot full understanding with Greek mythology because it permeated the training corpora so much so I’ve left with that for now. Almost all scenarios needed can be covered by this it seems. I also use movie references occasionally when describing stuff and found it really useful (eg I was talking about the LLM creating like a Leonard Shelby tattoo to remind itself of stuff post compaction and it got the reference well.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0psl2w","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, I did look at using other cultural references and they can work but I found zero shot full understanding with Greek mythology because it permeated the training corpora so much so I’ve left with that for now. Almost all scenarios needed can be covered by this it seems. I also use movie references occasionally when describing stuff and found it really useful (eg I was talking about the LLM creating like a Leonard Shelby tattoo to remind itself of stuff post compaction and it got the reference well.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lompd5","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lompd5/ive_built_a_spec_for_llmtollm_comms_by_combining/n0psl2w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751351444,"author_flair_text":null,"treatment_tags":[],"created_utc":1751351444,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ordvc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SkyFeistyLlama8","can_mod_post":false,"created_utc":1751335172,"send_replies":true,"parent_id":"t1_n0oe5r8","score":2,"author_fullname":"t2_1hgbaqgbnq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The river Temarc, in winter!\\n\\nOr any other meme, really.\\n\\nI love the weird stuff that pops up on here sometimes. Semantic compression is real because it works in humans and with LLMs being trained on human patterns, it wouldn't be farfetched for LLMs to understand tropes from human literature. I'm reminded of the controversial work of Jorn Barger from way back in the early days of the Web: he proposed training a future neural network on the contents of books like the Greek classics and Ulysses, to get it to understand human behavior by looking at common examples across a few thousand years of fiction.\\n\\nAnd here we are. The problem is that it's not universal, it only works for a specific cultural subset. If I put examples from ancient Chinese myths dating back to the Shang kingdoms, I might not get as good a result compared to using bits from the Odyssey.\\n\\nLLMs talking to LLMs... Wintermute, say hi to Neuromancer.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ordvc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The river Temarc, in winter!&lt;/p&gt;\\n\\n&lt;p&gt;Or any other meme, really.&lt;/p&gt;\\n\\n&lt;p&gt;I love the weird stuff that pops up on here sometimes. Semantic compression is real because it works in humans and with LLMs being trained on human patterns, it wouldn&amp;#39;t be farfetched for LLMs to understand tropes from human literature. I&amp;#39;m reminded of the controversial work of Jorn Barger from way back in the early days of the Web: he proposed training a future neural network on the contents of books like the Greek classics and Ulysses, to get it to understand human behavior by looking at common examples across a few thousand years of fiction.&lt;/p&gt;\\n\\n&lt;p&gt;And here we are. The problem is that it&amp;#39;s not universal, it only works for a specific cultural subset. If I put examples from ancient Chinese myths dating back to the Shang kingdoms, I might not get as good a result compared to using bits from the Odyssey.&lt;/p&gt;\\n\\n&lt;p&gt;LLMs talking to LLMs... Wintermute, say hi to Neuromancer.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lompd5","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lompd5/ive_built_a_spec_for_llmtollm_comms_by_combining/n0ordvc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751335172,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0oe5r8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Disposable110","can_mod_post":false,"created_utc":1751330499,"send_replies":true,"parent_id":"t3_1lompd5","score":4,"author_fullname":"t2_b8logo4s","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Temba, his arms wide!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0oe5r8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Temba, his arms wide!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lompd5/ive_built_a_spec_for_llmtollm_comms_by_combining/n0oe5r8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751330499,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lompd5","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0pn7q4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Not_your_guy_buddy42","can_mod_post":false,"created_utc":1751348547,"send_replies":true,"parent_id":"t3_1lompd5","score":4,"author_fullname":"t2_4m6vm3ghs","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This is like a sane version of what people on r / artificialsentience are doing lol. I played with semantic compression before and it works. Prompts like \\"Try and compress this similarly to how a seed contains all DNA of the tree\\" or \\"In Three Body Problem, A single photon is unfolded into the size of a planet, inscribed with information and folded back into a photon. Compress the information like that\\".","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0pn7q4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is like a sane version of what people on r / artificialsentience are doing lol. I played with semantic compression before and it works. Prompts like &amp;quot;Try and compress this similarly to how a seed contains all DNA of the tree&amp;quot; or &amp;quot;In Three Body Problem, A single photon is unfolded into the size of a planet, inscribed with information and folded back into a photon. Compress the information like that&amp;quot;.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lompd5/ive_built_a_spec_for_llmtollm_comms_by_combining/n0pn7q4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751348547,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lompd5","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0q5rii","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"sbuswell","can_mod_post":false,"created_utc":1751359298,"send_replies":true,"parent_id":"t1_n0pz9me","score":1,"author_fullname":"t2_wfcudj1nx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"So I use it to convert regular system prompts and docs I use a lot, or compress research docs that are heavy. Just use the user guide and get an llm to convert any doc that could do with compression or comms and use that instead. \\n\\nIf you make your system prompt in octave, it’s unlikely to respond in that language. Most of the time my responses I see are in natural language, especially if your user prompt is. Sometimes it does do octave, but that seems to happen more if you’re doing multi-agent stuff. I think that’s good but you can always just add “reply in natural language if you want output to not be octave and just utilise it for giving prompts or info in a condensed and rich way. \\n\\nFor single prompts, or just general individual things, the idea of getting an llm to convert the doc for another llm to read sort of defeats the point so it’s only really useful for files read regularly I find. \\n\\nMaybe others can find better uses for it, I don’t know. But it’s saved me a lot of space, and I’ve found the models are more focused as there’s less noise to deal with.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0q5rii","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So I use it to convert regular system prompts and docs I use a lot, or compress research docs that are heavy. Just use the user guide and get an llm to convert any doc that could do with compression or comms and use that instead. &lt;/p&gt;\\n\\n&lt;p&gt;If you make your system prompt in octave, it’s unlikely to respond in that language. Most of the time my responses I see are in natural language, especially if your user prompt is. Sometimes it does do octave, but that seems to happen more if you’re doing multi-agent stuff. I think that’s good but you can always just add “reply in natural language if you want output to not be octave and just utilise it for giving prompts or info in a condensed and rich way. &lt;/p&gt;\\n\\n&lt;p&gt;For single prompts, or just general individual things, the idea of getting an llm to convert the doc for another llm to read sort of defeats the point so it’s only really useful for files read regularly I find. &lt;/p&gt;\\n\\n&lt;p&gt;Maybe others can find better uses for it, I don’t know. But it’s saved me a lot of space, and I’ve found the models are more focused as there’s less noise to deal with.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lompd5","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lompd5/ive_built_a_spec_for_llmtollm_comms_by_combining/n0q5rii/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751359298,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0pz9me","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jaxupaxu","can_mod_post":false,"created_utc":1751355341,"send_replies":true,"parent_id":"t3_1lompd5","score":1,"author_fullname":"t2_zo4gm","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I don't get it, how is this supposed to be used? Am supposed to somehow \\"compress\\" my prompt into this and then send it over to the LLM? Won't it answer in a similar way? ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0pz9me","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t get it, how is this supposed to be used? Am supposed to somehow &amp;quot;compress&amp;quot; my prompt into this and then send it over to the LLM? Won&amp;#39;t it answer in a similar way? &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lompd5/ive_built_a_spec_for_llmtollm_comms_by_combining/n0pz9me/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751355341,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lompd5","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0qr67r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"sbuswell","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0qqedt","score":1,"author_fullname":"t2_wfcudj1nx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Oh, phi4 also totally got it all from what I can tell.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0qr67r","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Oh, phi4 also totally got it all from what I can tell.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lompd5","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lompd5/ive_built_a_spec_for_llmtollm_comms_by_combining/n0qr67r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751370571,"author_flair_text":null,"treatment_tags":[],"created_utc":1751370571,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0qqedt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"sbuswell","can_mod_post":false,"created_utc":1751370245,"send_replies":true,"parent_id":"t1_n0qnoft","score":1,"author_fullname":"t2_wfcudj1nx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Gemini 2.5 flash not only got all of the entire octave spec, it suggested improvements and said it could competently handle translations or conversions. \\n\\nGemini 2.5 Flash-Lite accurately summarised all the points in the big compressed research doc in the evidence folder, and again, completely understood how to not only apply but translate all docs (even explaining why manorial language would be counter to octave’s purpose). But I’ve been so busy with stuff I’ve not really tested it enough. I really do need to do some proper stress testing if I get the chance.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0qqedt","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Gemini 2.5 flash not only got all of the entire octave spec, it suggested improvements and said it could competently handle translations or conversions. &lt;/p&gt;\\n\\n&lt;p&gt;Gemini 2.5 Flash-Lite accurately summarised all the points in the big compressed research doc in the evidence folder, and again, completely understood how to not only apply but translate all docs (even explaining why manorial language would be counter to octave’s purpose). But I’ve been so busy with stuff I’ve not really tested it enough. I really do need to do some proper stress testing if I get the chance.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lompd5","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lompd5/ive_built_a_spec_for_llmtollm_comms_by_combining/n0qqedt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751370245,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0qnoft","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"wpg4665","can_mod_post":false,"created_utc":1751369050,"send_replies":true,"parent_id":"t3_1lompd5","score":1,"author_fullname":"t2_d3dy8","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What's the smallest sized model you've tried this on? I would imagine the smaller and less training material, the less this would work.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0qnoft","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What&amp;#39;s the smallest sized model you&amp;#39;ve tried this on? I would imagine the smaller and less training material, the less this would work.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lompd5/ive_built_a_spec_for_llmtollm_comms_by_combining/n0qnoft/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751369050,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lompd5","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0sd0w2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RMCPhoto","can_mod_post":false,"created_utc":1751388650,"send_replies":true,"parent_id":"t3_1lompd5","score":1,"author_fullname":"t2_ehhvb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Semantic compression definitely works, but it is model specific - meaning the decompression will only work well using the same model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0sd0w2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Semantic compression definitely works, but it is model specific - meaning the decompression will only work well using the same model.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lompd5/ive_built_a_spec_for_llmtollm_comms_by_combining/n0sd0w2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751388650,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lompd5","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),s=()=>e.jsx(t,{data:l});export{s as default};
