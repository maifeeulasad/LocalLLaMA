import{j as e}from"./index-Cd3v0jxz.js";import{R as l}from"./RedditPostRenderer-cI96YLhy.js";import"./index-DGBoKyQm.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"What's the current state of multi GPU use in local UIs? For example, GPUs such as 2x RX570/580/GTX1060, GTX1650, etc... I ask for future reference of the possibility of having twice VRam amount or an increase since some of these can still be found for half the price of a RTX.\\n\\nIn case it's possible, pairing AMD GPU with Nvidia one is a bad idea? And if pairing a ~8gb Nvidia with an RTX to hit nearly 20gb or more?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Multi GPUs?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1ls7vmb","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.62,"author_flair_background_color":null,"subreddit_type":"public","ups":3,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_eljq22kg","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":3,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751715462,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;What&amp;#39;s the current state of multi GPU use in local UIs? For example, GPUs such as 2x RX570/580/GTX1060, GTX1650, etc... I ask for future reference of the possibility of having twice VRam amount or an increase since some of these can still be found for half the price of a RTX.&lt;/p&gt;\\n\\n&lt;p&gt;In case it&amp;#39;s possible, pairing AMD GPU with Nvidia one is a bad idea? And if pairing a ~8gb Nvidia with an RTX to hit nearly 20gb or more?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1ls7vmb","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"WEREWOLF_BX13","discussion_type":null,"num_comments":10,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1ls7vmb/multi_gpus/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1ls7vmb/multi_gpus/","subreddit_subscribers":494986,"created_utc":1751715462,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1ghbhv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mitchins-au","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1gh8no","score":2,"author_fullname":"t2_4hjtgq5u","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Qwen3-14B\\nProblem solved in 9/10 cases","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1ghbhv","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen3-14B\\nProblem solved in 9/10 cases&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ls7vmb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ls7vmb/multi_gpus/n1ghbhv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751716776,"author_flair_text":null,"treatment_tags":[],"created_utc":1751716776,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1gh8no","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"WEREWOLF_BX13","can_mod_post":false,"created_utc":1751716738,"send_replies":true,"parent_id":"t1_n1gfhtm","score":1,"author_fullname":"t2_eljq22kg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I've tried Gemma models already, now I'm looking for something between 12-30b in a doable way since an RTX is pointless if it can't run AI as games aren't that all the focus.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1gh8no","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve tried Gemma models already, now I&amp;#39;m looking for something between 12-30b in a doable way since an RTX is pointless if it can&amp;#39;t run AI as games aren&amp;#39;t that all the focus.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ls7vmb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ls7vmb/multi_gpus/n1gh8no/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751716738,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1k5lbg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mitchins-au","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1k5cx7","score":1,"author_fullname":"t2_4hjtgq5u","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Iâ€™m honestly doubting it. You donâ€™t get speed up with more consumer cards without NVLINK, just the chance to run bigger models.\\nIâ€™ve also got two cards.\\nYouâ€™re better off running smaller quants of bigger models. IQ3 of 100+ B dense models are surprisingly good for example (Mistral large, Cohere Command)\\n\\nGiven deepseek is MOE, look to offload the expert tensors not relevant!\\n\\nEdit: right hybrid. Yes probably but not linearly I think,\\nLess. Is it worth the cost? Hard to say but itâ€™ll make things faster.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1k5lbg","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Iâ€™m honestly doubting it. You donâ€™t get speed up with more consumer cards without NVLINK, just the chance to run bigger models.\\nIâ€™ve also got two cards.\\nYouâ€™re better off running smaller quants of bigger models. IQ3 of 100+ B dense models are surprisingly good for example (Mistral large, Cohere Command)&lt;/p&gt;\\n\\n&lt;p&gt;Given deepseek is MOE, look to offload the expert tensors not relevant!&lt;/p&gt;\\n\\n&lt;p&gt;Edit: right hybrid. Yes probably but not linearly I think,\\nLess. Is it worth the cost? Hard to say but itâ€™ll make things faster.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ls7vmb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ls7vmb/multi_gpus/n1k5lbg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751762519,"author_flair_text":null,"treatment_tags":[],"created_utc":1751762519,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1k5cx7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"sourpatchgrownadults","can_mod_post":false,"created_utc":1751762424,"send_replies":true,"parent_id":"t1_n1gfhtm","score":1,"author_fullname":"t2_vct0oav1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Do you think upgrading from dual 3090s to quad 3090s would ser significant improvements for cpu+gpu hybrid inference? Say with Deepseek R1 0528 Q4, 512gb DDR4 ram. Currently getting about 2 to 4.5 ish t/s depending on context size. Wondering if upgrading to 4x3090 set up would be significant or not.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1k5cx7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Do you think upgrading from dual 3090s to quad 3090s would ser significant improvements for cpu+gpu hybrid inference? Say with Deepseek R1 0528 Q4, 512gb DDR4 ram. Currently getting about 2 to 4.5 ish t/s depending on context size. Wondering if upgrading to 4x3090 set up would be significant or not.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ls7vmb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ls7vmb/multi_gpus/n1k5cx7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751762424,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1gfhtm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mitchins-au","can_mod_post":false,"created_utc":1751715895,"send_replies":true,"parent_id":"t3_1ls7vmb","score":2,"author_fullname":"t2_4hjtgq5u","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Tensor splitting works with LLAMA.cpp or VLLM.\\nLM Studio will spread the model across the devices- usually. (It uses LLAMA.cpp but makes it easier).\\n\\nBut those devices are all really old and slow, and have low VRAM\\nThe best budget bang for buck is a 12GB RTX 3060.\\nAnything without tensor cores is quite slow. AMD is a world of hurt but people here so get it running.\\n\\nMaybe just play with Gemma 3N now? I hear itâ€™s good for edge devices or CPU","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1gfhtm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Tensor splitting works with LLAMA.cpp or VLLM.\\nLM Studio will spread the model across the devices- usually. (It uses LLAMA.cpp but makes it easier).&lt;/p&gt;\\n\\n&lt;p&gt;But those devices are all really old and slow, and have low VRAM\\nThe best budget bang for buck is a 12GB RTX 3060.\\nAnything without tensor cores is quite slow. AMD is a world of hurt but people here so get it running.&lt;/p&gt;\\n\\n&lt;p&gt;Maybe just play with Gemma 3N now? I hear itâ€™s good for edge devices or CPU&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ls7vmb/multi_gpus/n1gfhtm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751715895,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ls7vmb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1gh19s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"WEREWOLF_BX13","can_mod_post":false,"created_utc":1751716640,"send_replies":true,"parent_id":"t1_n1gfel8","score":1,"author_fullname":"t2_eljq22kg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Cool, that sounds promising, something 2 old gpus costs less than a full one.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1gh19s","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Cool, that sounds promising, something 2 old gpus costs less than a full one.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ls7vmb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ls7vmb/multi_gpus/n1gh19s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751716640,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1gfel8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Daniokenon","can_mod_post":false,"created_utc":1751715853,"send_replies":true,"parent_id":"t3_1ls7vmb","score":1,"author_fullname":"t2_ugryjft4e","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yes it is possible, I myself used radeon 6900xt and nvidia 1080ti for some time. Of course, you can only use vulkan - because it is the only one that can work on both cards at once. Recently vulkan support on amd cards has improved a lot, so this option now makes even more sense than before.\\n\\nCarefully divide the layers between all cards - leaving a reserve of about 1GB. The downside is that processing with many cards on vulkan is not so great - compared to CUDA or ROCM. Additionally, put as few layers as possible on the slowest card - it will slow down the rest (although it will still work much faster than the CPU).\\n\\n  \\n[https://github.com/ggml-org/llama.cpp/discussions/10879](https://github.com/ggml-org/llama.cpp/discussions/10879) This will give you a better idea of â€‹â€‹what to expect from certain cards.","edited":1751716128,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1gfel8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes it is possible, I myself used radeon 6900xt and nvidia 1080ti for some time. Of course, you can only use vulkan - because it is the only one that can work on both cards at once. Recently vulkan support on amd cards has improved a lot, so this option now makes even more sense than before.&lt;/p&gt;\\n\\n&lt;p&gt;Carefully divide the layers between all cards - leaving a reserve of about 1GB. The downside is that processing with many cards on vulkan is not so great - compared to CUDA or ROCM. Additionally, put as few layers as possible on the slowest card - it will slow down the rest (although it will still work much faster than the CPU).&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/ggml-org/llama.cpp/discussions/10879\\"&gt;https://github.com/ggml-org/llama.cpp/discussions/10879&lt;/a&gt; This will give you a better idea of â€‹â€‹what to expect from certain cards.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ls7vmb/multi_gpus/n1gfel8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751715853,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ls7vmb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1gjraf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1ghdp8","score":0,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; I never heard of p series, is this GPU intended for what?\\n\\nmining.\\n\\n&gt; Two of these would be worth it?\\n\\nprobably not, but a single one is a great combo for 3060 12 GiB or even 5060ti 16 GiB.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1gjraf","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;I never heard of p series, is this GPU intended for what?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;mining.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Two of these would be worth it?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;probably not, but a single one is a great combo for 3060 12 GiB or even 5060ti 16 GiB.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ls7vmb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ls7vmb/multi_gpus/n1gjraf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751717917,"author_flair_text":null,"treatment_tags":[],"created_utc":1751717917,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n1ghdp8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"WEREWOLF_BX13","can_mod_post":false,"created_utc":1751716805,"send_replies":true,"parent_id":"t1_n1gggn8","score":1,"author_fullname":"t2_eljq22kg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"They got me a little confused, so I made a little more specific question just to know, apologies ðŸ‘¤\\n\\n\\nI never heard of p series, is this GPU intended for what? Two of these would be worth it?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ghdp8","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They got me a little confused, so I made a little more specific question just to know, apologies ðŸ‘¤&lt;/p&gt;\\n\\n&lt;p&gt;I never heard of p series, is this GPU intended for what? Two of these would be worth it?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ls7vmb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ls7vmb/multi_gpus/n1ghdp8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751716805,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1gggn8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1751716363,"send_replies":true,"parent_id":"t3_1ls7vmb","score":-1,"author_fullname":"t2_uz37qfx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This question is literally asked twice a day every day. Yes you can use multiple GPUs. Do not invest in anything older than 30xx series as 10xx 20xx will soon be deprecated completely. If you are desperate to add 8 GiB VRAM buy p104-100, $25 on local marketplaces.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1gggn8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This question is literally asked twice a day every day. Yes you can use multiple GPUs. Do not invest in anything older than 30xx series as 10xx 20xx will soon be deprecated completely. If you are desperate to add 8 GiB VRAM buy p104-100, $25 on local marketplaces.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ls7vmb/multi_gpus/n1gggn8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751716363,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ls7vmb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}}]`),r=()=>e.jsx(l,{data:t});export{r as default};
